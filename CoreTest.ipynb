{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiled all imports\n",
    "import pandas as pd \n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from gpt_researcher import GPTResearcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the api key from the .env file\n",
    "load_dotenv(\"/Users/roberthand/Desktop/FDL/.env\")  \n",
    "api_key = os.getenv('CORE_API_KEY')\n",
    "api_endpoint = \"https://api.core.ac.uk/v3/\"\n",
    "\n",
    "def pretty_json(json_object): print(json.dumps(json_object, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity(url_fragment):\n",
    "    headers={\"Authorization\":\"Bearer \"+api_key}\n",
    "    response = requests.get(api_endpoint + url_fragment, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json(), response.elapsed.total_seconds()\n",
    "    else:\n",
    "        print(f\"Error code {response.status_code}, {response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On computable numbers, with an application to the entscheidungsproblem\n",
    "def query_api(url_fragment, query,limit=100):\n",
    "    headers={\"Authorization\":\"Bearer \"+api_key}\n",
    "    query = {\"q\":query, \"limit\":limit}\n",
    "    response = requests.post(f\"{api_endpoint}{url_fragment}\",data = json.dumps(query), headers=headers)\n",
    "    if response.status_code ==200:\n",
    "        return response.json(), response.elapsed.total_seconds()\n",
    "    else:\n",
    "        print(f\"Error code {response.status_code}, {response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_api2(url_fragment, query, limit=100):\n",
    "    headers = {\"Authorization\": \"Bearer \" + api_key}\n",
    "    query = {\"q\": query, \"limit\": limit}\n",
    "    response = requests.post(f\"{api_endpoint}{url_fragment}\", data=json.dumps(query), headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        citation_data = []\n",
    "        \n",
    "        for item in result['results']:\n",
    "            # Extract the necessary components\n",
    "            authors = ', '.join([author['name'] for author in item['authors']])\n",
    "            year = item.get('createdDate', '').split('-')[0]\n",
    "            title = item['title']\n",
    "            full_text = item.get('fullText', '')\n",
    "            \n",
    "            # Create a smaller JSON object with the necessary parameters\n",
    "            citation_object = {\n",
    "                \"authors\": authors,\n",
    "                \"year\": year,\n",
    "                \"title\": title,\n",
    "                \"full_text\": full_text\n",
    "            }\n",
    "            \n",
    "            citation_data.append(citation_object)\n",
    "        \n",
    "        return citation_data, response.elapsed.total_seconds()\n",
    "    else:\n",
    "        print(f\"Error code {response.status_code}, {response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code 500, b'{\"message\":\"token_mgr_error: Lexical error at line 1, column 43.  Encountered: \\\\u0022 \\\\u0022 (32), after : \\\\u0022\\\\u0022 [reason: all shards failed]\"}'\n",
      "Error processing title: Solving SDD Linear Systems in Nearly mlog^(1/2)n Time — cannot unpack non-iterable NoneType object\n"
     ]
    }
   ],
   "source": [
    "research_papers = [\n",
    "    \"The Power of Simple Tabulation Hashing\",\n",
    "    \"Solving SDD Linear Systems in Nearly mlog^(1/2)n Time\",\n",
    "    \"Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions\",\n",
    "    \"Sparser Johnson-Lindenstrauss Transforms\",\n",
    "    \"Multiplying Matrices Faster Than Coppersmith-Winograd\",\n",
    "    \"Greedy Sequential Maximal Independent Set and Matching Are Parallel on Average\",\n",
    "    \"Polynomial-Time Approximation Schemes for Packing and Piercing Fat Objects\",\n",
    "    \"Edit Distance Cannot Be Computed in Strongly Subquadratic Time (Unless SETH Is False)\",\n",
    "    \"Heuristic Algorithms in Computational Molecular Biology\",\n",
    "    \"Deterministic Discrepancy Minimization\",\n",
    "    \"A Near-Linear Pseudopolynomial Time Algorithm for Subset Sum\",\n",
    "    \"Popular Conjectures Imply Strong Lower Bounds for Dynamic Problems\",\n",
    "    \"Probabilistic Polynomials and Hamming Nearest Neighbors\",\n",
    "    \"Phase-Concurrent Hash Tables for Determinism\",\n",
    "    \"Energy-Efficient Algorithms\",\n",
    "    \"On the Complexity of Computing Gröbner Bases for Weighted Homogeneous Systems\",\n",
    "    \"Cache-Oblivious B-Trees\",\n",
    "    \"Higher Lower Bounds from the 3SUM Conjecture\",\n",
    "    \"Distributed Approximation Algorithms for Weighted Shortest Paths\",\n",
    "    \"Compressed Static Functions with Applications\",\n",
    "    \"Fully Dynamic Almost-Maximal Matching: Breaking the Polynomial Barrier for Worst-Case Time Bounds\",\n",
    "    \"Fault-Tolerant Compact Routing Schemes for General Graphs\",\n",
    "    \"Pattern Matching in Lempel-Ziv Compressed Strings: Fast, Simple, and Deterministic\",\n",
    "    \"Conditional Lower Bounds for Space/Time Tradeoffs\",\n",
    "    \"Minimal Suffix and Rotation of a Substring in Optimal Time\",\n",
    "    \"Fine-Grained Reductions from Approximate Counting to Decision\",\n",
    "    \"New Deterministic Approximation Algorithms for Fully Dynamic Matching\",\n",
    "    \"On Near-Linear-Time Algorithms for Dense Subset Sum\",\n",
    "    \"Deterministic APSP, Orthogonal Vectors, and More: Quickly Derandomizing Razborov-Smolensky\",\n",
    "    \"Local Search Yields Approximation Schemes for k-Means and k-Median in Euclidean and Minor-Free Metrics\",\n",
    "    \"On the Complexity of Solving Sparse Polynomials Over the p-adic Numbers\",\n",
    "    \"A Simple Algorithm for Approximating the Text-to-Pattern Hamming Distances\",\n",
    "    \"A New Subquadratic Algorithm for Constructing the Minimax Tree\",\n",
    "    \"Equivalence Classes and Conditional Hardness in Massively Parallel Computations\",\n",
    "    \"Popular Conjectures as a Barrier for Dynamic Planar Graph Algorithms\",\n",
    "    \"Limits on All Known (and Some Unknown) Approaches to Matrix Multiplication\",\n",
    "    \"Top-k-Convolution and the Quest for Near-Linear Output-Sensitive Subset Sum\",\n",
    "    \"Dynamic Graph Algorithms with Batch Updates in Sublinear Time\",\n",
    "    \"Optimal Las Vegas Approximate Near Neighbors in Low Dimensions\",\n",
    "    \"Approximating Edit Distance Within Constant Factor in Truly Sub-Quadratic Time\",\n",
    "    \"The Query Complexity of Mastermind with ℓ_p Distances\",\n",
    "    \"Faster Algorithms for All-Pairs Bounded Min-Cuts\",\n",
    "    \"Optimal Streaming Approximations for All Boolean Max-2CSPs\",\n",
    "    \"Tight Bounds for the Subspace Sketch Problem with Applications\",\n",
    "    \"A Deterministic Algorithm for Balanced Cut with Applications to Dynamic Connectivity\",\n",
    "    \"Decremental SSSP in Weighted Digraphs: Faster Against an Adaptive Adversary\",\n",
    "    \"Optimal Approximate Sampling from Discrete Probability Distributions\",\n",
    "    \"Nearly Optimal Static Las Vegas Succinct Dictionaries\",\n",
    "    \"Faster Algorithms for the Shortest Path Problem with Negative Weights\",\n",
    "    \"Dynamic Set Cover: Improved Amortized and Worst-Case Update Time\",\n",
    "    \"Exploiting non-constant safe memory in resilient algorithms and data structures\",\n",
    "    \"Geometric deep learning\",\n",
    "    \"Geometric deep learning: going beyond Euclidean data\",\n",
    "    \"Geometric Multi-Model Fitting by Deep Reinforcement Learning\",\n",
    "    \"Beyond Low Rank + Sparse: Multi-scale Low Rank Matrix Decomposition\",\n",
    "    \"Manifold Constrained Low-Rank Decomposition\", \n",
    "    \"Meta-Learning via Classifier(-free) Guidance\",\n",
    "    \"On Distillation of Guided Diffusion Models\",\n",
    "    \"Analysis of Classifier-Free Guidance Weight Schedulers\",\n",
    "    \"Accelerating cryptic pocket discovery using AlphaFold\",\n",
    "    \"Grokking phase transitions in learning local rules with gradient descent\"\n",
    "]\n",
    "\n",
    "# query = f\"Algorithms and data structures for adaptive multigrid elliptic solvers\" \n",
    "# for some reason we lost articles...?!\n",
    "\n",
    "json_objs = []\n",
    "for title in research_papers:\n",
    "    query = f\"{title}\"\n",
    "    try:\n",
    "        results, _ = query_api2(\"search/works\", query, limit=1)\n",
    "        json_objs.append(results)\n",
    "    except Exception as e: \n",
    "        print(f\"Error processing title: {title} — {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'authors': 'Thorup, Mikkel', 'year': '2015', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'authors': 'Andoni, Alexandr, Indyk, Piotr, R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'authors': 'Kane, Daniel M., Nelson, Jelani',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'authors': 'Rosowski, Andreas', 'year': '2019...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'authors': 'Fischer, Manuela, Noever, Andreas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'authors': 'Shahrokhi, Farhad', 'year': '2015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'authors': 'Backurs, Arturs, Indyk, Piotr', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'authors': 'Ahrabian, Hayedeh, Dalini, Abbas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'authors': 'Dudek, Bartłomiej, Gawrychowski, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'authors': 'Koiliaris, Konstantinos, Xu, Chao...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'authors': 'Abboud, Amir, Williams, Virginia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'authors': 'Alman, Josh, Williams, Ryan', 'ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'authors': 'Dementiev, Roman, Maier, Tobias, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'authors': 'Axelsen H. B., Frank M. P., Koome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'authors': 'Din, Mohab Safey El, Faugère, Jea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'authors': 'Barratt, Jeffrey, Zhang, Brian', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'authors': 'Kopelowitz, Tsvi, Pettie, Seth, P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'authors': 'Abram J., Demetrescu C., Lynch N....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'authors': 'Panizzi, Emanuele, Pastorelli, Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'authors': 'C. Demetrescu, D. Peleg, D. Peleg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'authors': 'A Abboud, A Amir, A Gajentaan, H ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'authors': 'Kociumaka, Tomasz', 'year': '2016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'authors': 'Beame Paul, Bhattacharya Anup, Bh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'authors': 'Bringmann, K., Wellnitz, P.', 'ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'authors': 'Cohen-Addad, Vincent, Klein, Phil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'authors': 'Eberly, Wayne, Giesbrecht, Mark, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'authors': 'Studený, Jan, Uznański, Przemysła...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'authors': 'Eppstein, David, Har-Peled, Sarie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'authors': 'Nanongkai, Danupon, Scquizzato, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{'authors': 'Martin, Daniel P.', 'year': '2017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'authors': 'Helfer, Brian, Kepner, Jeremy, Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'authors': 'Bringmann, K., Nakos, V.', 'year'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>{'authors': 'Italiano, Giuseppe F., Lattanzi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>{'authors': 'Ahle, Thomas Dybdahl', 'year': '2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>{'authors': 'Chakraborty, Diptarka, Das, Debar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>{'authors': 'Abboud, Amir, Georgiadis, Loukas,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>{'authors': 'Chou, Chi-Ning, Golovnev, Alexand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>{'authors': 'Li, Yi, Wang, Ruosong, Woodruff, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>{'authors': 'Chuzhoy, Julia, Gao, Yu, Li, Jaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>{'authors': 'Freer, Cameron E., Mansinghka, Vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>{'authors': 'Belazzougui, Djamal, Venturini, R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>{'authors': 'Agarwal, Udit, Ramachandran, Vija...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>{'authors': 'DE STEFANI, LORENZO, SILVESTRI, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>{'authors': 'Andreux M., Boscaini D., Bruna J....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>{'authors': 'Chen, Yiping, Li, Jonathan, Wang,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>{'authors': 'Chen, Chen, Del Bue, Alessio, Mur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>{'authors': 'Chang, Shih-Fu, Gao, Hang, Shou, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'authors': 'Ermon, Stefano, Gao, Ruiqi, Ho, J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>{'authors': 'Abrevaya, Victoria Fernandez, And...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>{'authors': 'Bhakat, Soumendranath, Bowman, Gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>{'authors': 'Ilievski, Enej, Žunkovič, Bojan',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0\n",
       "0   {'authors': 'Thorup, Mikkel', 'year': '2015', ...\n",
       "1   {'authors': 'Andoni, Alexandr, Indyk, Piotr, R...\n",
       "2   {'authors': 'Kane, Daniel M., Nelson, Jelani',...\n",
       "3   {'authors': 'Rosowski, Andreas', 'year': '2019...\n",
       "4   {'authors': 'Fischer, Manuela, Noever, Andreas...\n",
       "5   {'authors': 'Shahrokhi, Farhad', 'year': '2015...\n",
       "6   {'authors': 'Backurs, Arturs, Indyk, Piotr', '...\n",
       "7   {'authors': 'Ahrabian, Hayedeh, Dalini, Abbas ...\n",
       "8   {'authors': 'Dudek, Bartłomiej, Gawrychowski, ...\n",
       "9   {'authors': 'Koiliaris, Konstantinos, Xu, Chao...\n",
       "10  {'authors': 'Abboud, Amir, Williams, Virginia ...\n",
       "11  {'authors': 'Alman, Josh, Williams, Ryan', 'ye...\n",
       "12  {'authors': 'Dementiev, Roman, Maier, Tobias, ...\n",
       "13  {'authors': 'Axelsen H. B., Frank M. P., Koome...\n",
       "14  {'authors': 'Din, Mohab Safey El, Faugère, Jea...\n",
       "15  {'authors': 'Barratt, Jeffrey, Zhang, Brian', ...\n",
       "16  {'authors': 'Kopelowitz, Tsvi, Pettie, Seth, P...\n",
       "17  {'authors': 'Abram J., Demetrescu C., Lynch N....\n",
       "18  {'authors': 'Panizzi, Emanuele, Pastorelli, Be...\n",
       "19  {'authors': 'C. Demetrescu, D. Peleg, D. Peleg...\n",
       "20  {'authors': 'A Abboud, A Amir, A Gajentaan, H ...\n",
       "21  {'authors': 'Kociumaka, Tomasz', 'year': '2016...\n",
       "22  {'authors': 'Beame Paul, Bhattacharya Anup, Bh...\n",
       "23  {'authors': 'Bringmann, K., Wellnitz, P.', 'ye...\n",
       "24  {'authors': 'Cohen-Addad, Vincent, Klein, Phil...\n",
       "25  {'authors': 'Eberly, Wayne, Giesbrecht, Mark, ...\n",
       "26  {'authors': 'Studený, Jan, Uznański, Przemysła...\n",
       "27  {'authors': 'Eppstein, David, Har-Peled, Sarie...\n",
       "28  {'authors': 'Nanongkai, Danupon, Scquizzato, M...\n",
       "29  {'authors': 'Martin, Daniel P.', 'year': '2017...\n",
       "30  {'authors': 'Helfer, Brian, Kepner, Jeremy, Re...\n",
       "31  {'authors': 'Bringmann, K., Nakos, V.', 'year'...\n",
       "32  {'authors': 'Italiano, Giuseppe F., Lattanzi, ...\n",
       "33  {'authors': 'Ahle, Thomas Dybdahl', 'year': '2...\n",
       "34  {'authors': 'Chakraborty, Diptarka, Das, Debar...\n",
       "35  {'authors': 'Abboud, Amir, Georgiadis, Loukas,...\n",
       "36  {'authors': 'Chou, Chi-Ning, Golovnev, Alexand...\n",
       "37  {'authors': 'Li, Yi, Wang, Ruosong, Woodruff, ...\n",
       "38  {'authors': 'Chuzhoy, Julia, Gao, Yu, Li, Jaso...\n",
       "39  {'authors': 'Freer, Cameron E., Mansinghka, Vi...\n",
       "40  {'authors': 'Belazzougui, Djamal, Venturini, R...\n",
       "41  {'authors': 'Agarwal, Udit, Ramachandran, Vija...\n",
       "42  {'authors': 'DE STEFANI, LORENZO, SILVESTRI, F...\n",
       "43  {'authors': 'Andreux M., Boscaini D., Bruna J....\n",
       "44  {'authors': 'Chen, Yiping, Li, Jonathan, Wang,...\n",
       "45  {'authors': 'Chen, Chen, Del Bue, Alessio, Mur...\n",
       "46  {'authors': 'Chang, Shih-Fu, Gao, Hang, Shou, ...\n",
       "47  {'authors': 'Ermon, Stefano, Gao, Ruiqi, Ho, J...\n",
       "48  {'authors': 'Abrevaya, Victoria Fernandez, And...\n",
       "49  {'authors': 'Bhakat, Soumendranath, Bowman, Gr...\n",
       "50  {'authors': 'Ilievski, Enej, Žunkovič, Bojan',..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(json_objs)\n",
    "df.dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Establish connection with MongoDB\n",
    "client = MongoClient('mongodb://mongoadmin:password@localhost:27017/?authSource=admin')\n",
    "db = client['FDL']\n",
    "collection = db['DSA']\n",
    "\n",
    "# Insert the data into MongoDB\n",
    "def insert_to_mongodb(data):\n",
    "    if data:\n",
    "        collection.insert_many(data)\n",
    "        print(f\"Inserted {len(data)} records into the DSA collection.\")\n",
    "    else:\n",
    "        print(\"No data to insert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "No data to insert.\n",
      "Inserted 1 records into the DSA collection.\n",
      "No data to insert.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "No data to insert.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "No data to insert.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "No data to insert.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "No data to insert.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "No data to insert.\n",
      "Inserted 1 records into the DSA collection.\n",
      "No data to insert.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n",
      "Inserted 1 records into the DSA collection.\n"
     ]
    }
   ],
   "source": [
    "for item in json_objs:\n",
    "    insert_to_mongodb(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Nearest Neighbor Search in High Dimensions\n",
      "Sparser Johnson-Lindenstrauss Transforms\n",
      "Fast Commutative Matrix Algorithm\n",
      "Tight Analysis of Randomized Greedy MIS\n",
      "New separation theorems and sub-exponential time algorithms for packing\n",
      "  and piercing of fat objects\n",
      "Edit Distance Cannot Be Computed in Strongly Subquadratic Time (unless\n",
      "  SETH is false)\n",
      "Molecular solutions for double and partial digest problems in polynomial time\n",
      "A Faster Pseudopolynomial Time Algorithm for Subset Sum\n",
      "Probabilistic Polynomials and Hamming Nearest Neighbors\n",
      "Concurrent Hash Tables: Fast and General?(!)\n",
      "Energy-Efficient Algorithms\n",
      "Cache-Friendly Search Trees; or, In Which Everything Beats std::set\n",
      "Higher Lower Bounds from the 3SUM Conjecture\n",
      "Distributed Approximation Algorithms for Weighted Shortest Paths\n",
      "Multimethods and separate static typechecking in a language with\n",
      "  C++-like object model\n",
      "Sparse Fault-Tolerant BFS Trees\n",
      "Conditional Lower Bounds for Space/Time Tradeoffs\n",
      "Fine-Grained Reductions from Approximate Counting to Decision\n",
      "New deterministic approximation algorithms for fully dynamic matching\n",
      "On Near-Linear-Time Algorithms for Dense Subset Sum\n",
      "Local search yields approximation schemes for k-means and k-median in\n",
      "  Euclidean and minor-free metrics\n",
      "Solving Sparse Integer Linear Systems\n",
      "Approximating Approximate Pattern Matching\n",
      "Approximate Greedy Clustering and Distance Selection for Graph Metrics\n",
      "Dynamic Shortest Path and Transitive Closure Algorithms: A Survey\n",
      "A Linear Algebra Approach to Fast DNA Mixture Analysis Using GPUs\n",
      "Top-k-Convolution and the Quest for Near-Linear Output-Sensitive Subset Sum\n",
      "Parallel Batch-Dynamic Graphs: Algorithms and Lower Bounds\n",
      "Approximating Edit Distance Within Constant Factor in Truly\n",
      "  Sub-Quadratic Time\n",
      "Faster Algorithms for All-Pairs Bounded Min-Cuts\n",
      "Optimal Streaming Approximations for all Boolean Max-2CSPs and Max-kSAT\n",
      "Tight Bounds for the Subspace Sketch Problem with Applications\n",
      "A Deterministic Algorithm for Balanced Cut with Applications to Dynamic\n",
      "  Connectivity, Flows, and Beyond\n",
      "Optimal Approximate Sampling from Discrete Probability Distributions\n",
      "Compressed String Dictionary Search with Edit Distance One\n",
      "New and Simplified Distributed Algorithms for Weighted All Pairs\n",
      "  Shortest Paths\n",
      "Exploiting non-constant safe memory in resilient algorithms and data structures\n",
      "Geometric deep learning\n",
      "Geometric Multi-Model Fitting by Deep Reinforcement Learning\n",
      "Manifold Constrained Low-Rank Decomposition\n",
      "Discriminative Adversarial Domain Generalization with Meta-learning\n",
      "  based Cross-domain Validation\n",
      "On Distillation of Guided Diffusion Models\n",
      "Analysis of Classifier-Free Guidance Weight Schedulers\n",
      "Accelerating cryptic pocket discovery using AlphaFold\n",
      "Grokking phase transitions in learning local rules with gradient descent\n"
     ]
    }
   ],
   "source": [
    "titles = collection.find({}, {\"title\": 1, \"_id\": 0}) # prints all of the titles in the db\n",
    "for doc in titles:\n",
    "    print(doc[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Fast Commutative Matrix Algorithm'}\n"
     ]
    }
   ],
   "source": [
    "# find one articles in the db\n",
    "titles = collection.find({}, {\"title\": \"Fast Commutative Matrix Algorithm\", \"_id\": 0})\n",
    "print(titles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd0c'), 'authors': 'Andoni, Alexandr, Indyk, Piotr, Razenshteyn, Ilya', 'year': '2018', 'title': 'Approximate Nearest Neighbor Search in High Dimensions', 'full_text': 'ar\\nX\\niv\\n:1\\n80\\n6.\\n09\\n82\\n3v\\n1 \\n [c\\ns.D\\nS]\\n  2\\n6 J\\nun\\n 20\\n18\\nApproximate Nearest Neighbor Search in High Dimensions\\nAlexandr Andoni Piotr Indyk Ilya Razenshteyn\\nAbstract\\nThe nearest neighbor problem is defined as follows: Given a set P of n points in some metric\\nspace (X,D), build a data structure that, given any point q, returns a point in P that is closest\\nto q (its “nearest neighbor” in P ). The data structure stores additional information about the\\nset P , which is then used to find the nearest neighbor without computing all distances between q\\nand P . The problem has a wide range of applications in machine learning, computer vision,\\ndatabases and other fields.\\nTo reduce the time needed to find nearest neighbors and the amount of memory used by the\\ndata structure, one can formulate the approximate nearest neighbor problem, where the the goal\\nis to return any point p′ ∈ P such that the distance from q to p′ is at most c ·minp∈P D(q, p),\\nfor some c ≥ 1. Over the last two decades, many efficient solutions to this problem were\\ndeveloped. In this article we survey these developments, as well as their connections to questions\\nin geometric functional analysis and combinatorial geometry.\\n1 Introduction\\nThe nearest neighbor problem is defined as follows: Given a set P of n points in a metric space\\ndefined over a set X with distance function D, build a data structure1 that, given any “query” point\\nq ∈ X, returns its “nearest neighbor” argminp∈P D(q, p). A particularly interesting and well-studied\\ncase is that of nearest neighbor in geometric spaces, where X = Rd and the metric D is induced by\\nsome norm. The problem has a wide range of applications in machine learning, computer vision,\\ndatabases and other fields, see [SDI06, AI08] for an overview.\\nA simple solution to this problem would store the set P in memory, and then, given q, compute\\nall distances D(q, p) for p ∈ P and select the point p with the minimum distance. Its disadvantage\\nis the computational cost: computing all n distances requires at least n operations. Since in many\\napplications n can be as large as 109 (see e.g., [STS+13]), it was necessary to develop faster methods\\nthat find the nearest neighbors without explicitly computing all distances from q. Those methods\\ncompute and store additional information about the set P , which is then used to find nearest\\nneighbors more efficiently. To illustrate this idea, consider another simple solution for the case\\nwhere X = {0, 1}d. In this case, one could precompute and store in memory the answers to all\\n2d queries q ∈ X; given q, one could then return its nearest neighbor by performing only a single\\nmemory lookup. Unfortunately, this approach requires memory of size 2d, which again is inefficient\\n(d is at least 103 or higher in many applications).\\nThe two aforementioned solutions can be viewed as extreme points in a tradeoff between the time\\nto answer a query (“query time”) and the amount of memory used (“space”)2. The study of this\\ntradeoff dates back to the work of Minsky and Papert ([MP69], p. 222), and has become one of the\\nkey topics in the field of computational geometry [PS85]. During the 1970s and 1980s many efficient\\n1See Section 1.1 for a discussion about the computational model.\\n2There are other important data structure parameters, such as the time needed to construct it. For the sake of\\nsimplicity, we will mostly focus on query time and space.\\n1\\nsolutions have been discovered for the case when (X,D) = (Rd, ℓ2) and d is a constant independent\\nof n. For example, for d = 2, one can construct a data structure using O(n) space with O(log n)\\nquery time [LT80]. Unfortunately, as the dimension d increases, those data structures become less\\nand less efficient. Specifically, it is known how construct data structures with O(dO(1) log n) query\\ntime, but using nO(d) space ([Mei93], building on [Cla88]).3 Furthermore, there is evidence that\\ndata structures with query times of the form n1−αdO(1) for some constant α > 0 might be difficult\\nto construct efficiently.4\\nThe search for efficient solutions to the nearest neighbor problem has led to the question whether\\nbetter space/query time bounds could be obtained if the data structure was allowed to report ap-\\nproximate answers. In the c-approximate nearest neighbor problem, the data structure can report\\nany point p′ ∈ P within distance c ·minp∈P D(q, p) from q; the parameter c ≥ 1 is called “approx-\\nimation factor”. The work of Arya, Mount and Bern [AM93, Ber93] showed that allowing c > 1\\nindeed leads to better data structures, although their solutions still retained exponential dependen-\\ncies on d in the query time or space bounds [AM93] or required that the approximation factor c is\\npolynomial in the dimension d [Ber93]. These bounds have been substantially improved over the\\nnext few years, see e.g., [Cla94, Cha98, AMN+98, Kle97] and the references therein.\\nIn this article we survey the “second wave” of approximate nearest neighbor data structures,\\nwhose query time and space bounds are polynomial in the dimension d. 5 At a high level, these data\\nstructures are obtained in two steps. In the first step, the approximate nearest neighbor problem\\nis reduced to its “decision version”, termed approximate near neighbor (see e.g. [HPIM12]). The\\nsecond step involves constructing a data structure for the latter problem. In this survey we focus\\nmostly on the second step.\\nThe approximate near neighbor problem is parameterized by an approximation factor c > 1 as\\nwell as a “scale parameter” r > 0, and defined as follows.\\nDefinition 1.1 ((c, r)-Approximate Near Neighbor). Given a set P of n points in a metric space\\n(X,D), build a data structure S that, given any query point q ∈ X such that the metric ball\\nBD(q, r) = {p ∈ X : D(p, q) ≤ r} contains a point in P , S returns any point in BD(q, cr) ∩ P .\\nNote that the definition does not specify the behavior of the data structure if the ball BD(q, r)\\ndoes not contain any point in P . We omit the index D when it is clear from the context.\\nThe above definition applies to algorithms that are deterministic, i.e., do not use random\\nbits. However, most of the approximate near neighbor algorithms in the literature are randomized,\\ni.e., generate and use random bits while constructing the data structure. In this case, the data\\nstructure S is a random variable, sampled from some distribution. This leads to the following\\ngeneralization.\\nDefinition 1.2 ((c, r, δ)-Approximate Near Neighbor). Given a set P of n points in a metric space\\n(X,D), build a data structure S that, given any query point q ∈ X such that B(q, r) ∩ P 6= ∅,\\nPr\\nS\\n[S returns any point in B(q, cr) ∩ P ] ≥ 1− δ\\n3This exponential dependence on the dimension is due to the fact that those data structures compute and store\\nthe Voronoi decomposition of P , i.e., the decomposition of Rd into cells such that all points in each cell have the same\\nnearest neighbor in P . The combinatorial complexity of this decomposition could be as large as nΩ(d) [Car11].\\n4If such a data structure could be constructed in polynomial time nO(1)dO(1), then the Strong Exponential Time\\nHypothesis [Wil18] would be false. This fact essentially follows from [Wil05], see the discussion after Theorem 1\\nin [APRS16].\\n5Due to the lack of space, we will not cover several important related topics, such as data structures for point-sets\\nwith low intrinsic dimension [Cla06], approximate furthest neighbor, approximate nearest line search [Mah14] and\\nother variants of the problem.\\n2\\nThe probability of failure δ of the data structure can be reduced by independently repeating\\nthe process several times, i.e., creating several data structures. Therefore, in the rest of the survey\\nwe will set δ to an arbitrary constant, say, 1/3. We will use (c, r)-ANN to denote (c, r, 1/3)-\\nApproximate Near Neighbor.\\n1.1 Computational model\\nFor the purpose of this survey, a data structure of size M is an array A[1 . . . M ] of numbers (“the\\nmemory”), together with an associated algorithm that, given a point q, returns a point in P as\\nspecified by the problem. The entries A[i] of A are called “memory cells”. Due to lack of space, we\\nwill not formally define other details of the computational model, in particular what an algorithm\\nis, how to measure its running time, what is the range of the array elements A[i], etc. There are\\nseveral ways of defining these notions, and the material in this survey is relatively robust to the\\nvariations in the definitions. We note, however, that one way to formalize these notions is to restrict\\nall numbers, including point coordinates, memory entries, etc, to rational numbers of the form a/b,\\nwhere a ∈ {−nO(1) . . . nO(1)} and b = nO(1), and to define query time as the maximum number of\\nmemory cells accessed to answer any query q.\\nFor an overview of these topics and formal definitions, the reader is referred to [Mil99]. For\\na discussion specifically geared towards mathematical audience, see [FK09].\\n2 Data-independent approach\\nThe first approach to the approximate near neighbor problem has been via data-independent data\\nstructures. These are data structures where the memory cells accessed by the query algorithm do\\nnot depend on the data set P , but only on q and (for randomized data structures) the random bits\\nused to construct the data structure. In this section, we describe two methods for constructing\\nsuch data structures, based on oblivious dimension-reduction, and on randomized space partitions.\\nThese methods give ANN data structures for the ℓ1 and ℓ2 spaces in particular.\\n2.1 ANN via dimension reduction\\nAs described in the introduction, there exist ANN data structures with space and query time at\\nmost exponential in the dimension d. Since exponential space/time bounds are unaffordable for\\nlarge d, a natural approach is to perform a dimension reduction beforehand, and then solve the\\nproblem in the lower, reduced dimension. The main ingredient of such an approach is a map\\nf : Rd → Rk that preserves distances up to a c = 1 + ε factor, where k = O(log n). Then a space\\nbound exponential in k becomes polynomial in n.\\nSuch dimension-reducing maps f indeed exist for the ℓ2 norm if we allow randomization, as first\\nshown in the influential paper by Johnson and Lindenstrauss:\\nLemma 2.1 ([JL84]). Fix dimension d ≥ 1 and a “target” dimension k < d. Let A be the projection\\nof Rd to its k-dimensional subspace selected uniformly at random (with respect to the Haar measure),\\nand define f : Rd → Rk as f(x) =\\n√\\nd√\\nk\\nAx. Then, there is a universal constant C > 0, such that for\\nany ε ∈ (0, 1/2), and any x, y ∈ Rd, we have that\\nPr\\nA\\n[‖f(x)−f(y)‖\\n‖x−y‖ ∈ (1− ε, 1 + ε)\\n]\\n≥ 1− e−Cε2k.\\n3\\nWe can now apply this lemma, with k = O\\n(\\nlogn\\nε2\\n)\\n, to a set of points P to show that the\\nmap f has a (1 + ε) distortion on P , with probability at least 2/3. Most importantly the map f is\\n“oblivious”, i.e., it does not depend on P .\\nWe now show how to use Lemma 2.1 to design a (1 + O(ε), r)-ANN data structure with the\\nfollowing guarantees.\\nTheorem 2.2 ([IM98, HPIM12]). Fix ε ∈ (0, 1/2) and dimension d ≥ 1. There is a (1 +O(ε), r)-\\nANN data structure over (Rd, ℓ2) achieving Q = O(d · lognε2 ) query time, and S = nO(log(1/ε)/ε\\n2) +\\nO(d(n + k)) space. The time needed to build the data structure is O(S + ndk).\\nProof sketch. First, assume there is a (1 + ε, r)-ANN data structure A for the k-dimensional ℓ2\\nspace, achieving query time Q(n, k) and space bounded by S(n, k). For k = O( lognε2 ), we consider\\nthe map f from Lemma 2.1. For the dataset P , we compute f(P ) and preprocess this set using A\\n(with the scale parameter r(1+ ε)). Then, for a query point q ∈ Rd, we query the data structure A\\non f(q). This algorithm works for a fixed dataset P and query q with 5/6 probability, by applying\\nLemma 2.1 to the points in the set P ∪ {q}. The map f preserves all distances between P and q\\nup to a factor of 1 + ε.\\nWe now construct A with space S(n, k) = n · (1/ε)O(k) and Q(n, k) = O(k), which yields the\\nstated bound for k = O( lognε2 ). Given the scale parameter r, we discretize the space R\\nk into cubes of\\nsidelength εr/\\n√\\nk, and consider the set S of cubes that intersect any ball B(p′, r) where p′ ∈ f(P ).\\nUsing standard estimates on the volume of ℓ2 balls, one can prove that |S| ≤ n ·(1/ε)O(k). The data\\nstructure then stores the set S in a dictionary data structure.6 For a query f(q), we just compute\\nthe cube that contains f(q), and check whether it is contained in set S using the dictionary data\\nstructure. We note that there is an additional 1+ε factor loss from discretization since the diameter\\nof a cube is εr.\\nA similar approach was introduced [KOR00] in the context of the Hamming space {0, 1}d. An\\nimportant difference is that that there is no analog of Lemma 2.1 for the Hamming space [BC05].7\\nTherefore, [KOR00] introduce a weaker notion of randomized dimension reduction, which works\\nonly for a fixed scale r.\\nLemma 2.3 ([KOR00]). Fix the error parameter ε ∈ (0, 1/2), dimension d ≥ 1, and scale r ∈ [1, d].\\nFor any k ≥ 1, there exists a randomized map f : {0, 1}d → {0, 1}k and an absolute constant C > 0,\\nsatisfying the following for any fixed x, y ∈ {0, 1}d:\\n• if ‖x− y‖1 ≤ r, then Prf [‖f(x)− f(y)‖1 ≤ k/2] ≥ 1− e−Cε2k;\\n• if ‖x− y‖1 ≥ (1 + ε)r, then Prf [‖f(x)− f(y)‖1 > (1 + ε/2) · k/2] ≥ 1− e−Cε2k.\\nThe map f can be constructed via a random projection over GF (2). That is, take f(x) = Ax,\\nwhereA is a k×dmatrix for k = O(log(n)/ε2), with each entry being 1 with some fixed probability p,\\nand zero otherwise. The probability p depends solely on r. The rest of the algorithm proceeds as\\nbefore, with the exception that the “base” data structure A is particularly simple: just store the\\nanswer for any dimension-reduced query point f(q) ∈ {0, 1}k . Since there are only 2k = nO(1/ε2)\\nsuch possible queries, and computing f(q) takes O(dk) time, we get the following result.\\n6In the dictionary problem, we are given a set S of elements from a discrete universe U , and we need to answer\\nqueries of the form “given x, is x ∈ S?”. This is a classic data structure problem and has many solutions. One\\nconcrete solution is via a hashing [CLRS01], which achieves space of O(|S|) words, each of O(log |U |) bits, and query\\ntime of O(1) in expectation.\\n7In fact, it has been shown that spaces for which analogs of Lemma 2.1 hold are “almost” Hilbert spaces [JN09].\\n4\\nTheorem 2.4 ([KOR00]). Fix ε ∈ (0, 1/2) and dimension d ≥ 1. There is a (1 + O(ε), r)-ANN\\ndata structure over ({0, 1}d, ℓ1) using nO(1/ε2) +O(d(n + k)) space and O(d · lognε2 ) query time.\\nAs a final remark, we note we cannot obtain improved space bounds by improving the dimension\\nreduction lemmas 2.1 and 2.3. Indeed the above lemmas are tight as proven in [JW13]. There was\\nhowever work on improving the run-time complexity for computing a dimension reduction map,\\nimproving over the naïve bound of O(dk); see [AC09, DKS10, AL13, KW11, NPW14, KN14].\\n2.2 ANN via space partitions: Locality-Sensitive Hashing\\nWhile dimension reduction yields ANN data structure with polynomial space, this is not enough\\nin applications, where one desires space as close as possible to linear in n. This consideration led\\nto the following, alternative approach, which yields smaller space bounds, albeit at the cost of\\nincreasing the query time to something of the form nρ where ρ ∈ (0, 1).\\nThe new approach is based on randomized space partitions, and specifically on Locality-Sensitive\\nHashing, introduced in [IM98].\\nDefinition 2.5 (Locality-Sensitive Hashing (LSH)). Fix a metric space (X,D), scale r > 0, ap-\\nproximation c > 1 and a set U . Then a distribution H over maps h : X → U is called (r, cr, p1, p2)-\\nsensitive if the following holds for any x, y ∈ X:\\n• if D(x, y) ≤ r, then Prh[h(x) = h(y)] ≥ p1;\\n• if D(x, y) > cr, then Prh[h(x) = h(y)] ≤ p2.\\nThe distribution H is called an LSH family, and has quality ρ = ρ(H) = log 1/p1log 1/p2 .\\nIn what follows we require an LSH family to have p1 > p2, which implies ρ < 1. Note that LSH\\nmappings are also oblivious: the distribution H does not depend on the point-set P or the query q.\\nUsing LSH, [IM98] show how to obtain the following ANN data structure.\\nTheorem 2.6 ([IM98]). Fix a metric M = (X,D), a scale r > 0, and approximation factor c > 1.\\nSuppose the metric admits a (r, cr, p1, p2)-sensitive LSH family H, where the map h(·) can be stored\\nin σ space, and, for given x, can be computed in τ time; similarly, assume that computing distance\\nD(x, y) takes O(τ) time. Let ρ = ρ(H) = log 1/p1log 1/p2 . Then there exists a (c, r)-ANN data structure\\nover M achieving query time Q = O(nρ · τ log1/p2 np1 ) and space S = O(n1+ρ · 1p1 +nρ 1p1 · σ · log1/p2 n)\\n(in addition to storing the original dataset P ). The time needed to build this data structure is\\nO(S · τ).\\nWhile we describe some concrete LSH families later on, for now, one can think of the param-\\neters τ, σ as being proportional to the dimension of the space (although this is not always the\\ncase).\\nThe overall idea of the algorithm is to use an LSH family as a pre-filter for the dataset P . In\\nparticular, for a random partition h from the family H, the query point q will likely collide with its\\nnear neighbor (with probability at least p1), but with few points at a distance ≥ cr, in expectation\\nat most p2 · n of them. Below we show how an extension of this idea yields Theorem 2.6.\\nProof sketch. Given an LSH family H, we can build a new, derived LSH family via a certain\\ntensoring operation. In particular, for an integer k ≥ 1, consider a new distribution Gk over maps\\ng : X → U , where g(·) is obtained by picking k i.i.d. functions h1, . . . hk chosen from H and setting\\n5\\ng(x) = (h1(x), h2(x), . . . hk(x)) Then, if H is (r, cr, p1, p2)-sensitive, Gk is (r, cr, pk1 , pk2)-sensitive.\\nNote that the parameter ρ of the hash family does not change, i.e., ρ(Gk) = ρ(H).\\nThe entire ANN data structure is now composed of L dictionary data structures (e.g., hash\\ntables discussed in the previous section), where L, k ≥ 1 are parameters to fix later. The i-th\\nhash table is constructed as follows. Pick a map gi uniformly at random from Gk, and store\\nthe set gi(P ) in the dictionary structure. At the query time, we iterate over i = 1 . . . L. For\\na given i, we compute gi(q), and use the dictionary structure to obtain the set of “candidate”\\npoints Qi = {p ∈ P : gi(p) = gi(q)}. For each candidate point we compute the distance from q to\\nthat point. The process is stopped when all Qi’s are processed, or when a point within distance cr\\nto q is found, whichever happens first.\\nTo analyze the success probability, we note that the dictionary structure i succeeds if p∗ ∈ Qi,\\nwhere p∗ is the assumed point at distance at most r from q. This happens with probability at\\nleast pk1. Thus, we can take L = O(1/p\\nk\\n1) such dictionary structures, and thus guarantee success\\nwith a constant probability.\\nThe expected query time is O(L(kτ + Ln · pk2 · τ)), which includes both the computation of\\nthe maps g1(q), . . . gL(q) and the of distances to the candidates in sets Q1, . . . QL. We can now\\nderive the value of k that minimizes the above, obtaining k = ⌈log1/p2 n⌉ ≤ log1/p2 n+1, and hence\\nL = O(nρ/p1).\\nFinally, note that the space usage is O(Ln) for the dictionary structures, plus O(Lkσ) for the\\ndescription of the maps.\\n2.3 Space partitions: LSH constructions\\nTheorem 2.6 assumes the existence of an LSH family H with a parameter ρ < 1. In what follows\\nwe show a few examples of such families.\\n1. Hamming space {0, 1}d, with ρ = 1/c. The distribution H is simply a projection on a random\\ncoordinate i: H = {hi : hi(x) = xi, i = 1, . . . d}. This family is (r, cr, 1 − r/d, 1 − cr/d)-\\nsensitive, and hence ρ ≤ 1/c [IM98].\\nThis LSH scheme is near-optimal for the Hamming space, as described in Section 2.4. We\\nalso note that, since ℓ2 embeds isometrically into ℓ1 (see Section 6), this result extends to ℓ2\\nas well.\\n2. Euclidean space (Rd, ℓ2), with ρ < 1/c. In [DIIM04], the authors introduced an LSH family\\nwhich slightly improves over the above construction. It is based on random projections in ℓ2.\\nIn particular, define a random map h(x) as h(x) = ⌊ 〈x,g〉wr + b⌋, where g is a random d-\\ndimensional Gaussian vector, b ∈ [0, 1], and w > 0 is a fixed parameter. It can be shown that,\\nfor any fixed c > 1, there exists w > 0 such that ρ < 1/c.\\n3. Euclidean space (Rd, ℓ2), with ρ → 1/c2. In [AI06], the authors showed an LSH family with\\na much better ρ, which later turned out to be optimal (see Section 2.4). At its core, the\\nmain idea is to partition the space into Euclidean balls.8 It proceeds in two steps: 1) perform\\na random dimension reduction A to dimension t (a parameter), and 2) partition Rt into balls.\\nSince it is impossible to partition the space Rt into balls precisely9 when t ≥ 2, instead one\\nperforms “ball carving”. The basic idea is to consider a sequence of randomly-centered balls\\n8In contrast, the above LSH family can be seen as partitioning the space into cubes: when considering the k-\\ntensored family G = Hk, the resulting map g ∈ G is equivalent to performing a random dimension reduction (by\\nmultiplying by a random k × d Gaussian matrix), followed by discretization of the space into cubes.\\n9This is also termed tessellation of the space.\\n6\\nB1, B2, . . ., each of radius wr for some parameter w > 1, and define the map h(x), for x ∈ Rd,\\nto be the index i of the first ball Bi containing the point Ax. Since we want to cover an\\ninfinite space with balls of finite volume, the above procedure needs to be modified slightly\\nto terminate in finite time. The modified procedure runs in time T = tO(t).\\nOverall, optimizing for w, one can obtain ρ = 1/c2 + O(log t)√\\nt\\nwhich tends to 1/c2 as t → ∞.\\nThe time to hash is τ = O(Tt+ dt), where T depends exponentially on the parameter t, i.e.,\\nT = tΘ(t). For the ANN data structure, the optimal choice is t = O(log n)2/3, resulting in\\nρ = 1/c2 + O(log logn)\\n(logn)1/3\\n.\\nThe ℓ2 LSH families can be extended to other ℓp’s. For p < 1, [DIIM04] showed one can use\\nmethod 2 as described above, but using p-stable distributions instead of Gaussians. See Section 6\\nfor other extensions for p > 1.\\nWe remark that there is a number of other widely used LSH families, including min-hash\\n[Bro97, BGMZ97] and simhash [Cha02], which apply to different notions of similarity between\\npoints. See [AI08] for an overview.\\n2.4 Space partitions: impossibility results\\nIt is natural to explore the limits of LSH families and ask what is the best ρ one can obtain for a\\ngiven metric space as a function of the approximation c > 1. In [MNP07, OWZ14], it was proven\\nthat the LSH families [IM98, AI06] from the previous section are near-optimal: for the Hamming\\nspace, we must have ρ ≥ 1/c − o(1), and for the Euclidean space, ρ ≥ 1/c2 − o(1). Below is the\\nformal statement from [OWZ14].\\nTheorem 2.7. Fix dimension d ≥ 1 and approximation c ≥ 1. Let H be a (r, cr, p1, p2)-sensitive\\nLSH family over the Hamming space, and suppose p2 ≥ 2−o(d). Then ρ ≥ 1/c − od(1).\\nNote that the above theorem also immediately implies ρ ≥ 1/c2 − o(1) for the Euclidean space,\\nby noting that ‖x− y‖1 = ‖x− y‖22 for binary vectors x and y.\\nFinally, we remark that some condition on p2 is necessary, as there exists an LSH family with\\np2 = 0, p1 = 2−O(d) and hence ρ = 0. To obtain the latter, one can use the “ball carving” family of\\n[AI06], where the balls have radius wr = cr/2. Note however that such a family results in query\\ntime that is at least exponential in d, which LSH algorithms are precisely designed to circumvent.\\n3 (More) Deterministic algorithms\\nA drawback of data structures described in the previous section is that they allow “false negatives”:\\nwith a controllable but non-zero probability, the data structure can report nothing even if the ball\\nB(q, r) is non-empty. Although most of the data structures described in the literature have this\\nproperty, it is possible to design algorithms with stronger guarantees, including deterministic ones.\\nThe first step in this direction was an observation (already made in [KOR00]) that for a finite\\nmetric (X,D) supported by (c, r)-ANN data structures, it is possible to construct a data structure\\nthat provides accurate answers to all queries q ∈ X. This is because one can construct and use\\nO(log |X|) independent data structures, reducing the probability of failure to 13|X| . By taking a\\nunion bound over all q ∈ X, the constructed data structure works, with probability at least 2/3,\\nfor all queries X. Note that the space and query time bounds of the new data structure are\\nO(log |X|) times larger than the respective bounds for (c, r)-ANN . Unfortunately, the algorithm\\n7\\nfor constructing such data structures has still a non-zero failure probability, and no deterministic\\npolynomial-time algorithm for this task is known.\\nThe first deterministic polynomial-time algorithm for constructing a data structure that works\\nfor all queries q ∈ X appeared in [Ind00a]. It was developed for d-dimensional Hamming spaces,\\nand solved a (c, r)-ANN with an approximation factor c = 3+ ε for any ε > 0. The data structure\\nhad d(1/ε)O(1) query time and used dn(1/ε)\\nO(1)\\nspace. It relied on two components. The first\\ncomponent, “densification”, was a deterministic analog of the mapping in Lemma 2.3, which was\\nshown to hold with k = (d/ε)O(1). Retrospectively, the mapping can be viewed as being induced\\nby an adjacency matrix of an unbalanced expander [GUV09].\\nDefinition 3.1 (Expander). An (r, α)-unbalanced expander is a bipartite simple graphG = (U, V,E),\\n|U | = d, |V | = k, with left degree ∆ such that for any X ⊂ U with |X| ≤ r, the set of neighbors\\nN(X) of X has size |N(X)| ≥ (1− α)∆|X|.\\nGiven such a graph G, one can construct a mapping f = fG : {0, 1}d → Σk for some finite\\nalphabet Σ by letting f(x)j to be the concatenation of all symbols xi such that (i, j) ∈ E. Let\\nH(x, y) be the Hamming distance between x and y, i.e., the number of coordinates on which x and\\ny differ. We have that:\\n• H(f(x), f(y)) ≤ ∆H(x, y), since each difference between a and b contributes to at most ∆\\ndifferences between f(x) and f(y). In particular H(f(x), f(y)) ≤ ∆r(1 − ε) if H(x, y) ≤\\nr(1− ε).\\n• if H(x, y) ≥ r, then H(f(x), f(y)) ≥ (1− α)∆r (from the expansion property).\\nThus, setting α = ε/2 yields guarantees analogous to Lemma 2.3, but using a deterministic\\nmapping, and with coordinates of f(x) in Σ, not {0, 1}. To map into binary vectors, we further\\nreplace each symbol f(x)j by C(f(x)j), where C : Σ → {0, 1}s is an error-correcting code, i.e.,\\nhaving the property that for any distinct a, b ∈ Σ we have H(C(a), C(b)) ∈ [s(1/2− ε), s(1/2 + ε)].\\nWe then use off-the-shelf constructions of expanders [GUV09] and codes [GRS14] to obtain the\\ndesired mapping g = C ◦ f : {0, 1}d → {0, 1}ks.\\nThe second component partitions the coordinates of points g(x) into blocks S1 . . . St of size\\nlog(n)/εO(1) such that an analog of Lemma 2.3 holds for all projections g(x)Sl and g(y)Sl where\\nx, y ∈ P , l = 1 . . . t. Such a partitioning can be shown to exist using the probabilistic method,\\nand can be computed deterministically in time polynomial in n via the method of conditional\\nprobabilities. Unfortunately, this property does not extend to the case where one of the points\\n(say, x) is a query point from X − P . Nevertheless, by averaging, there must be at least one block\\nSl such that H(g(x)Sl , g(y)Sl) ≤ H(g(x), g(y))/t, where y is the nearest neighbor of x in P . It can\\nbe then shown that an approximate near neighbor of g(x)Sl in {g(y)Sl : y ∈ P} is an approximate\\nnearest neighbor of x in P . Finding the nearest neighbor in the space restricted to a single block\\nSl can be solved via exhaustive storage using n1/ε\\nO(1)\\nspace, as in Theorem 2.4.\\nPerhaps surprisingly, the above construction is the only known example of a polynomial-size\\ndeterministic approximate near neighbor data structure with a constant approximation factor.\\nHowever, more progress has been shown for an “intermediary” problem, where the data structure\\navoids false negatives by reporting a special symbol ⊥.\\nDefinition 3.2 ((c, r, δ)-Approximate Near Neighbor Without False Negatives (ANNWFN)). Given\\na set P of n points in a metric space (X,D), build a data structure S that, given any query\\npoint q ∈ X such that B(q, r) ∩ P 6= ∅, S returns an element of (B(q, cr) ∩ P ) ∪ {⊥}, and\\nPrS [S returns ⊥] ≤ δ.\\n8\\nA (1+ ε, r, δ)-ANNWFN data structure with bounds similar to those in Theorem 2.4 was given\\nin [Ind00a]. It used densification and random block partitioning as described above. However,\\nthanks to randomization, block partitioning could be assumed to hold even for the query point\\nwith high probability.\\nObtaining “no false negatives” analogs of Theorem 2.2 turned out to be more difficult. The first\\nsuch data structure was presented in [Pag16], for the Hamming space, achieving query time of the\\nform (roughly) dn1.38/c. Building on that work, very recently, Ahle [Ahl17] improved the bound to\\n(roughly) dn1/c, achieving the optimal runtime exponent.\\nIn addition to variants of densification and random block partitioning, the latter algorithm uses\\na generalization of the space partitioning method from Section 2.2, called locality sensitive filtering.\\nSuch objects can be constructed deterministically in time and space roughly exponential in the\\ndimension. Unfortunately, random block partitioning leads to blocks whose length is larger than\\nlog n by at least a (large) constant, which results in large (although polynomial) time and space\\nbounds. To overcome this difficulty, [Ahl17] shows how to combine filters constructed for dimension\\nd to obtain a filter for dimension 2d. This is achieved by using splitters [NSS95], which can be\\nviewed as families of partitions of {1 . . . 2d} into pairs of sets (S1, S1), (S2, S2), . . . of size d, such\\nthat for any x, y, there is a pair (Sl, Sl) for which H(xSl , ySl) = H(xSl , ySl)± 1. The construction\\nmultiplies the space bound by a factor quadratic in d, which makes it possible to apply it a small\\nbut super-constant number of times to construct filters for (slightly) super-logarithmic dimension.\\n4 Data-dependent approach\\nIn the earlier sections, we considered ANN data structures that are based on random and determin-\\nistic space partitions. The unifying feature of all of the above approaches is that the partitions used\\nare independent of the dataset. This “data-independence” leads to certain barriers: for instance,\\nthe best possible LSH exponent is ρ ≥ 1/c− o(1) for the ℓ1 distance and ρ ≥ 1/c2 − o(1) for ℓ2 (see\\nSection 2.4). In this section, we show how to improve upon the above results significantly if one\\nallows partitions to depend on the dataset.\\nThis line of study has been developed in a sequence of recent results [AINR14, AR15, ALRW17].\\nHowever, even before these works, the data-dependent approach had been very popular in practice\\n(see, e.g., surveys [WSSJ14, WLKC16]). Indeed, real-world datasets often have some implicit or\\nexplicit structure, thus it pays off to tailor space partitions to a dataset at hand. However, the\\ntheoretical results from [AINR14, AR15, ALRW17] improve upon data-independent partitions for\\narbitrary datasets. Thus, one must show that any set of n points has some structure that makes\\nthe ANN problem easier.\\n4.1 The result\\nIn [AR15] (improving upon [AINR14]), the following result has been shown.\\nTheorem 4.1. For every c > 1, there exists a data structure for (c, r)-ANN over (Rd, ℓ2) with\\nspace n1+ρ +O(nd) and query time nρ + dno(1), where\\nρ ≤ 1\\n2c2 − 1 + o(1).\\nThis is much better than the best LSH-based data structure, which has ρ = 1c2 + o(1). For\\ninstance, for c = 2, the above theorem improves the query time from n1/4+o(1) to n1/7+o(1), while\\nusing less memory.\\nNext, we describe the new approach at a high level.\\n9\\n4.2 Simplification of the problem\\nBefore describing new techniques, it will be convenient to introduce a few simplifications. First,\\nwe can assume that d = log1+o(1) n, by applying Lemma 2.1. Second, we can reduce the general\\nANN problem over (Rd, ℓ2) to the spherical case: where dataset and queries lie on the unit sphere\\nSd−1 ⊂ Rd (see [Raz17], pages 55–56). Both the dimension reduction and the reduction to the\\nspherical case incur a negligible loss in the approximation10. After the reduction to the spherical\\ncase, the distance to the near neighbor r can be made to be any function of the number of points\\nn that tends to zero as n→∞ (for example, r = 1log logn).\\n4.3 Data-independent partitions for a sphere\\nIn light of the above discussion, we need to solve the (c, r)-ANN problem for Sd−1, where d =\\nlog1+o(1) n and r = o(1). Even though the final data structure is based on data-dependent partitions,\\nwe start with developing a data-independent LSH scheme for the unit sphere, which will be later\\nused as a building block.\\nThe LSH scheme is parametrized by a number η > 0. Consider a sequence of i.i.d. samples\\nfrom a standard d-dimensional Gaussian distribution N(0, 1)d: g1, g2, . . . , gt, . . . ∈ Rd. The hash\\nfunction h(x) of the point x ∈ Sd−1 is then defined as mint{t ≥ 1 | 〈x, gt〉 ≥ η}. This LSH family\\ngives the following exponent ρ for distances r and cr:\\nρ =\\nlog 1/p1\\nlog 1/p2\\n=\\n4− c2r2\\n4− r2 ·\\n1\\nc2\\n+ δ(r, c, η), (1)\\nwhere δ(r, c, η) > 0 and δ(r, c, η) → 0 as η → ∞. Thus, the larger the value of the threshold η is,\\nthe more efficient the resulting LSH scheme is. At the same time, η affects the efficiency of hash\\nfunctions. Indeed, one can show that with very high probability maxx∈Sd−1 h(x) ≤ e(1+o(1))η2/2 ·\\ndO(1), which bounds the hashing time as well as the number of Gaussian vectors to store.\\nConsider the expression (1) for the exponent ρ in more detail. If r = o(1), then we obtain\\nρ = 1c2 +o(1), which matches the guarantee of the best data-independent LSH for ℓ2. This is hardly\\nsurprising, since, as was mentioned above, the general ANN problem over ℓ2 can be reduced to the\\n(c, r)-ANN problem over the sphere for r = o(1). If r ≈ 2/c, then ρ is close to zero, and, indeed,\\nthe (c, 2/c)-ANN problem on the sphere is trivial (any point can serve as an answer to any valid\\nquery).\\nBetween these two extremes, there is a point r ≈\\n√\\n2\\nc that is crucial for the subsequent discussion.\\nSince the distance between a pair of random points on Sd−1 is close to\\n√\\n2 with high probability, the\\nproblem where r is slightly smaller than\\n√\\n2\\nc has the following interpretation: if one is guaranteed\\nto have a data point within distance r from the query, find a data point that is a bit closer to the\\nquery than a typical point on the sphere. For r ≈\\n√\\n2\\nc , the equation (1) gives exponent ρ ≈ 12c2−1 ,\\nwhich is significantly smaller than the bound 1c2 one is getting for r = o(1). Later, using a certain\\ndata-dependent partitioning procedure, we will be able to reduce the general ANN problem on\\nthe sphere to this intermediate case of r ≈\\n√\\n2\\nc , thus obtaining the ANN data structure with the\\nexponent ρ = 12c2−1 + o(1). This significantly improves upon the best possible LSH for ℓ2 from\\nSection 2, which yields ρ = 1c2 + o(1).\\n10Approximation c reduces to approximation c− o(1).\\n10\\n4.4 Data-dependent partitions\\nWe now describe at a high level how to obtain a data structure with space n1+ρ and query time nρ,\\nwhere ρ = 12c2−1+o(1), for the (c, r)-ANN problem on the sphere for general r > 0. If r ≥\\n√\\n2\\nc −o(1),\\nthen we can simply use the data-independent LSH described above. Now suppose r is nontrivially\\nsmaller than\\n√\\n2\\nc .\\nWe start with finding and removing dense low-diameter clusters. More precisely, we repeatedly\\nfind a point u ∈ Sd−1 such that |P ∩ B(u,√2 − ε)| ≥ τn, where ε, τ = o(1), and set P :=\\nP \\\\ B(u,√2 − ε). We stop when there are no more dense clusters remaining. Then we proceed\\nwith clusters and the remainder separately. Each cluster is enclosed in a ball of radius 1 − Ω(ε2)\\nand processed recursively. For the remainder, we sample one partition from the data-independent\\nLSH family described above, apply it to the dataset, and process each resulting part of the dataset\\nrecursively. During the query stage, we (recursively) query the data structure for every cluster (note\\nthat the number of clusters is at most 1/τ), and for the remainder we query (again, recursively) a\\npart of the partition, where the query belongs to. Each step of the aforementioned procedure makes\\nprogress as follows. For clusters, we decrease the radius by a factor of 1 − Ω(ε2). It means that\\nwe come slightly closer to the ideal case of r ≈\\n√\\n2\\nc , and the instance corresponding to the cluster\\nbecomes easier. For the remainder, we use the fact that there are at most τn data points closer\\nthan\\n√\\n2− ε to the query. Thus, when we apply the data-independent LSH, the expected number\\nof data points in the same part as the query is at most (τ + p2)n, where p2 is the probability of\\ncollision under the LSH family for points at the distance\\n√\\n2− ε. We set τ ≪ p2, thus the number\\nof colliding data points is around p2n. At the same time, the probability of collision with the near\\nneighbor is at least p1, where p1 corresponds to the distance r. Since r <\\n√\\n2\\nc , we obtain an effective\\nexponent of at most 12c2−1 + o(1). Note that we need to keep extracting the clusters recursively to\\nbe able to apply the above reasoning about the remainder set in each step.\\nOne omission in the above high-level description is that the clusters are contained in smaller\\nballs rather than spheres. This is handled by partitioning balls into thin annuli and treating them\\nas spheres (introducing negligible distortion).\\n4.5 Time–space trade-off\\nIn [ALRW17], Theorem 4.1 has been extended to provide a smooth time–space trade-off for the\\nANN problem. Namely, it allows to decrease the query time at a cost of increasing the space and\\nvice versa.\\nTheorem 4.2. For every c > 1 and every ρs, ρq such that\\nc2\\n√\\nρq + (c2 − 1)√ρs ≥\\n√\\n2c2 − 1, (2)\\nthere exists a data structure for (c, r)-ANN over (Rd, ℓ2) with space n1+ρs+o(1) + O(nd) and query\\ntime nρq+o(1) + dno(1).\\nThe bound (2) interpolates between:\\n• The near-linear space regime: ρs = 0, ρq = 2c2 − 1c4 ;\\n• The “balanced” regime: ρs = ρq = 12c2−1 , where it matches Theorem 4.1;\\n• The fast queries regime: ρs =\\n(\\nc2\\nc2−1\\n)2\\n, ρq = 0.\\n11\\nFor example, for c = 2, one can obtain any of the following trade-offs: space n1+o(1) and query\\ntime n7/16+o(1), space n8/7+o(1) and query time n1/7+o(1), and space n16/9+o(1) and query time no(1).\\nTheorem 4.2 significantly improves upon the previous ANN data structures in various regimes [IM98,\\nKOR00, Ind00b, Pan06, Kap15]. For example, it improves the dependence on ε in Theorem 2.2\\nfrom O(log(1/ε)/ε2) to O(1/ε2).\\n4.6 Impossibility results\\nSimilarly to the data-independent case, it is natural to ask whether exponent ρ = 12c2−1 + o(1)\\nfrom Theorem 4.1 is optimal for data-dependent space partitions. In [AR16], it was shown that\\nthe above ρ is near-optimal in a properly formalized framework of data-dependent space partitions.\\nThis impossibility result can be seen as an extension of the results discussed in Section 2.4.\\nSpecifically, [AR16] show that ρ ≥ 1\\n2c2−1 , where ρ =\\nlog 1/p1\\nlog 1/p2\\nfor p1 and p2 being certain natural\\ncounterparts of the LSH collision probabilities for the data-dependent case, even when we allow\\nthe distribution on the partitions to depend on a dataset. This result holds under two further\\nconditions. First, as in Section 2.4, we need to assume that p2 is not too small.\\nThe second condition is specific to the data-dependent case, necessary to address another nec-\\nessary aspect of the space partition. For any dataset, where all the points are sufficiently well\\nseparated, we can build an “ideal” space partition, with ρ = 0, simply by considering its Voronoi\\ndiagram. However, this is obviously not a satisfactory space partition: it is algorithmically hard to\\ncompute fast where in the partition a fixed query point q falls to — in fact, it is precisely equivalent\\nto the original nearest neighbor problem! Hence, to be able to prove a meaningful lower bound\\non ρ, we would need to restrict the space partitions to have low run-time complexity (e.g., for a\\ngiven point q, we can compute the part where q lies in, in time no(1)). This precise restriction is\\nwell beyond reach of the current techniques (it would require proving computational lower bounds).\\nInstead, [AR16] use a different, proxy restriction: they require that the description complexity of\\npartitions is n1−Ω(1). The latter restriction is equivalent to saying that the distribution of partitions\\n(which may depend on the given dataset) is supported on a fixed (universal) family of partitions of\\nthe size 2n\\n1−Ω(1)\\n. This restriction, for instance, rules out the Voronoi diagram, since the latter has a\\ndescription complexity of Ω(n). Furthermore, the description complexity of a randomized partition\\nis a good proxy for the run-time complexity of a partition because in all the known constructions of\\nrandom space partitions with a near-optimal ρ, the run-time complexity is at least the description\\ncomplexity, which makes the requirement meaningful.\\nOverall, under the above two conditions, [AR16] show that ρ ≥ 1\\n2c2−1 − o(1) for data-dependent\\nrandom space partitions, and hence Theorem 4.1 is essentially optimal in this framework.\\n4.7 ANN for ℓ∞\\nIn this subsection we will describe another type of data-dependent data structure, for the ℓ∞ norm.\\nHistorically, this was the first example of a data-dependent partitioning procedure used for ANN\\nover high-dimensional spaces.\\nTheorem 4.3 ([Ind01]). For every 0 < ε < 1, there exists a deterministic data structure for (c, 1)-\\nANN for (Rd, ℓ∞) with approximation c = O\\n(\\nlog log d\\nε\\n)\\n, space O(dn1+ε) and query time O(d log n).\\nThe algorithm relies on the following structural lemma.\\nLemma 4.4. Let P ⊂ Rd be a set of n points and 0 < ε < 1. Then:\\n1. Either there exists an ℓ∞-ball of radius O\\n(\\nlog log d\\nε\\n)\\nthat contains Ω(n) points from P , or\\n12\\n2. There exists a “good” coordinate i ∈ {1, 2, . . . , d} and a threshold u ∈ R such that for the sets\\nA = {p ∈ P | pi < u − 1}, B = {p ∈ P | u − 1 ≤ pi ≤ u+ 1} and C = {p ∈ P | pi > u + 1}\\none has: ( |A|+ |B|\\nn\\n)1+ε\\n+\\n( |B|+ |C|\\nn\\n)1+ε\\n≤ 1 (3)\\nand |A|/n, |C|/n ≥ Ω(1/d).\\nUsing this lemma, we can build the data structure for (c, 1)-ANN for (Rd, ℓ∞) recursively. If\\nthere exists a ball B(x,R) with R = O\\n(\\nlog log d\\nε\\n)\\nsuch that |P ∩B(x,R)| ≥ Ω(n) (Case 1), then we\\nstore x and R and continue partitioning P \\\\B(x,R) recursively. If there exists a good coordinate\\ni ∈ {1, 2, . . . , d} and a threshold u ∈ R (Case 2), then we define sets A, B, C as in the above lemma\\nand partition A∪B and B ∪C recursively. We stop as soon as we reach a set that consists of O(1)\\npoints.\\nThe query procedure works as follows. Suppose there is a point in P within distance 1 from q\\n(“the near neighbor”). If we are in Case 1, we check if the query point q lies in B(x,R + 1). If it\\ndoes, we return any data point from B(x,R); f not, we query the remainder recursively. On the\\nother hand, if we are in Case 2, we query A ∪ B if qi ≤ u, and B ∪ C otherwise. In this case we\\nrecurse on the part which is guaranteed to contain a near neighbor.\\nOverall, we always return a point within distance O\\n(\\nlog log d\\nε\\n)\\n, and it is straightforward to bound\\nthe query time by bounding the depth of the tree. We obtain the space bound of O(dn1+ε) by using\\nthe property (3) to bound the number of times points that are replicated in the Case 2 nodes.\\nSurprisingly, the approximation O(log log d) turns out to be optimal in certain restricted models\\nof computation [ACP08, KP12], including for the approach from [Ind01].\\n5 Closest pair\\nA problem closely related to ANN is the closest pair problem, which can be seen as an “offline”\\nversion of ANN. Here, we are given a set P of n points, and we need to find a pair p, q ∈ P of\\ndistinct points that minimize their distance.\\nA trivial solution is to compute the distance between all possible\\n(n\\n2\\n)\\npairs of points and take\\nthe one that minimizes the distance. However this procedure has quadratic running time. As for\\nthe nearest neighbor problem, there is evidence that for, say, d-dimensional ℓ2 space, the closest\\npair problem cannot be solved in time n2−αdO(1) for any constant α > 0.\\nAs with c-ANN, we focus on the approximate version of the problem. Furthermore, we consider\\nthe decision version, where we need to find a pair of points that are below a certain threshold r.\\nThe formal definition (for the randomized variant) follows.\\nDefinition 5.1 ((c, r)-approximate close pair problem, or (c, r)-CP). Given a set of points P ⊂ X\\nof size n, if there exist distinct points p∗, q∗ ∈ X with D(p∗, q∗) ≤ r, find a pair of distinct points\\np, q ∈ P such that D(p, q) ≤ cr, with probability at least 2/3.\\nThe (c, r)-CP problem is closely related to the (c, r)-ANN problem because we can solve the\\nformer using a data structure for the latter. In particular, one can run the following procedure:\\npartition P into two sets A, B randomly; build (c, r)-ANN on the set A; query every point q ∈ B. It\\nis easy to see that one such run succeeds in solving a (c, r)-approximate close pair with probability\\nat least 1/2 · 2/3. Repeating the procedure 3 times is enough to guarantee a success probability of\\n2/3. If (c, r)-ANN under the desired metric can be solved with query time Q(n) and preprocessing\\ntime S(n), we obtain a solution for (c, r)-CP running in time O(S(n) + nQ(n)). For example,\\n13\\napplying the reduction from above for (Rd, ℓp) space for p ∈ {1, 2}, we immediately obtain an\\nalgorithm running in O(dn1+ρ) time, where ρ = 12cp−1 + o(1) (Section 4).\\nFocusing on the case of ℓ2, and approximation c = 1 + ε, the above algorithm has runtime\\nO(n2−4ε+O(ε2)d). It turns out that, for the ℓ2 norm, one can obtain algorithms with a better\\ndependance on ε, for small ε. In particular, the line of work from [Val15, KKK16, ACW16] led to\\nthe following algorithm:\\nTheorem 5.2 ([ACW16]). Fix dimension d ≥ 1, r > 0, and ε ∈ (0, 1/2). Then, for any set of\\nn points in Rd, one can solve the (1 + ε, r)-CP over ℓ2 in time O(n2−Ω(ε\\n1/3/ log(1/ε)) + dn), with\\nconstant probability.\\nNote that the running time bound in the above theorem is better than that obtained using LSH\\ndata structures, for small enough ε.\\nThe main new technical ingredient is the fast matrix multiplication algorithm. In particular,\\nsuppose we want to multiply two matrices of size n ×m and m × n. Doing so naïvely takes time\\nO(n2m). Starting with the work of [Str69], there has been substantial work to improve this run-\\ntime; see also [Wil12]. Below we state the running time of a fast matrix multiplication algorithm\\ndue to [Cop82], which is most relevant for this section.\\nTheorem 5.3 ([Cop82]). Fix n ≥ 1 and let m ≥ 1 be such that m ≤ n0.172. One can compute the\\nproduct of two matrices of sizes n×m and m× n in O(n2 log2 n) time.\\n5.1 Closest pair via matrix multiplication\\nWe now sketch the algorithm for the closest pair from [Val15], which obtains O(n2−Ω(\\n√\\nε)d) time.\\nThe algorithm is best described in terms of inner products, as opposed to distances as before. In\\nparticular, suppose we have a set of points P ⊂ Sd of unit norm, where all pairs of points have\\ninner product in the range [−θ, θ], except for one “special” pair that has inner product at least\\ncθ, for some scale θ > 0 and approximation c = 1 + ε. Now the problem is to find this special\\npair—we term this problem (c, θ)-IP problem. We note that we can reduce (1+ ε, r)-CP over ℓ2 to\\n(1 + Ω(ε), 1/2)-IP , by using the embedding of [Sch42], or Lemma 2.3 of [KOR00].\\nA natural approach to the the IP problem is to multiply two n × d matrices: if we consider\\nthe matrix M where the rows are the points of P , then MM t will have a large off-diagonal entry\\nprecisely for the special pair of points. This approach however requires at least n2 computation\\ntime, since even the output of MM t has size n2. Nevertheless, an extension of this approach gives\\na better run-time when c is very large (and hence θ < 1/c very small, i.e., all points except for the\\nspecial pair are near-orthogonal). In particular, partition randomly the vectors from P into n/g\\ngroups S1, . . . Sn/g, each of size O(g). For each group i, we sum the vectors Si with random signs,\\nobtaining vectors vi =\\n∑\\npj∈Si χjpj, where pj are the points in P and χj are Rademacher random\\nvariables. Now the algorithm forms a matrix M with vi’s as rows, and computes MM t using fast\\nmatrix multiplication (Theorem 5.3). The two special points are separated with probability 1−g/n.\\nConditioning on this event, without loss of generality, we can assume that they are in group 1 and\\n2 respectively. Then, it is easy to note that |(MM t)12| ≈ Θ(c · θ), whereas, for (i, j) 6= (1, 2) and\\ni 6= j, we have that |(MM t)ij | ≈ O(g · θ) with constant probability. Hence, we can identify the\\nspecial pair in the product MM t as long as c ≫ g, and yields runtime O(n2/g2), i.e., a g2 ≪ c2\\nspeed-up over the naïve algorithm (note that Theorem 5.3 requires that d < n0.172).\\nThe above approach requires c to be very large, and hence the challenge is whether we can\\nreduce the case of c = 1 + ε to the case of large c. Indeed, one method is to use tensoring:\\nfor a fixed parameter k and any two vectors x, y ∈ Rd, we consider x⊗k, y⊗k ∈ Rdk , for which\\n14\\n〈x⊗k, y⊗k〉 = (〈x, y〉)k. Thus tensoring reduces the problem of (1 + ε, 1/2)-IP to ((1 + ε)k, 2−k)-IP,\\nand hence we hope to use the above algorithm for c = (1+ ε)k ≈ eεk. If we use t = ζ lnn, for small\\nconstant ζ, we obtain c = nεζ , and hence we obtain a speed-up of g2 ≈ c2 = n2εζ . One caveat here\\nis that, after tensoring the vectors, we obtain vectors of dimension dk, which could be much larger\\nthan n—then even writing down such vectors would take Ω(n2) time. Yet, one can use a dimension\\nreduction method, like Lemma 2.1, to reduce dimension to O( logn\\nθk\\n) = O˜(nζ ln 2), which is enough\\nto preserve all inner products up to additive, say, 0.1 ·θk. There are further details (e.g., we cannot\\nafford to get high-dimensional vectors in the first place, even if we perform dimension-reduction),\\nsee [Val15, KKK16] for more details.\\nThe above algorithm yields a speed-up of the order of nO(ε), i.e., comparable to the speed-up\\nvia the LSH methods. To obtain a better speed-up, like in the Theorem 5.2, one can replace\\nthe tensoring transformation with a more efficient one. Indeed, one can employ an asymmetric\\nembedding f, g : Rd → Rm, with the property that for any unit-norm vectors x, y, we have that\\n〈f(x), g(y)〉 = p(〈x, y〉), where p(·) is a polynomial of choice. In particular, we require a polynomial\\np(·) that is small on the interval [−θ, θ], as large as possible on [(1 + ε)θ, 1], and p(1) is bounded.\\nNote that the tensoring operation implements such an embedding with p(a) = ak and where\\nf(x) = g(x) = x⊗k. However, there are more efficient polynomials: in fact, the optimal such\\npolynomial is the Chebyshev polynomial. For example, for the degree-k Chebyshev polynomial\\nTk(·), we have that Tk(1+ε)/Tk(1) ≈ e\\n√\\nεk, which is in contrast to the above polynomial p(a) = ak,\\nfor which p(1 + ε)/p(1) ≈ eεk.\\nUsing the Chebyshev polynomials, one can obtain a runtime of n2−Ω(\\n√\\nε) for the IP and hence\\nCP problem. To obtain the improved result from Theorem 5.2, [ACW16] employ randomized\\npolynomials, i.e., a distribution over polynomials where p(·) is small/large only with a certain\\nprobability. Without going into further details, the theorem below states the existence of such\\npolynomials, which are used to obtain n2−Ω(ε1/3/ log(1/ε)) run-time for the (1 + ε, r)-CP problem.\\nTheorem 5.4 ([ACW16]). Fix d ≥ 1, θ ≥ 1, s ≥ 1, and ε > 0. There exists a distribution\\nover polynomials P : {0, 1}d → R of degree O(ε−1/3 log s), such that we have the following for any\\nx ∈ {0, 1}d:\\n• if ∑di=1 xi ≤ θ, then |P (x)| ≤ 1 with probability at least 1− 1/s;\\n• if ∑di=1 xi ∈ (θ, (1 + ε)θ), then |P (x)| > 1 with probability at least 1− 1/s;\\n• if ∑di=1 xi > (1 + ε)θ, then |P (x)| ≥ s with probability at least 1− 1/s.\\n6 Extensions\\nIn this section, we discuss several techniques that significantly extend the class of spaces which\\nadmit efficient ANN data structures.\\n6.1 Metric embeddings\\nSo far, we have studied the ANN problem over the ℓ1, ℓ2 and ℓ∞ distances. A useful approach is\\nto embed a metric of interest into ℓ1/ℓ2/ℓ∞ and use one of the data structures developed for the\\nlatter spaces.\\n15\\n6.1.1 Deterministic embeddings\\nDefinition 6.1. For metric spaces M = (X,DX), N = (Y,DY ) and for D ≥ 1, we say that a map\\nf : X → Y is a bi-Lipschitz embedding with distortion D if there exists λ > 0 such that for every\\nx1, x2 ∈ X one has:\\nλdX(x1, x2) ≤ DY (f(x1), f(x2)) ≤ D · λDX(x1, x2).\\nA bi-Lipschitz embedding of M into N with distortion D together with a data structure for\\n(c, r)-ANN over N immediately implies a data structure for (cD, r′)-ANN over M, where r′ = rλD .\\nHowever, space and query time of the resulting data structure depend crucially on the computa-\\ntional efficiency of the embedding, since, in particular, the query procedure requires evaluating the\\nembedding on a query point.\\nAs the following classic results show, any finite-dimensional normed or finite metric space can\\nbe embedded into finite-dimensional ℓ∞ with small distortion.\\nTheorem 6.2 (Fréchet–Kuratowski, [Fré06, Kur35]). If M is a finite metric space, which consists\\nof N points, then M embeds into (RN , ℓ∞) with distortion D = 1 (isometrically).\\nTheorem 6.3 (see, e.g., [Woj96]). For every ε > 0, every normed space (Rd, ‖ · ‖) embeds with\\ndistortion 1 + ε into (Rd\\n′\\n, ℓ∞), where d′ = O(1/ε)d, via a linear map.\\nHowever, the utility of Theorems 6.2 and 6.3 in the context of the ANN problem is limited,\\nsince the required dimension of the target ℓ∞ space is very high (in particular, Theorem 6.3 gives\\na data structure with exponential dependence on the dimension). Moreover, even if we allow the\\ndistortion D of an embedding to be a large constant, the target dimension can not be improved\\nmuch. As has been shown in [Mat97], one needs at least NΩ(1/D)-dimensional ℓ∞ to “host” all the\\nN -point metrics with distortion D. For d-dimensional norms, even as simple as ℓ2, the required\\ndimension is 2ΩD(d) [FLM77, Bal97].\\nMore generally, (lower-dimensional) ℓ∞ turns out to be not so useful of a target space, and only\\na handful of efficient embeddings into ℓ∞ are known (for instance, such an embedding has been\\nconstructed in [FCI99] for the Hausdorff distance). Luckily, the situation drastically improves, if\\nwe allow randomized embeddings, see Section 6.1.2 for the examples.\\nInstead of ℓ∞, one can try to embed a metric of interest into ℓ1 or ℓ2. Let us list a few cases,\\nwhere such embeddings lead to efficient ANN data structures.\\n• Using the result from [JS82], one can embed (Rd, ℓp) for 1 < p ≤ 2 into (Rd′ , ℓ1) with distortion\\n1 + ε, where d′ = O(d/ε2). Moreover, the corresponding map is linear and hence efficient to\\nstore and apply. This reduction shows that the ANN problem over ℓp for 1 < p ≤ 2 is no\\nharder than for the ℓ1 case. However, later in this section we will show how to get a better\\nANN algorithm for the ℓp case using a different embedding.\\n• For the Wasserstein-1 distance (a.k.a. the Earth-Mover distance in the computer science\\nliterature) between probability measures defined on {1, 2, . . . , d}k, one can use the results\\nfrom [Cha02, IT03, NS07], to embed it into (Rd\\nO(k)\\n, ℓ1) with distortion O(k log d).\\n• The Levenshtein distance (a.k.a. edit distance) over the binary strings {0, 1}d can be embed-\\nded into (Rd\\nO(1)\\n, ℓ1) with distortion 2O(\\n√\\nlog d log log d) [OR07].\\nLet us note that there exist generic results concerned with embeddings into ℓ1/ℓ2 similar to\\nTheorem 6.2 and Theorem 6.3.\\n16\\nTheorem 6.4 ([Bou85, LLR95]). Any N -point metric embeds into (RO(logN), ℓ2) with distortion\\nO(logN).\\nTheorem 6.5 ([Joh48, Bal97]). Any normed space (Rd, ‖ · ‖) embeds into (Rd, ℓ2) with distortion√\\nd via a linear map.\\nTheorem 6.4 does not give an embedding efficient enough for the ANN applications: computing\\nit in one point requires time Ω(N). At the same time, Theorem 6.5 is efficient and, together with ℓ2\\ndata structures, gives an ANN data structure for a general d-dimensional norm with approximation\\nO(\\n√\\nd).\\nSince the ANN problem is defined for two specific distance scales (r and cr), we do not need\\nthe full power of bi-Lipschitz embeddings and sometimes can get away with weaker notions of\\nembeddability. For example, the following theorem follows from the results of [Sch37].\\nIn the theorem, ℓ2(N) denotes the space of infinite sequences (ai)∞i=1 such that\\n∑\\ni |ai|2 < +∞\\nand the norm of the sequence ‖a‖2 is equal to\\n(∑\\ni |ai|2\\n)1/2.\\nTheorem 6.6. For every 1 ≤ p < 2 and every d ≥ 1, there exists a map f : Rd → ℓ2(N) such that\\nfor every x, y ∈ Rd, one has:\\n‖f(x)− f(y)‖22 = ‖x− y‖pp.\\nThis embedding allows to use an ANN data structure for ℓ2 with approximation c to get an ANN\\ndata structure for ℓp with approximation c2/p. However, for this we need to make the embedding\\ncomputationally efficient. In particular, the target must be finite-dimensional. This can be done,\\nsee [Ngu14] for details. As a result, for the ℓp distance for 1 ≤ p < 2, we are able to get the result\\nsimilar to the one given by Theorem 4.2, where in (2) c2 is replaced with cp everywhere.\\n6.1.2 Randomized embeddings\\nIt would be highly desirable to utilize the fact that every metric embeds well into ℓ∞ (Theorems 6.2\\nand 6.3) together with the ANN data structure for ℓ∞ from Section 4.7. However, as discussed\\nabove, spaces as simple as (Rd, ℓ1) or (Rd, ℓ2) require the target ℓ∞ to have 2Ω(d) dimensions to\\nbe embedded with small distortion. It turns out, this can be remedied by allowing embeddings\\nto be randomized. In what follows, we will consider the case of (Rd, ℓ1), and then generalize the\\nconstruction to other metrics.\\nThe randomized embedding of (Rd, ℓ1) into (Rd, ℓ∞) is defined as follows: we generate d i.i.d.\\nsamples u1, u2, . . . , ud from the exponential distribution with parameter 1, and then the embedding\\nf maps a vector x ∈ Rd into (\\nx1\\nu1\\n,\\nx2\\nu2\\n, . . . ,\\nxd\\nud\\n)\\n.\\nThus, the resulting embedding is linear. Besides that, it is extremely efficient to store (d numbers)\\nand apply (O(d) time).\\nLet us now understand how ‖f(x)‖∞ is related to ‖x‖1. The analysis uses (implicitly) the\\nmin-stability property of the exponential distribution. One has for every t > 0:\\nPrf [‖f(x)‖∞ ≤ t] =\\nd∏\\ni=1\\nPr\\n[ |xi|\\nui\\n≤ t\\n]\\n=\\nd∏\\ni=1\\nPr\\n[\\nui ≥ |xi|\\nt\\n]\\n=\\nd∏\\ni=1\\ne−|xi|/t = e−‖x‖1/t.\\nThe random variable ‖f(x)‖∞ does not have a finite first moment, however its mode is in the\\npoint t = ‖x‖1, which allows us to use ‖f(x)‖∞ to estimate ‖x‖1. It is immediate to show that for\\n17\\nevery δ > 0, there exist C1, C2 > 1 with C1 = O(log(1/δ)) and C2 = O(1/δ) such that for every x,\\none has:\\nPrf\\n[\\n‖f(x)‖∞ ≥ ‖x‖1\\nC1\\n]\\n≥ 1− δ (4)\\nand\\nPrf [‖f(x)‖∞ ≤ C2 · ‖x‖1] ≥ 1− δ (5)\\nThus, the map f has distortion O\\n(\\nlog(1/δ)\\nδ\\n)\\nwith probability 1− δ. However, unlike the deter-\\nministic case, the randomized guarantees (4) and (5) are not sufficient for the reduction between\\nANN data structures (if δ ≫ 1/n). This is because the lower bound on ‖f(x)‖∞ must apply si-\\nmultaneously to all “far” points. In order to obtain a desired reduction, we need to use slightly\\ndifferent parameters. Specifically, for 0 < ε < 1 one has:\\nPrf\\n[\\n‖f(x)‖∞ ≥ Ω\\n(‖x‖1\\nlog n\\n)]\\n≥ 1− 1\\n10n\\nand\\nPrf\\n[\\n‖f(x)‖∞ ≤ O\\n( ‖x‖1\\nε · log n\\n)]\\n≥ n−ε.\\nThis allows us to reduce the (c/ε, r)-ANN problem over (Rd, ℓ1) to nO(ε) instances of the (c, r′)-ANN\\nproblem over (Rd, ℓ∞). Indeed, we sample nO(ε) i.i.d. maps fi as described above and solve the ANN\\nproblem over ℓ∞ on the image of fi. Far points remain being far with probability 1− 1/10n each.\\nUsing the linearity of expectation and the Markov inequality, we observe that, with probability at\\nleast 0.9, no far point come close enough to the query point. At the same time, with probability at\\nleast n−ε, the near neighbor does not move too far away, so, with high probability, at least one of\\nthe nO(ε) data structures succeeds. This reduction is quite similar to the use of Locality-Sensitive\\nHashing in Section 2.2.\\nAs a result, we get an ANN data structure for (Rd, ℓ1) with approximation O\\n(\\nlog log d\\nε2\\n)\\n, query\\ntime O(dnε) and space O(dn1+ε). This is worse than the best ANN data structure for ℓ1 based on\\n(data-dependent) space partitions. However, the technique we used is very versatile and generalizes\\neasily to many other distances. The ℓ1 embedding was first used in [AIK09]. Later, it was general-\\nized [And09] to ℓp spaces for p ≥ 1. To get such an embedding, one can divide every coordinate by\\nthe (1/p)-th power of an exponential random variable. Finally, in [ANN+17b] the same technique\\nhas been shown to work for Orlicz norms and top-k norms, which we define next.\\nDefinition 6.7. Let ψ : [0;+∞)→ [0;+∞) be a non-negative monotone increasing convex function\\nwith ψ(0) = 0. Then, an Orlicz norm ‖ · ‖ψ over Rd is given by its unit ball Kψ, defined as follows:\\nKψ =\\n{\\nx ∈ Rd\\n∣∣∣∣∣\\nd∑\\ni=1\\nψ(|xi|) ≤ 1\\n}\\n.\\nClearly, ℓp norm for p <∞ is Orlicz for ψ(t) = tp.\\nDefinition 6.8. For 1 ≤ k ≤ d, we define the top-k norm of a vector from Rd as the sum of k\\nlargest absolute values of the coordinates.\\nThe top-1 norm is simply ℓ∞, while top-d corresponds to ℓ1.\\nTo embed an Orlicz norm ‖ · ‖ψ into ℓ∞, we divide the coordinates using a random variable X\\nwith the c.d.f. FX(t) = Pr[X ≤ t] = 1 − e−ψ(t). To embed the top-k norm, we use a truncated\\nexponential distribution. All of the above embeddings introduce only a constant distortion.\\nLet us note that for the ℓp norms one can achieve approximation 2O(p) [NR06, BG15], which is\\nan improvement upon the above O(log log d) bound if p is sufficiently small.\\n18\\n6.2 ANN for direct sums\\nIn this section we describe a vast generalization of the ANN data structure for ℓ∞ from Section 4.7.\\nNamely, we will be able to handle direct sums of metric spaces.\\nDefinition 6.9. Let M1 = (X1,D1), M2 = (X2,D2), . . . , Mk = (Xk,Dk) be metric spaces and let\\n‖ · ‖ be a norm over Rk. Then the ‖ · ‖-direct sum of M1, M2, . . . , Mk denoted by\\n(⊕k\\ni=1Mi\\n)\\n‖·‖\\nis a metric space defined as follows. The ground set is the Cartesian product X1 ×X2 × . . .×Xk.\\nThe distance function D is given by the following formula.\\nD ((x1, x2, . . . , xk), (y1, y2, . . . , yk)) = ‖(D1(x1, y1),D2(x2, y2), . . . ,Dk(xk, yk))‖ .\\nIt turns out that for many interesting norms ‖ · ‖ the following holds. If for metrics M1, M2,\\n. . . , Mk there exist efficient ANN data structures, then the same holds for\\n(⊕k\\ni=1Mi\\n)\\n‖·‖\\n(with a\\nmild loss in the parameters).\\nThe first result of this kind was shown in [Ind02]11 for the case of ℓ∞-direct sums. In what\\nfollows we denote by d the “complexity” of each metric Mi. That is, we assume it takes O(d) time\\nto compute the distance between two points, and that a point requires O(d) space to store.\\nTheorem 6.10. Let c > 1, r > 0 and 0 < ε < 1. Suppose that each Mi admits a (c, r)-ANN\\ndata structure for n-point sets with space n1+ρ (in addition to storing the dataset) for some ρ ≥ 0\\nand query time Q(n). Then, there exists a data structure for (c′, r)-ANN over\\n(⊕k\\ni=1Mi\\n)\\n∞\\n, where\\nc′ = O\\n(\\nc log logn\\nε\\n)\\n, the space is O(n1+ρ+ε) (in addition to storing the dataset), and the query time\\nis Q(n) · logO(1) n+O(dk log n).\\nInformally speaking, compared to data structures forMi, the data structure for (\\n⊕\\niMi)∞ loses\\nlog logn\\nε in approximation, n\\nε in space, and logO(1) n in query time.\\nLater, the result of [Ind02] was significantly extended [Ind04, AIK09, And09, ANN+17b], to\\nsupport ‖ · ‖-direct sums where ‖ · ‖ is an ℓp norm, an Orlicz norm, or a top-k norm. The main\\ninsight is that we can use the randomized embeddings of various norms into ℓ∞ developed in\\nSection 6.1.2, to reduce the case of ‖ · ‖-direct sums to the case of ℓ∞-direct sums. Indeed, we\\ndescribed how to reduce the ANN problem over several classes of norms to nε instances of ANN\\nover the ℓ∞ distance at a cost of losing O(1/ε) in the approximation. It is not hard to see that the\\nexact same approach can be used to reduce the ANN problem over\\n(⊕k\\ni=1Mi\\n)\\n‖·‖ to n\\nε instances\\nof ANN over\\n(⊕k\\ni=1Mi\\n)\\n∞ also at a cost of losing O(1/ε) in approximation.\\n11In [Ind02], a slightly weaker version of Theorem 6.10 has been stated. First, it assumed deterministic data\\nstructures for the spaces Mi. This is straightforward to address by boosting the probability of success for data\\nstructures for Mi using repetition. Second, the resulting space bound [Ind02] was worse. An improvement to the\\nspace bound has been described in Appendix A of the arXiv version of [ANN+17b]. Finally, the paper [Ind02] assumes\\nANN data structures for Mi with a slightly stronger guarantee. Namely, each point is assigned a priority from 1 to\\nn, and if the near neighbor has priority t, we must return a point with priority at most t. It is not hard to solve the\\nversion with priorities using a standard ANN data structure (with logO(1) n loss in space and query time). A naïve\\nreduction builds an ANN data structure for points with priority at most t for every t. Then, we can run a binary\\nsearch over the resulting priority. However, this gives a linear in n loss in space. To rectify this, we use a standard\\ndata structure technique: the decomposition of an interval into O(log n) dyadic intervals, i.e., intervals of the form\\n[2k · l + 1; 2k · (l + 1)] for integer k, l.. Thus, we build an ANN data structure for every dyadic interval of priorities.\\nThis still gives O(n) ANN data structures, however, each data point participates in at most O(log n) of them.\\n19\\n6.3 Embeddings into direct sums\\nAs Section 6.2 shows, for a large class of norms ‖ · ‖, we can get an efficient ANN data structure\\nfor any ‖ · ‖-direct sum of metrics that admit efficient ANN data structures. This gives a natural\\napproach to the ANN problem: embed a metric of interest into such a direct sum.\\nThis approach has been successful in several settings. In [Ind02], the Fréchet distance between\\ntwo sequences of points in a metric space is embedded into an ℓ∞-direct sums of Fréchet distances\\nbetween shorter sequences. Together with Theorem 6.10, this was used to obtain an ANN data\\nstructure for the Fréchet distance. In [AIK09], it is shown how to embed the Ulam metric (which\\nis the edit distance between permutations of length d) into\\n(⊕d (⊕O(log d)(Rd, ℓ1))\\nℓ∞\\n)\\nℓ22\\nwith a\\nconstant distortion which gives an ANN data structure with doubly-logarithmic approximation. At\\nthe same time, the Ulam distance requires distortion Ω\\n(\\nlog d\\nlog log d\\n)\\nto embed into ℓ1 [AK10]. This\\nshows that (lower-dimensional) direct sums form a strictly more “powerful” class of spaces than ℓ1\\nor ℓ2. Finally, in [ANN+17b], it is shown that any symmetric norm over Rd is embeddable into(⊕dO(1)\\ni=1\\n(⊕d\\nj=1Xij\\n)\\n1\\n)\\n∞\\nwith constant distortion, where Xij is Rd equipped with the top-j norm.\\nTogether with the results from Section 6.1.2 and Section 6.2, this gives an ANN algorithm with\\napproximation (log log n)O(1) for general symmetric12 norms.\\n6.4 ANN for general norms\\nFor general d-dimensional norms, the best known ANN data structure is obtained by combining\\nTheorem 6.5 with an efficient ANN data structure for ℓ2 (for example, the one given by The-\\norem 4.1). This approach gives approximation O(\\n√\\nd/ε) for space dO(1) · n1+ε and query time\\ndO(1) · nε for every constant 0 < ε < 1. Very recently, the approximation O(√d/ε) has been\\nimproved to O\\n(\\nlog d\\nε2\\n)\\n[ANN+17a] for the same space and time bounds if one is willing to relax\\nthe model of computation to the cell-probe model, where the query procedure is charged for mem-\\nory accesses, but any computation is free. This ANN data structure heavily builds on a recent\\ngeometric result from [Nao17]: a bi-Lipschitz embedding (see Definition 6.1) of the shortest-path\\nmetric of any N -node expander graph [HLW06] into an arbitrary d-dimensional normed space must\\nhave distortion at least Ω (logdN). At a very high level, this non-embeddability result is used to\\nclaim that any large bounded-degree graph, which does embed into a normed space, can not be\\nan expander, and hence it must have a sparse cut. The existence of the sparse cut is then used,\\nvia a duality argument, to build a (data-dependent) random space partition family for a general\\nd-dimensional normed space. The latter family is used to obtain the final data structure.\\nThis approach can be further extended for several norms of interest to obtain proper, time-\\nefficient ANN data structures, with even better approximations. For instance, [ANN+17a] show\\nhow to get ANN with approximation O(p) for the ℓp norms, improving upon the bound 2O(p)\\nfrom [NR06, BG15]. Finally, for the Schatten-p norms of matrices, defined as the ℓp norm of\\nthe vector of singular values, one obtains approximation O(p) as well, while the previous best\\napproximation was polynomial in the matrix size (by relating the Schatten-p norm to the Frobenius\\nnorm).\\nAcknowledgements The authors would like to thank Assaf Naor, Tal Wagner, Erik Waingarten\\nand Fan Wei for many helpful comments. This research was supported by NSF and Simons Foun-\\ndation.\\n12Under permutations and negations of the coordinates.\\n20\\nReferences\\n[AC09] Nir Ailon and Bernard Chazelle. The fast Johnson–Lindenstrauss transform and ap-\\nproximate nearest neighbors. SIAM J. Comput., 39(1):302–322, 2009.\\n[ACP08] Alexandr Andoni, Dorian Croitoru, and Mihai Patrascu. Hardness of nearest neighbor\\nunder l-infinity. In Proceedings of the 49th Annual IEEE Symposium on Foundations\\nof Computer Science, pages 424–433. IEEE, 2008.\\n[ACW16] Josh Alman, Timothy M Chan, and Ryan Williams. Polynomial representations of\\nthreshold functions and algorithmic applications. In Proceedings of the 57th Annual\\nIEEE Symposium on Foundations of Computer Science, pages 467–476. IEEE, 2016.\\n[Ahl17] Thomas Dybdahl Ahle. Optimal las vegas locality sensitive data structures. In Pro-\\nceedings of the 58th Annual IEEE Symposium on Foundations of Computer Science.\\nIEEE, 2017.\\n[AI06] Alexandr Andoni and Piotr Indyk. Near-optimal hashing algorithms for approximate\\nnearest neighbor in high dimensions. In Proceedings of the 47th Annual IEEE Sympo-\\nsium on Foundations of Computer Science, pages 459–468. IEEE, 2006.\\n[AI08] Alexandr Andoni and Piotr Indyk. Near-optimal hashing algorithms for approximate\\nnearest neighbor in high dimensions. Communications of the ACM, 51(1):117, 2008.\\n[AIK09] Alexandr Andoni, Piotr Indyk, and Robert Krauthgamer. Overcoming the ℓ1 non-\\nembeddability barrier: algorithms for product metrics. In Proceedings of the twentieth\\nAnnual ACM-SIAM Symposium on Discrete Algorithms, pages 865–874. SIAM, 2009.\\n[AINR14] Alexandr Andoni, Piotr Indyk, Huy L Nguyên, and Ilya Razenshteyn. Beyond locality-\\nsensitive hashing. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium\\non Discrete Algorithms, pages 1018–1028. SIAM, 2014.\\n[AK10] Alexandr Andoni and Robert Krauthgamer. The computational hardness of estimating\\nedit distance. SIAM Journal on Computing, 39(6):2398–2429, 2010.\\n[AL13] Nir Ailon and Edo Liberty. An almost optimal unrestricted fast johnson-lindenstrauss\\ntransform. ACM Transactions on Algorithms, 9(3):21, 2013.\\n[ALRW17] Alexandr Andoni, Thijs Laarhoven, Ilya Razenshteyn, and Erik Waingarten. Optimal\\nhashing-based time-space trade-offs for approximate near neighbors. In Proceedings\\nof the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages\\n47–66. SIAM, 2017.\\n[AM93] Sunil Arya and David M Mount. Approximate nearest neighbor queries in fixed di-\\nmensions. In Proceedings of the Fourth Annual ACM-SIAM Symposium on Discrete\\nAlgorithms, volume 93, pages 271–280, 1993.\\n[AMN+98] Sunil Arya, David M Mount, Nathan S Netanyahu, Ruth Silverman, and Angela YWu.\\nAn optimal algorithm for approximate nearest neighbor searching fixed dimensions.\\nJournal of the ACM (JACM), 45(6):891–923, 1998.\\n[And09] Alexandr Andoni. Nearest neighbor search: the old, the new, and the impossible. PhD\\nthesis, Massachusetts Institute of Technology, 2009.\\n21\\n[ANN+17a] Alexandr Andoni, Assaf Naor, Aleksandar Nikolov, Ilya Razenshteyn, and Erik Wain-\\ngarten. Data-dependent hashing via nonlinear spectral gaps. Manuscript, 2017.\\n[ANN+17b] Alexandr Andoni, Huy L Nguyên, Aleksandar Nikolov, Ilya Razenshteyn, and Erik\\nWaingarten. Approximate near neighbors for general symmetric norms. In Proceedings\\nof the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 902–\\n913. ACM, 2017.\\n[APRS16] Thomas D Ahle, Rasmus Pagh, Ilya Razenshteyn, and Francesco Silvestri. On the\\ncomplexity of maximum inner product search. In Proc. 8th 35th ACM Symposium on\\nPrinciples of Database Systems (PODS), 2016.\\n[AR15] Alexandr Andoni and Ilya Razenshteyn. Optimal data-dependent hashing for approx-\\nimate near neighbors. In Proceedings of the forty-seventh annual ACM symposium on\\nTheory of computing, pages 793–801. ACM, 2015.\\n[AR16] Alexandr Andoni and Ilya P. Razenshteyn. Tight lower bounds for data-dependent\\nlocality-sensitive hashing. In Proceedings of the 32nd International Symposium on\\nComputational Geometry, SoCG 2016, June 14-18, 2016, Boston, MA, USA, pages\\n9:1–9:11, 2016.\\n[Bal97] Keith Ball. An elementary introduction to modern convex geometry. Flavors of ge-\\nometry, 31:1–58, 1997.\\n[BC05] Bo Brinkman and Moses Charikar. On the impossibility of dimension reduction in ℓ1.\\nJournal of the ACM (JACM), 52(5):766–788, 2005.\\n[Ber93] Marshall Bern. Approximate closest-point queries in high dimensions. Information\\nProcessing Letters, 45(2):95–99, 1993.\\n[BG15] Yair Bartal and Lee-Ad Gottlieb. Approximate nearest neighbor search for ℓp-spaces\\n(2 < p <∞) via embeddings. Available as arXiv:1512.01775, 2015.\\n[BGMZ97] Andrei Z Broder, Steven C Glassman, Mark S Manasse, and Geoffrey Zweig. Syntactic\\nclustering of the web. Computer Networks and ISDN Systems, 29(8-13):1157–1166,\\n1997.\\n[Bou85] Jean Bourgain. On lipschitz embedding of finite metric spaces in hilbert space. Israel\\nJournal of Mathematics, 52(1):46–52, 1985.\\n[Bro97] Andrei Z Broder. On the resemblance and containment of documents. In Compression\\nand Complexity of Sequences 1997. Proceedings, pages 21–29. IEEE, 1997.\\n[Car11] Constantin Carathéodory. Über den variabilitätsbereich der fourierÕschen konstanten\\nvon positiven harmonischen funktionen. Rendiconti Del Circolo Matematico di Palermo\\n(1884-1940), 32(1):193–217, 1911.\\n[Cha98] Timothy M Chan. Approximate nearest neighbor queries revisited. Discrete & Com-\\nputational Geometry, 20(3):359–373, 1998.\\n[Cha02] Moses S Charikar. Similarity estimation techniques from rounding algorithms. In\\nProceedings of the thirty-fourth annual ACM symposium on Theory of computing, pages\\n380–388. ACM, 2002.\\n22\\n[Cla88] Kenneth L Clarkson. A randomized algorithm for closest-point queries. SIAM Journal\\non Computing, 17(4):830–847, 1988.\\n[Cla94] Kenneth L Clarkson. An algorithm for approximate closest-point queries. In Proceed-\\nings of the tenth annual symposium on Computational geometry, pages 160–164. ACM,\\n1994.\\n[Cla06] Kenneth L Clarkson. Nearest-neighbor searching and metric space dimensions.\\nNearest-neighbor methods for learning and vision: theory and practice, pages 15–59,\\n2006.\\n[CLRS01] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. In-\\ntroduction to Algorithms. MIT Press, 2nd edition, 2001.\\n[Cop82] Don Coppersmith. Rapid multiplication of rectangular matrices. SIAM Journal on\\nComputing, 11(3):467–471, 1982.\\n[DIIM04] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. Locality-sensitive\\nhashing scheme based on p-stable distributions. In Proceedings of the twentieth annual\\nsymposium on Computational geometry, pages 253–262. ACM, 2004.\\n[DKS10] Anirban Dasgupta, Ravi Kumar, and Tamás Sarlós. A sparse johnson: Lindenstrauss\\ntransform. In Proceedings of the forty-second ACM symposium on Theory of computing,\\npages 341–350. ACM, 2010.\\n[FCI99] Martin Farach-Colton and Piotr Indyk. Approximate nearest neighbor algorithms for\\nhausdorff metrics via embeddings. In Proceedings of the 40th Annual IEEE Symposium\\non Foundations of Computer Science, pages 171–179. IEEE, 1999.\\n[FK09] Charles Fefferman and Bo’az Klartag. Fitting a Cm-smooth function to data I. Annals\\nof Mathematics, pages 315–346, 2009.\\n[FLM77] Tadeusz Figiel, Joram Lindenstrauss, and Vitali D Milman. The dimension of almost\\nspherical sections of convex bodies. Acta Mathematica, 139(1):53–94, 1977.\\n[Fré06] M Maurice Fréchet. Sur quelques points du calcul fonctionnel. Rendiconti del Circolo\\nMatematico di Palermo (1884-1940), 22(1):1–72, 1906.\\n[GRS14] Venkatesan Guruswami, Atri Rudra, and Madhu Sudan. Essential coding theory. Draft\\navailable at http://www. cse. buffalo. edu/atri/courses/coding-theory/book/index. html,\\n2014.\\n[GUV09] Venkatesan Guruswami, Christopher Umans, and Salil Vadhan. Unbalanced expanders\\nand randomness extractors from parvaresh–vardy codes. Journal of the ACM (JACM),\\n56(4):20, 2009.\\n[HLW06] Shlomo Hoory, Nathan Linial, and Avi Wigderson. Expander graphs and their appli-\\ncations. Bulletin of the American Mathematical Society, 43(4):439–561, 2006.\\n[HPIM12] Sariel Har-Peled, Piotr Indyk, and Rajeev Motwani. Approximate nearest neighbor:\\nTowards removing the curse of dimensionality. Theory of computing, 8(1):321–350,\\n2012.\\n23\\n[IM98] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing\\nthe curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on\\nTheory of computing, pages 604–613. ACM, 1998.\\n[Ind00a] Piotr Indyk. Dimensionality reduction techniques for proximity problems. In Pro-\\nceedings of the eleventh annual ACM-SIAM symposium on Discrete algorithms, pages\\n371–378. SIAM, 2000.\\n[Ind00b] Piotr Indyk. High-dimensional computational geometry. PhD thesis, stanford univer-\\nsity, 2000.\\n[Ind01] Piotr Indyk. On approximate nearest neighbors under ℓ∞ norm. Journal of Computer\\nand System Sciences, 63(4):627–638, 2001.\\n[Ind02] Piotr Indyk. Approximate nearest neighbor algorithms for Fréchet distance via prod-\\nuct metrics. In Proceedings of the eighteenth annual symposium on Computational\\ngeometry, pages 102–106. ACM, 2002.\\n[Ind04] Piotr Indyk. Approximate nearest neighbor under edit distance via product metrics.\\nIn Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms,\\npages 646–650. SIAM, 2004.\\n[IT03] Piotr Indyk and Nitin Thaper. Fast image retrieval via embeddings. Workshop on\\nStatistical and Computational Theories of Vision, 2003.\\n[JL84] William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a\\nhilbert space. Contemporary mathematics, 26(189-206):1, 1984.\\n[JN09] William B Johnson and Assaf Naor. The johnson-lindenstrauss lemma almost charac-\\nterizes hilbert space, but not quite. In Proceedings of the twentieth Annual ACM-SIAM\\nSymposium on Discrete Algorithms, pages 885–891. SIAM, 2009.\\n[Joh48] Fritz John. Extremum problems with inequalities as subsidiary conditions. In Studies\\nand Essays Presented to R. Courant on his 60th Birthday, January 8, 1948, pages\\n187–204. Interscience Publishers, Inc., New York, N. Y., 1948.\\n[JS82] William B Johnson and Gideon Schechtman. Embedding ℓmp into ℓ\\nn\\n1 . Acta Mathemat-\\nica, 149(1):71–85, 1982.\\n[JW13] Thathachar S Jayram and David P Woodruff. Optimal bounds for johnson-\\nlindenstrauss transforms and streaming problems with subconstant error. ACM Trans-\\nactions on Algorithms (TALG), 9(3):26, 2013.\\n[Kap15] Michael Kapralov. Smooth tradeoffs between insert and query complexity in nearest\\nneighbor search. In Proceedings of the 34th ACM SIGMOD-SIGACT-SIGAI Sympo-\\nsium on Principles of Database Systems, pages 329–342. ACM, 2015.\\n[KKK16] Matti Karppa, Petteri Kaski, and Jukka Kohonen. A faster subquadratic algorithm for\\nfinding outlier correlations. In Proceedings of the Twenty-Seventh Annual ACM-SIAM\\nSymposium on Discrete Algorithms, pages 1288–1305. SIAM, 2016.\\n[Kle97] Jon M Kleinberg. Two algorithms for nearest-neighbor search in high dimensions.\\nIn Proceedings of the twenty-ninth annual ACM symposium on Theory of computing,\\npages 599–608. ACM, 1997.\\n24\\n[KN14] Daniel M. Kane and Jelani Nelson. Sparser johnson-lindenstrauss transforms. J. ACM,\\n61(1):4, 2014.\\n[KOR00] Eyal Kushilevitz, Rafail Ostrovsky, and Yuval Rabani. Efficient search for approximate\\nnearest neighbor in high dimensional spaces. SIAM Journal on Computing, 30(2):457–\\n474, 2000.\\n[KP12] Michael Kapralov and Rina Panigrahy. Nns lower bounds via metric expansion for lâĹđ\\nand emd. In International Colloquium on Automata, Languages, and Programming,\\npages 545–556. Springer, 2012.\\n[Kur35] Casimir Kuratowski. Quelques problèmes concernant les espaces métriques non-\\nséparables. Fundamenta Mathematicae, 25(1):534–545, 1935.\\n[KW11] Felix Krahmer and Rachel Ward. New and improved johnson-lindenstrauss embeddings\\nvia the restricted isometry property. SIAM J. Math. Analysis, 43(3):1269–1281, 2011.\\n[LLR95] Nathan Linial, Eran London, and Yuri Rabinovich. The geometry of graphs and some\\nof its algorithmic applications. Combinatorica, 15(2):215–245, 1995.\\n[LT80] Richard J Lipton and Robert E Tarjan. Applications of a planar separator theorem.\\nSIAM Journal on Computing, 9(3):615–627, 1980.\\n[Mah14] Sepideh Mahabadi. Approximate nearest line search in high dimensions. In Proceedings\\nof the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, pages\\n337–354. SIAM, 2014.\\n[Mat97] Jiří Matoušek. On embedding expanders into ℓp spaces. Israel Journal of Mathematics,\\n102(1):189–197, 1997.\\n[Mei93] Stefan Meiser. Point location in arrangements of hyperplanes. Information and Com-\\nputation, 106(2):286–303, 1993.\\n[Mil99] Peter Bro Miltersen. Cell probe complexity-a survey. In Proceedings of the 19th Con-\\nference on the Foundations of Software Technology and Theoretical Computer Science,\\nAdvances in Data Structures Workshop, page 2, 1999.\\n[MNP07] Rajeev Motwani, Assaf Naor, and Rina Panigrahy. Lower bounds on locality sensitive\\nhashing. SIAM Journal on Discrete Mathematics, 21(4):930–935, 2007.\\n[MP69] Marvin Minsky and Seymour A Papert. Perceptrons: An introduction to computational\\ngeometry. MIT press, 1969.\\n[Nao17] Assaf Naor. A spectral gap precludes low-dimensional embeddings. In Proceedings of\\nthe 33rd International Symposium on Computational Geometry (SoCG ’2017), 2017.\\n[Ngu14] Huy L. Nguyên. Algorithms for High Dimensional Data. PhD thesis, Princeton Univer-\\nsity, 2014. Available at http://arks.princeton.edu/ark:/88435/dsp01b8515q61f.\\n[NPW14] Jelani Nelson, Eric Price, and Mary Wootters. New constructions of RIP matrices\\nwith fast multiplication and fewer rows. In Proceedings of the Twenty-Fifth Annual\\nACM-SIAM Symposium on Discrete Algorithms, SODA 2014, Portland, Oregon, USA,\\nJanuary 5-7, 2014, pages 1515–1528, 2014.\\n25\\n[NR06] Assaf Naor and Yuval Rabani. On approximate nearest neighbor search in ℓp, p > 2.\\nManuscript, 2006.\\n[NS07] Assaf Naor and Gideon Schechtman. Planar earthmover is not in l1. SIAM Journal\\non Computing, 37(3):804–826, 2007.\\n[NSS95] Moni Naor, Leonard J Schulman, and Aravind Srinivasan. Splitters and near-optimal\\nderandomization. In Proceedings of the 36th Annual IEEE Symposium on Foundations\\nof Computer Science, pages 182–191. IEEE, 1995.\\n[OR07] Rafail Ostrovsky and Yuval Rabani. Low distortion embeddings for edit distance.\\nJournal of the ACM (JACM), 54(5):23, 2007.\\n[OWZ14] Ryan O’Donnell, Yi Wu, and Yuan Zhou. Optimal lower bounds for locality-sensitive\\nhashing (except when q is tiny). ACM Transactions on Computation Theory (TOCT),\\n6(1):5, 2014.\\n[Pag16] Rasmus Pagh. Locality-sensitive hashing without false negatives. In Proceedings of the\\nTwenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1–9.\\nSIAM, 2016.\\n[Pan06] Rina Panigrahy. Entropy based nearest neighbor search in high dimensions. In Proceed-\\nings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm, pages\\n1186–1195. SIAM, 2006.\\n[PS85] Franco P Preparata and Michael Ian Shamos. Introduction. In Computational Geom-\\netry, pages 1–35. Springer, 1985.\\n[Raz17] Ilya Razenshteyn. High-Dimensional Similarity Search and Sketching: Algorithms and\\nHardness. PhD thesis, Massachusetts Institute of Technology, 2017.\\n[Sch37] Isaac J Schoenberg. On certain metric spaces arising from euclidean spaces by a change\\nof metric and their imbedding in hilbert space. Annals of mathematics, pages 787–793,\\n1937.\\n[Sch42] I. J. Schoenberg. Positive definite functions on spheres. Duke Math. J., 9(1):96–108,\\n03 1942.\\n[SDI06] Gregory Shakhnarovich, Trevor Darrell, and Piotr Indyk. Nearest-Neighbor Methods\\nin Learning and Vision: Theory and Practice (Neural Information Processing). The\\nMIT press, 2006.\\n[Str69] Volker Strassen. Gaussian elimination is not optimal. Numerische mathematik,\\n13(4):354–356, 1969.\\n[STS+13] Narayanan Sundaram, Aizana Turmukhametova, Nadathur Satish, Todd Mostak, Pi-\\notr Indyk, Samuel Madden, and Pradeep Dubey. Streaming similarity search over\\none billion tweets using parallel locality-sensitive hashing. Proceedings of the VLDB\\nEndowment, 6(14):1930–1941, 2013.\\n[Val15] Gregory Valiant. Finding correlations in subquadratic time, with applications to learn-\\ning parities and the closest pair problem. Journal of the ACM (JACM), 62(2):13, 2015.\\n26\\n[Wil05] Ryan Williams. A new algorithm for optimal 2-constraint satisfaction and its implica-\\ntions. Theoretical Computer Science, 348(2-3):357–365, 2005.\\n[Wil12] Virginia Vassilevska Williams. Multiplying matrices faster than coppersmith-winograd.\\nIn Proceedings of the forty-fourth annual ACM symposium on Theory of computing,\\npages 887–898. ACM, 2012.\\n[Wil18] V. Williams. On some fine-grained questions in algorithms and complexity. These\\nproceedings, 2018.\\n[WLKC16] Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. Learning to hash for indexing\\nbig data: a survey. Proceedings of the IEEE, 104(1):34–57, 2016.\\n[Woj96] Przemyslaw Wojtaszczyk. Banach spaces for analysts, volume 25. Cambridge Univer-\\nsity Press, 1996.\\n[WSSJ14] Jingdong Wang, Heng Tao Shen, Jingkuan Song, and Jianqiu Ji. Hashing for similarity\\nsearch: A survey. arXiv preprint arXiv:1408.2927, 2014.\\n27\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd0d'), 'authors': 'Kane, Daniel M., Nelson, Jelani', 'year': '2012', 'title': 'Sparser Johnson-Lindenstrauss Transforms', 'full_text': 'ar\\nX\\niv\\n:1\\n01\\n2.\\n15\\n77\\nv6\\n  [\\ncs\\n.D\\nS]\\n  5\\n Fe\\nb 2\\n01\\n4\\nSparser Johnson-Lindenstrauss Transforms\\nDaniel M. Kane\\n∗\\nJelani Nelson\\n†\\nAbstract\\nWe give two different and simple constructions for dimensionality reduction in ℓ2 via linear\\nmappings that are sparse: only an O(ε)-fraction of entries in each column of our embedding\\nmatrices are non-zero to achieve distortion 1 + ε with high probability, while still achieving the\\nasymptotically optimal number of rows. These are the first constructions to provide subconstant\\nsparsity for all values of parameters, improving upon previous works of Achlioptas (JCSS 2003)\\nand Dasgupta, Kumar, and Sarlo´s (STOC 2010). Such distributions can be used to speed up\\napplications where ℓ2 dimensionality reduction is used.\\n1 Introduction\\nThe Johnson-Lindenstrauss lemma states:\\nLemma 1 (JL Lemma [21]). For any integer d > 0, and any 0 < ε, δ < 1/2, there exists a\\nprobability distribution on k × d real matrices for k = Θ(ε−2 log(1/δ)) such that for any x ∈ Rd,\\nP\\nS\\n((1− ε)‖x‖2 ≤ ‖Sx‖2 ≤ (1 + ε)‖x‖2) > 1− δ.\\nProofs of the JL lemma can be found in [1, 6, 7, 13, 14, 17, 21, 23, 28]. The value of k in the\\nJL lemma is optimal [20] (also see a later proof in [22]).\\nThe JL lemma is a key ingredient in the JL flattening theorem, which states that any n points\\nin Euclidean space can be embedded into O(ε−2 log n) dimensions so that all pairwise Euclidean\\ndistances are preserved up to 1 ± ε. The JL lemma is a useful tool for speeding up solutions to\\nseveral high-dimensional problems: closest pair, nearest neighbor, diameter, minimum spanning\\ntree, etc. It also speeds up some clustering and string processing algorithms, and can further be\\nused to reduce the amount of storage required to store a dataset, e.g. in streaming algorithms.\\nRecently it has also found applications in approximate numerical algebra problems such as linear\\nregression and low-rank approximation [10, 34]. See [19, 36] for further discussions on applications.\\nStandard proofs of the JL lemma take a distribution over dense matrices (e.g. i.i.d. Gaussian or\\nBernoulli entries), and thus performing the embedding na¨ıvely takes O(k · ‖x‖0) time where x has\\n‖x‖0 non-zero entries. Several works have devised other distributions which give faster embedding\\ntimes [2, 3, 4, 18, 27, 38], but all these methods require Ω(d log d) embedding time even for sparse\\n∗Stanford University, Department of Mathematics. dankane@math.stanford.edu. This work was done while the\\nauthor was supported by an NSF Graduate Research Fellowship.\\n†Harvard University, School of Engineering and Applied Sciences. minilek@seas.harvard.edu. This work was\\ndone while the author was supported by a Xerox-MIT Fellowship, and in part by the Center for Massive Data\\nAlgorithmics (MADALGO) - a center of the Danish National Research Foundation.\\n1\\nvectors (even when ‖x‖0 = 1). This feature is particularly unfortunate in streaming applications,\\nwhere a vector x receives coordinate-wise updates of the form x ← x + v · ei, so that to maintain\\nsome linear embedding Sx of x we should repeatedly calculate Sei during updates. Since ‖ei‖0 = 1,\\neven the na¨ıve O(k · ‖ei‖0) embedding time method is faster than these approaches.\\nEven aside from streaming applications, several practical situations give rise to vectors with\\n‖x‖0 ≪ d. For example, a common similarity measure for comparing text documents in data\\nmining and information retrieval is cosine similarity [33], which is approximately preserved under\\nany JL embedding. Here, a document is represented as a bag of words with the dimensionality\\nd being the size of the lexicon, and we usually would not expect any single document to contain\\nanywhere near d distinct words (i.e., we expect sparse vectors). In networking applications, if xi,j\\ncounts bytes sent from source i to destination j in some time interval, then d is the total number\\nof IP pairs, whereas we would not expect most pairs of IPs to communicate with each other. In\\nlinear algebra applications, a rating matrix A may for example have Ai,j as user i’s score for item\\nj (e.g. the Netflix matrix where columns correspond to movies), and we would expect that most\\nusers rate only small fraction of all available items.\\nOne way to speed up embedding time in the JL lemma for sparse vectors is to devise a distribu-\\ntion over sparse embedding matrices. This was first investigated in [1], which gave a JL distribution\\nwhere only one third of the entries of each matrix in its support was non-zero, without increasing\\nthe number of rows k from dense constructions. Later, the works [9, 35] gave a distribution over\\nmatrices with only O(log(1/δ)) non-zero entries per column, but the algorithm for estimating ‖x‖2\\ngiven the linear sketch then relied on a median calculation, and thus these schemes did not provide\\nan embedding into ℓ2. In several applications, such as nearest-neighbor search [17] and approximate\\nnumerical linear algebra [10, 34], an embedding into a normed space or even ℓ2 itself is required,\\nand thus median estimators cannot be used. Median-based estimators also pose a problem when\\none wants to learn classifiers in the dimension-reduced space via stochastic gradient descent, since\\nin this case the estimator needs certain differentiability properties [39]. In fact, the work of [39]\\ninvestigated JL distributions over sparse matrices for this reason, in the context of collaborative\\nspam filtering. The work [12] later analyzed the JL distribution in [39] and showed that it can\\nbe realized where for each matrix in the support of the distribution, each column has at most\\ns = O˜(ε−1 log3(1/δ))1 non-zero entries, thus speeding up the embedding time to O(s · ‖x‖0). This\\n“DKS construction” requires O(ds log k) bits of random seed to sample a matrix from their distri-\\nbution. The work of [12] left open two main directions: (1) understand the sparsity parameter s\\nthat can be achieved in a JL distribution, and (2) devise a sparse JL transform distribution which\\nrequires few random bits to sample from, for streaming applications where storing a long random\\nseed requires prohibitively large memory.\\nThe previous work [23] of the current authors made progress on both these questions by showing\\nO˜(ε−1 log2(1/δ)) sparsity was achievable by giving an alternative analysis of the scheme of [12]\\nwhich also only required O(log(1/(εδ)) log d) seed length. The work of [7] later gave a tighter\\nanalysis under the assumption ε < 1/ log2(1/δ), improving the sparsity and seed length further by\\nlog(1/ε) and log log(1/δ) factors in this case. In Section 5 we show that the DKS scheme requires\\ns = Ω˜(ε−1 log2(1/δ)), and thus a departure from their construction is required to obtain better\\nsparsity. For a discussion of other previous work concerning the JL lemma see [23].\\n1We say g = Ω˜(f) when g = Ω(f/polylog(f)), g = O˜(f) when g = O(f ·polylog(f)), and g = Θ˜(f) when g = Ω˜(f)\\nand g = O˜(f) simultaneously.\\n2\\n(a) (b)\\nk/s\\n(c)\\nFigure 1: In all three constructions above, a vector in Rd is projected down to Rk. Figure (a) is\\nthe DKS construction in [12], and the two constructions we give in this work are represented in (b)\\nand (c). The out-degree in each case is s, the sparsity.\\nMain Contribution: In this work, we give two new constructions which achieve sparsity s =\\nΘ(ε−1 log(1/δ)) for ℓ2 embedding into optimal dimension k = Θ(ε\\n−2 log(1/δ)). This is the first\\nsparsity bound which is always o(k) for the asymptotically optimal value of k for all ranges of ε, δ.\\nOne of our distributions can be sampled from using O(log(1/δ) log d) uniform random bits.\\nIt is also worth nothing that after the preliminary version of this work was published in [24],\\nit was shown in [32] that our bound is optimal up to an O(log(1/ε)) factor. That is, for any\\nfixed constant c > 0, any distribution satisfying Lemma 1 that is supported on matrices with k =\\nO(ε−c log(1/δ)) and at most s non-zero entries per column must have s = Ω(ε−1 log(1/δ)/ log(1/ε))\\nas long as k = O(d/ log(1/ε)). Note that once k ≥ d one can always take the distribution supported\\nsolely on the d× d identity matrix, giving s = 1 and satisfying Lemma 1 with ε = 0.\\nWe also describe variations on our constructions which achieve sparsity O˜(ε−1 log(1/δ)), but\\nwhich have much simpler analyses. We describe our simpler constructions in Section 3, and our\\nbetter constructions in Section 4. We show in Section 5 that our analyses of the required sparsity\\nin our schemes are tight up to a constant factor. In Section 6 we discuss how our new schemes\\nspeed up the numerical linear algebra algorithms in [10] for approximate linear regression and best\\nrank-k approximation in the streaming model of computation. We also show in Section 6 that a\\nwide range of JL distributions automatically provides sketches for approximate matrix product as\\ndefined in [34]. While [34] also showed this, it lost a logarithmic factor in the target dimension\\ndue to a union bound in its reduction; the work of [10] avoided this loss, but only for the JL\\ndistribution of random sign matrices. We show a simple and general reduction which incurs no loss\\nin parameters. Plugging in our sparse JL transform then yields faster linear algebra algorithms\\nusing the same space. In Section 7 we state two open problems for future work.\\n1.1 Our Approach\\nOur constructions are depicted in Figure 1. Figure 1(a) represents the DKS construction of [12]\\nin which each item is hashed to s random target coordinates with replacement. Our two schemes\\nachieving s = Θ(ε−1 log(1/δ)) are as follows. Construction (b) is much like (a) except that we hash\\ncoordinates s times without replacement; we call this the graph construction, since hash locations\\n3\\nare specified by a bipartite graph with d left vertices, k right vertices, and left-degree s. In (c), the\\ntarget vector is divided into s contiguous blocks each of equal size k/s, and a given coordinate in the\\noriginal vector is hashed to a random location in each block (essentially this is the CountSketch\\nof [9], though we use a higher degree of independence in our hash functions); we call this the block\\nconstruction. In all cases (a), (b), and (c), we randomly flip the sign of a coordinate in the original\\nvector and divide by\\n√\\ns before adding it in any location in the target vector.\\nWe give two different analyses for both our constructions (b) and (c). Since we consider linear\\nembeddings, without loss of generality we can assume ‖x‖2 = 1, in which case the JL lemma follows\\nby showing that ‖Sx‖22 ∈ [(1 − ε)2, (1 + ε)2], which is implied by |‖Sx‖22 − 1| ≤ 2ε − ε2. Thus it\\nsuffices to show that for any unit norm x,\\nP\\nS\\n(|‖Sx‖22 − 1| > 2ε− ε2) < δ. (1)\\nWe furthermore observe that both our graph and block constructions have the property that the\\nentries of our embedding matrix S can be written as\\nSi,j = ηi,jσi,j/\\n√\\ns, (2)\\nwhere the σi,j are independent and uniform in {−1, 1}, and ηi,j is an indicator random variable\\nfor the event Si,j 6= 0 (in fact in our analyses we will only need that the σi,j are O(log(1/δ))-wise\\nindependent). Note that the ηi,j are not independent, since in both constructions we have that\\nthere are exactly s non-zero entries per column. Furthermore in the block construction, knowing\\nthat ηi,j = 1 for j in some block implies that ηi,j′ = 0 for all other j\\n′ in the same block.\\nTo outline our analyses, look at the random variable\\nZ\\ndef\\n= ‖Sx‖22 − 1 =\\n1\\ns\\n·\\nk∑\\nr=1\\n∑\\ni 6=j∈[d]\\nηr,iηr,jσr,iσr,jxixj. (3)\\nOur proofs all use Markov’s bound on the ℓth moment Zℓ to give P(|Z| > 2ε−ε2) < (2ε−ε2)−ℓ ·EZℓ\\nfor ℓ = log(1/δ) an even integer. The task is then to bound EZℓ. In our first approach, we observe\\nthat Z is a quadratic form in the σi,j of Eq. (2), and thus its moments can be bounded via the\\nHanson-Wright inequality [16]. This analysis turns out to reveal that the hashing to coordinates\\nin the target vector need not be done randomly, but can in fact be specified by any sufficiently\\ngood code (i.e. the ηi,j need not be random). Specifically, it suffices that for any j 6= j′ ∈ [d],∑k\\ni=1 ηi,jηi,j′ = O(s\\n2/k). That is, no two columns have their non-zero entries in more than O(s2/k)\\nof the same rows. In (b), this translates to the columns of the embedding matrix (ignoring the\\nrandom signs and division by\\n√\\ns) to be codewords in a constant-weight binary code of weight s\\nand minimum distance 2s−O(s2/k). In (c), if for each j ∈ [d] we let Cj be a length-s vector with\\nentries in [k/s] specifying where coordinate j is mapped to in each block, it suffices for {Cj}dj=1 to\\nbe a code of minimum distance s−O(s2/k). It is fairly easy to see that if one wants a deterministic\\nhash function, it is necessary for the columns of the embedding matrix to be specified by a code:\\nif two coordinates have their non-zeroes in many of the same rows, it means those coordinates\\ncollide often. Since collision is the source of error, an adversary in this case could ask to embed a\\nvector which has its mass equally spread on these two coordinates, causing large error with large\\nprobability over the choice of random signs. What our analysis shows is that not only is a good\\ncode necessary, but it is also sufficient.\\n4\\nIn our second analysis approach, we define\\nZr =\\n∑\\ni 6=j∈[d]\\nηr,iηr,jσr,iσr,jxixj . (4)\\nso that\\nZ =\\n1\\ns\\nk∑\\nr=1\\nZr. (5)\\nWe show that to bound EZℓ it suffices to bound EZtr for each r ∈ [k], t ∈ [ℓ]. To bound EZtr,\\nwe expand expand Ztr to obtain a polynomial with roughly d\\n2t terms. We view its monomials\\nas being in correspondence with graphs, group monomials that map to the same graph, then do\\nsome combinatorics to make the expectation calculation feasible. We remark that a similar tactic\\nof mapping monomials to graphs then carrying out combinatorial arguments is frequently used to\\nanalyze the eigenvalue spectrum of random matrices; see for example work of Wigner [40], or the\\nwork of Fu¨redi and Komlo´s [15]. In our approach here, we assume that the random signs as well\\nas the hashing to coordinates in the target vector are done O(log(1/δ))-wise independently. This\\ncombinatorial approach of mapping to graphs played a large role in our previous analysis of the\\nDKS construction [23], as well as a later analysis of that construction in [7].\\nWe point out here that Figure 1(c) is somewhat simpler to implement, since there are simple\\nconstructions of O(log(1/δ))-wise hash families [8]. Figure 1(b) on the other hand requires hashing\\nwithout replacement, which amounts to using random permutations and can be derandomized using\\nalmost O(log(1/δ))-wise independent permutation families [26] (see Remark 14).\\n2 Conventions and Notation\\nDefinition 2. For A ∈ Rn×n, the Frobenius norm of A is ‖A‖F =\\n√∑\\ni,j A\\n2\\ni,j.\\nDefinition 3. For A ∈ Rn×n, the operator norm of A is ‖A‖2 = sup‖x‖2=1 ‖Ax‖2. In the case A\\nis symmetric, this is also the largest magnitude of an eigenvalue of A.\\nHenceforth, all logarithms are base-2 unless explicitly stated otherwise. For a positive integer\\nn we use [n] to denote the set {1, . . . , n}. We will always be focused on embedding a vector x ∈ Rd\\ninto Rk, and we assume ‖x‖2 = 1 without loss of generality (since our embeddings are linear).\\nAll vectors v are assumed to be column vectors, and vT denotes its transpose. We often implicitly\\nassume that various quantities, such as 1/δ, are powers of 2 or 4, which is without loss of generality.\\nSpace complexity bounds (as in Section 6), are always measured in bits.\\n3 Code-Based Constructions\\nIn this section, we provide analyses of our constructions (b) and (c) in Figure 1 when the non-zero\\nentry locations are deterministic but satisfy a certain condition. In particular, in the analysis in\\nthis section we assume that for any i 6= j ∈ [d],\\nk∑\\nr=1\\nηr,iηr,j = O(s\\n2/k). (6)\\n5\\nThat is, no two columns have their non-zero entries in more than O(s2/k) of the same rows. We\\nshow how to use error-correcting codes to ensure Eq. (6) in Remark 8 for the block construction,\\nand in Remark 9 for the graph construction. Unfortunately this step will require setting s to be\\nslightly larger than the desired O(ε−1 log(1/δ)). We give an alternate analysis in Section 4 which\\navoids assuming Eq. (6) and obtains an improved bound for s by not using deterministic ηr,i.\\nIn what follows, we assume k = C · ε−2 log(1/δ) for a sufficiently large constant C, and that s\\nis some integer dividing k satisfying s ≥ 2(2ε − ε2)−1 log(1/δ) = Θ(ε−1 log(1/δ)). We also assume\\nthat the σi,j are 2ℓ-wise independent for ℓ = log(1/δ), so that E(‖Sx‖22 − 1)ℓ is fully determined.\\nAnalysis of Figure 1(b) and Figure 1(c) code-based constructions: Recall from Eq. (3)\\nZ\\ndef\\n= ‖Sx‖22 − 1 =\\n1\\ns\\nk∑\\nr=1\\n∑\\ni 6=j∈[d]\\nηr,iηr,jσr,iσr,jxixj.\\nNote Z is a quadratic form in σ which can be written as σTTσ for a kd× kd block-diagonal matrix\\nT . There are k blocks, each d × d, where in the rth block Tr we have (Tr)i,j = ηr,iηr,jxixj/s for\\ni 6= j and (Tr)i,i = 0 for all i. Now, P(|Z| > 2ε − ε2) = P(|σTTσ| > 2ε − ε2). To obtain an upper\\nbound for this probability, we use the Hanson-Wright inequality combined with a Markov bound.\\nTheorem 4 (Hanson-Wright inequality [16]). Let z = (z1, . . . , zn) be a vector of i.i.d. Rademacher\\n±1 random variables. For any symmetric B ∈ Rn×n and ℓ ≥ 2,\\nE\\n∣∣zTBz − trace(B)∣∣ℓ ≤ Cℓ ·max{√ℓ · ‖B‖F , ℓ · ‖B‖2}ℓ\\nfor some universal constant C > 0 independent of B,n, ℓ.\\nWe prove our construction satisfies the JL lemma by applying Theorem 4 with z = σ,B = T .\\nLemma 5. ‖T‖2F = O(1/k).\\nProof.\\n‖T‖2F =\\n1\\ns2\\n·\\n∑\\ni 6=j∈[d]\\nx2ix\\n2\\nj ·\\n(\\nk∑\\nr=1\\nηr,iηr,j\\n)\\n≤ O(1/k) ·\\n∑\\ni 6=j∈[d]\\nx2ix\\n2\\nj ≤ O(1/k) · ‖x‖42 = O(1/k),\\nwhere the first inequality used Eq. (6). \\x04\\nLemma 6. ‖T‖2 ≤ 1/s.\\nProof. Since T is block-diagonal, its eigenvalues are the eigenvalues of each block. For a block Tr,\\nwrite Tr = (1/s) · (Sr −Dr). Dr is diagonal with (Dr)i,i = ηr,ix2i , and (Sr)i,j = ηr,iηr,jxixj . Since\\nSr and Dr are both positive semidefinite, we have ‖T‖2 ≤ (1/s) · max{‖Sr‖2, ‖Dr‖2}. We have\\n‖Dr‖2 ≤ ‖x‖2∞ ≤ 1. Define u ∈ Rd by ui = ηr,ixi so Sr = uuT . Thus ‖Sr‖2 = ‖u‖22 ≤ ‖x‖22 = 1. \\x04\\nBy Eq. (1), it now suffices to prove the following theorem.\\nTheorem 7. Pσ(|Z| > 2ε− ε2) < δ.\\n6\\nProof. By a Markov bound applied to Zℓ for ℓ an even integer,\\nP\\nσ\\n(|Z| > 2ε− ε2) < (2ε − ε2)−ℓ · E\\nσ\\nZℓ.\\nSince Z = σTTσ and trace(T ) = 0, applying Theorem 4 with B = T , z = σ, and ℓ = log(1/δ) gives\\nP\\nσ\\n(|Z| > ε) < Cℓ ·max\\n{\\nO(ε−1) ·\\n√\\nℓ\\nk\\n, (2ε − ε2)−1 ℓ\\ns\\n}ℓ\\n. (7)\\nsince the ℓth moment is determined by 2 log(1/δ)-wise independence of σ. We conclude the proof\\nby noting that the expression in Eq. (7) is at most δ for our choices for s, k, ℓ. \\x04\\nWe now discuss how to choose the non-zero locations in S to ensure Eq. (6).\\nRemark 8. Consider the block construction, and for i ∈ [d] let Ci ∈ [k/s]s specify the locations\\nof the non-zero entries for column i of S in each of the s blocks. Then Eq. (6) is equivalent to\\nC = {C1, . . . , Cd} being an error-correcting code with relative distance 1 − O(s/k), i.e. that no\\nCi, Cj pair for i 6= j agree in more than O(s2/k) coordinates. It is thus important to know whether\\nsuch a code exists. Let h : [d]× [s]→ [k/s] be such that h(i, r) gives the non-zero location in block\\nr for column i, i.e. (Ci)r = h(i, r). Note that having relative distance 1−O(s/k) is to say that for\\nevery i 6= j ∈ [d], h(i, r) = h(j, r) for at most O(s2/k) values of r. For r ∈ [s] let Xr be an indicator\\nrandom variable for the event h(i, r) = h(j, r), and define X =\\n∑s\\nr=1Xr. Then EX = s\\n2/k, and\\nif s2/k = Ω(log(d/δ)), then a Chernoff bound shows that X = O(s2/k) with probability at least\\n1− δ/d2 over the choice of h (in fact it suffices to use Markov’s bound applied to the O(log(d/δ))th\\nmoment implied by the Chernoff bound so that h can be O(log(d/δ))-wise independent, but we do\\nnot dwell on this issue here since Section 4 obtains better parameters). Thus by a union bound over\\nall\\n(d\\n2\\n)\\npairs i 6= j, C is a code with the desired properties with probability at least 1−δ/2. Note that\\nthe condition s2/k = Ω(log(d/δ)) is equivalent to s = Ω(ε−1\\n√\\nlog(d/δ) log(1/δ)). We also point\\nout that we may assume without loss of generality that d = O(ε−2/δ). This is because there exists\\nan embedding into this dimension with sparsity 1 using only 4-wise independence with distortion\\n(1 + ε) and success probability 1 − δ/2 [9, 35]. It is worth noting that in the construction in this\\nsection, potentially h could be deterministic given an explicit code with our desired parameters.\\nRemark 9. It is also possible to use a code to specify the hash locations in the graph construction.\\nIn particular, let the jth entry of the ith column of the embedding matrix be the jth symbol of the\\nith codeword (which we call h(i, j)) in a weight-s binary code of minimum distance 2s−O(s2/k) for\\ns ≥ 2ε−1 log(1/δ). Define ηi,j,r for i, j ∈ [d], r ∈ [s] as an indicator variable for h(i, r) = h(j, r) = 1.\\nThen, the error is again exactly as in Eq. (3). Also, as in Remark 8, such a code can be shown to\\nexist via the probabilistic method (the Chernoff bound can be applied using negative dependence,\\nfollowed by a union bound) as long as s = Ω(ε−1\\n√\\nlog(d/δ) log(1/δ)). We omit the details since\\nSection 4 obtains better parameters.\\nRemark 10. Only using Eq. (6), it is impossible to improve our sparsity bound further. For\\nexample, consider an instantiation of the block construction in which Eq. (6) is satisfied. Create\\na new set of ηr,i which change only in the case r = 1 so that η1,i = 1 for all i, so that Eq. (6)\\nstill holds. In our construction this corresponds to all indices colliding in the first chunk of k/s\\ncoordinates, which creates an error term of (1/s) ·∑i 6=j xixjσr,iσr,j. Now, suppose x consists of\\n7\\nt = (1/2) · log(1/δ) entries each with value 1/√t. Then, with probability √δ ≫ δ, all these entries\\nreceive the same sign under σ and contribute a total error of Ω(t/s) in the first chunk alone. We\\nthus need t/s = O(ε), which implies s = Ω(ε−1 log(1/δ)).\\n4 Random Hashing Constructions\\nIn this section, we show that if the hash functions h described in Remark 8 and Remark 9 are\\nnot specified by fixed codes, but rather are chosen at random from some family of sufficiently high\\nindependence, then one can achieve sparsity O(ε−1 log(1/δ)) (in the case of Figure 1(b), we actually\\nneed almost k-wise independent permutations). Recall our bottleneck in reducing the sparsity in\\nSection 3 was actually obtaining the codes, discussed in Remark 8 and Remark 9.\\nWe perform our analysis by bounding the ℓth moment of Z = ‖Sx‖22−1 from first principles for\\nℓ = Θ(log(1/δ)) an even integer (for this particular scheme, it seems the Hanson-Wright inequality\\ndoes not simplify any details of the proof). To show Eq. (1) we then use Markov’s inequality to say\\nP(|Z| > λ) < λ−ℓ ·EZℓ. Although the ηi,j are specified differently in the two constructions, in both\\ncases they are easily seen to be negatively correlated; that is, for any subset T ⊆ [k] × [d] (in fact\\nin our proof we will only be concerned with |T | ≤ ℓ) we have E∏(i,j)∈T ηi,j ≤ (s/k)|T |. Also, each\\nconstruction has\\n∑k\\ni=1 ηi,j = s with probability 1 for all j ∈ [d], and thus, recalling the definition\\nof Zr from Eq. (4),\\nZ =\\n1\\ns\\n·\\nk∑\\nr=1\\n∑\\ni 6=j∈[d]\\nxixjσr,iσr,jηr,iηr,j =\\n1\\ns\\n·\\nk∑\\nr=1\\nZr.\\nWe first bound the tth moment of each Zr for 1 ≤ t ≤ ℓ. As in the Frobenius norm moment\\nbound of [23], and also used later in [7], the main idea is to construct a correspondence between\\nthe monomials appearing in Ztr and certain graphs. Notice\\nZtr =\\n∑\\ni1,...,it,j1,...,jt∈[d]\\ni1 6=j1,...,it 6=jt\\nt∏\\nu=1\\nηr,iuηr,juxiuxjuσr,iuσr,ju. (8)\\nTo each monomial above we associate a directed multigraph with labeled edges whose vertices\\ncorrespond to the distinct iu and ju. An xiuxju term corresponds to a directed edge with label u\\nfrom the vertex corresponding to iu to the vertex corresponding to ju. The basic idea we use to\\nbound EZtr is to group these monomials based on their associated graphs.\\nLemma 11. For t > 1 an integer, Eη,σ Z\\nt\\nr ≤ t(2e2)t ·\\n{\\n(s/k)2 t < 2 ln(k/s)\\n(t/ ln(k/s))t otherwise\\n.\\nProof. We have\\nE\\nη,σ\\nZtr =\\n∑\\ni1,...,it,j1,...,jt∈[d]\\ni1 6=j1,...,it 6=jt\\n(\\nt∏\\nu=1\\nxiuxju\\n)\\n·\\n(\\nE\\nσ\\nt∏\\nu=1\\nσr,iuσr,ju\\n)\\n·\\n(\\nE\\nη\\nt∏\\nu=1\\nηr,iuηr,ju\\n)\\n. (9)\\nDefine Gt as the set of directed multigraphs with t edges having distinct labels in [t] and no\\nself-loops, with between 2 and t vertices (inclusive), and where every vertex has non-zero and even\\n8\\n45\\n1\\n2\\n3\\n6\\n7\\n4\\n5\\n1\\n2\\n3\\n6\\n7\\n5 3\\n2\\n41\\nFigure 2: Example of a graph in Gt on the left with v = 5, t = 7 and j1 = j5, i1 = j4, i4 = i5, j2 =\\ni3 = i6 = i7, i2 = j3 = j6 = j7. Example graph with the same restrictions on the right, but in G′t.\\ndegree (we use degree to denote the sum of in- and out-degrees). Let f map variable sequences to\\ntheir corresponding graph. That is, we draw a directed edge labeled u from the vertex representing\\niu to that representing ju for u = 1, . . . , t, where one vertex represents all the iu, ju which are\\nassigned the same element of [d] (see Figure 2). For a graph G, let v be its number of vertices,\\nand let du be the degree of vertex u. By construction every monomial maps to a graph with t\\nedges. Also we need only consider graphs with all even vertex degrees since a monomial whose\\ngraph has at least one vertex with odd degree will have at least one random sign σi,ru appearing\\nan odd number of times and thus have expectation zero. Then,\\nE\\nη,σ\\nZtr =\\n∑\\nG∈Gt\\n∑\\ni1 6=j1,...,it 6=jt∈[d]\\nf((iu,ju)tu=1)=G\\n(\\nt∏\\nu=1\\nxiuxju\\n)\\n· E\\nη\\nt∏\\nu=1\\nηr,iuηr,ju\\n=\\n∑\\nG∈Gt\\n∑\\ni1 6=j1,...,it 6=jt∈[d]\\nf((iu,ju)tu=1)=G\\n(\\nt∏\\nu=1\\nxiuxju\\n)\\n·\\n( s\\nk\\n)v\\n(10)\\n≤\\n∑\\nG∈Gt\\n( s\\nk\\n)v\\n· v! · 1( t\\nd1/2,...,dv/2\\n) (11)\\n=\\n∑\\nG∈G′t\\n( s\\nk\\n)v\\n· 1( t\\nd1/2,...,dv/2\\n) (12)\\n≤ (e/2)t ·\\nt∑\\nv=2\\n( s\\nk\\n)v\\n· 1\\ntt\\n·\\n\\uf8eb\\n\\uf8ed∑\\nG∈G′t\\nv∏\\nu=1\\n√\\ndu\\ndu\\n\\uf8f6\\n\\uf8f8 , (13)\\nwhere G′t is the set of all directed multigraphs as in Gt, but in which vertices are labeled as well,\\nwith distinct labels in [v] (see Figure 2; the vertex labels can be arbitrarily permuted).\\nEq. (10) used that ηr,1, . . . , ηr,d are independent for any r. For Eq. (11), note that (‖x‖22)t = 1,\\nand the coefficient of\\n∏v\\nu=1 x\\ndu\\nau in its expansion for\\n∑v\\nu=1 du = 2t is\\n(\\nt\\nd1/2,...,dv/2\\n)\\n. Meanwhile, the\\ncoefficient of this monomial when summing over all i1 6= j1, . . . , it 6= jt for a particular G ∈ Gt is\\nat most v!. For Eq. (12), we move from graphs in Gt to those in G′t, and for any G ∈ Gt there are\\nexactly v! ways to label vertices. This is because for any graph G ∈ Gt there is a canonical way\\nof labeling the vertices as 1, . . . , v since there are no isolated vertices. Namely, the vertices can\\nbe labeled in increasing order of when they are first visited by an edge when processing edges in\\n9\\norder of increasing label (if two vertices are both visited for the first time simultaneously by some\\nedge, then we can break ties consistently using the direction of the edge). Thus the vertices are all\\nidentified by this canonical labeling, implying that the v! vertex labelings all give distinct graphs\\nin G′t. Eq. (13) follows since t! ≥ tt/et and\\nv∏\\nu=1\\n(du/2)! ≤\\nv∏\\nu=1\\n2−du/2\\n√\\ndu\\ndu\\n= 2−\\n∑v\\nu=1 du/2\\nv∏\\nu=1\\n√\\ndu\\ndu\\n= 2−t\\nv∏\\nu=1\\n√\\ndu\\ndu\\n.\\nThe summation over G in Eq. (13) is over the G ∈ G′t with v vertices. Let us bound this\\nsummation for some fixed choice of vertex degrees d1, . . . , dv . For any given i, consider the set of\\nall graphs G′′i on v labeled vertices with distinct labels in [v], and with i edges with distinct labels\\nin [i] (that is, we do not require even edge degrees, and some vertices may even have degree 0). For\\na graph G ∈ G′′i , let d′u represent the degree of vertex u in G. For a1, . . . , av > 0 define the function\\nSi(a1, . . . , av) =\\n∑\\nG∈G′′\\ni\\nv∏\\nu=1\\n√\\nau\\nd′u . (14)\\nLet G′t(d1, . . . , dv) be those graphs G ∈ G′t with v vertices such that vertex u has degree du. Then\\n∑\\nG∈G′t(d1,...,dv)\\nv∏\\nu=1\\n√\\ndu\\ndu ≤ St(d1, . . . , dv)\\nsince G′t(d1, . . . , dv) ⊂ G′′t . To upper bound St(a1, . . . , av), note S0(a1, . . . , av) = 1. For i > 1, note\\nany graph in G′′i can be formed by taking a graph G ∈ G′′i−1 and adding an edge labeled i from\\nu to w for some vertices u 6= w in G. This change causes d′u, d′w to both increase by 1, whereas all\\nother degrees stay the same. Thus considering Eq. (14),\\nSi+1(a1, . . . , av)/Si(a1, . . . , av) ≤\\n\\uf8eb\\n\\uf8ed ∑\\nu 6=w∈[v]\\n√\\nau · √aw\\n\\uf8f6\\n\\uf8f8 ≤\\n(\\nv∑\\nu=1\\n√\\nau\\n)2\\n≤\\n(\\nv∑\\nu=1\\nau\\n)\\n· v,\\nwith the last inequality using Cauchy-Schwarz. Thus by induction, St(a1, . . . , av) ≤ (\\n∑v\\nu=1 au)\\nt ·vt.\\nSince\\n∑v\\nu=1 du = 2t, we have St(d1, . . . , dv) ≤ (2tv)t. We then have that the summation in Eq. (13)\\nis at most the number of choices of even d1, . . . , dv summing to 2t (there are\\n(t−1\\nv−1\\n)\\n< 2t such\\nchoices), times (2tv)t, implying\\nE\\nη,σ\\nZtr ≤ (2e)t ·\\nt∑\\nv=2\\n( s\\nk\\n)v\\n· vt.\\nBy differentiation, the quantity (s/k)vvt is maximized for v = max {2, t/ ln(k/s)} (recall v ≥ 2),\\ngiving our lemma. \\x04\\nCorollary 12. For t > 1 an integer, Eη,σ Z\\nt\\nr ≤ t(2e3)t(s/k)2tt.\\nProof. We use Lemma 11. In the case t < 2 ln(k/s) we can multiply the (s/k)2 term by tt and\\nstill obtain an upper bound, and in the case of larger t we have (t/ ln(k/s))t ≤ tt since k ≥ s. Also\\nwhen t ≥ 2 ln(k/s) we have et(s/k)2 ≥ 1, so that t(2e2)ttt ≤ t(2e3)t(s/k)2tt. \\x04\\n10\\nTheorem 13. For some s ∈ Θ(ε−1 log(1/δ)), k ∈ Θ(ε−2 log(1/δ)), we have Ph,σ(|Z| > 2ε−ε2) < δ.\\nProof. We choose ℓ an even integer to be specified later. Using Eq. (5) and EZr = 0 for all r,\\nEZℓ =\\n1\\nsℓ\\n·\\nℓ/2∑\\nq=1\\n∑\\nr1<...<rq∈[k]\\nℓ1,...,ℓq\\n∀i ℓi>1∑\\ni ℓi=ℓ\\n(\\nℓ\\nℓ1, . . . , ℓq\\n)\\n· E\\nq∏\\ni=1\\nZℓiri\\n≤ 1\\nsℓ\\n·\\nℓ/2∑\\nq=1\\n∑\\nr1<...<rq∈[k]\\nℓ1,...,ℓq\\n∀i ℓi>1∑\\ni ℓi=ℓ\\n(\\nℓ\\nℓ1, . . . , ℓq\\n)\\n·\\nq∏\\ni=1\\nEZℓiri (15)\\n≤ 1\\nsℓ\\nℓ/2∑\\nq=1\\n∑\\nr1<...<rq∈[k]\\nℓ1,...,ℓq\\n∀i ℓi>1∑\\ni ℓi=ℓ\\nℓ!∏q\\ni=1 ℓi!\\n·\\n(\\nq∏\\ni=1\\nℓi\\n)\\n· (2e3)ℓ ·\\n( s\\nk\\n)2q\\n·\\nq∏\\ni=1\\nℓℓii (16)\\n≤ 1\\nsℓ\\nℓ/2∑\\nq=1\\n∑\\nr1<...<rq∈[k]\\nℓ1,...,ℓq\\n∀i ℓi>1∑\\ni ℓi=ℓ\\ne−q · ℓ! ·\\n(\\nq∏\\ni=1\\nℓi\\n)\\n· (2e4)ℓ ·\\n( s\\nk\\n)2q\\n(17)\\n≤ 1\\nsℓ\\nℓ/2∑\\nq=1\\n∑\\nr1<...<rq∈[k]\\nℓ1,...,ℓq\\n∀i ℓi>1∑\\ni ℓi=ℓ\\ne−q · ℓ! · (4e4)ℓ ·\\n( s\\nk\\n)2q\\n(18)\\n≤\\n(\\n4e3(ℓ+ 1)\\ns\\n)ℓ\\n· (ℓ+ 1) ·\\nℓ/2∑\\nq=1\\n∑\\nr1<...<rq∈[k]\\nℓ1,...,ℓq\\n∀i ℓi>1∑\\ni ℓi=ℓ\\ne−q ·\\n( s\\nk\\n)2q\\n(19)\\n≤\\n(\\n8e3(ℓ+ 1)\\ns\\n)ℓ\\n· (ℓ+ 1) ·\\nℓ/2∑\\nq=1\\ne−q ·\\n(\\nk\\nq\\n)\\n·\\n( s\\nk\\n)2q\\n(20)\\n≤\\n(\\n8e3(ℓ+ 1)\\ns\\n)ℓ\\n· (ℓ+ 1) ·\\nℓ/2∑\\nq=1\\n(\\ns2\\nqk\\n)q\\n(21)\\nEq. (15) follows since the expansion of\\n∏\\ni Z\\nℓi\\nri into monomials contains all nonnegative terms,\\nin which the participating ηr,i terms are negatively correlated, and thus E\\n∏\\ni Z\\nℓi\\nri is term-by-term\\ndominated when expanding into a sum of monomials by the case when the ηr,i are independent.\\nEq. (16) uses Corollary 12, and Eq. (17) uses ℓi! ≥ e(ℓi/e)ℓi . Eq. (18) compares geometric and\\narithmetic means, giving\\n∏q\\ni=1 ℓi ≤ (\\n∑q\\ni=1 ℓi/q)\\nq ≤ (ℓ/q)q ≤ (ℓq) < 2ℓ. Eq. (19) bounds ℓ! ≤\\n11\\n(ℓ + 1) · ((ℓ + 1)/e)ℓ. Eq. (20) follows since there are (kq) ways to choose the ri, and there are at\\nmost 2ℓ−1 ways to choose the ℓi summing to ℓ. Taking derivatives shows that the right hand side\\nof Eq. (21) is maximized for q = max{1, s2/(ek)}, which will be bigger than 1 and less than ℓ/2 by\\nour choices of s, k, ℓ that will soon be specified. Then q = s2/(ek) gives a summand of eq ≤ eℓ/2.\\nWe choose ℓ ≥ ln(δ−1(ℓ+ 1)ℓ/2) = Θ(log(1/δ)) and s ≥ 8e4√e(ℓ+ 1)/(2ε − ε2) = Θ(ε−1 log(1/δ))\\nso that Eq. (21) is at most (2ε − ε2)ℓ · δ. Then to ensure s2/(ek) ≤ ℓ/2 we choose k = 2s2/(eℓ) =\\nΘ(ε−2 log(1/δ)). The theorem then follows by Markov’s inequality. \\x04\\nRemark 14. In order to use fewer random bits to sample from the graph construction, we\\ncan use the following implementation. We realize the distribution over S via two hash functions\\nh : [d] × [k] → {0, 1} and σ : [d] × [s]→ {−1, 1}. The function σ is drawn from from a 2 log(1/δ)-\\nwise independent family. The function h has the property that for any i, exactly s distinct r ∈ [k]\\nhave h(i, r) = 1; in particular, we pick d seeds log(1/δ)-wise independently to determine hi for\\ni = 1, . . . , d, and where each hi is drawn from a γ-almost 2 log(1/δ)-wise independent family\\nof permutations on [d] for γ = (εs/(d2k))Θ(log(1/δ)). The seed length required for any one such\\npermutation is O(log(1/δ) log d + log(1/γ)) = O(log(1/δ) log d) [26], and thus we can pick d such\\nseeds 2 log(1/δ)-wise independently using total seed length O(log2(1/δ) log d). We then let h(i, r) =\\n1 iff some j ∈ [s] has hi(j) = r. Recall that a γ-almost ℓ-wise independent family of permutations\\nfrom [d] onto itself is a family of permutations F where the image of any fixed ℓ elements in [d]\\nhas statistical distance at most γ when choosing a random f ∈ F when compared with choosing\\na uniformly random permutation f . Now, there are (kd2)ℓ monomials in the expansion of Zℓ. In\\neach such monomial, the coefficient of the E\\n∏\\nu h(iu, ru)h(ju, ru) term is at most s\\n−ℓ. In the end,\\nwe want Eh,σ Z\\nℓ < O(ε)ℓ to apply Markov’s inequality. Thus, we want (kd2/s)ℓ · γ < O(ε)ℓ.\\nRemark 15. It is worth noting that if one wants distortion 1 ± εi with probability 1 − δi si-\\nmultaneously for all i in some set S, our proof of Theorem 13 reveals that it suffices to set\\ns = C · supi∈S ε−1i log(1/δi) and k = C · supi∈S ε−2i log(1/δi).\\n5 Tightness of analyses\\nIn this section we show that sparsity Ω(ε−1 log(1/δ)) is required in Figure 1(b) and Figure 1(c),\\neven if the hash functions used are completely random. We also show that sparsity Ω˜(ε−1 log2(1/δ))\\nis required in the DKS construction (Figure 1(a)), nearly matching the upper bounds of [7, 23].\\nInterestingly, all three of our proofs of (near-)tightness of analyses for these three constructions use\\nthe same hard input vectors. In particular, if s = o(1/ε), then we show that a vector with t =\\n⌊1/(sε)⌋ entries each of value 1/√t incurs large distortion with large probability. If s = Ω(1/ε) but\\nis still not sufficiently large, we show that the vector (1/\\n√\\n2, 1/\\n√\\n2, 0, . . . , 0) incurs large distortion\\nwith large probability (in fact, for the DKS scheme one can even take the vector (1, 0, . . . , 0)).\\n5.1 Near-tightness for DKS Construction\\nThe main theorem of this section is the following.\\nTheorem 16. The DKS construction of [12] requires sparsity s = Ω(ε−1 · ⌈log2(1/δ)/ log2(1/ε)⌉)\\nto achieve distortion 1± ε with success probability 1− δ.\\n12\\nBefore proving Theorem 16, we recall the DKS construction (Figure 1(a)). First, we repli-\\ncate each coordinate s times while preserving the ℓ2 norm. That is, we produce the vector\\nx˜ = (x1, . . . , x1, x2, . . . , x2, . . . , xd, . . . , xd)/\\n√\\ns, where each xi is replicated s times. Then, pick\\na random k × ds embedding matrix A for k = Cε−2 log(1/δ) where each column has exactly one\\nnon-zero entry, in a location defined by some random function h : [ds] → [k], and where this non-\\nzero entry is ±1, determined by some random function σ : [ds]→ {−1, 1}. The value C > 0 is some\\nfixed constant. The final embedding is A applied to x˜. We are now ready to prove Theorem 16.\\nThe proof is similar to that of Theorem 19.\\nOur proof will use the following standard fact.\\nFact 17 ([30, Proposition B.3]). For all t, n ∈ R with n ≥ 1 and |t| ≤ n,\\net(1− t2/n) ≤ (1 + t/n)n ≤ et.\\nProof (of Theorem 16). First suppose s ≤ 1/(2ε). Consider a vector with t = ⌊1/(sε)⌋ non-\\nzero coordinates each of value 1/\\n√\\nt. If there is exactly one pair {i, j} that collides under h,\\nand furthermore the signs agree under σ, the ℓ2 norm squared of our embedded vector will be\\n(st− 2)/(st) + 4/(st). Since 1/(st) ≥ ε, this quantity is at least 1 + 2ε. The event of exactly one\\npair {i, j} colliding occurs with probability\\n(\\nst\\n2\\n)\\n· 1\\nk\\n·\\nst−2∏\\ni=0\\n(1− i/k) ≥ Ω\\n(\\n1\\nlog(1/δ)\\n)\\n· (1− ε/2)1/ε\\n= Ω(1/ log(1/δ)),\\nwhich is much larger than δ/2 for δ smaller than some constant. Now, given a collision, the colliding\\nitems have the same sign with probability 1/2.\\nWe next consider the case 1/(2ε) < s ≤ 4/ε. Consider the vector x = (1, 0, . . . , 0). If there\\nare exactly three pairs {i1, j1}, . . . , {i3, j3} that collide under h in three distinct target coodinates,\\nand furthermore the signs agree under σ, the ℓ2 norm squared of our embedded vector will be\\n(s− 6)/(s) + 12/(s) > 1 + 3ε/2. The event of three pairs colliding occurs with probability\\n(\\ns\\n2\\n)(\\ns− 2\\n2\\n)(\\ns− 4\\n2\\n)\\n· 1\\n3!\\n· 1\\nk3\\n·\\ns−4∏\\ni=0\\n(1− i/k) ≥ Ω\\n(\\n1\\nlog3(1/δ)\\n)\\n· (1− ε/8)4/ε\\n= Ω(1/ log3(1/δ)),\\nwhich is much larger than δ/2 for δ smaller than some constant. Now, given a collision, the colliding\\nitems have the same sign with probability 1/8.\\nWe lastly consider the case 4/ε < s ≤ 2cε−1 log2(1/δ)/ log2(1/ε) for some constant c > 0\\n(depending on C) to be determined later. First note this case only exists when δ = O(ε). Define\\nx = (1, 0, . . . , 0). Suppose there exists an integer q so that\\n1. q2/s ≥ 4ε\\n2. q/s < ε\\n3. (s/(qk))q(1− 1/k)s > δ1/3.\\n13\\nFirst we show it is possible to satisfy the above conditions simultaneously for our range of s.\\nWe set q = 2\\n√\\nεs, satisfying item 1 trivially, and item 2 since s > 4/ε. For item 3, Fact 17 gives\\n(s/(qk))q · (1− 1/k)s ≥\\n(\\ns\\nqk\\n)q\\n· e−s/k ·\\n(\\n1− s\\nk2\\n)\\n.\\nThe e−s/k · (1 − (s/k2)) term is at least δ1/6 by the settings of s, k, and the (s/(qk))q term is also\\nat least δ1/6 for c sufficiently small.\\nNow, consider the event E that exactly q of the s copies of x1 are hashed to 1 by h, and to +1\\nby σ. If E occurs, then coordinate 1 in the target vector contributes q2/s ≥ 4ε to ℓ22 in the target\\nvector by item 1 above, whereas these coordinates only contribute q/s < ε to ‖x‖22 by item 2 above,\\nthus causing error at least 3ε. Furthermore, the s− q coordinates which do not hash to 1 are being\\nhashed to a vector of length k − 1 = ω(1/ε2) with random signs, and thus these coordinates have\\ntheir ℓ22 contribution preserved up to 1± o(ε) with constant probability by Chebyshev’s inequality.\\nIt thus just remains to show that P(E)≫ δ. We have\\nP(E) =\\n(\\ns\\nq\\n)\\n· k−q ·\\n(\\n1− 1\\nk\\n)s−q\\n· 1/2q\\n≥\\n(\\ns\\nqk\\n)q\\n·\\n(\\n1− 1\\nk\\n)s\\n· 1\\n2q\\n> δ1/3 · 1\\n2q\\n.\\nThe 2−q term is ω(δ1/3) and thus overall P(E) = ω(δ2/3)≫ δ. \\x04\\n5.2 Tightness of Figure 1(b) analysis\\nTheorem 18. For δ smaller than a constant depending on C for k = Cε−2 log(1/δ), the graph\\nconstruction of Section 4 requires s = Ω(ε−1 log(1/δ)) to obtain distortion 1 ± ε with probability\\n1− δ.\\nProof. First suppose s ≤ 1/(2ε). We consider a vector with t = ⌊1/(sε)⌋ non-zero coordinates\\neach of value 1/\\n√\\nt. If there is exactly one set i, j, r with i 6= j such that Sr,i, Sr,j are both non-zero\\nfor the embedding matrix S (i.e., there is exactly one collision), then the total error is 2/(ts) ≥ 2ε.\\nIt just remains to show that this happens with probability larger than δ. The probability of this\\noccurring is\\ns2 ·\\n(\\nt\\n2\\n)\\n· 1\\nk\\n· k − s\\nk − 1 · · ·\\nk − 2s+ 2\\nk − s+ 1 ·\\n(\\n(k − 2s+ 1)!\\n(k − ts+ 1)!\\n)\\n·\\n(\\n(k − s)!\\nk!\\n)t−2\\n≥ s\\n2t2\\n2k\\n·\\n(\\nk − st\\nk\\n)st\\n≥ s\\n2t2\\n2k\\n·\\n(\\n1− s\\n2t2\\nk\\n)\\n= Ω(1/ log(1/δ)).\\nNow consider the case 1/(2ε) < s < c · ε−1 log(1/δ) for some small constant c. Consider the\\nvector (1/\\n√\\n2, 1/\\n√\\n2, 0, . . . , 0). Suppose there are exactly 2sε collisions, i.e. 2sε distinct values of\\nr such that Sr,i, Sj,r are both non-zero (to avoid tedium we disregard floors and ceilings and just\\nassume sε is an integer). Also, suppose that in each colliding row r we have σ(1, r) = σ(2, r). Then,\\n14\\nthe total error would be 2ε. It just remains to show that this happens with probability larger than\\nδ. The probability of signs agreeing in exactly 2εs chunks is 2−2εs > 2−2c log(1/δ), which is larger\\nthan\\n√\\nδ for c < 1/4. The probability of exactly 2εs collisions is\\n(\\ns\\n2εs\\n)\\n·\\n(\\n2εs−1∏\\ni=0\\ns− i\\nk − i\\n)\\n·\\n(\\ns−2εs−1∏\\ni=0\\nk − i− s\\nk − i− 2εs\\n)\\n≥\\n(\\n1\\n2ε\\n)2εs\\n·\\n(\\n(1− 2ε)s\\nk\\n)2εs\\n·\\n(\\n1− s\\nk − s\\n)s−2εs\\n≥\\n( s\\n4εk\\n)2εs\\n·\\n(\\n1− 2s\\nk\\n)s\\n. (22)\\nIt suffices for the right hand side to be at least\\n√\\nδ since h is independent of σ, and thus the\\ntotal probability of error larger than 2ε would be greater than\\n√\\nδ\\n2\\n= δ. Taking natural logarithms,\\nit suffices to have\\n2εs ln\\n(\\n4εk\\ns\\n)\\n− s ln\\n(\\n1− 2s\\nk\\n)\\n≤ ln(1/δ)/2.\\nWriting s = q/ε and a = 4C log(1/δ), the left hand side is 2q ln(a/q)+Θ(s2/k). Taking a derivative\\nshows 2q ln(a/q) is monotonically increasing for q < a/e. Thus as long as q < ca for a sufficiently\\nsmall constant c, 2q ln(a/q) < ln(1/δ)/4. Also, the Θ(s2/k) term is at most ln(1/δ)/4 for c suffi-\\nciently small. \\x04\\n5.3 Tightness of Figure 1(c) analysis\\nTheorem 19. For δ smaller than a constant depending on C for k = Cε−2 log(1/δ), the block\\nconstruction of Section 4 requires s = Ω(ε−1 log(1/δ)) to obtain distortion 1 ± ε with probability\\n1− δ.\\nProof. First suppose s ≤ 1/(2ε). Consider a vector with t = ⌊1/(sε)⌋ non-zero coordinates each\\nof value 1/\\n√\\nt. If there is exactly one set i, j, r with i 6= j such that h(i, r) = h(j, r) (i.e. exactly\\none collision), then the total error is 2/(ts) ≥ 2ε. It just remains to show that this happens with\\nprobability larger than δ.\\nThe probability of exactly one collision is\\ns ·\\n[\\nt! · (k/st )\\n(k/s)t\\n]s−1\\n·\\n(\\nt\\n2\\n)\\n·\\n(\\nk\\ns\\n)\\n·\\n[\\n(t− 2)! · (k/s−1t−2 )\\n(k/s)t\\n]\\n≥ s ·\\n(\\n1− st\\nk\\n)t(s−1)\\n·\\n(\\nt\\n2\\n)\\n·\\n( s\\nk\\n)(\\n1− st\\nk\\n)t−2\\n=\\ns2t(t− 1)\\n2k\\n·\\n(\\n1− st\\nk\\n)st−2\\n≥ s\\n2t(t− 1)\\n2k\\n·\\n(\\n1− s\\n2t2\\nk\\n)\\n= Ω(1/ log(1/δ)),\\nwhich is larger than δ for δ smaller than a universal constant.\\nNow consider 1/(2ε) < s < c · ε−1 log(1/δ) for some small constant c. Consider the vector\\nx = (1/\\n√\\n2, 1/\\n√\\n2, 0, . . . , 0). Suppose there are exactly 2sε collisions, i.e. 2sε distinct values of r\\nsuch that h(1, r) = h(2, r) (to avoid tedium we disregard floors and ceilings and just assume sε is\\nan integer). Also, suppose that in each colliding chunk r we have σ(1, r) = σ(2, r). Then, the total\\n15\\nerror would be 2ε. It just remains to show that this happens with probability larger than δ. The\\nprobability of signs agreeing in exactly 2εs chunks is 2−2εs > 2−2c log(1/δ), which is larger than\\n√\\nδ\\nfor c < 1/4. The probability of exactly 2εs collisions is(\\ns\\n2εs\\n)( s\\nk\\n)2εs (\\n1− s\\nk\\n)(1−2ε)s\\n≥\\n( s\\n2εk\\n)2εs (\\n1− s\\nk\\n)(1−2ε)s\\nThe above is at most\\n√\\nδ, by the analysis following Eq. (22). Since h is independent of σ, the\\ntotal probability of having error larger than 2ε is greater than\\n√\\nδ\\n2\\n= δ. \\x04\\n6 Faster numerical linear algebra streaming algorithms\\nThe works of [10, 34] gave algorithms to solve various approximate numerical linear algebra problems\\ngiven small memory and a only one or few passes over an input matrix. They considered models\\nwhere one only sees a row or column at a time of some matrix A ∈ Rd×n. Another update model\\nconsidered was the turnstile streaming model. In this model, the matrix A starts off as the all\\nzeroes matrix. One then sees a sequence of m updates (i1, j1, v1), . . . , (im, jm, vm), where each\\nupdate (i, j, v) triggers the change Ai,j ← Ai,j+v. The goal in all these models is to compute some\\nfunctions of A at the end of seeing all rows, columns, or turnstile updates. The algorithm should\\nuse little memory (much less than what is required to store A explicitly). Both works [10, 34]\\nsolved problems such as approximate linear regression and best rank-k approximation by reducing\\nto the problem of sketches for approximate matrix products. Before delving further, first we give\\na definition.\\nDefinition 20. Distribution D over Rk×d has (ε, δ, ℓ)-JL moments if for all x with ‖x‖2 = 1,\\nE\\nS∼D\\n∣∣‖Sx‖22 − 1∣∣ℓ ≤ εℓ · δ.\\nNow, the following theorem is a generalization of [10, Theorem 2.1]. The theorem states that any\\ndistribution with JL moments also provides a sketch for approximate matrix products. A similar\\nstatement was made in [34, Lemma 6], but that statement was slightly weaker in its parameters\\nbecause it resorted to a union bound, which we avoid by using Minkowski’s inequality.\\nTheorem 21. Given ε, δ ∈ (0, 1/2), let D be any distribution over matrices with d columns with\\nthe (ε, δ, ℓ)-JL moment property for some ℓ ≥ 2. Then for A,B any real matrices with d rows,\\nP\\nS∼D\\n(‖ATSTSB −ATB‖F > 3ε‖A‖F ‖B‖F ) < δ.\\nProof. Let x, y ∈ Rd each have ℓ2 norm 1. Then\\n〈Sx, Sy〉 = ‖Sx‖\\n2\\n2 + ‖Sy‖22 − ‖S(x− y)‖22\\n2\\n16\\nso that, defining ‖X‖p = (E |X|p)1/p (which is a norm for p ≥ 1 by Minkowski’s inequality),\\n‖〈Sx, Sy〉 − 〈x, y〉‖ℓ = 1\\n2\\n· ∥∥(‖Sx‖22 − 1) + (‖Sy‖22 − 1)− (‖S(x− y)‖22 − ‖x− y‖22)∥∥ℓ\\n≤ 1\\n2\\n· (∥∥‖Sx‖22 − 1∥∥ℓ + ∥∥‖Sy‖22 − 1∥∥ℓ + ∥∥‖S(x− y)‖22 − ‖x− y‖22∥∥ℓ)\\n≤ 1\\n2\\n·\\n(\\nε · δ1/ℓ + ε · δ1/ℓ + ‖x− y‖22 · ε · δ1/ℓ\\n)\\n≤ 3ε · δ1/ℓ\\nNow, if A has n columns and B has m columns, label the columns of A as x1, . . . , xn ∈ Rd and the\\ncolumns of B as y1, . . . , ym ∈ Rd. Define the random variable Xi,j = 1/(‖xi‖2‖yj‖2) · (〈Sxi, Syj〉 −\\n〈xi, yj〉). Then ‖ATSTSB −ATB‖2F =\\n∑n\\ni=1\\n∑m\\nj=1 ‖xi‖22 · ‖yj‖22 ·X2i,j . Then again by Minkowski’s\\ninequality since ℓ/2 ≥ 1,\\n∥∥‖ATSTSB −ATB‖2F∥∥ℓ/2 =\\n∥∥∥∥∥∥\\nn∑\\ni=1\\nm∑\\nj=1\\n‖xi‖22 · ‖yj‖22 ·X2i,j\\n∥∥∥∥∥∥\\nℓ/2\\n≤\\nn∑\\ni=1\\nm∑\\nj=1\\n‖xi‖22 · ‖yj‖22 · ‖X2i,j‖ℓ/2\\n=\\nn∑\\ni=1\\nm∑\\nj=1\\n‖xi‖22 · ‖yj‖22 · ‖Xi,j‖2ℓ\\n≤ (3εδ1/ℓ)2 ·\\n\\uf8eb\\n\\uf8ed n∑\\ni=1\\nm∑\\nj=1\\n‖xi‖22 · ‖yj‖22\\n\\uf8f6\\n\\uf8f8\\n= (3εδ1/ℓ)2 · ‖A‖2F ‖B‖2F\\nThen by Markov’s inequality and using E ‖ATSTSB −ATB‖ℓF = ‖‖ATSTSB −ATB‖2F ‖ℓ/2ℓ/2,\\nP\\n(‖ATSTSB −ATB‖F > 3ε‖A‖F ‖B‖F ) ≤\\n(\\n1\\n3ε‖A‖F ‖B‖F\\n)ℓ\\n· E ‖ATSTSB −ATB‖ℓF ≤ δ.\\n\\x04\\nRemark 22. Often when one constructs a JL distribution D over k× d matrices, it is shown that\\nfor all x with ‖x‖2 = 1 and for all ε > 0,\\nP\\nS∼D\\n(∣∣‖Sx‖22 − 1∣∣ > ε) < e−Ω(ε2k+εk).\\nAny such distribution automatically satisfies the (ε, e−Ω(ε\\n2k+εk),min{ε2k, εk})-JL moment property\\nfor any ε > 0 by converting the tail bound into a moment bound via integration by parts.\\nRemark 23. After this work there was interest in finding sparse oblivious subspace embeddings,\\ni.e. a randomized and sparse S ∈ Rk×n such that for any U ∈ Rn×d with orthonormal columns,\\n17\\nP(‖(SU)T (SU)−I‖ > ε) < δ. Here the norm is ℓ2 to ℓ2 operator norm, and thus ‖(SU)T (SU)−I‖ ≤\\nε implies that (1− ε)‖x‖22 ≤ ‖Sx‖22 ≤ (1 + ε)‖x‖22 for all x in the column span of U . It was shown\\nin [11, 29, 31] that such S exists with one non-zero entry per column and k = O(d2/(ε2δ)) rows. It\\nhas sinced been pointed out to us by Huy Leˆ Nguy˜ˆen that this result also follows from Theorem 21.\\nIndeed, [35] provides a distribution with (ε′, δ, 2)-JL moments with k = O(ε′−2δ−1) rows, and\\nsupported on matrices each with exactly one non-zero entry per column. The claim then follows\\nby applying Theorem 21 with A = B = U and ε′ = ε/(3d) by noting that ‖U‖F =\\n√\\nd and that\\noperator norm is upper bounded by Frobenius norm.\\nNow we arrive at the main point of this section. Several algorithms for approximate linear\\nregression and best rank-k approximation in [10] simply maintain SA as A is updated, where S\\ncomes from the JL distribution with Ω(log(1/δ))-wise independent ±1/√k entries. In fact though,\\ntheir analyses of their algorithms only use the fact that this distribution satisfies the approximate\\nmatrix product sketch guarantees of Theorem 21. Due to Theorem 21 though, we know that any\\ndistribution satisfying the (ε, δ)-JL moment condition gives an approximate matrix product sketch.\\nThus, random Bernoulli matrices may be replaced with our sparse JL distributions in this work. We\\nnow state some of the algorithmic results given in [10] and describe how our constructions provide\\nimprovements in the update time (the time to process new columns, rows, or turnstile updates).\\nAs in [10], when stating our results we will ignore the space and time complexities of storing\\nand evaluating the hash functions in our JL distributions. We discuss this issue later in Remark 26.\\n6.1 Linear regression\\nIn this problem we have an A ∈ Rd×n and b ∈ Rd. We would like to compute a vector x˜ such that\\n‖Ax˜− b‖F ≤ (1+ε) ·minx∗ ‖Ax∗− b‖F with probability 1− δ. In [10], it is assumed that the entries\\nof A, b require O(log(nd)) bits of precision to store precisely. Both A, b receive turnstile updates.\\nTheorem 3.2 of [10] proves that such an x˜ can be computed with probability 1 − δ from SA\\nand Sb, where S is drawn from a distribution that simultaneously satisfies both the (1/2, η−rδ)\\nand (\\n√\\nε/r, δ)-JL moment properties for some fixed constant η > 1 in their proof, and where\\nrank(A) ≤ r ≤ n. Thus due to Remark 15, we have the following.\\nTheorem 24. There is a one-pass streaming algorithm for linear regression in the turnstile model\\nwhere one maintains a sketch of size O(n2ε−1 log(1/δ) log(nd)). Processing each update requires\\nO(n+\\n√\\nn/ε · log(1/δ)) arithmetic operations and hash function evaluations.\\nTheorem 24 improves the update complexity of [10], which was O(nε−1 log(1/δ)).\\n6.2 Low rank approximation\\nIn this problem, we have an A ∈ Rd×n of rank ρ with entries that require precision O(log(nd)) to\\nstore. We would like to compute the best rank-r approximation Ar to A. We define ∆r\\ndef\\n= ‖A−Ar‖F\\nas the error of Ar. We relax the problem by only requiring that we compute a matrix A\\n′\\nr such that\\n‖A−A′r‖F ≤ (1 + ε)∆r with probability 1− δ over the randomness of the algorithm.\\nTwo-pass algorithm: Theorem 4.4 of [10] gives a 2-pass algorithm where in the first pass,\\none maintains SA where S is drawn from a distribution that simultaneously satisfies both the\\n(1/2, η−rδ) and (\\n√\\nε/r, δ)-JL moment properties for some fixed constant η > 1 in their proof. It is\\nalso assumed that ρ ≥ 2r + 1. The first pass is thus sped up again as in Theorem 24.\\n18\\nOne-pass algorithm for column/row-wise updates: Theorem 4.5 of [10] gives a one-pass\\nalgorithm in the case that A is seen either one whole column or row at a time. The algorithm\\nmaintains both SA and SAAT where S is drawn from a distribution that simultaneously satisfies\\nboth the (1/2, η−rδ) and (\\n√\\nε/r, δ)-JL moment properties. This implies the following.\\nTheorem 25. There is a one-pass streaming algorithm for approximate low rank approximation\\nwith row/column-wise updates where one maintains a sketch of size O(rε−1(n+d) log(1/δ) log(nd)).\\nProcessing each update requires O(r +\\n√\\nr/ε · log(1/δ)) amortized arithmetic operations and hash\\nfunction evaluations per entry of A.\\nTheorem 25 improves the amortized update complexity of [10], which was O(rε−1 log(1/δ)).\\nThree-pass algorithm for row-wise updates: Theorem 4.6 of [10] gives a three-pass algorithm\\nusing less space in the case that A is seen one row at a time. Again, the first pass simply maintains\\nSA where S is drawn from a distribution that satisfies both the (1/2, η−rδ) and (\\n√\\nε/r, δ)-JL\\nmoment properties. This pass is sped up using our sparser JL distribution.\\nOne-pass algorithm in the turnstile model, bi-criteria: Theorem 4.7 of [10] gives a one-pass\\nalgorithm under turnstile updates where SA and RAT are maintained in the stream. S is drawn\\nfrom a distribution satisfying both the (1/2, η−r log(1/δ)/εδ) and (ε/\\n√\\nr log(1/δ), δ)-JL moment prop-\\nerties. R is drawn from a distribution satisfying both the (1/2, η−rδ) and (\\n√\\nε/r, δ)-JL moment\\nproperties. Theorem 4.7 of [10] then shows how to compute a matrix of rank O(rε−1 log(1/δ))\\nwhich achieves the desired error guarantee given SA and RAT .\\nOne-pass algorithm in the turnstile model: Theorem 4.9 of [10] gives a one-pass algorithm\\nunder turnstile updates where SA and RAT are maintained in the stream. S is drawn from a distri-\\nbution satisfying both the (1/2, η−r log(1/δ)/ε\\n2\\nδ) and (ε\\n√\\nε/(r log(1/δ)), δ)-JL moment properties. R\\nis drawn from a distribution satisfying both the (1/2, η−rδ) and (\\n√\\nε/r, δ)-JL moment properties.\\nTheorem 4.9 of [10] then shows how to compute a matrix of rank r which achieves the desired error\\nguarantee given SA and RAT .\\nRemark 26. In the algorithms above, we counted the number of hash function evaluations that\\nmust be performed. We use our construction in Figure 1(c), which uses 2 log(1/δ)-wise independent\\nhash functions. Standard constructions of t-wise independent hash functions over universes with\\nelements fitting in a machine word require O(t) time to evaluate [8]. In our case, this would blow\\nup our update time by factors such as n or r, which could be large. Instead, we use fast multipoint\\nevaluation of polynomials. The standard construction [8] of our desired hash functions mapping\\nsome domain [z] onto itself for z a power of 2 takes a degree-(t − 1) polynomial p with random\\ncoefficients in Fz. The hash function evaluation at some point y is then the evaluation p(y) over\\nFz. Theorem 27 below states that p can be evaluated at t points in total time O˜(t). We note that\\nin the theorems above, we are always required to evaluate some t-wise independent hash function\\non many more than t points per stream update. Thus, we can group these evaluation points into\\ngroups of size t then perform fast multipoint evaluation for each group. We borrow this idea from\\n[25], which used it to give a fast algorithm for moment estimation in data streams.\\nTheorem 27 ([37, Ch. 10]). Let R be a ring, and let q ∈ R[x] be a degree-t polynomial. Then, given\\ndistinct x1, . . . , xt ∈ R, all the values q(x1), . . . , q(xt) can be computed using O(t log2 t log log t)\\noperations over R.\\n19\\n7 Open Problems\\nIn this section we state two explicit open problems. For the first, observe that our graph construction\\nis quite similar to a sparse JL construction of Achlioptas [1]. The work of [1] proposes a random\\nnormalized sign matrix where each column has an expected number s of non-zero entries, so that in\\nthe notation of this work, the ηi,j are i.i.d. Bernoulli with expectation s/k. Using this construction,\\n[1] was able to achieve s = k/3 without causing k to increase over analyses of dense constructions,\\neven by a constant factor. Meanwhile, our graph construction requires that there be exactly s non-\\nzero entries per column. This sole change was the reason we were able to obtain better asymptotic\\nbounds on the sparsity of S in this work, but in fact we conjecture an even stronger benefit than\\njust asymptotic improvement. The first open problem is to resolve the following conjecture.\\nConjecture 28. Fix a positive integer k. For x ∈ Rd, define ZAx,s as the error random variable\\n|‖Sx‖22 − ‖x‖22| when S is the sparse construction of [1] with sparsity parameter s. Let ZGx,s be\\nsimilarly defined, but when using our graph construction. Then for any x ∈ Rd and any s ∈ [k], ZAx,s\\nstochastically dominates ZGx,s. That is, for all x ∈ Rd, s ∈ [k], λ > 0, P(ZAx,s > λ) ≥ P(ZGx,s > λ).\\nA positive resolution of this conjecture would imply that not only does our graph construction\\nobtain better asymptotic performance than [1], but in fact obtains stronger performance in a very\\ndefinitive sense.\\nThe second open problem is the following. Recall that the “metric Johnson-Lindenstrauss\\nlemma” [21] states that for any n vectors in Rd, there is a linear map into Rk for k = O(ε−2 log n)\\nwhich preserves all pairwise Euclidean distances of the n vectors up to 1± ε. Lemma 1 implies this\\nmetric JL lemma by setting δ < 1/\\n(\\nn\\n2\\n)\\nthen performing a union bound over all\\n(\\nn\\n2\\n)\\npairwise difference\\nvectors. Alon showed that k = Ω(ε−2 log n/ log(1/ε)) is necessary [5]. Our work shows that metric\\nJL is also achievable where every column of the embedding matrix has at most s = O(ε−1 log n)\\nnon-zeroes, and this is also known to be tight up to an O(log(1/ε)) factor [32]. Thus, for metric\\nJL, the lower bounds for both k and s are off by O(log(1/ε)) factors. Meanwhile, for the form of\\nthe JL lemma in Lemma 1 where one wants to succeed on any fixed vector with probability 1 − δ\\n(the “distributional JL lemma”), the tight lower bound on k of Ω(ε−2 log(1/δ)) is known [20, 22].\\nThus it seems that obtaining lower bounds for distributional JL is an easier task.\\nQuestion: Can we obtain a tight lower bound of s = Ω(ε−1 log(1/δ)) for distributional JL in the\\ncase that k = O(ε−2 log(1/δ)) < d/2, thus removing the O(log(1/ε)) factor gap?\\nAcknowledgments\\nWe thank Venkat Chandar, Venkatesan Guruswami, Swastik Kopparty, and Madhu Sudan for\\nuseful discussions about error-correcting codes, David Woodruff for answering several questions\\nabout [10], Piotr Indyk and Eric Price for useful comments and discussion, and Mark Rudelson\\nand Dan Spielman for both pointing out the similarity of our proof of Lemma 11 to the types of\\narguments that are frequently used to analyze the eigenvalue spectrum of random matrices. We\\nthank Huy Leˆ Nguy˜ˆen for pointing out Remark 23. We also thank the anonymous referees for many\\nhelpful comments.\\n20\\nReferences\\n[1] Dimitris Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with bi-\\nnary coins. J. Comput. Syst. Sci., 66(4):671–687, 2003.\\n[2] Nir Ailon and Bernard Chazelle. The fast Johnson–Lindenstrauss transform and approximate\\nnearest neighbors. SIAM J. Comput., 39(1):302–322, 2009.\\n[3] Nir Ailon and Edo Liberty. Fast dimension reduction using Rademacher series on dual BCH\\ncodes. Discrete Comput. Geom., 42(4):615–630, 2009.\\n[4] Nir Ailon and Edo Liberty. An almost optimal unrestricted fast Johnson-Lindenstrauss trans-\\nform. ACM Transactions on Algorithms, 9(3):21, 2013.\\n[5] Noga Alon. Problems and results in extremal combinatorics I. Discrete Mathematics, 273(1-\\n3):31–53, 2003.\\n[6] Rosa I. Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts\\nand random projection. Machine Learning, 63(2):161–182, 2006.\\n[7] Vladimir Braverman, Rafail Ostrovsky, and Yuval Rabani. Rademacher chaos, random Eule-\\nrian graphs and the sparse Johnson-Lindenstrauss transform. CoRR, abs/1011.2590, 2010.\\n[8] J. Lawrence Carter and Mark N. Wegman. Universal classes of hash functions. J. Comput.\\nSyst. Sci., 18(2):143–154, 1979.\\n[9] Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data\\nstreams. Theor. Comput. Sci., 312(1):3–15, 2004.\\n[10] Kenneth L. Clarkson and David P. Woodruff. Numerical linear algebra in the streaming model.\\nIn Proceedings of the 41st ACM Symposium on Theory of Computing (STOC), pages 205–214,\\n2009.\\n[11] Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input\\nsparsity time. In Proceedings of the 45th ACM Symposium on Theory of Computing (STOC),\\npages 81–90, 2013.\\n[12] Anirban Dasgupta, Ravi Kumar, and Tama´s Sarlo´s. A sparse Johnson-Lindenstrauss trans-\\nform. In Proceedings of the 42nd ACM Symposium on Theory of Computing (STOC), pages\\n341–350, 2010.\\n[13] Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of Johnson and\\nLindenstrauss. Random Struct. Algorithms, 22(1):60–65, 2003.\\n[14] Peter Frankl and Hiroshi Maehara. The Johnson-Lindenstrauss lemma and the sphericity of\\nsome graphs. J. Comb. Theory. Ser. B, 44(3):355–362, 1988.\\n[15] Zolta´n Fu¨redi and Ja´nos Komlo´s. The eigenvalues of random symmetric matrices. Combina-\\ntorica, 1(3):233–241, 1981.\\n[16] David Lee Hanson and Farroll Tim Wright. A bound on tail probabilities for quadratic forms\\nin independent random variables. Ann. Math. Statist., 42(3):1079–1083, 1971.\\n21\\n[17] Sariel Har-Peled, Piotr Indyk, and Rajeev Motwani. Approximate nearest neighbor: Towards\\nremoving the curse of dimensionality. Theory of Computing, 8(1):321–350, 2012.\\n[18] Aicke Hinrichs and Jan Vyb´ıral. Johnson-Lindenstrauss lemma for circulant matrices. Random\\nStruct. Algorithms, 39(3):391–398, 2011.\\n[19] Piotr Indyk. Algorithmic applications of low-distortion geometric embeddings. In Proceedings\\nof the 42nd Annual Symposium on Foundations of Computer Science (FOCS), pages 10–33,\\n2001.\\n[20] T. S. Jayram and David P. Woodruff. Optimal bounds for Johnson-Lindenstrauss transforms\\nand streaming problems with subconstant error. ACM Transactions on Algorithms, 9(3):26,\\n2013.\\n[21] William B. Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert\\nspace. Contemporary Mathematics, 26:189–206, 1984.\\n[22] Daniel M. Kane, Raghu Meka, and Jelani Nelson. Almost optimal explicit Johnson-\\nLindenstrauss transformations. In Proceedings of the 15th International Workshop on Ran-\\ndomization and Computation (RANDOM), pages 628–639, 2011.\\n[23] Daniel M. Kane and Jelani Nelson. A derandomized sparse Johnson-Lindenstrauss transform.\\nCoRR, abs/1006.3585, 2010.\\n[24] Daniel M. Kane and Jelani Nelson. Sparser Johnson-Lindenstrauss transforms. In Proceedings\\nof the 23rd Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1195–1206,\\n2012.\\n[25] Daniel M. Kane, Jelani Nelson, Ely Porat, and David P. Woodruff. Fast moment estimation\\nin data streams in optimal space. In Proceedings of the 43rd ACM Symposium on Theory of\\nComputing (STOC), pages 745–754, 2011.\\n[26] Eyal Kaplan, Moni Naor, and Omer Reingold. Derandomized constructions of k-wise (almost)\\nindependent permutations. Algorithmica, 55(1):113–133, 2009.\\n[27] Felix Krahmer and Rachel Ward. New and improved Johnson-Lindenstrauss embeddings via\\nthe Restricted Isometry Property. SIAM J. Math. Anal., 43(3):1269–1281, 2011.\\n[28] Jir´ı Matousek. On variants of the Johnson-Lindenstrauss lemma. Random Struct. Algorithms,\\n33(2):142–156, 2008.\\n[29] Xiangrui Meng and Michael W. Mahoney. Low-distortion subspace embeddings in input-\\nsparsity time and applications to robust linear regression. In Proceedings of the 45th ACM\\nSymposium on Theory of Computing (STOC), pages 91–100, 2013.\\n[30] Rajeev Motwani and Prabakar Raghavan. Randomized Algorithms. Cambridge University\\nPress, 1995.\\n[31] Jelani Nelson and Huy L. Nguy˜ˆen. OSNAP: Faster numerical linear algebra algorithms via\\nsparser subspace embeddings. In Proceedings of the 54th Annual IEEE Symposium on Foun-\\ndations of Computer Science (FOCS), 2013.\\n22\\n[32] Jelani Nelson and Huy L. Nguy˜ˆen. Sparsity lower bounds for dimensionality reducing maps.\\nIn Proceedings of the 45th ACM Symposium on Theory of Computing (STOC), pages 101–110,\\n2013.\\n[33] Vipin Kumar Pang-Ning Tan, Michael Steinbach. Introduction to Data Mining. Addison-\\nWesley, 2005.\\n[34] Tama´s Sarlo´s. Improved approximation algorithms for large matrices via random projections.\\nIn Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science\\n(FOCS), pages 143–152, 2006.\\n[35] Mikkel Thorup and Yin Zhang. Tabulation-based 5-independent hashing with applications to\\nlinear probing and second moment estimation. SIAM J. Comput., 41(2):293–331, 2012.\\n[36] Santosh Vempala. The random projection method, volume 65 of DIMACS Series in Discrete\\nMathematics and Theoretical Computer Science. American Mathematical Society, 2004.\\n[37] Joachim von zur Gathen and Ju¨rgen Gerhard. Modern Computer Algebra. Cambridge Univer-\\nsity Press, 1999.\\n[38] Jan Vyb´ıral. A variant of the Johnson-Lindenstrauss lemma for circulant matrices. J. Funct.\\nAnal., 260(4):1096–1105, 2011.\\n[39] Kilian Q. Weinberger, Anirban Dasgupta, John Langford, Alexander J. Smola, and Josh At-\\ntenberg. Feature hashing for large scale multitask learning. In Proceedings of the 26th Annual\\nInternational Conference on Machine Learning (ICML), pages 1113–1120, 2009.\\n[40] Eugene P. Wigner. Characteristic vectors of bordered matrices with infinite dimensions. Ann.\\nMath., 62:548–564, 1955.\\n23\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd0e'), 'authors': 'Rosowski, Andreas', 'year': '2019', 'title': 'Fast Commutative Matrix Algorithm', 'full_text': 'ar\\nX\\niv\\n:1\\n90\\n4.\\n07\\n68\\n3v\\n1 \\n [c\\ns.C\\nC]\\n  1\\n6 A\\npr\\n 20\\n19\\nFast Commutative Matrix Algorithm\\nAndreas Rosowski\\nUniversity of Siegen, Germany\\nandreas.rosowski@student.uni-siegen.de\\nApril 17, 2019\\nAbstract\\nWe show that the product of an n× 3 matrix and a 3× 3 matrix over a commutative ring\\ncan be computed using 6n+3 multiplications. For two 3×3 matrices this gives us an algorithm\\nusing 21 multiplications. This is an improvement with respect to Makarov algorithm using\\n22 multiplications[10]. We generalize our result for n × 3 and 3 × 3 matrices and present\\nan algorithm for computing the product of an l × n matrix and an n × m matrix over a\\ncommutative ring for odd n using n(lm+ l +m− 1)/2 multiplications if m is odd and using\\n(n(lm+ l+m−1)+ l−1)/2 multiplications if m is even. Waksman algorithm for odd n needs\\n(n− 1)(lm+ l+m− 1)/2+ lm multiplications[16], thus in both cases less multiplications are\\nrequired by our algorithm.\\n1 Introduction\\nIn 1969 Strassen showed that the product of two n×n matrices can be computed using O(nlog2(7))\\narithmetic operations [15]. This work opened a new field of research and over the years better\\nupper bounds for the exponent of matrix multiplication were published. In 1990 Coppersmith and\\nWinograd obtained an upper bound of 2.375477 for the exponent [2]. For a long time this was the\\nbest result. Since 2010 further improvements were obtained in a series of papers [7, 8, 14, 17, 18].\\nThe best result so far was published in 2014 by Le Gall who obtained an upper bound of 2.3728639\\nfor the exponent [8].\\nIn this paper we first study the product of an n × 3 matrix A and a 3 × 3 matrix B over a\\ncommutative ring and show that we can compute the product AB using 6n + 3 multiplications.\\nThe basic idea is to improve the computation of the product of a 1× 3 vector a and a 3× 3 matrix\\nB over a commutative ring in the sense that we try to obtain as much as possible multiplications\\nthat contain only entries of the matrix B but without using more than 9 multiplications overall.\\nThe multiplications which contain only entries of the matrix B only need to be calculated once\\nand can therefore be reused in the matrix multiplication. In the special case n = 3 we obtain an\\nalgorithm using 21 multiplications which improves the best result so far from Makarov using 22\\nmultiplications [10]. Our next step is to generalize this result to the computation of the product\\nof an l × n matrix A and an n ×m matrix B over a commutative ring for odd n. We show that\\nthe product AB can be computed using n(lm + l + m − 1)/2 multiplications if m is odd and\\n(n(lm + l +m − 1) + l − 1)/2 multiplications if m is even. This improves Waksman’s algorithm\\nwhich requires (n− 1)(lm+ l +m− 1)/2 + lm multiplications for odd n [16].\\nAll algorithms we present in this paper do not make use of any additional multiplications with\\nconstants.\\n1.1 Related Work\\nIn this section we present some related work. We start with presenting some results about multi-\\nplication of two square matrices. Note that a matrix multiplication algorithm can only be applied\\n1\\nrecursively if commutativity is not used. Since Strassen showed in 1969 that the product of two\\nmatrices can be computed using O(nlog2(7)) arithmetic operations [15] and since it is shown that\\nfor 2 × 2 matrices 7 is the optimal number of multiplications [5, 19], it is interesting to study\\nn× n matrices for n ≥ 3, to obtain an even faster algorithm for matrix multiplication. For 3× 3\\nmatrices 21 multiplications would be needed to obtain an even faster algorithm than Strassen’s\\nsince log3(21) ≈ 2.7712 < 2.807 ≈ log2(7). In 1976 Laderman obtained a non-commutative 3 × 3\\nalgorithm using only 23 multiplications [9]. It is not known if there exists a non-commutative 3×3\\nalgorithm that uses 22 or less multiplications. For 5× 5 matrices the best non-commutative result\\nso far is 99 multiplications by Sedoglavic [12] which is an improvement on Makarov’s algorithm\\nfor 5× 5 matrices [11].\\nHopcroft and Musinski showed in [6] that the number of multiplications to compute the product\\nof an l×n matrix and an n×m matrix is the same number that is required to compute the product\\nof an n×m matrix and an m× l matrix and of an l ×m matrix and an m× n matrix etc. This\\nmeans if one computes an algorithm for the product of an l × m matrix and an m × n matrix\\nusing x multiplications there exists a matrix product algorithm for lnm × lnm matrices using\\nx3 multiplications overall. This algorithm for square matrices will then have an exponent of\\nloglnm(x\\n3).\\nWe present some examples of non-square matrix multiplication algorithms. In [4] Hopcroft\\nand Kerr showed that the product of a p × 2 matrix and a 2 × n matrix can be multiplied using\\n⌈(3pn +max{n, p})/2⌉ multiplications without using commutativity. In the case p = 3 = n this\\ngives an algorithm using 15 multiplications. Combined with the results of [6] this gives an algo-\\nrithm for 18×18 matrices using 153 = 3375 multiplications and an exponent of log18(3375) ≈ 2.811.\\nSmirnov obtained an algorithm for the product of a 3 × 3 matrix and a 3 × 6 matrix using 40\\nmultiplications[13]. By [6] this gives an algorithm for 54 × 54 matrices using 403 = 64000 multi-\\nplications and an exponent of log54(64000) ≈ 2.7743.\\nCariow et al. developed a high-speed parallel 3 × 3 matrix multiplier structure based on the\\ncommutative 3 × 3 matrix algorithm using 22 multiplications obtained by Makarov [1, 10]. We\\nsuppose that the structure could be improved by using our commutative 3 × 3 matrix algorithm\\nusing 21 multiplications.\\nIn [3] Drevet et al. optimized the number of required multiplications of small matrices up\\nto 30× 30 matrices. They considered non-commutative and commutative algorithms. Combined\\nwith our results for commutative rings we suppose that some results could be improved.\\n2 Matrix Product over a Commutative Ring\\nLet R denote a commutative ring throughout this Section.\\n2.1 Product of n× 3 and 3× 3 Matrices\\nConsider the vector-matrix product of an 1× 3 vector a and a 3× 3 matrix B over a commutative\\nring.\\na =\\n[\\na1 a2 a3\\n]\\nB =\\n\\uf8ee\\n\\uf8f0b11 b12 b13b21 b22 b23\\nb31 b32 b33\\n\\uf8f9\\n\\uf8fb (1)\\nIn the usual way the vector-matrix product of a and B would be computed as:\\naB =\\n[\\na1b11 + a2b21 + a3b31 a1b12 + a2b22 + a3b32 a1b13 + a2b23 + a3b33\\n]\\nBut it can also be computed by first computing these 9 products:\\n2\\nAlgorithm 1. Input: Vector a and Matrix B as in (1).\\nLet\\np1 := (a2 + b12)(a1 + b21)\\np2 := (a3 + b13)(a1 + b31)\\np3 := (a3 + b23)(a2 + b32)\\np4 := a1(b11 − b12 − b13 − a2 − a3)\\np5 := a2(b22 − b21 − b23 − a1 − a3)\\np6 := a3(b33 − b31 − b32 − a1 − a2)\\np7 := b12b21\\np8 := b13b31\\np9 := b23b32\\nOutput:\\naB =\\n[\\np4 + p1 + p2 − p7 − p8 p5 + p1 + p3 − p7 − p9 p6 + p2 + p3 − p8 − p9\\n]\\nTheorem 1. Let R be a commutative ring, let n ≥ 1, let A be an n× 3 matrix over R and let B\\nbe a 3× 3 matrix over R. Then the product AB can be computed using 6n+ 3 multiplications.\\nProof. Consider Algorithm 1. The products p7, p8 and p9 contain only entries of the matrix B.\\nOne can observe that for all n ≥ 1 the multiplications p7, p8 and p9 can be reused for the product\\nAB and therefore 3(n− 1) multiplications are saved.\\nWe give an example. In the case n = 3 we obtain an algorithm with 21 multiplications for the\\nmatrix-matrix product. This algorithm needs one multiplication less than Makarov’s [10].\\nCorollary 1. Let R be a commutative ring and let A and B be 3 × 3 matrices over R as shown\\nbelow. Then the product AB can be computed using 21 multiplications as follows:\\nA =\\n\\uf8ee\\n\\uf8f0a11 a12 a13a21 a22 a23\\na31 a32 a33\\n\\uf8f9\\n\\uf8fb B =\\n\\uf8ee\\n\\uf8f0b11 b12 b13b21 b22 b23\\nb31 b32 b33\\n\\uf8f9\\n\\uf8fb\\np1 := (a12 + b12)(a11 + b21)\\np2 := (a13 + b13)(a11 + b31)\\np3 := (a13 + b23)(a12 + b32)\\np4 := a11(b11 − b12 − b13 − a12 − a13)\\np5 := a12(b22 − b21 − b23 − a11 − a13)\\np6 := a13(b33 − b31 − b32 − a11 − a12)\\np7 := (a22 + b12)(a21 + b21)\\np8 := (a23 + b13)(a21 + b31)\\np9 := (a23 + b23)(a22 + b32)\\np10 := a21(b11 − b12 − b13 − a22 − a23)\\np11 := a22(b22 − b21 − b23 − a21 − a23)\\np12 := a23(b33 − b31 − b32 − a21 − a22)\\n3\\np13 := (a32 + b12)(a31 + b21)\\np14 := (a33 + b13)(a31 + b31)\\np15 := (a33 + b23)(a32 + b32)\\np16 := a31(b11 − b12 − b13 − a32 − a33)\\np17 := a32(b22 − b21 − b23 − a31 − a33)\\np18 := a33(b33 − b31 − b32 − a31 − a32)\\np19 := b12b21\\np20 := b13b31\\np21 := b23b32\\nHence,\\nAB =\\n\\uf8ee\\n\\uf8f0 p4 + p1 + p2 − p19 − p20 p5 + p1 + p3 − p19 − p21 p6 + p2 + p3 − p20 − p21p10 + p7 + p8 − p19 − p20 p11 + p7 + p9 − p19 − p21 p12 + p8 + p9 − p20 − p21\\np16 + p13 + p14 − p19 − p20 p17 + p13 + p15 − p19 − p21 p18 + p14 + p15 − p20 − p21\\n\\uf8f9\\n\\uf8fb\\n2.2 General Matrix Product\\nAlgorithm 1 from Section 2.1 is the basic idea of a general algorithm for the matrix-matrix product\\nof l×n and n×m matrices over a commutative ring for odd n. This general algorithm makes use\\nof Waksman algorithm [16] for even n. The algorithm we present below is split into two cases. In\\nCase 1 m is odd and in Case 2 m is even. This leads us to the following:\\nTheorem 2. Let R be a commutative ring, let n ≥ 3 be odd, l ≥ 1, m ≥ 3 and let A ∈ Rl×n,\\nB ∈ Rn×m be matrices. Then the following holds:\\n• If m is odd the product AB can be computed using n(lm+ l +m− 1)/2 multiplications.\\n• If m is even the product AB can be computed using (n(lm+l+m−1)+l−1)/2multiplications.\\nProof. Let A and B be matrices as in the Theorem. Now split A and B in submatrices in the\\nfollowing way:\\nA =\\n[\\nA1 A2\\n]\\n, with A1 ∈ R\\nl×3 and A2 ∈ R\\nl×n−3,\\nB =\\n[\\nB1\\nB2\\n]\\n, with B1 ∈ R\\n3×m and B2 ∈ R\\nn−3×m.\\nThen AB = A1B1 + A2B2. With Waksman algorithm [16] mentioned before A2B2 can be\\ncomputed using (n − 3)(lm+ l +m − 1)/2 multiplications. Let aij denote the entries of A1 and\\nlet bij denote the entries of B1 and let cij denote the entries of A1B1. The matrix A1B1 can be\\ncomputed as follows.\\nCase 1: m is odd.\\nFor i = 1, . . . , l let\\nci1 = (ai1 + b21)(ai2+ b12)+ (ai1 + b31)(ai3 + b13)+ ai1(b11− b12− b13− ai2− ai3)− b12b21− b13b31\\nci2 = (ai1 + b21)(ai2+ b12)+ (ai2 + b32)(ai3 + b23)+ ai2(b22− b21− b23− ai1− ai3)− b12b21− b23b32\\nci3 = (ai1 + b31)(ai3+ b13)+ (ai2 + b32)(ai3 + b23)+ ai3(b33− b31− b32− ai1− ai2)− b13b31− b23b32\\nand for i = 1, . . . , l and j = 4, 6, 8, . . . ,m− 1 let\\n4\\ncij = (ai1 + b21)(ai2 + b12) + (ai1 + b31)(ai3 + b13) + (ai1 + b21 − b2j)(−ai2 − b12 + b1j − b1(j+1))\\n+(ai1 + b31 − b3j)(−ai3 − b13 + b1(j+1))− b12b21 − b13b31 − (b21 − b2j)(−b12 + b1j − b1(j+1))\\n−(b31 − b3j)(−b13 + b1(j+1))\\nci(j+1) = (ai1 + b31)(ai3 + b13) + (ai2 + b32)(ai3 + b23) + (ai1 + b31 − b3j)(−ai3 − b13 + b1(j+1))\\n+(ai2 + b32 + b3j − b3(j+1))(−ai3 − b23 + b2(j+1))− b13b31 − b23b32 − (b31 − b3j)(−b13 + b1(j+1))\\n−(b32 + b3j − b3(j+1))(−b23 + b2(j+1))\\nIt can easily be seen that 6l+3+3l(m−3)/2+3(m−3)/2 = 3(lm+ l+m−1)/2 multiplications\\nare required to compute A1B1.\\nThus, AB can be computed using 3(lm + l + m − 1)/2 + (n − 3)(lm + l + m − 1)/2 =\\nn(lm+ l +m− 1)/2 multiplications.\\nCase 2: m is even.\\nFor i = 1, . . . , l let\\nci1 = (ai1 + b21)(ai2+ b12)+ (ai1 + b31)(ai3 + b13)+ ai1(b11− b12− b13− ai2− ai3)− b12b21− b13b31\\nci2 = (ai1 + b21)(ai2+ b12)+ (ai2 + b32)(ai3 + b23)+ ai2(b22− b21− b23− ai1− ai3)− b12b21− b23b32\\nci3 = (ai1 + b31)(ai3+ b13)+ (ai2 + b32)(ai3 + b23)+ ai3(b33− b31− b32− ai1− ai2)− b13b31− b23b32\\nci4 = (ai1+b21)(ai2+b12)+(ai1+b21−b24)(−ai2−b12+b14)+ai3b34−b12b21−(b21−b24)(−b12+b14)\\nand for i = 1, . . . , l and j = 5, 7, 9, . . . ,m− 1 let\\ncij = (ai1 + b21)(ai2 + b12) + (ai1 + b31)(ai3 + b13) + (ai1 + b21 − b2j)(−ai2 − b12 + b1j − b1(j+1))\\n+(ai1 + b31 − b3j)(−ai3 − b13 + b1(j+1))− b12b21 − b13b31 − (b21 − b2j)(−b12 + b1j − b1(j+1))\\n−(b31 − b3j)(−b13 + b1(j+1))\\nci(j+1) = (ai1 + b31)(ai3 + b13) + (ai2 + b32)(ai3 + b23) + (ai1 + b31 − b3j)(−ai3 − b13 + b1(j+1))\\n+(ai2 + b32 + b3j − b3(j+1))(−ai3 − b23 + b2(j+1))− b13b31 − b23b32 − (b31 − b3j)(−b13 + b1(j+1))\\n−(b32 + b3j − b3(j+1))(−b23 + b2(j+1))\\nOne can easily verify that in this case 8l+4+3l(m−4)/2+3(m−4)/2 = 2(l−1)+3(lm+m)/2\\nmultiplications are required to compute A1B1.\\nThus, AB can be computed using 2(l − 1) + 3(lm + m)/2 + (n − 3)(lm + l + m − 1)/2 =\\n(n(lm+ l +m− 1) + l − 1)/2 multiplications.\\nIn both cases less multiplications are required to compute AB than Waksman algorithm [16]\\nfor odd n requires.\\n3 Acknowledgment\\nI am grateful to Michael Figelius and Markus Lohrey for helpful comments.\\nReferences\\n[1] A. Cariow, W. Sys lo, G. Cariowa, M. Gliszczyn´ski. A rationalized structure of processing unit\\nto multiply 3 × 3 matrices. Journal Pomiary Automatyka Kontrola, Volume R. 58, Number\\n7, (2012), 677–680\\n5\\n[2] D. Coppersmith, S. Winograd. Matrix multiplication via arithmetic progressions. Journal of\\nSymbolic Computation 9, 3 (1990), 251–280\\n[3] C.-E´. Drevet, M. N. Islam, E´. Schost. Optimization techniques for small matrix multiplication.\\nTheoretical Computer Science, Volume 412, Issue 22, (2011), 2219–2236\\n[4] J. E. Hopcroft, L. R. Kerr. On minimizing the number of multiplications necessary for matrix\\nmultiplication. SIAM Journal on Applied Mathematics, Volume 20, Number 1, (1971), 30–35\\n[5] J. E. Hopcroft, L. R. Kerr. Some techniques for proving certain simple programs optimal.\\nProc. Tenth Ann. Symposium on Switching and Automata Theory, 1969, 36–45\\n[6] J. E. Hopcroft, J. Musinski. Duality applied to the complexity of matrix multiplications and\\nother bilinear forms. STOC ’73 Proceedings of the fifth annual ACM symposium on Theory\\nof computing, (1973), 73–87, New York, NY, USA, ACM Press\\n[7] A. M. Davie, A. J. Stothers. Improved bound for complexity of matrix multiplication. Pro-\\nceedings of the Royal Society of Edinburgh 143A, 2013, 351–370\\n[8] F. Le Gall. Powers of tensors and fast matrix multiplication. Proceedings of the 39th Inter-\\nnational Symposium on Symbolic and Algebraic Computation (ISSAC 2014), (2014), 296–303\\n[9] J. D. Laderman. A Non-Commutative Algorithm for Multiplying 3×3 Matrices Using 23 Mul-\\ntiplications. Bulletin of the American Mathematical Society, Volume 82, Number 1, (1976),\\n126–128\\n[10] O. M. Makarov. An algorithm for multiplication of 3 × 3 matrices. Zh. Vychisl. Mat. Mat.\\nFiz., 26:2 (1986), 293–294\\n[11] O. M. Makarov. A non-commutative algorithm for multiplying 5× 5 matrices using one hun-\\ndred multiplications. USSR Computational Mathematics and Mathematical Physics, Volume\\n27, Issue 1, (1987), 205–207\\n[12] A. Sedoglavic. A non-commutative algorithm for multiplying 5 × 5 matrices using 99 multi-\\nplications. https://www.researchgate.net/publication/318652755\\n[13] A. V. Smirnov. The bilinear complexity and practical algorithms for matrix multiplication.\\nZh. Vychisl. Mat. Mat. Fiz., Volume 53, Number 12, (2013), 1970–1984\\n[14] A. J. Stothers. On the Complexity of Matrix Multiplication. PhD thesis, University of\\nEdinburgh, 2010\\n[15] V. Strassen. Gaussian elimination is not optimal. Numerische Mathematik 13 (1969), 354–356\\n[16] A. Waksman. On Winograds algorithm for inner products. In IEEE Transactions on Com-\\nputers, C-19(1970), 360–361.\\n[17] V. V. Williams. Multiplying matrices faster than Coppersmith-Winograd. In Proceedings of\\nthe 44th ACM Symposium on Theory of Computing, 887–898, 2012\\n[18] V. V. Williams. Multiplying matrices faster than Coppersmith-Winograd. Version available\\nat http://theory.stanford.edu/~virgi/matrixmult-f.pdf, retrieved on August 03, 2018\\n[19] S. Winograd. On multiplication of 2×2 matrices. Linear Algebra and its Applications, Volume\\n4, Issue 4, (1971), 381–388\\n6\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd0f'), 'authors': 'Fischer, Manuela, Noever, Andreas', 'year': '2017', 'title': 'Tight Analysis of Randomized Greedy MIS', 'full_text': 'ar\\nX\\niv\\n:1\\n70\\n7.\\n05\\n12\\n4v\\n3 \\n [c\\ns.D\\nS]\\n  1\\n6 M\\nay\\n 20\\n19\\nTight Analysis of Parallel Randomized Greedy MIS\\nManuela Fischer\\nETH Zurich\\nmanuela.fischer@inf.ethz.ch\\nAndreas Noever\\nETH Zurich\\nanoever@inf.ethz.ch\\nAbstract\\nWe provide a tight analysis which settles the round complexity of the well-studied parallel\\nrandomized greedy MIS algorithm, thus answering the main open question of Blelloch, Fineman,\\nand Shun [SPAA’12].\\nThe parallel/distributed randomized greedyMaximal Independent Set (MIS) algorithmworks\\nas follows. An order of the vertices is chosen uniformly at random. Then, in each round, all\\nvertices that appear before their neighbors in the order are added to the independent set and\\nremoved from the graph along with their neighbors. The main question of interest is the number\\nof rounds it takes until the graph is empty. This algorithm has been studied since 1987, initiated\\nby Coppersmith, Raghavan, and Tompa [FOCS’87], and the previously best known bounds were\\nO(log n) rounds in expectation for Erdo˝s-Re´nyi random graphs by Calkin and Frieze [Random\\nStruc. & Alg. ’90] and O(log2 n) rounds with high probability for general graphs by Blelloch,\\nFineman, and Shun [SPAA’12].\\nWe prove a high probability upper bound of O(log n) on the round complexity of this algo-\\nrithm in general graphs, and that this bound is tight. This also shows that parallel randomized\\ngreedy MIS is as fast as the celebrated algorithm of Luby [STOC’85, JALG’86].\\n1 Introduction and Related Work\\nThe Maximal Independent Set (MIS) problem plays a central role in parallel and distributed com-\\nputing [Val82,Coo83], and has—due to its many applications in symmetry breaking [Lub85]—been\\nextensively studied for more than three decades [KW85,Lub85,ABI86,Gol86,GPS87,CRT87,GS89b,\\nGS89a,CF90,Lin92,BFS12,BEPS12,Gha16]. We refer to [BEPS16] for a thorough review of the\\nstate of the art.\\nOne strikingly simple algorithm addressing this problem is the parallel/distributed randomized\\ngreedy MIS algorithm, which works as follows. An order of the vertices is chosen uniformly at\\nrandom. Then, in each round, all local minima—i.e., all vertices that appear before their neighbors\\nin the order—are added to the independent set and removed from the graph along with their\\nneighbors. The main question of interest is the number of rounds it takes until the graph is empty.\\nThis algorithm is particularly easy to implement and requires only a small amount of communi-\\ncation. Indeed, a vertex only needs to inform its neighbors about its position in the random order\\nand then, in the round of its removal from the graph, about its decision (whether to join the MIS).\\nFor practical implementation-related details, we refer to [BFS12].\\nAnother nice property of this algorithm is that—once an order is fixed—it always yields the\\nso-called lexicographically first MIS, i.e., the same as the sequential greedy MIS algorithm that goes\\nthrough the vertices in this order one by one and adds a vertex to the MIS if none of its neighbors\\nhas been added in a previous step. Such determinism can be an important feature of parallel\\nalgorithms [BAAS09,BFGS12].\\nThese practical advantages are mainly owed to the fact that the same random order is used\\nthroughout. This, however, comes with the drawback of complicating the analysis significantly, due\\nto the lack of independence among different rounds. Indeed, while for Luby’s algorithm—which is\\nthe same algorithm but with regenerated random order in each iteration—the round complexity was\\nestablished at O(log n) for general n-node graphs more than 30 years ago [Lub85,ABI86], no similar\\nresult is known for parallel randomized greedy MIS. For Erdo˝s-Re´nyi random graphs, Calkin and\\nFrieze [CF90] could prove an (in expectation) upper bound of O(log n), resolving the conjecture of\\nCoppersmith, Raghavan, and Tompa [CRT87] who themselves arrived at O\\n(\\nlog2 n\\nlog logn\\n)\\n. A matching\\nlower bound of Ω(log n) was proven by Calkin, Frieze and Kucˇera [CFK92] two years later.\\nFor general graphs, Blelloch, Fineman, and Shun [BFS12] proved 5 years ago that w.h.p.1\\nO(log2 n) rounds are enough. The authors stated as one of their main open questions\\n“whether the dependence length [...] can be improved to O(log n)”,\\nthus whether the analysis of parallel greedy MIS’s round complexity can be improved to O(log n).\\n1.1 Our Results\\nWe show that the parallel greedy MIS’s round complexity is indeed in O(log n). This in particular\\nalso shows that parallel randomized greedy MIS is as fast as the celebrated algorithm of Luby,\\nconfirming a widespread belief.\\nTheorem 1.1. The parallel/distributed randomized greedy MIS algorithm terminates in O(log n)\\nrounds on any n-node graph with high probability.\\n1As standard, with high probability, abbreviated as w.h.p., indicates a probability at least 1− 1\\nnc\\n, for any desirably\\nlarge constant c ≥ 2.\\n1\\nThe result of Calkin, Frieze and Kucˇera [CFK92] proves that this is asymptotically best possible\\nand we also provide an alternative short proof of the lower bound in Appendix A. In Section 3, we\\npresent implications of Theorem 1.1 for maximal matching and (∆ + 1)-vertex-coloring as well as\\nthe correlation clustering problem.\\n1.2 Overview of Our Method\\nIt is a well-known fact that the removal of all local minima along with their neighbors from a\\ngraph for a random order in expectation leads to a constant factor decrease in the total number of\\nedges (see, e.g., [MRSDZ10] for a simple proof). When—as it is the case in Luby’s algorithm—the\\nrandom order is regenerated in every iteration, repeated application of this argument directly yields\\nan upper bound of O(log n) on the round complexity. However, if the order is kept fixed between\\nrounds, then the order of the remaining vertices is no longer uniformly distributed.\\nTo overcome this problem of dependencies among different iterations, Blelloch, Fineman, and\\nShun [BFS12]—inspired by an approach of [CRT87] and [CF90]—divide the algorithm into several\\nphases. In each phase they only expose a prefix of the remaining order and run the parallel algorithm\\non these vertices only (whilst still deleting a vertex in the suffix if it is adjacent to a vertex added to\\nthe MIS). This way, in each phase the order among the unprocessed (but possibly already deleted)\\nvertices in the suffix remains random, leading to a sequence of “independent” problems.\\nThey exploit this independence to argue that after having processed a prefix of length Ω(t·log n),\\nthe maximum remaining degree (among the not yet deleted vertices) in the suffix w.h.p. is d = n\\nt\\n.\\nThis is because in every step2 in which a vertex v has more than d neighbors, the probability that\\none of these is chosen to be exposed next (which causes the deletion of v) is at least d\\nn\\n. In the end\\nof the phase, the probability of v not being deleted is at most\\n(\\n1− d\\nn\\n)Ω(t·logn)\\n= 1polyn . A union\\nbound over all vertices concludes the argument. By doubling the parameter t after each phase they\\nensure that after O(log n) phases the whole graph has been processed. Inside each phase, they\\nuse the maximum degree to bound the round complexity by the length of a longest monotonically\\nincreasing path3, which is O(log n).\\nThe main shortcoming of Blelloch et al.’s approach is that it relies heavily on the property\\nthat in each phase the remaining degree of all vertices with high probability falls below a certain\\nvalue. This imposed union bound unavoidably stretches each prefix by a factor of log n. We will\\ncircumvent this problem using the following core ideas.\\n(i) Instead of bounding the degree of all vertices in the graph, we will consider a fixed set\\nof O(log n) positions, that is, indices in {1, . . . , n}, and only analyze the degree of vertices\\nassigned to these positions in the random order.\\n(ii) Instead of using one prefix for all these vertices simultaneously, we will essentially have one\\ndistinct “prefix” for each position, that is, one distinct set of vertices which we will use to argue\\nabout the degree drop of the vertex assigned to this position. This will preserve independence\\namong positions, and hence spare us the need of a union bound.\\n(iii) Instead of bounding the probability of a long monotonically increasing path of vertices based\\non the with high probability upper bound on the degree, we will restrict our attention to the\\nmuch stronger concept of so-called dependency paths4 . Roughly speaking, a dependency path\\nis a monotonically increasing path which alternates between vertices in the MIS and vertices\\n2For the sake of analysis, we think of the prefix being processed sequentially. This does not change the outcome.\\n3A monotonically increasing path with respect to an order is a path along which the order ranks are increasing.\\n4Note that this is not the same as dependence path in [BFS12].\\n2\\nnot in the MIS, and the predecessor of a vertex v not in the MIS is the first vertex that\\nknocked v out. These additional properties of dependency paths allow us to execute a more\\nnuanced analysis of their occurrence probability. In particular, we will not need to argue that\\nthe degrees of our vertices drop according to some process with high probability. It suffices\\nto show that for any degree of this vertex its chances of being part of a dependency path are\\nlow enough.\\nProof Outline: Summarized, our method—comprising all the aforementioned ideas—can be out-\\nlined as follows. We will show that the parallel round complexity is bounded by the largest depen-\\ndency path length. It is then enough to show that w.h.p. there cannot be a dependency path of\\nlength L = Ω(log n). In a first step, we will analyze the probability that a fixed set P of positions\\nforms a dependency path. To this end, we assign each position a position set (which will play the\\nrole of a “prefix” for this position’s vertex and thus serve the purpose of controlling its degree) of\\na certain size and at a certain place, both carefully chosen depending on the position. Then we\\nwill argue for each position5 that its probability of being part of and continuing the dependency\\npath is not too high, based on the randomness of its associated position set. Being careful about\\nonly exposing positions that have not already been exposed for other positions, we will be able to\\ncombine these probabilities to obtain a bound on the probability that P forms a dependency path.\\nFinally, we union bound over all choices for P .\\n1.3 Notation\\nWe use [n] := {1, . . . , n} and [x, y] := {x, . . . , y}. For two sets X,Y ⊆ N, we write X < Y if\\nmaxX < minY . We use X[i] for the ith element in the set X (we think of all sets as ordered)\\nand X[I] the (ordered) set of the elements in X at positions I ⊆ [|X|]. For an order pi : [n] → V ,\\nwe say that vertex v ∈ V has position i if pi(i) = v, use pi(I) := ⋃i∈I pi(i), and write pi(I) = pi′(I)\\nwhen pi(i) = pi′(i) for all i ∈ I. We say that we expose a position i when we fix the vertex pi(i)\\nin a random order pi. Moreover, we say a vertex v is exposed if there is a position i such that i is\\nexposed and pi(i) = v. If a set I of positions is already exposed, this means that the considered\\nprobabilities are all conditioned on pi(I). We use subscript I to indicate that the probability is\\nover the randomness in the positions I. For a graph G = (V,E), we use E(X,Y ) to denote the set\\nof edges in E between X and Y , for X,Y ⊆ V , and write e(X,Y ) = |E(X,Y )|. Moreover, we let\\nN(v) := {u ∈ V : {u, v} ∈ E} denote the neighborhood of vertex v ∈ V .\\n2 Proof of Theorem 1.1: Upper Bound\\n2.1 Framework\\nWe introduce the concept of dependency paths, and show that this notion is closely related to the\\nround complexity of the parallel greedy MIS algorithm.\\nDependency Path: For a fixed permutation pi : [n] → V , let V ∗ ⊆ V denote the MIS generated\\nby the (sequential) greedy algorithm that processes the vertices in the order (pi(1), . . . , pi(n)). For\\nevery vertex v not in V ∗, we use inhib(v) to denote the neighbor of v in V ∗ of minimum position,\\nthat is, setting inhib(v) := argmin{pi−1(u) : u ∈ N(v) ∩ V ∗}, and call it v’s inhibitor.\\nDefinition 2.1. A sequence 1 ≤ p1 < · · · < p2l+1 ≤ n of positions forms a dependency path of\\nlength 2l + 1 for l ≥ 0 if\\n5In fact, to get a strong enough bound, we will have to argue not only about one position but about two positions\\nsimultaneously.\\n3\\n(i) (pi(p1), . . . , pi(p2l+1)) is a path in G,\\n(ii) {pi(pk) | k is odd} ⊆ V ∗,\\n(iii) {pi(pk) | k is even} ⊆ V \\\\ V ∗,\\n(iv) pi(pk−1) = inhib(pi(pk)) for even k.\\nWe write p1 ∼ · · · ∼ p2l+1.\\nConnection to Parallel Algorithm: In the following, we establish a connection between the\\nround complexity of the parallel algorithm and the dependency length, defined as the length of the\\nlongest dependency path in a graph.\\nLemma 2.2. If the dependency length is 2l + 1, the parallel algorithm takes at most l + 1 rounds.\\nProof. Consider a slowed-down version of the parallel algorithm in which the deletion of a vertex\\nv /∈ V ∗ is delayed until the round in which inhib(v) enters the independent set. That is, even if a\\nneighbor u of v enters the independent set, v is not deleted from the graph unless u = inhib(v).\\nThis algorithm takes at least as many rounds as the original parallel algorithm, as the time in\\nwhich a vertex is processed can only be delayed. Furthermore the slowed-down version produces\\nthe same independent set as in the original algorithm.\\nWe show by induction that every vertex entering the independent set in round i in this modified\\nparallel algorithm must be the last vertex of a dependency path of length 2(i − 1) + 1. The base\\ncase i = 1 is immediate. If a vertex w enters V ∗ in round i+1, then there is a neighbor v of w with\\npi(v) < pi(w) that was deleted from the graph (but not added to the MIS) in round i, as otherwise\\nw could have been added to V ∗ in an earlier round. This means that inhib(v) was added to V ∗ in\\nround i. By the induction hypothesis, inhib(v) is the last vertex of a dependency path of length\\n2(i− 1) + 1, and it is easy to check that v and w extend this dependency path.\\n2.2 Proof Outline\\nThe number of rounds taken by the randomized parallel greedy MIS algorithm in the beginning and\\nin the end—that is, for vertices with positions in [1,Θ(log n)] and [βn, n], for a constant β ∈ (0, 1)\\nwhich is given by Lemma 2.4 below—can be handled easily, as we will discuss in the proof of\\nTheorem 1.1 in Section 2.3. We thus focus on the technically more interesting range of positions\\nin [Θ(log n), βn] here. By Lemma 2.2, we know that the dependency length constitutes an upper\\nbound on the round complexity of the parallel greedy MIS algorithm. It is thus enough to show\\nthe following.\\nTheorem 2.3. W.h.p. there is no dependency path of length L = Ω(log n) in the interval [βn].\\nOne main part of this theorem’s proof, which appears in Section 2.3, is to argue that the\\nprobability that a fixed set {p1, . . . , pL} of positions form a dependency path is low. This, in turn,\\nis proved by bounding the probability that—when already having exposed pk−1—the positions pk\\nand pk+1 continue a dependency path, thus pk−1 ∼ pk ∼ pk+1, for all segments (pk−1, pk, pk+1)\\nfor even k ∈ [L]. By being careful about dependencies among segments, thus in particular about\\nwhich randomness to expose when, these probabilities for the segments then can be combined to a\\nbound for the probability of p1 ∼ · · · ∼ pL. We will achieve this by assigning disjoint position sets\\nPi to positions pi and exposing, roughly speaking, only Pk−1, Pk, and {pk, pk+1} when analyzing\\nthe probability of the segment (pk−1, pk, pk+1). More formally, this reads as follows.\\n4\\nLemma 2.4. There exist absolute constants β, ε ∈ (0, 0.01) such that the following holds. Fix three\\npositions p1, p2, p3 ∈ [βn] and two disjoint sets P1, P2 ⊆ [βn] of equal size l := |P1| = |P2| ≥ 10000\\nwhich satisfy P1 < P2 < p1 < p2 < p3, and let t2 := maxP2. Consider S ⊆ [βn]\\\\(P1∪P2∪{p2, p3})\\nwith p1 ∈ S. Suppose that the positions in S have already been exposed. Then the probability that\\np1, p2, p3 can still form a dependency path when additionally exposing S2\\\\S for S2 := S∪[t2]∪{p2, p3}\\nis PrS2 [p1 ∼ p2 ∼ p3 | pi (S)] ≤ 1−ε(e·l)2 .\\nThe proof of this lemma is deferred to Section 2.4.\\n2.3 Proofs of Theorems 1.1 and 2.3\\nWe will bound the probability that there exists a dependecy path of length Ω(log n) in the interval\\n[A log n, βn] for some large A, thereby proving Theorem 2.3.\\nProof. We first upper bound the probability that a fixed set P of L = Θ(log n) (for odd L) positions\\nforms a dependency path by iteratively applying Lemma 2.4. We then take a union bound over all\\npossible choices for P .\\nProbability for Fixed Positions: Let I = [1, . . . , n] and let A denote a large constant which\\nwe will fix later. For a fixed set P = {p1, . . . , pL} ⊆ [A log n, n] of L =\\n√\\nA log n positions, we will\\nchoose position sets Pk to associate with each pk which satisfy the conditions\\n(1) P1 < · · · < PL,\\n(2) Pk < pk for all k ∈ [L], and additionally\\n(3) Pk < pk−1 and |Pk| = |Pk−1| for all even k ∈ [L].\\nThis will ensure the applicability of Lemma 2.4 to (pk−1, pk, pk+1) for even k ∈ [L].\\nBreak [n] into exponentially growing sub-intervals Ii :=\\n[\\n(1 + α)i, (1 + α)i+1\\n)\\nfor some small\\nenough constant 0 < α < 1. Let si := |Ii ∩ (P \\\\ {pL})| denote the number of positions in P \\\\ {pL}\\nthat intersect Ii. Observe that si = 0 for all i + 1 ≤ log1+α(A log n). For convenience, we use pi,j\\nto refer to pk if pk is the j\\nth position among Ii ∩ P .\\nIf si > 0 then we split Ii−1 \\\\ P into si + 1 position sets Ii−1,1 < . . . < Ii−1,si+1, each of size\\nli :=\\n⌊ |Ii−1 \\\\ P |\\nsi + 1\\n⌋\\n=\\n⌊⌊α(1 + α)i−1⌋ − |P ∩ Ii−1|\\nsi + 1\\n⌋\\n.\\nNote that these subsets are not necessarily contiguous and do not contain any of the positions\\npk ∈ P . We claim that li ≥ α(1 + α)i−2/(si + 1) ≥ 10000. Indeed for si to be non-zero i must\\nsatisfy (1 + α)i+1 ≥ A log n = √A|P |. Thus the first inequality holds for A(α) large enough.\\nSecondly si ≤ |P | thus the second inequality holds for A large enough as well.\\nNext we assign each position pk = pi,j ∈ P a position set\\nPk = I(pk) = I(pi,j) :=\\n{\\nIi−1,j, if k is odd or j 6= 1,\\nIi−hi−1,s(i−hi)+1, if k is even, j = 1,\\nwhere hi := i−max{j | j < i : sj 6= 0} is the distance i−j of Ii to the next smaller interval Ij which\\ncontains at least one position from P .6 In words, we assign the jth position pi,j in the interval Ii\\n6Note that, since we are interested in hi only for positions pk = pi,1, where k is even. Thus in particular not for\\np1, there will actually always be a j < i with sj 6= 0.\\n5\\npk−3 pk−2 pk−1 pk pk+1\\nPk−3 Pk−2 Pk−1 Pk\\nIi−2 Ii−1 Ii\\nFigure 1: The situation before applying Lemma 2.4 to (pk−1, pk, pk+1). The set Ii−2 \\\\ P has been split into\\nsi−1+1 = 4 parts. The first three parts are associated with the points in P ∩ Ii−1. Since k is even and pk is\\nthe first position in the interval Ii, its associated set is Pk = Ii−hi−1,si−hi+1 = Ii−2,4 (and not Ii−1,1). This\\nensures that |Pk−1| = |Pk|. The positions before Pk−1 = Ii−2,3 as well as pk−3, pk−2, pk−1 have already been\\nexposed. Invoking Lemma 2.4 will additionally expose Pk−1, Pk, pk and pk+1.\\nthe jth position set Ii−1,j in the previous interval Ii−1, except for positions pk for which k is even\\nand pk is the first position in some interval Ii. Since in that case pk−1 is in the interval Ii−hi , and\\nthus is assigned a position set Ii−hi−1,s(i−hi) , we have to assign pk the position set Ii−hi−1,s(i−hi)+1\\nin order to not violate condition (3).\\nLet S0 = {p1}, and for every even k ∈ [L], let Sk = Sk−2∪[maxPk]∪{pk, pk+1}. Then, iteratively\\nfor every even k ∈ [L], apply Lemma 2.4 with (p1, p2, p3)← (pk−1, pk, pk+1), (P1, P2)← (Pk−1, Pk),\\nl ← |Pk−1| = |Pk|, S ← Sk−2. Observe that indeed (Pk−1 ∪ Pk) ∩ Sk−2 = ∅, pk−1 ∈ Sk−2,\\nand Sk = Sk−2∪ [maxPk]∪{pk, pk+1}, which—together with properties (1)–(3)—makes the lemma\\napplicable. Thus for every even k we obtain a bound of 1−ε\\ne2·|Pk−1|·|Pk| on the probability that pk−1, pk,\\nand pk+1 form a dependency path when exposing Sk, conditioned on already having exposed Sk−2.\\nThus, P forms a dependency path with probability at most\\n∏\\nk∈[L] : k even\\nPrSk [pk−1 ∼ pk ∼ pk+1 | pi(Sk−2)] ≤\\n∏\\nk∈[L] : k even\\n1− ε\\ne2 · |Pk−1| · |Pk| =\\n∏\\ni : si 6=0\\nsi∏\\nj=1\\n√\\n1− ε\\ne · |I(pi,j)| .\\nLet I˜ = {i : si 6= 0 ∧ pi,1 = pk for some even k}. Then for i ∈ I˜\\nli\\n|I(pi,1)| =\\nli\\nli−hi\\n≤ (1 + α)hi si−hi + 1\\nsi + 1\\n· (1 + α) ≤ (1 + α)hi(si−hi + 1).\\nSince\\n∑\\ni∈I˜ hi ≤ log1+α n and since the mapping f : I˜ → {i : si ≥ 1}, f(i) = i − hi is an injection\\nwe can bound the product by\\n∏\\ni∈I˜\\nli\\n|I(pi,1)| ≤\\n∏\\ni∈I˜\\n(1 + α)hi(si−hi + 1) ≤ n\\n∏\\ni : si≥1\\n(si + 1) ≤ n\\n(\\n2L\\nlog1+α(n)\\n)log1+α(n)\\n.\\nFinally by definition |I(pi,j)| = li whenever i /∈ I˜ or j > 1. Therefore we obtain\\n∏\\ni : si 6=0\\nsi∏\\nj=1\\n√\\n1− ε\\ne · |I(pi,j)| ≤ n\\n(\\n2L\\nlog1+α(n)\\n)log1+α(n) ∏\\ni : si 6=0\\n(√\\n1− ε\\ne · li\\n)si\\n.\\n6\\nUnion Bound Over All Positions: For fixed values of {si} there are at most\\nn ·\\n∏\\ni\\n(\\nα · (1 + α)i\\nsi\\n)\\n≤ n ·\\n∏\\ni : si 6=0\\n(\\ne · α · (1 + α)i\\nsi\\n)si\\n= n ·\\n∏\\ni : si 6=0\\n((\\nsi + 1\\nsi\\n)\\n·\\n(\\ne · α · (1 + α)i\\nsi + 1\\n))si\\n≤ n·\\n∏\\ni : si 6=0\\ne · (e · li · (1 + α)2)si ≤ n1+ 1log(1+α) · ∏\\ni : si 6=0\\n(\\ne · li · (1 + α)2\\n)si\\nchoices for positions P with |Ii ∩ (P \\\\ {pL})| = si (counting n choices for pL), using\\n(\\nsi+1\\nsi\\n)si ≤ e\\nin the second inequality and |{i : si 6= 0}| ≤ log1+α n in the third inequality.\\nTherefore, by a union bound, the probability of having a dependency path of length L for\\nprescribed {si} is at most n2+\\n1\\nlog(1+α) ·\\n(\\n2L\\nlog1+α n\\n)log1+α n · (√1− ε · (1 + α)2)L. Since we can as-\\nsign values to {si} in at most\\n(L+log1+α n\\nlog1+α n\\n) ≤ (e · ( Llog1+α n + 1\\n))log1+α n\\nways, it follows that the\\nprobability of a dependency path of length L is at most\\nn\\n2+ 1\\nlog(1+α) ·\\n(\\n2L\\nlog1+α n\\n· e ·\\n(\\nL\\nlog1+α n\\n+ 1\\n))log1+α n\\n· (√1− ε · (1 + α)2)L ,\\nwhich is 1polyn for a constant α small enough depending on ε, and L =\\n√\\nA log n large enough\\ndepending on α.\\nWe now use the bound on the dependency length from the previous result to prove Theorem 1.1.\\nProof. By Theorem 2.3, w.h.p. the length of any dependency path in the interval [βn] is O(log n),\\nand by Lemma 2.2, this thus bounds the number of rounds needed by the parallel algorithm to\\nprocess this interval by O(log n).\\nSuffix [βn, n]: For the suffix [βn, n], it can be shown that w.h.p. after processing the first βn\\npositions the maximum degree among all remaining vertices is at most d = O(log n). See e.g.\\nLemma 3.1 in [BFS12]. Since each possible path of length L is monotonically increasing with\\nprobability 1\\nL! ≤ ( eL)L, the probability of having such a path in the suffix is at most n ·\\n(\\ne·d\\nL\\n)L\\n,\\nwhich is 1polyn for L = Ω(log n) large enough. Finally, observe that for the parallel algorithm to\\ntake L rounds there indeed must be a monotonically increasing path of length L.\\n2.4 Proof of Lemma 2.4: Continuing a Dependency Path\\nProof. As we will see next, the probability of p1 ∼ p2 ∼ p3 can be bounded by considering an\\nexecution of the sequential greedy MIS algorithm on a random ordering.\\nConnection to Sequential Algorithm: We will work with the following sequential algorithm.\\nInitially, in step t = 1, all vertices are called alive. Then, in each step t ∈ [n], position t is exposed\\n(if not already exposed) and vertex pi(t) processed as follows. If pi(t) is alive in step t, then pi(t) is\\nadded to the MIS, and pi(t) as well as all its neighbors are called dead (i.e., not alive) for all steps\\nt′ > t after t. If pi(t) is dead in step t, then we proceed to the next step. We say that a vertex dies\\nin step t if it is alive in step t and dead in step t+ 1, and say that it is dead after t.\\nLet t1 = maxP1 and t2 = maxP2. Consider the events\\nE1: pi(p1), pi(p2) are neighbors and alive in step t1 + 1 ≤ p1, and\\n7\\nE2: pi(p2), pi(p3) are neighbors and alive in step t2 + 1 ≤ p1, and pi(p1), pi(p3) are not neighbors.\\nObserve that these two events are necessary for p1 ∼ p2 ∼ p3. Indeed for both pi(p1) and pi(p3)\\nto enter the independent set they must not share an edge and must be alive in step t2+1 ≤ p1 < p3.\\nFurthermore by definition of a dependency path pi(p2) dies in step p1 and therefore must be alive\\nin step t1 + 1 as well.\\nIn the following, we call an alive and unexposed (with respect to a step and a set of exposed\\npositions) vertex active, and let the active degree of a vertex be its number of active neighbors.\\nProof Sketch: By the above observation it is enough to bound the probability that during the\\nexecution of the sequential algorithm up to step t2 the events E1 and E2 occur. First, we will\\ninvestigate how the active degree of the vertex v := pi(p1) evolves and thereby affects the probability\\nof E1 when the sequential algorithm runs through the position set P1 up to step t1. The main\\nobservation is that on the one hand, if the active degree d of v in step t1 is low, then it is unlikely\\nthat pi(p2) is an active neighbor of v (this happens with probability ≈ dn).7 On the other hand,\\nif the active degree of v is high in step t1 (and thus during all steps P1) then v is likely to die\\nbecause one of its active neighbors enters the MIS (v stays alive with probability ≈ (1− d\\nn\\n)l\\n). The\\ncombined probability (over P1 ∪ {p2}) that v stays alive and one of its active neighbors is selected\\nfor pi(p2) is ≈ dn ·\\n(\\n1− d\\nn\\n)l\\n. Therefore the probability of E1 is maximized for d ≈ nl , yielding a\\nvalue 1\\ne·l for one additional position, and hence\\n1\\n(e·l)2 for two positions, which falls just short of our\\ndesired bound.\\nIn that seemingly bad case, however, we will argue that pi(p2) is likely to have a low active\\ndegree, which in turn will make E2 improbable (by employing a similar argument for v′ := pi(p2)\\nwhen running the algorithm through P2 up to step t2, also exposing pi(p3)). Taken together, E1\\nand E2 will have low probability in all cases, i.e., for any active degree d of v.\\nFormal Proof: We may assume without loss of generality that [t2] \\\\ (P1 ∪ P2) ⊆ S. If not, we\\nexpose the missing positions and add them to S. Let t0 = minP1 and S1 = S ∪P1 ∪{p2}. Suppose\\nthat S is exposed and that the sequential algorithm has run up to step t0 − 1. We then run the\\nsequential algorithm through the steps [t0, t1] ⊇ P1. For i ∈ {0, . . . , l− 1}, let N1i denote the set of\\nactive neighbors of v in step P1[i+1], and N\\n1\\nl the set of active neighbors of v after step t1 = P1[l].\\nWe use di for the corresponding degrees, let N\\n2\\ni denote the set of active neighbors of (vertices in)\\nN1i without N\\n1\\ni , and set Ei := E(N\\n1\\ni , N\\n2\\ni ) and ei := |Ei|, in the respective step.\\nWe now analyze the probability of E1 by distinguishing several cases based on the set Ω of all\\nvertex sequences ω : P1 → V \\\\ pi(S) that can be encountered when exposing P1. We restrict our\\nattention to ω ∈ Ω∗ := Ω \\\\Ω0 for Ω0 := {ω ∈ Ω: v dead in steps after t1 under ω}, as otherwise E1\\nis impossible. We introduce the following auxiliary algorithm. Let A be the algorithm that works\\nexactly as the sequential greedy algorithm, except that it picks a vertex for P1[i] uniformly at\\nrandom from V \\\\(pi(S) ∪N1i−1) instead of from V \\\\pi(S). For ω /∈ Ω0 this set never becomes empty.\\nIf V \\\\ (pi(S) ∪N1i−1) = ∅ at some step then define A to pick vertices in some arbitrary way. We\\nuse PrA[ω | pi(S)] to denote the probability of the algorithm A picking the sequence ω ∈ Ω∗ when\\nexposing P1, conditioned on pi(S). Note that PrA[ω | pi(S)] =\\n∏l−1\\ni=0\\n(\\n1\\nn−|S|−i−di\\n)\\nand therefore\\nPrS1 [pi(P1) = ω(P1) | pi(S)] =\\nl−1∏\\ni=0\\n(\\n1\\nn− |S| − i\\n)\\n= PrA[ω | pi(S)] ·\\nl−1∏\\ni=0\\n(\\n1− di\\nn− |S| − i\\n)\\n.\\n7Note that since there are always at least (1−β)n unexposed positions the probability that one of d alive vertices\\nis exposed in the next step is always Θ\\n(\\nd\\nn\\n)\\n.\\n8\\nMoreover, the probability of having an active neighbor of v at p2 under ω is at most\\ndl\\nn−|S|−l . Thus,\\nPrS1 [E1 and pi(P1) = ω(P1) | pi(S)] ≤\\ndl\\nn− |S| − l · PrA[ω | pi(S)] · exp\\n(\\n−\\nl−1∑\\ni=0\\ndi\\nn\\n)\\n. (1)\\nSince the di are decreasing (in i) this term is maximized for d0 = · · · = dl = nl , yielding an upper\\nbound of\\nPrA[ω | pi(S)] · n\\ne · l · (n− |S| − l) ≤ PrA[ω | pi(S)] ·\\n1\\ne · l · (1− β) .\\nSumming up over ω ∈ Ω∗ yields PrS1 [E1 | pi(S)] ≤ 1e·l·(1−β) . Note that the same upper bound of\\n1\\ne·l·(1−β) can be obtained for PrS2 [E2 | pi(S1)], where S2 = S1∪P2∪{p3}, by repeating the argument\\nwhile exposing P2.\\nThus we obtain our first bound of\\n(\\n1\\ne·l·(1−β)\\n)2\\n. The next step is to improve it to the claimed\\nbound of 1−ε\\n(e·l)2 . To that end, we distinguish three different cases for ω, mainly based on the active\\ndegree d⌊δl⌋ of v after ⌊δl⌋ steps under ω, for δ = 0.01, say. Note that our assumption of l ≥ 10000\\nensures that ⌊δl⌋ ≥ 0.99δl.\\nDeviation from Degree ≈ n\\nl\\n: Ω1 = {ω ∈ Ω∗ : d⌊δl⌋ > 1.1nl or dl < 0.8nl under ω}:\\nEasy calculations show that when the degree deviates from the optimizer n\\nl\\n, then the upper bound\\non (1) can be improved by a small constant factor (1− ε1).\\nDegree ≈ n\\nl\\nand Few Edges: Ω2 = {ω ∈ Ω∗ : d⌊δl⌋ ≤ 1.1nl , dl ≥ 0.8nl , el ≤ 0.6n\\n2\\nl2\\nunder ω}:\\nWe will argue that because el is small, the active degree of pi(p2) in steps > t1 is likely to be low.\\nIn that case, E2 will have a low probability by an argument analogous to the one in the previous\\ncase where the active degree of v deviates from n\\nl\\n.\\nThere can be at most 0.9375dl vertices in N\\n1\\nl with degree into N\\n2\\nl larger than 0.8\\nn\\nl\\nafter step\\nt1. All other at least 0.0625dl many vertices in N\\n1\\nl thus have degree into N\\n2\\nl at most 0.8\\nn\\nl\\n. For\\npi(p2) such that this is the case, analogously\\n8 to the case ω ∈ Ω1 from before, we improve upon\\nthe trivial bound by a constant factor (1− ε1). For pi(p2) with larger degree we obtain the trivial\\nbound of PrA′ [ω′ | pi(S1)] · 1e·l·(1−β) . Thus, on average we improve by some factor (1− ε2).\\nDegree ≈ n\\nl\\nand Many Edges: Ω3 = {ω ∈ Ω∗ : d⌊δl⌋ ≤ 1.1nl , dl ≥ 0.8nl , el > 0.6n\\n2\\nl2\\nunder ω}:\\nWe will argue that\\n∑\\nω∈Ω3 PrA[ω | pi(S)] is bounded away from 1 by a constant. Intuitively speaking,\\nthe more edges there are, the likelier it is that vertices in N1i die. It is thus not too likely to have\\nonly a small drop in the active degree over all (1− δ)l steps if in every step the number of edges in\\nEi is large.\\nRun A for ⌊δl⌋ steps and suppose that d⌊δl⌋ ≤ 1.1nl . We will bound the probability that\\ndl ≥ 0.8nl and el > 0.6n\\n2\\nl2\\nif we continue to run A.\\nIf ei ≥ 0.6n2l2 then let Mi ⊆ Ei be a subset of exactly ⌈0.6n\\n2\\nl2\\n⌉ edges, and let Xi+1 denote the\\nnumber of vertices in N1i which are connected through Mi with the vertex selected in step i + 1\\nscaled down by α :=\\n(\\nn− ∣∣S ∪N1i ∣∣− i) /n. To shorten notation, we use ξi := pi (S ∪ P1 ([i])).\\nThen, for (say) β ≤ 0.01,\\nEA [Xi+1 | ξi] = α|Mi|\\nn− |S ∪N1i | − i\\n=\\n|Mi|\\nn\\n=\\n⌈0.6n2\\nl2\\n⌉\\nn\\n.\\n8The main observations needed for adapting the proof is that we can ignore all sequences for P2 under which a\\nvertex in N1l or N\\n2\\nl is exposed, since then either v or v\\n′ would die, and that pi(p3) ∈ N\\n2\\nl \\\\N\\n1\\nl is necessary for E2.\\n9\\nIf ei ≤ 0.6n2l2 then define Xi to be a constant random variable (with the same expectation). Note\\nthat these random variables are uncorrelated. Indeed since Xi is fully determined by ξi while (for\\nj > i) EA[Xj | ξi] does not depend on ξi we have:\\nEA[XiXj ] =\\n∑\\nξi\\nEA[XiXj | ξi]PrA[ξi] =\\n∑\\nξi\\nEA[Xi | ξi]EA[Xj | ξi]PrA[ξi] = EA[Xi]EA[Xj ].\\nNow let X :=\\n∑l\\ni=⌊δl⌋Xi (note that if el > 0.6\\nn2\\nl2\\nthen this is a lower bound on d⌊δl⌋ − dl) with\\nexpectation EA[X | ξ⌊δl⌋] ≥ 0.5nl and variance\\nVarA[Xi+1 | ξ⌊δl⌋] ≤ EA[X2i+1 | ξ⌊δl⌋] ≤ d⌊δl⌋ · EA[Xi+1 | ξ⌊δl⌋] ≤\\nn2\\nl3\\n,\\nwhere we have used Xi+1 ≤ d⌊δl⌋ ≤ 1.1nl .\\nSince the Xi are uncorrelated the variance of the sum is at most VarA[X | ξ⌊δl⌋] ≤ n2l2 . Hence,\\nby Cantelli ’s inequality (the one-sided version of Chebyshev ’s inequality),\\nPrA\\n[\\nX ≥ 0.4n\\nl\\n| ξ⌊δl⌋\\n]\\n≥ Pr\\n[\\nX − EA[X | ξ⌊δl⌋] ≥ −0.1\\nn\\nl\\n| ξ⌊δl⌋\\n]\\n≥ 1− VarA[X | ξ⌊δl⌋]\\n0.001n\\n2\\nl2\\n+VarA[X | ξ⌊δl⌋]\\n≥ C > 0,\\nfor some constant C.\\nThus, if we have d⌊δl⌋ ≤ 1.1nl then with probability at least C either el ≤ 0.6n\\n2\\nl2\\nor the active\\ndegree drops by at least 0.4n\\nl\\n. Both cases are excluded from Ω3. Therfore\\n∑\\nω∈Ω3 PrA[ω | pi(S)] ≤\\n1− C.\\nWrap-Up: Combining these three cases, we obtain∑\\nω∈Ω\\nPrS2 [E1 and E2 and pi(P1) = ω(P1) | pi(S)]\\n≤\\n∑\\nω∈Ω1∪Ω2\\nPrA[ω | pi(S)] · 1−min {ε1, ε2}\\n((1− β) · e · l)2 +\\n∑\\nω∈Ω3\\nPrA[ω | pi(S)] · 1\\n((1 − β) · e · l)2 .\\nwhich is at most 1−ε\\n(e·l)2 for some absolute small constants ε, β > 0, since\\n∑\\nω∈Ω3 PrA[ω | pi(S)] ≤\\n1− C.\\n3 Corollaries\\nDue to the well-known reductions of maximal matching and (∆ + 1)-vertex-coloring to MIS, our\\nanalysis in Theorem 1.1 directly applies to the parallel/distributed randomized greedy algorithms\\nfor maximal matching and vertex coloring. For a review of the state of the art for these problems,\\nwe refer to [BEPS16].\\n3.1 Randomized Greedy Maximal Matching\\nThe parallel/distributed randomized greedy maximal matching algorithm works as follows: A random\\norder of the edges is chosen. Then, in each round, all locally minimal edges are removed from the\\ngraph along with all their incident edges.\\n10\\nCorollary 3.1. The parallel/distributed randomized greedy maximal matching algorithm has round\\ncomplexity O(log n) with high probability on graphs with n vertices.\\nProof. For a graph G = (V,E), the line graph L = (E,F ) is defined to be a graph with vertex set\\nE and an edge {e, e′} ∈ F iff e∩ e′ 6= ∅. Running the randomized greedy MIS algorithm on the line\\ngraph L corresponds to running the randomized greedy maximal matching algorithm on G.\\n3.2 Distributed Randomized Greedy (∆ + 1)-Vertex-Coloring\\nThe parallel/distributed randomized greedy (∆+1)-vertex-coloring algorithm on a graph G = (V,E)\\nwith maximum degree ∆ works as follows: A random order of the vertex-color pairs V × [∆ + 1]\\nis chosen. Then, in each round, all locally minimal pairs (v, c) are removed along with all (v′, c′)\\nsuch that either v′ = v or {v, v′} ∈ E and c′ = c. Vertex v is assigned color c.\\nCorollary 3.2. The parallel/distributed randomized greedy (∆ + 1)-vertex-coloring algorithm, as\\ndefined above,9 has round complexity O(log n) with high probability on graphs with n vertices and\\nmaximum degree ∆.\\nProof. Luby [Lub85, Lin87] presented the following reduction from (∆ + 1)-vertex-coloring in a\\ngraph G to MIS in a graph H: To construct H, take ∆ + 1 copies of G and add a clique among\\nall ∆ + 1 copies of the same vertex, for all vertices in G. It is easy to observe that a MIS in H\\ncorresponds to a proper (∆+1)-vertex-coloring of G, when we assign vertex v the color i iff the ith\\ncopy of v is in the MIS. Indeed, due to maximality, every vertex in G is assigned at least one color,\\nand because of the added cliques and the independence of the MIS, at most one. Moreover, having\\na copy of G for every color guarantees that all the edges must be proper (due to independence).\\n3.3 Correlation Clustering\\nCorrelation clustering has the goal to partition nodes into clusters so that the number of miss-\\nclassified edges—that is, edges with its two endpoints in two different clusters or non-edges with\\nendpoints in the same cluster—is minimized. More formally, we are given a complete graph on n\\nnodes where each edge is either labeled + or −, indicating that the corresponding nodes should\\nbe in the same or in different clusters, respectively. The goal is to group the nodes into (an\\narbitrary number of) clusters so that the number of − edges within clusters and + edges crossing\\nclusters is minimized [BBC04]. Ailon, Charikar, and Newman [ACN08] showed that the greedy MIS\\nalgorithm, called CC-Pivot in their paper, provides a 3-approximation for correlation clustering,\\nwhen each non-MIS node is clustered with its inhibitor, that is, its lowest-rank neighbor in the MIS.\\nMoreover, [CDK14] argues how an iteration of this (or a similar) algorithm can be implemented in\\nO(1) rounds of MapReduce or in O(1) passes of the streaming model.\\nCorollary 3.3. A 3-approximation for correlation clustering can be computed in O(log n) rounds\\nin the PRAM, LOCAL, and MapReduce model, and in O(log n) passes in the streaming model.\\nAcknowledgment\\nThe authors want to thank Mohsen Ghaffari for suggesting this problem, providing the construction\\nof the lower bound, and valuable comments.\\n9This is not the greedy coloring algorithm where the largest available color is picked greedily.\\n11\\nReferences\\n[ABI86] Noga Alon, La´szlo´ Babai, and Alon Itai. A Fast and Simple Randomized Parallel\\nAlgorithm for the Maximal Independent Set Problem. Journal of Algorithms, 7(4):567–\\n583, 1986.\\n[ACN08] Nir Ailon, Moses Charikar, and Alantha Newman. Aggregating inconsistent informa-\\ntion: ranking and clustering. Journal of the ACM (JACM), 55(5):23, 2008.\\n[BAAS09] Robert Bocchino, Vikram Adve, Sarita Adve, and Marc Snir. Parallel Programming\\nMust Be Deterministic By Default. In USENIX Conference on Hot Topics in Paral-\\nlelism, pages 4–4, 2009.\\n[BBC04] Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine\\nlearning, 56(1-3):89–113, 2004.\\n[BEPS12] Leonid Barenboim, Michael Elkin, Seth Pettie, and Johannes Schneider. The Lo-\\ncality of Distributed Symmetry Breaking. In the Proceedings of the Symposium on\\nFoundations of Computer Science (FOCS), pages 321–330. IEEE, 2012.\\n[BEPS16] Leonid Barenboim, Michael Elkin, Seth Pettie, and Johannes Schneider. The Locality\\nof Distributed Symmetry Breaking. Journal of the ACM, 63(3):20:1–20:45, 2016.\\n[BFGS12] Guy E. Blelloch, Jeremy T. Fineman, Phillip B. Gibbons, and Julian Shun. Internally\\nDeterministic Parallel Algorithms Can Be Fast. In ACM SIGPLAN Notices, pages\\n181–192, 2012.\\n[BFS12] Guy E. Blelloch, Jeremy T. Fineman, and Julian Shun. Greedy Sequential Maximal\\nIndependent Set and Matching are Parallel on Average. In the Proceedings of the\\nSymposium on Parallel Algorithms and Architectures (SPAA), pages 308–317, 2012.\\n[CDK14] Flavio Chierichetti, Nilesh Dalvi, and Ravi Kumar. Correlation clustering in mapre-\\nduce. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge\\ndiscovery and data mining, pages 641–650. ACM, 2014.\\n[CF90] Neil Calkin and Alan Frieze. Probabilistic Analysis of a Parallel Algorithm for Finding\\nMaximal Independent Sets. Random Structures & Algorithms, 1(1):39–50, 1990.\\n[CFK92] Neil Calkin, Alan Frieze, and Luˇdek Kucˇera. On the expected performance of a par-\\nallel algorithm for finding maximal independent subsets of a random graph. Random\\nStructures & Algorithms, 3(2):215–221, 1992.\\n[Coo83] Stephen A. Cook. An Overview of Computational Complexity. Communications of\\nthe ACM, 26(6):400–408, 1983.\\n[CRT87] Don Coppersmith, Prabhakar Raghavan, and Martin Tompa. Parallel Graph Al-\\ngorithms that Are Efficient on Average. In the Proceedings of the Symposium on\\nFoundations of Computer Science (FOCS), pages 260–269, 1987.\\n[Gha16] Mohsen Ghaffari. An Improved Distributed Algorithm for Maximal Independent Set.\\nIn the Proceedings of ACM-SIAM Symposium on Discrete Algorithms (SODA), 2016.\\n12\\n[Gol86] Mark K. Goldberg. Parallel Algorithms for Three Graph Problems. Congressus Nu-\\nmerantium, 54(111-121):4–1, 1986.\\n[GPS87] Andrew Goldberg, Serge Plotkin, and Gregory Shannon. Parallel Symmetry-Breaking\\nin Sparse Graphs. In Proceedings of the Symposium on Theory of Computing (STOC),\\npages 315–324. ACM, 1987.\\n[GS89a] Mark K. Goldberg and Thomas Spencer. A New Parallel Algorithm for the Maximal\\nIndependent Set Problem. SIAM Journal on Computing, 18(2):419–427, 1989.\\n[GS89b] Mark K. Goldberg and Thomas Spencer. Constructing a Maximal Independent Set in\\nParallel. SIAM Journal on Discrete Mathematics, 2(3):322–328, 1989.\\n[KW85] Richard M. Karp and Avi Wigderson. A Fast Parallel Algorithm for the Maximal\\nIndependent Set Problem. Journal of the ACM (JACM), 32(4):762–773, 1985.\\n[Lin87] Nathan Linial. Distributive Graph Algorithms - Global Solutions From Local Data.\\nIn the Proceedings of the Symposium on Foundations of Computer Science (FOCS),\\npages 331–335. IEEE, 1987.\\n[Lin92] Nathan Linial. Locality in Distributed Graph Algorithms. SIAM Journal on Comput-\\ning, 21(1):193–201, 1992.\\n[Lub85] Michael Luby. A Simple Parallel Algorithm for the Maximal Independent Set Problem.\\nIn Proceedings of the Symposium on Theory of Computing (STOC), pages 1–10, 1985.\\n[MRSDZ10] Yves Me´tivier, John Michael Robson, Nasser Saheb-Djahromi, and Akka Zemmari.\\nAn optimal bit complexity randomized distributed mis algorithm. In Structural Infor-\\nmation and Communication Complexity, pages 323–337. Springer, 2010.\\n[Val82] Leslie G. Valiant. Parallel Computation. In the Proceedings of the Symposium on\\nFoundations of Computer Science (FOCS), 1982.\\n13\\nA Lower Bound\\nLemma A.1. There exists an n-node graph on which the parallel randomized greedy MIS algorithm\\nwith high probability takes Ω(log n) rounds.\\nProof. Consider a graph consisting of\\n√\\nn connected components, each connected component made\\nof l+1 layers for l := logn5 as follows. The i\\nth layer for i ∈ [0, l] is a clique on 2i vertices, and there\\nis a full bipartite graph between any two consecutive layers. The remaining vertices not part of a\\nlayer are just isolate.\\nWe call a path of length l strictly increasing if the ith vertex on the path for 0 ≤ i ≤ l is in\\nlayer l − i and the of the path vertices are sorted (in the random order). We will argue that the\\nprobability that a connected component contains such a strictly increasing path is at least n−0.05.\\nConsider the layers U0, . . . , Ul of one connected component U and a random order. The prob-\\nability that Ul contains the minimum among\\n⋃l\\ni=0 Ui is at least\\n|Ul|\\n|U | ≥ 14 . Then, conditioned on\\nthe previous event, the probability of Ul−1 containing the minimum among\\n⋃l−1\\ni=0 Ui is at least\\n|Ul−1|\\n|⋃l−1\\ni=0 Ui|\\n≥ 14 . Continuing this argument, and combining the conditional probabilities, we get a\\nlower bound of\\n(\\n1\\n4\\n)l\\n= n−0.05 on the probability that there is a strictly increasing path in U .\\nSince all the\\n√\\nn connected components are independent, the probability of no such component\\ncontaining a strictly increasing path is at most\\n(\\n1− n−0.05)√n ≪ 1/polyn. Thus, with high\\nprobability, the considered graph contains such a path.\\nFinally, observe that the parallel/distributed randomized greedy MIS algorithm will take at\\nleast (l + 1)/2 rounds until it has processed such a strictly increasing path, since the algorithm\\nprocesses only 2 layers in each round.\\n14\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd10'), 'authors': 'Shahrokhi, Farhad', 'year': '2015', 'title': 'New separation theorems and sub-exponential time algorithms for packing\\n  and piercing of fat objects', 'full_text': 'ar\\nX\\niv\\n:1\\n50\\n2.\\n06\\n17\\n6v\\n1 \\n [c\\ns.C\\nG]\\n  2\\n2 F\\neb\\n 20\\n15\\nEuroCG 2012, Assisi, Italy, March 19–21, 2012\\nNew separation theorems and sub-exponential time algorithms for\\npacking and piercing of fat objects\\nFarhad Shahrokhi\\nDepartment of Computer Science and Engineering, UNT\\nP.O.Box 13886, Denton, TX 76203-3886, USA farhad@cs.unt.edu\\nAbstract\\nFor C a collection of n objects in Rd, let the pack-\\ning and piercing numbers of C, denoted by Pack(C),\\nand Pierce(C), respectively, be the largest number\\nof pairwise disjoint objects in C, and the smallest\\nnumber of points in Rd that are common to all el-\\nements of C, respectively. When elements of C are fat\\nobjects of arbitrary sizes, we derive sub-exponential\\ntime algorithms for the NP-hard problems of com-\\nputing Pack(C) and Pierce(C), respectively, that run\\nin nOd(Pack(C)\\nd−1\\nd ) and nOd(Pierce(C)\\nd−1\\nd ) time, respec-\\ntively, and O(n logn) storage. Our main tool which is\\ninteresting in its own way, is a new separation the-\\norem. The algorithms readily give rise to polyno-\\nmial time approximation schemes (PTAS) that run in\\nnO((\\n1\\nǫ )\\nd−1) time and O(n log n) storage. The results\\nfavorably compare with many related best known re-\\nsults. Specifically, our separation theorem signifi-\\ncantly improves the splitting ratio of the previous re-\\nsult of Chan, whereas, the sub-exponential time algo-\\nrithms significantly improve upon the running times\\nof very recent algorithms of Fox and Pach for packing\\nof spheres.\\n1 Introduction and Summary\\nAn effective tool in the design of divide and conquer\\ngraph algorithms is separation. The separator theo-\\nrem of Lipton and Tarjan [15],[16] asserts that any n\\nvertex planar graph can be separated into two sub-\\ngraphs with a splitting ratio of 1/3 − 2/3, that is,\\neach subgraph having at most 2n/3 vertices, by re-\\nmoving O(\\n√\\nn) vertices. Additional results for graphs\\nand geometric objects have been obtained by Alon et\\nal [3], Alber and Fiala [2], Miller et al [17], Smith and\\nWormald [18], Chan [4], Fox and Pach [8] [9],[10],[12],\\nFox, Pach and Toth [11], and Shahrokhi [19].\\n1.1 Past Related Results\\nThroughout this paper C denotes a finite collection\\nof subsets of Rd of cardinality n. Miller et al [17]\\nproved that given C a set of spheres in Rd, where\\nd is fixed and each point is common to only a con-\\nstant number of spheres, there is a sphere S in Rd,\\nso that at most d+1\\nd+2n of the spheres in C are entirely\\ninside of S, at most d+1\\nd+2n are entirely outside, and at\\nmost Od(n\\nd−1\\nd ) intersect the boundary of S. Smith\\nand Wormald [18], among other results, derived vari-\\nations of this fundamental result, where the separator\\nis a box, and hence, reduced the splitting ratio to\\n1/3− 2/3 in any dimension d. Nonetheless, their re-\\nsult required *disjointness* assumption of the spheres,\\nwhich could be weakened to the assumption that there\\nis a very small overlapping among the objects. Since\\nthe intersection graphs of many geometric objects ex-\\nhibit a large vertex connectivity, it is impossible to\\nseparate them with the removal of a small number of\\nvertices. Consequently, researchers, have focused on\\nseparating intersection graphs of geometric objects,\\nincluding spheres, with respect to other measures.\\nAlber and Fiala [2] studied the separation proper-\\nties of unit discs in R2 with respect to the area which\\ncoincides with the packing number, in case of unit\\ndiscs. For a set C of unit discs in R2, they derived\\na separator theorem with a constant splitting ratio,\\nused it to compute the Pack(C) in nO(\\n√\\nPack(C)) time,\\nand also derived a PTAS for computing the packing\\nnumber.\\nChan [4] studied the packing and piercing numbers\\nof fat objects of arbitrary sizes in Rd. He introduced\\nthe concept of a measure on fat objects that coincides\\nwith the packing and piercing numbers, and general-\\nized the separation result of Smith and Wormald for\\nthis measure. In simple words, Chan’s beautiful sep-\\naration theorem asserts that given a set C of n fat\\nobjects in Rd, with a sufficiently large measure µ(C),\\nthere is a cube R so that the measure of objects inside\\nof R is at most (1 − α)µ(C), the measure of objects\\noutside of R is at most (1 − α)µ(C), and the mea-\\nsure of objects intersecting R is nOd(µ(C)\\nd−1\\nd ), where\\nα is a constant whose value is about 12d+1 . Chan\\nthen used his separation theorem to design Polyno-\\nmial time Approximation Schemes (PTAS) for pack-\\ning and piercing problems that run in nO((\\n1\\nǫ )\\nd) time\\nand O(n) space. Specifically, he improved the running\\ntime of the best previous PTAS for packing problem\\nthat was due to Erlebach et al [5].\\nIn [19], we obtained a combinatorial measure sep-\\nThis is an extended abstract of a presentation given at EuroCG 2012. It has been made public for the benefit of the community and should be considered a\\npreprint rather than a formally reviewed paper. Thus, this work is expected to appear in a conference with formal proceedings and/or in a journal.\\n28th European Workshop on Computational Geometry, 2012\\naration theorem for a class of graphs containing the\\nintersection graphs of nearly fat objects in Rd, and\\nobtained sub-exponential algorithms and PTAS for\\npacking and piercing number of unit hight rectangles,\\nunit discs, and other related problems.\\nFox and Pach [8], [9], [10] have developed a series of\\nnew separator theorems for planar curves and string\\ngraphs. These results highly generalize the planar\\nseparator theorem and have striking applications in\\ncombinatorial geometry. Very recently, they discov-\\nered that their methods can be used to solve the max-\\nimum independent set problem, in sub-exponential\\ntime, for a variety of geometric graphs [12]. Specifi-\\ncally, they have just reported a sub-exponential time\\nalgorithm with the running time of 2Od\\n(\\nn\\nd\\nd+1 polylogn\\n)\\n(or nOd\\n(\\nn\\nd\\nd+1\\n)\\n), for computing the packing number of\\nspheres.\\n1.2 Our Results\\nWe present several main results. First, we present\\nan improvement to Chan’s separation theorem which\\nis obtained by combining some of his ideas with the\\noriginal work of Smith and Wormald. Specifically,\\nusing a box of aspect ratio ratio 2 in Rd, as the\\nseparator, we obtain a parametric splitting ratio of\\n1/3 − 2/3(1 + ǫ), for any 0 < ǫ ≤ 12 , for separating\\nthe measure, independent of the dimension. More-\\nover, we provide an explicit (constant) lower bound\\nfor µ(C), in terms of d and ǫ, for obtaining such a suit-\\nable separation, contrasting the result in [4] that was\\nobtained for *sufficiently large values*. We use this\\nseparation theorem to derive sub-exponential time al-\\ngorithms for packing and piercing problems, running\\nin nOd(Pack(C)\\nd−1\\nd ) and nOd(Pierce(C)\\nd−1\\nd ) time, respec-\\ntively, and O(n log n) storage. Finally, we convert\\nthese algorithms to PTAS that run in nO((\\n1\\nǫ )\\nd−1) time\\nand O(n log n) storage.\\n2 Preliminaries\\nFor a closed setB in Rd, let δB denote the boundary of\\nB, and note that δB ⊆ B. Let B¯ denote Rd−B, that\\nis, B¯ is the set of points *outside* of B. A collection\\nC of objects in Rd is fat, if for every r and size r box\\nR, we can choose a constant number c of points such\\nthat every object in C that intersects R and has size\\nat least r contains one of these points [4]. It should\\nbe noted that the class of fat objects contains spheres,\\ncubes, and boxes with bounded aspect ratios.\\nLet C be a collection of subsets of Rd, and let µ be\\na mapping that assigns non-negative values to subsets\\nof C. Chan [4] calls µ a measure, if for any A,B ⊆ C\\nthe following hold.\\n(i) µ(A) ≤ µ(B), if A ⊆ B.\\n(ii) µ(A ∪ B) ≤ µ(A) + µ(B).\\n(iii) µ(A ∪ B) = µ(A) + µ(B), if no object in A\\nintersects an object in B.\\n(iv) Given any r > 0 and any size−r box R in Rd,\\nif every object in A has size at least R and intersects\\nR, then µ(A) ≤ c, for a constant c.\\n(v) A constant-factor approximation to µ(A) can\\nbe computed in |A|O(1) time. Moreover, if µ(A) ≤ b,\\nthen, µ(A) can be computed exactly in |A|O(b) time.\\nNote that feasible solutions to the packing and\\npiercing problems gives rise to measures.\\nLet A ⊆ C, and let B be a closed subset of Rd.\\nLet AB−δB and AB¯ denote, respectively, the set of\\nall objects in A that are contained in B − δB, or are\\ncompletely inside of B, and the set of all objects in A\\nthat are contained in B¯, or are completely outside of\\nB, respectively. Let AB , and AδB , denote the set of\\nall objects in A that have their centers in B, and the\\nset of all objects in A that have a point in common\\nwith δB.\\nAspect ratio of a box in Rd is the ratio of its longest\\nside to its shortest side. Chan [4] proved the following\\nseparation theorem.\\nTheorem 1 Given a measure µ satisfying (i) − (iv)\\nand a collection C of n objects in Rd with sufficiently\\nlarge µ(C) there is a box R with µ(CR−δR), µ(CR¯) ≥\\nαµ(C), and µ(CδR) = Od(nµ(C)\\nd−1\\nd ), where α is some\\nfixed constant. Moreover, if (v) is satisfied then, R can\\nbe computed in polynomial time and linear space.\\n3 The Separation Theorem\\nTheorem 2 Let C be a set of n objects in Rd, d ≥ 2\\nand let µ be a measure on C satisfying (i)− (iv), and\\nlet 0 < ǫ ≤ 12 . If µ(C) ≥ (3.c.d\\n2.8d\\nǫ\\n)\\nd\\n, then, there is a\\nbox R so that\\nµ(CR−δR) ≤ 2\\n3\\n(1 + ǫ)µ(C), µ(CR¯) ≤\\n2\\n3\\n(1 + ǫ)µ(C),\\nand\\nµ(CδR) = Od(nµ(C)\\nd−1\\nd ).\\nProof. Let B be a minimum volume box with as-\\npect ratio at most 2 with µ(CB) ≥ (1+ǫ3 )µ(C), whose\\nside lengths are l1 ≤ l2 ≤ .... ≤ ld, ld ≤ 2l1. Let s de-\\nnote the center of B. For any m with 1 ≤ m < 2 1d , let\\nBm be the box that is the magnified version of B by\\nthe magnification factor m. Thus, Bm is a box of side\\nlengths lm1 ≤ lm2 ≤ ... ≤ lmd , lmd ≤ 2lm1 , that has center\\ns, and contains B. Note that lmi = mli < 2\\n1\\nd li, for\\ni = 1, 2, ..., d, and hence the volume of Bm is strictly\\nless that 2 times the volume of B. By cutting Bm,\\nEuroCG 2012, Assisi, Italy, March 19–21, 2012\\nin the middle of its longest side, we can decompose B\\ninto two boxes Bim, i = 1, 2, of aspect ratio at most\\ntwo, each having a volume strictly smaller than vol-\\nume of B. Thus, µ(CBim) < (1+ǫ3 )µ(C), i = 1, 2, since\\nB has the minimum volume. Consequently, using (ii),\\nwe deduce that µ(CBm) < 23 (1 + ǫ)µ(C). Therefore,\\nµ(CBm−δBm) < 23 (1 + ǫ)µ(C). Next, let Cm be a cube\\nof side length lmd having center s, and note that area\\nof δCm is 2.dl\\nm\\nd\\nd−1 < 2\\n2d−1\\nd .d.ld\\nd−1 < 4.d.ld\\nd−1. Now,\\nlet l = ld\\n8µ(C)\\n1\\nd\\n, and note that δCm , and hence δBm\\ncan be covered with at most d.8d.µ(C) d−1d cubes of\\nsize l. Let C1 denote the set of all objects in C of\\nsize at least l, and let 1 ≤ m < 2 1d . Then, by (iv),\\nµ(C1δBm) ≤ c.d.8d.µ(C)\\nd−1\\nd . Similarly, let C2 denote\\nthe set of all objects in C of size strictly smaller than\\nl. We need the following claim to finish the proof.\\nClaim. Let 1 ≤ m1 < m2 < 2 1d , with m2 −m1 ≥\\n1\\nµ(C)\\n1\\nd\\n, let A1 ∈ C2δBm1 , and let A2 ∈ C\\n2\\nδBm2\\n. Then,\\nA1 ∩ A2 = ∅.\\nJustification. For any x, y ∈ Rd, let\\ndistance(x, y) denote the distance between x and y.\\nNote that m2 − m1 ≥ 1\\nµ(C)\\n1\\nd\\nimplies that for any\\ntwo points a ∈ δBm2 and b ∈ δBm1 , we must have\\ndistance(a, b) ≥ l1\\n2µ(C)\\n1\\nd\\n≥ ld\\n4µ(C)\\n1\\nd\\n≥ 2.l. Assume to\\nthe contrary that A1 ∩ A2 6= ∅, and let x ∈ A1 ∩ A2.\\nLet x1 ∈ A1 ∩ δBm1 , and let x2 ∈ A2 ∩ δBm2 . Note\\nthat distance(x, x1) < l and distance(x, x2) < l, and\\nthus, distance(x1, x2) < 2.l which is a contradiction.\\n\\x03\\nNow use the claim and employ (ii) to conclude that\\n⌊(2\\n1\\nd−1)µ(C)\\n1\\nd ⌋∑\\nj=0\\nµ(C2δB\\n1+j/µ(C)\\n1\\nd\\n) ≤ µ(C).\\nIt follows that there is a j so that for m∗ = 1 +\\nj\\nµ(C)\\n1\\nd\\n, we have µ(C2δBm∗ ) ≤\\nµ(C)\\nd−1\\nd\\n2\\n1\\nd−1\\n≤ d2µ(C) d−1d ,\\nsince, 1\\n2\\n1\\nd−1\\n≥ 1\\nd2\\n. We conclude that µ(CδBm∗ ) ≤\\nc.d28d.nµ(C)\\nd−1\\nd . Now let R = Bm∗ , and to finish\\nthe proof note that for µ(C) ≥ (3c.d2.8d\\nǫ\\n)\\nd\\n, we have\\nµ(CδR) ≤ ǫµ(CR)3 . \\x03.\\n4 Sub-Exponential Time Algorithms\\nOur next result is the following.\\nTheorem 3 Let C be a set of n fat objects in Rd,\\nd ≥ 2, then Pack(C) and Pierce(C) can be computed\\nin nO(Pack(C)\\nd−1\\nd ) and nO(Pierce(C)\\nd−1\\nd ), respectively,\\nand O(n log n) storage.\\nProof. We provide the details for computing packing\\nnumber; The details relevant to the computation of\\npiercing number are similar, but slightly different. We\\nuse the following recursive algorithm adopted from\\n[16] and tailored to our needs, which we previously\\nutilized in [19].\\nStep 1. Determine an approximate solution µ to\\nthe packing number using (iv). Choose a separation\\nparameter α = (1 + ǫ) for Theorem 2, and let C =\\nC(d) be the Lower bound on µ(C) in Theorem 2. Test\\nusing (v) to determine if Pack(C) ≤ C. If so, then\\ncompute a solution in O(nC) time and return it. If\\nnot, proceed to the recursive step.\\nStep 2 (Recursive Step). Find box R by applying\\nTheorem 2 to µ. For any independent set of objects I\\nin CδB compute Pack(CB−N(I)), Pack(CB¯−N(I)),\\nand return\\nmax\\nI\\n{Pack(CB −N(I) + Pack(CB¯ −N(I)) + |I|}\\nwhere the maximum is taken overall independent sets\\nof objects I in CδB.\\nIt is easy to verify that the algorithm computes\\nPack(C) correctly. For the running time, let T (n, p)\\ndenote the execution time of the algorithm on an in-\\nstance of the problem with Pack(C) = p. Note that,\\nT (n, p) ≤ nO(p\\nd−1\\nd )T (n, ((1 − α)p), if p ≥ b; Other-\\nwise T (n, p) ≤ O(nb). It is not difficult to verify that\\nT (n, p) = nO(p\\nd−1\\nd ) as claimed. \\x03\\n5 Approximation Algorithms\\nThe algorithms in the previous section gives rise to\\npolynomial time approximation schemes (PTAS) for\\nboth of stated problems with nO((\\n1\\nǫ )\\nd−1) time and\\nO(n logn) storage. For the PTAS, one can slightly\\nmodify the original divide and conquer approach in\\n[15].\\nSpecifically, one must first obtain an constant time\\napproximation solution to the problem using (v), use\\nit to define the measure µ, and apply the separation\\ntheorem, or Theorem 2, to this µ. In the recursive\\nstep, the divide and conquer algorithm stops when the\\nvalue of the measure is *small*. That is, if the value\\nof the measure is O((1\\nǫ\\n)\\n2\\n), then an exact solution is\\ncomputed by the application of our sub-exponential\\ntime algorithm. The claims concerning the running\\ntime, storage, and quality of the approximation are\\neasy to verify.\\nReferences\\n[1] Agarwal, P.K., Kreveld, M., Suri, S., Label place-\\nment by maximum independent sets in rectan-\\n28th European Workshop on Computational Geometry, 2012\\ngles, Comput. Geometry:Theory and Appl. 11(3-\\n4) 209-218, 1998.\\n[2] Alber J., Fiala J., Geometric Separation and Ex-\\nact Solutions for the Parameterized Independent\\nSet Problem on Disk Graphs,Journal of Algo-\\nrithms, Volume 52 , Issue 2, 134 - 151, 2004\\n[3] N. Alon, P. Seymour, and R. Thomas, A sep-\\narator theorem for graphs with an excluded\\nminor and its applications, Proceedings 22nd\\nACM Symposium on the Theory of Computing,\\nSTOC90, 1990, 293-299.\\n[4] Chan T., Polynomial-time approximation\\nschemes for packing and piercing fat objects ,\\nJournal of Algorithms, 46(2), 178 - 189, 2003.\\n[5] Erlebach T., Jansen K. and Seidel E.,\\nPolynomial-time approximation schemes for\\ngeometric graphs. Proc. 12th ACM-SIAM\\nSymposium on Discrete Algorithms (SODA’01),\\n671-679, 2001.\\n[6] Fox J., Pach J., A separator theorem for\\nstring graphs and its applications, Combina-\\ntorics, Probability and Computing, 2009.\\n[7] Fox J., Pach J., Separator theorems and Turn-\\ntype results for planar intersection graphs, Ad-\\nvances in Mathematics 219, 1070-1080, 2008.\\n[8] Fox J., Pach J., A separator theorem for\\nstring graphs and its applications, Combina-\\ntorics, Probability and Computing 19(2010), 371-\\n390.\\n[9] Fox J., Pach J., Separator theorems and Turn-\\ntype results for planar intersection graphs, Ad-\\nvances in Mathematics 219, 1070-1080, 2008.\\n[10] Fox J., Pach J., Coloring kk-free intersection\\ngraphs of geometric objects in the plane, Pro-\\nceedings of the twenty-fourth annual symposium\\non Computational geometry, 346-354, 2008.\\n[11] Fox J, Pach J, To´th C.D, Intersection patterns\\nof curves, J. London Math. Soc. (2008)\\n[12] J. Fox and J. Pach, Computing the independence\\nnumber of intersection graphs. SODA 2011, 1161-\\n1165.\\n[13] Harry B. Hunt III, Marathe M. V., Radhakrish-\\nnan V., Ravi S. S, Rosenkrantz D. J., and Stearns\\nR. E.. A Unified Approach to Approximation\\nSchemes for NP- and PSPACE-Hard Problems\\nfor Geometric Graphs. Journal of Algorithms,\\n26, 135-149, 1996.\\n[14] Hochbaum D.S., Maass W., Approximation\\nSchemes for Covering and Packing Problems in\\nImage Processing and VLSI. Journal of the As-\\nsociation for Computing Machinery, 32(1), 130-\\n136, 1985.\\n[15] Lipton, R.J., Tarjan, R.E., Applications of a\\nplanar separator theorem, SIAM J. Comput.,\\n9(3),615-628, 1980.\\n[16] Lipton, R.J., Tarjan R.E, A separator theorem\\nfor planar graphs, SIAM Journal on Applied\\nMathematics 36, 1979, 177-189. .\\n[17] Miller, G.L., Teng, S., Thurston W., and Vava-\\nsis, S. A., Separators for Sphere-Packings and\\nNearest Neighborhood graphs, JACM, 44(1), 1-\\n29, 1997\\n[18] Smith and N.C. Wormald, Geometric separator\\ntheorems and applications. In 39th Annual Sym-\\nposium on Foundations of Computer Science:\\nFOCS ’98, pages 232-243, Palo Alto, CA, 1998.\\n[19] Shahrokhi F., A New Separation Theorem with\\nGeometric Applications, EuroCG2010.\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd11'), 'authors': 'Backurs, Arturs, Indyk, Piotr', 'year': '2015', 'title': 'Edit Distance Cannot Be Computed in Strongly Subquadratic Time (unless\\n  SETH is false)', 'full_text': 'ar\\nX\\niv\\n:1\\n41\\n2.\\n03\\n48\\nv4\\n  [\\ncs\\n.C\\nC]\\n  1\\n5 A\\nug\\n 20\\n17\\nEdit Distance Cannot Be Computed\\nin Strongly Subquadratic Time\\n(unless SETH is false)∗\\nArturs Backurs†\\nMIT\\nPiotr Indyk‡\\nMIT\\nAbstract\\nThe edit distance (a.k.a. the Levenshtein distance) between two strings is defined as the\\nminimum number of insertions, deletions or substitutions of symbols needed to transform one\\nstring into another. The problem of computing the edit distance between two strings is a\\nclassical computational task, with a well-known algorithm based on dynamic programming.\\nUnfortunately, all known algorithms for this problem run in nearly quadratic time.\\nIn this paper we provide evidence that the near-quadratic running time bounds known for\\nthe problem of computing edit distance might be tight. Specifically, we show that, if the edit\\ndistance can be computed in time O(n2−δ) for some constant δ > 0, then the satisfiability\\nof conjunctive normal form formulas with N variables and M clauses can be solved in time\\nMO(1)2(1−ǫ)N for a constant ǫ > 0. The latter result would violate the Strong Exponential Time\\nHypothesis, which postulates that such algorithms do not exist.\\n∗A preliminary version of this paper appeared in Proceedings of the Forty-Seventh Annual ACM Symposium on\\nTheory of Computing, 2015.\\n†\\nbackurs@mit.edu\\n‡\\nindyk@mit.edu\\n1 Introduction\\nThe edit distance (a.k.a. the Levenshtein distance) between two strings is defined as the minimum\\nnumber of insertions, deletions or substitutions of symbols needed to transform one string into\\nanother. The distance and its generalizations have many applications in computational biology,\\nnatural language processing and information theory. The problem of computing the edit distance\\nbetween two strings is a classical computational task, with a well-known algorithm based on the\\ndynamic programming. Unfortunately, that algorithm runs in quadratic time, which is prohibitive\\nfor long sequences1. A considerable effort has been invested into designing faster algorithms, either\\nby assuming that the edit distance is bounded, by considering the average case or by resorting to\\napproximation2. However, the fastest known exact algorithm, due to [MP80], has a running time\\nof O(n2/ log2 n) for sequences of length n, which is still nearly quadratic.\\nIn this paper we provide evidence that the (near)-quadratic running time bounds known for this\\nproblem might, in fact, be tight. Specifically, we show that if the edit distance can be computed\\nin time O(n2−δ) for some constant δ > 0, then the satisfiability of conjunctive normal form (CNF)\\nformulas with N variables and M clauses can be solved in time MO(1)2(1−ǫ)N for a constant ǫ >\\n0. The latter result would violate the Strong Exponential Time Hypothesis (SETH), introduced\\nin [IP01, IPZ01], which postulates that such algorithms do not exist3. The rationale behind this\\nhypothesis is that, despite decades of research on fast algorithms for satisfiability and related\\nproblems, no algorithm was yet shown to run in time faster than 2N(1−o(1)). Because of this\\nstate of affairs, SETH has served as the basis for proving conditional lower bounds for several\\nimportant computational problems, including k-Dominating Set [PW10], the diameter of sparse\\ngraphs [RVW13], local alignment [AVWW14], dynamic connectivity problems [AVW14], and the\\nFrechet distance computation [Bri14]. Our paper builds on these works, identifying a new important\\nmember of the class of “SETH-hard” problems.\\nOur techniques and related work This work has been stimulated by the recent result of\\nKarl Bringmann [Bri14], who showed an analogous hardness result for computing the Frechet\\ndistance4, and listed SETH-hardness of edit distance as an open problem. There are notable\\nsimilarities between the edit distance and the Frechet distance. In particular, both can be computed\\nin quadratic time, via dynamic programming over an n × n table T where each entry T [i, j] holds\\nthe distance between the first i elements of the first sequence and the first j elements of the second\\nsequence. Furthermore, in both cases each entry T [i, j] can be computed locally given T [i, j − 1],\\nT [i−1, j] and T [i−1, j−1]. The key difference between the two distances is that while the recursive\\nformula for the Frechet distance uses the max function, the formula for the edit distance involves\\nthe sum. As a result, the Frechet distance is effectively determined by a single pair of sequence\\nelements, while the edit distance is determined by many pairs of elements. As we describe below,\\n1For example, the analysis given in [Fri08] estimates that aligning human and mouse genomes using this approach\\nwould take about 95 years.\\n2There is a rather large body of work devoted to edit distance algorithms and we will not attempt to list all\\nrelevant works here. Instead, we refer the reader to the survey [Nav01] for a detailed overview of known exact and\\nprobabilistic algorithms, and to the recent paper [AKO10] for an overview of approximation algorithms.\\n3Technically, our results relies on an even weaker conjecture. See Preliminaries for more details.\\n4Given two sequences of points P1 and P2, the Frechet distance between them is defined as the minimum, over\\nall monotone traversals of P1 and P2, of the largest distance between the corresponding points at any stage of the\\ntraversal.\\n1\\nthis makes the reduction to edit distance much more subtle.5\\nOur result is obtained by a reduction from the Orthogonal Vectors Problem, which is defined as\\nfollows. Given two sets A,B ⊆ {0, 1}d such that |A| = |B| = N , the goal is to determine whether\\nthere exists x ∈ A and y ∈ B such that the dot product x · y =\\n∑d\\nj=1 xjyj (taken over reals) is\\nequal to 0. It is known [Wil05] that an O(dO(1) ·N2−δ)-time algorithm for the Orthogonal Vectors\\nProblem would imply that SETH is false (even in the setting d = ω(log n)). Therefore, in what\\nfollows we focus on reducing the Orthogonal Vectors Problem to the Edit Distance problem.\\nThe first step of our reduction mimics the approaches in [Bri14] (as well as [AVWW14]). In\\nparticular, each x ∈ A and y ∈ B is assigned a “gadget” sequence. Then, the gadget sequences for\\nall a ∈ A are concatenated together to form the first input sequence, and the gadget sequences for\\nall b ∈ B are concatenated to form the second input sequence. The correctness of the reduction is\\nproven by showing that:\\n• If there is a pair of orthogonal vectors x ∈ A and y ∈ B, then one can traverse the two\\nsequences in a way that the gadgets assigned to x and y are aligned, which implies that the\\ndistance induced by this traversal is “small”.\\n• If there is no orthogonal pair, then no such traversal exists, which implies that the distance\\ninduced by any traversal is “large”.\\nThe mechanics of this argument depends on the specific distance function. In the case of Frechet\\ndistance, the output value is determined by the maximum distance between the aligned elements,\\nso it suffices to show that the distance between two vector gadgets is smaller than C if they are\\northogonal and at least C if they are not, for some value of C. In contrast, edit distance sums up\\nthe distances between all aligned gadgets (as well as the costs of insertions and deletions used to\\ncreate the alignment), which imposes stronger requirements on the construction. Specifically, we\\nneed to show that if two vectors x and y are not orthogonal, i.e., they have at least one overlapping\\n1, then the distance between their gadgets is equal to C, not just at least C. Since we need to\\nensure that the distance between two gadgets cannot grow in the number of overlapping 1s, our\\ngadget design and analysis become more complex.\\nFortunately, the edit distance is expressive enough to support this functionality. The basic idea\\nbehind the gadget construction is to use the fact that the edit distance between two gadget strings,\\nsay V G1 (from the first sequence) and V G2 (from the second sequence), is the minimum cost over\\nall possible alignments between V G1 and V G2. Specifically, we construct gadgets that allow two\\nalignment options. The first option results in a cost that is linear in the number of overlapping\\n1s of the corresponding vectors (this is easily achieved by using substitutions only). On the other\\nhand, the second “fallback” option has a fixed cost (say C1) that is slightly higher than the cost of\\nthe first option when no 1s are overlapping (say, C0). Thus, by taking the minimum of these two\\noptions, the resulting cost is equal to C0 when the vectors are orthogonal and equal to C1 (> C0)\\notherwise, which is what is needed. See Theorems 1 and 2 for the details of the construction.\\nFurther developments Following this work, two recent publications showed multiple results\\ndemonstrating conditional hardness of the edit distance, the longest common subsequence problem\\n(LCS), dynamic time warping problem and other similarity measures between sequences [ABVW15,\\n5This also means that our hardness argument does not extend to the approximate edit distance computation, in\\ncontrast to the argument in [Bri14].\\n2\\nBK15]. Among other results, [BK15] showed hardness of computing the edit distance over the bi-\\nnary alphabet, which improves over the alphabet size of 7 required for our reduction. In another\\ndevelopment, [AHVWW16] showed that the quadratic hardness of LCS and edit distance computa-\\ntion can be based on a weaker (and therefore more plausible) assumption than SETH, by replacing\\nCNF formulas with more general circuits.\\n2 Preliminaries\\nEdit distance For any two sequences x and y over an alphabet Σ, the edit distance EDIT(x, y)\\nis equal to the minimum number of symbol insertions, symbol deletions or symbol substitutions\\nneeded to transform x into y. It is well known that the EDIT function induces a metric; in\\nparticular, it is symmetric and satisfies the triangle inequality.\\nIn the remainder of this paper we will use use an equivalent definition of EDIT that will make\\nthe analysis of our reductions more convenient.\\nObservation 1. For any two sequences x, y, EDIT(x, y) is equal to the minimum, over all sequences\\nz, of the number of deletions and substitutions needed to transform x into z and y into z.\\nProof. It follows directly from the metric properties of the edit distance that EDIT(x, y) is equal to\\nthe minimum, over all sequences z, of the number of insertions, deletions and substitutions needed\\nto transform x into z and y into z. Furthermore, observe that if, while transforming x, we insert a\\nsymbol that is later aligned with some symbol of y, we can instead delete the corresponding symbol\\nin y. Thus, it suffices to allow deletions and substitutions only.\\nDefinition 1. We define the following similarity distance between sequences P1 and P2 and we call\\nit the pattern matching distance between P1 and P2.\\nPATTERN(P1, P2) = min\\nx is a contiguous\\nsubsequence of P2\\nEDIT(P1, x).\\nFor a symbol a and an integer i we use ai to denote symbol a repeated i times.\\nOrthogonal Vectors Problem The Orthogonal Vectors Problem (OVP) is defined as follows:\\ngiven two sets A,B ⊆ {0, 1}d such that |A| = |B| = N , determine whether there exists x ∈ A and\\ny ∈ B such that the dot product x · y =\\n∑d\\nj=1 xjyj (taken over reals) is equal to 0. An alternative\\nformulation of this problem is: given two collections of N sets each, determine if there is a set in\\nthe first collection that does not intersect a set from the second collection.6\\nThe Orthogonal Vectors Problem has an easy O(N2d)-time solution. The currently best known\\nalgorithm for this problem runs in time n2−1/O(log c), where c = d/ log n [CW16, AWY15]. The\\nOrthogonal Vector Conjecture [Wil05, Wil15] postulates that there is no strongly sub-quadratic7\\nrunning time algorithm for OVP. Moreover, it is known that any algorithm for this problem with\\nstrongly sub-quadratic running time would also yield a more efficient algorithm for CNF-SAT,\\nbreaking SETH [Wil05]. Thus, in what follows, we focus on reducing the Orthogonal Vectors\\nProblem to EDIT.\\n6Equivalently, after complementing sets from the second collection, determine if there is a set in the first collection\\nthat is contained in a set from the second collection.\\n7“Strongly sub-quadratic” means dO(1) ·N2−δ for some constant δ > 0.\\n3\\nSimplifying assumption We assume that in the Orthogonal Vectors Problem, for all vectors\\nb ∈ B, b1 = 1, that is, the first coordinate of any vector b ∈ B is equal to 1. We can make this\\nassumption w.l.o.g. because we can always add a 1 to the beginning of each b ∈ B, and add a 0 to\\nthe beginning of each a ∈ A.\\n3 Reductions\\n3.1 Vector gadgets\\nWe now describe vector gadgets as well as provide some intuition behind the construction.\\nWe will construct sequences over an alphabet Σ = {0, 1, 2, 3, 4}.\\nWe start by defining an integer parameter l0 = 1000 · d, where d is the dimensionality of the\\nvectors in the Orthogonal Vectors Problem. We then define coordinate gadget sequences CG1 and\\nCG2 as follows. For integer x ∈ {0, 1} we define\\nCG1(x) :=\\n{\\n2l0 0 1 1 1 2l0 if x = 0;\\n2l0 0 0 0 1 2l0 if x = 1,\\nCG2(x) :=\\n{\\n2l0 0 0 1 1 2l0 if x = 0;\\n2l0 1 1 1 1 2l0 if x = 1.\\nThe coordinate gadgets were designed so that they have the following properties. For any two\\nintegers x1, x2 ∈ {0, 1},\\nEDIT(CG1(x1),CG2(x2)) =\\n{\\n1 if x1 · x2 = 0;\\n3 if x1 · x2 = 1.\\nFurther, we define another parameter l1 = (1000 · d)\\n2. We use Σ-style notation to denote the\\nconcatenation of sequences. For example, given d sequences s1, . . . , sd, we denote the concatenation\\ns1 . . . sd by ©i∈[d]si. For vectors a, a\\n′, b ∈ {0, 1}d, we define the vector gadget sequences as\\nVG1(a, a\\n′) = Z1L(a)V0R(a\\n′)Z2 and VG2(b) = V1D(b)V2,\\nwhere\\nV1 = V2 = V0 = 3\\nl1 , Z1 = Z2 = 4\\nl1 ,\\nL(a) =©i∈[d]CG1(ai), R(a\\n′) =©i∈[d]CG1(a\\n′\\ni), D(b) =©i∈[d]CG2(bi).\\nIn what follows we skip the arguments of L, R and D. We denote the length of L, R and D by\\nl = |L| = |R| = |D| = d(4 + 2l0).\\nWe visualize the defined vector gadgets in Figure 1.\\nIntuition behind the construction Before going into the analysis of the gadgets in Section\\n3.1.1, we will first provide some intuition behind the construction. Given three vectors a, a′, b ∈\\n{0, 1}d, we want that EDIT(VG1(a, a\\n′),VG2(b)) grows linearly in the minimum of a · b and a\\n′ · b.\\nMore precisely, we want that\\nEDIT(VG1(a, a\\n′),VG2(b)) = C + t ·min(a · b, a\\n′ · b), (1)\\n4\\nZ1 = 4\\nl1\\nL =©i∈[d]CG1(ai)\\nV0 = 3\\nl1\\nR =©i∈[d]CG1(a\\n′\\ni)\\nZ2 = 4\\nl1\\nV1 = 3\\nl1 V2 = 3\\nl1\\nD =©i∈[d]CG2(bi)\\nVG1(a, a\\n′)\\nVG2(b)\\nFigure 1: A visualisation of the vector gadgets. A black rectangle denotes a run of 3s, while a white rectangle denotes\\na run of 4s. A gray rectangle denotes a sequence that contains 0s, 1s and 2s. A short rectangle denotes a sequence\\nof length l, while a long one denotes a sequence of length l1.\\nwhere the integers C, t > 0 are functions of d only. In fact, we will have that t = 2. To realize this,\\nwe construct our vector gadgets VG1 and VG2 such that there are only two possibilities to achieve\\nsmall edit distance. In the first case, the edit distance grows linearly in a · b. In the second case,\\nthe edit distance grows linearly in a′ · b. Because the edit distance is equal to the minimum over all\\npossible alignments, we take the minimum of the two inner products. After taking the minimum,\\nthe edit distance will satisfy the properties stated in (1). More precisely, we achieve the minimum\\nedit distance cost between VG1 and VG2 by following one of the following two possible sequences\\nof operations:\\n• Case 1: Delete Z1 and L. Substitute Z2 with V2. This costs C\\n′ := l1+d·(2l0+4)+l1. Transform\\nR and D into the same sequence by transforming the corresponding coordinate gadgets into\\nthe same sequences. By the construction of the coordinate gadgets, the cost of this step is\\nd+2·(a′·b). Therefore, this case corresponds to edit distance cost C ′+d+2·(a′·b) = C+2·(a′·b).\\n• Case 2: Delete R and Z2. Substitute Z1 with V1. This costs C\\n′. Transform L and D into the\\nsame sequence by transforming the corresponding coordinate gadgets. Similarly as before,\\nthe cost of this step is d + 2 · (a · b). Therefore, this case corresponds to edit distance cost\\nC ′ + d+ 2 · (a · b) = C + 2 · (a · b).\\nTaking the minimum of these two cases yields the desired formula (1).\\nIn the reduction given in Section 3.2, we ensure that the dot product a′ · b is always equal to 1.\\nAs a result we have that EDIT(VG1(a),VG2(b)) is small (equal to C0) if the vectors a and b are\\northogonal, and is large (equal to C1) otherwise. That is:\\nEDIT(VG1(a),VG2(b)) =\\n{\\nC0 if a · b = 0\\nC1 otherwise\\n(2)\\nfor C1 > C0. This property is crucial for our construction, as it guarantees that the sum of several\\nterms EDIT(VG1(a),VG2(b)) is smaller than some threshold if and only if a · b = 0 for at least\\none pair of vectors a and b. This enables us to detect whether such a pair exists. In contrast, this\\nproperty would not hold if EDIT(VG1(a),VG2(b)) depended linearly on the value of a · b.\\n5\\n3.1.1 Properties of the vector gadgets\\nTheorem 1. For any vectors a, a′, b ∈ {0, 1}d,\\nEDIT(VG1(a, a\\n′),VG2(b)) = 2l1 + l + d+ 2 ·min\\n(\\na · b, a′ · b\\n)\\n.\\nProof. Follows from lemmas 1 and 2 below.\\nLemma 1. For any vectors a, a′, b ∈ {0, 1}d,\\nEDIT(VG1(a, a\\n′),VG2(b)) ≤ 2l1 + l + d+ 2 ·min\\n(\\na · b, a′ · b\\n)\\n.\\nProof. W.l.o.g., a · b ≤ a′ · b. We delete R and Z2 from VG1(a, a\\n′). This costs l1 + l. We transform\\nZ1LV0 into V1DV2 by using substitutions only. This costs l1 + d + 2 · (a · b). We get the upper\\nbound on the EDIT cost and this finishes the proof.\\nLemma 2. For any vectors a, a′, b ∈ {0, 1}d,\\nEDIT(VG1(a, a\\n′),VG2(b)) ≥ 2l1 + l + d+ 2 ·min\\n(\\na · b, a′ · b\\n)\\n=: X.\\nProof. Consider an optimal transformation of VG1(a, a\\n′) and VG2(b) into the same sequence. Every\\nsymbol (say s) in the first sequence is either substituted, preserved or deleted in the process. If\\na symbol is not deleted but instead is preserved or substituted by another symbol (say t), we say\\nthat s is aligned with t, or that s and t have an alignment.\\nWe state the following fact without a proof.\\nFact 1. Suppose we have two sequences x and y of symbols. Let i1 < j1 and i2 < j2 be four positive\\nintegers. If xi1 is aligned with yj2, then xj1 cannot be aligned with yi2 .\\nFrom now on we proceed by considering three cases.\\nCase 1. The subsequence D has alignments with both Z1L and RZ2. In this case, the cost\\ninduced by symbols from Z1 and Z2, and V0 is l1 for each one of these sequences because the\\nsymbols must be deleted or substituted. This implies that EDIT(VG1(a),VG2(b)) ≥ 3l1, which\\ncontradicts an easy upper bound. We have an upper bound EDIT(VG1(a),VG2(b)) ≤ 2l1 + 3l,\\nwhich is obtained by deleting L, R, D, Z1 and replacing Z2 with V2 symbol by symbol. Remember\\nthat l0 = 1000 · d and l1 = (1000 · d)\\n2, and l = d(4 + l0). Thus, l1 ≥ 3l and the lower bounds\\ncontradicts the upper bound. Therefore, this case cannot occur.\\nCase 2. D does not have any alignments with Z1L. We will show that, if this case happens,\\nthen\\nEDIT(VG1(a, a\\n′),VG2(b)) ≥ 2l1 + l + d+ 2 ·\\n(\\na′ · b\\n)\\n.\\nWe start by introducing the following notion. Let v and z be two sequences that decompose\\nas v = xV and z = yZ. Consider two sequences T and R of deletions and substitutions that\\ntransform v into u and z into u, respectively. An operation in T or R is called internal to V and\\nZ if it is either a (1) deletion of a symbol in V or Z, or (2) a substitution of a symbol in V so that\\nit aligns with a symbol in Z, or vice versa. All other operations, including substitutions that align\\nwith symbols in V (Z, resp.) to those outside of Z (V , resp.) are called external to V and Z.\\nWe state the following fact without a proof.\\n6\\nFact 2. Let xV and yZ be sequences such that V = 4t, Z = 3t and x and y are arbitrary sequences\\nover an arbitrary alphabet not including 3 or 4. Consider EDIT(xV, yZ) and the corresponding\\noperations minimizing the distance. Among those operations, the number of operations that are\\ninternal to V and Z is at least t.\\nGiven that |Z2| = |V2| = l1 and Z2 consists only of 4s and V2 consists of only 3s, Fact 2 implies\\nthat the number of operations that are internal to Z2 and V2 is at least S1 := l1.\\nBecause D does not have any alignments with Z1L, we must have that every symbol in Z1L gets\\ndeleted or substituted. Thus, the total contribution from symbols in Z1L to an optimal alignment\\nis S2 := |Z1L| = l1 + l. Now we will lower bound the contribution to an optimal alignment from\\nsymbols in sequences R and D. First, observe that both R and D have d runs of 1s. We consider\\nthe following two sub-cases.\\nCase 2.1. There exist i, j ∈ [d] with i 6= j such that the ith run in D has alignments with the\\njth run in R. The number of symbols of type 2 to the right of the ith run in D and the number\\nof symbols of type 2 to the right of the jth run in R differ by at least 2l0. Therefore, the induced\\nEDIT cost of symbols of type 2 in R and D is at least 2l0 ≥ d + 2 · (a\\n′ · b) =: S3, from which we\\nconclude that\\nEDIT(VG1(a, a\\n′),VG2(b)) ≥ S1 + S2 + S3\\n= l1 + (l1 + l) +\\n(\\nd+ 2 ·\\n(\\na′ · b\\n))\\n= X.\\nIn the inequality we used the fact that the contributions from S1, S2 and S3 are disjoint. This\\nfollows from the definitions of the quantities. More precisely, the contribution from S1 comes from\\noperations that are internal to V2 and Z2. Thus, it remains to show that the contributions from S2\\nand S3 are disjoint. This follows from the fact that the contribution from S2 comes from symbols\\nZ1L and the assumption that D does not have any alignments with Z1L.\\nCase 2.2. (The complement of Case 2.1.) Consider any i ∈ [d]. If a symbol of type 1 from the\\nith run in D is aligned with a symbol of type 1 in R, then the symbol of type 1 comes from the ith\\nrun in R. Define the set P as the set of all numbers i ∈ [d] such that the ith run of 1s in D has\\nalignment with the ith run of 1s in R.\\nFor all i ∈ P , the ith run in R aligns with the ith run in D. By the construction of coordinate\\ngadgets, the ith run in R and D incur EDIT cost ≥ 1 + 2a′ibi.\\nFor all i 6∈ P , the ith run in D incurs EDIT cost at least 2 (since there are at least two symbols\\nof type 1). Similarly, the ith run in R incurs EDIT cost at least 1 (since there is at least one symbol\\nof type 1). Therefore, for every i 6∈ P , the ith run in R and D incur EDIT cost ≥ 1+2 ≥ 1+2a′ibi.\\nWe get that the total contribution to the EDIT cost from the d runs in D and the d runs in R\\nis ∑\\ni∈P\\n(\\n1 + 2a′ibi\\n)\\n+\\n∑\\ni∈[d]\\\\P\\n3 ≥\\nd∑\\ni=1\\n(\\n1 + 2a′ibi\\n)\\n= d+ 2 ·\\n(\\na′ · b\\n)\\n=: S4.\\nWe conclude:\\nEDIT(VG1(a, a\\n′),VG2(b)) ≥ S1 + S2 + S4\\n= l1 + (l1 + l) +\\n(\\nd+ 2 ·\\n(\\na′ · b\\n))\\n= X.\\nWe used the fact that the contributions from S1, S2 and S4 are disjoint. The argument is\\nanalogous as in the previous case.\\n7\\nCase 3. The symbols of D are not aligned with any symbols in RZ2. If this case happens, then\\nEDIT(VG1(a, a\\n′),VG2(b)) ≥ 2l1 + l + d+ 2 · (a · b) .\\nThe analysis of this case is analogous to the analysis of Case 2. More concretely, for any sequence\\nx, define reverse(x) to be the sequence y of length |x| such that yi = x|x|+1−i for all i = 1, 2, . . . , |x|.\\nNow we repeat the proof in Case 2 but for\\nEDIT(reverse(VG1(a, a\\n′)), reverse(VG2(b))).\\nThis yields exactly the lower bound that we need.\\nThe proof of the lemma follows. We showed that Case 1 cannot happen. By combining lower\\nbounds corresponding to Cases 2 and 3, we get the lower bound stated in the lemma.\\nWe set a′ := 1 0d−1, that is, a′ is a binary vector of length d such that a′1 = 1 and a\\n′\\ni = 0 for\\ni = 2, . . . , d. We define\\nV G1(a) := V G1(a, a\\n′).\\nTheorem 2. Let a ∈ {0, 1}d be any binary vector and b ∈ {0, 1}d be any binary vector that starts\\nwith 1, that is, b1 = 1. Then,\\nEDIT(VG1(a),VG2(b)) =\\n{\\nEs := 2l1 + l + d if a · b = 0;\\nEu := 2l1 + l + d+ 2 if a · b ≥ 1.\\nProof. Follows from Theorem 1 by setting a′ = 10d−1 and observing that a′ · b = 1 because\\nb1 = 1.\\n3.2 Reducing the Orthogonal Vectors Problem to PATTERN\\nWe proceed by concatenating vector gadgets into sequences.\\nWe note that the length of the vector gadgets produced by VG1 depends on the dimensionality\\nd of the vectors but not on the entries of the vectors. The same is true about VG2. We set t\\nto be the maximum of the two lengths. Furthermore, we set T = 1000d · t = Θ(d3). We define\\nVG′k(a) = 5\\nTVGk(a)5\\nT for k ∈ {1, 2}. Let f = 1d be a vector consisting of d entries equal to 1.\\nLet A and B be sets from the Orthogonal Vectors instance. By definition |A| = |B|.\\nWe define sequences\\nP1 =©a∈AVG\\n′\\n1(a),\\nP2 =\\n(\\n©\\n|A|−1\\ni=1 VG\\n′\\n2(f)\\n) (\\n©b∈BVG\\n′\\n2(b)\\n) (\\n©\\n|A|−1\\ni=1 VG\\n′\\n2(f)\\n)\\n.\\nTheorem 3. Let X := |A| ·Eu. If there are two orthogonal vectors, one from set A, another from\\nset B, then PATTERN(P1, P2) ≤ X − (Eu − Es); otherwise we have PATTERN(P1, P2) = X.\\nProof. Follows from Lemmas 3 and 4 below.\\nLemma 3. If there are two orthogonal vectors, one from A, another from B, then\\nPATTERN(P1, P2) ≤ X − (Eu − Es) = X − 2.\\n8\\nProof. Let a ∈ A and b ∈ B be vectors such that a · b = 0.\\nWe can choose a contiguous subsequence s of P2 consisting of a sequence of |A| vector gadgets\\nVG′2 such that s has the following property: transforming the vector gadgets VG\\n′\\n1 from P1 and\\ntheir corresponding vector gadgets VG′2 from s into the same sequence one by one as per Theorem\\n2, we achieve a cost smaller than the upper bound. We use the fact that at least one transformation\\nis cheap because a · b = 0 and we choose s so that VG′1(a) and VG\\n′\\n2(b) get transformed into the\\nsame sequence.\\nLemma 4. If there are no two orthogonal vectors, one from A, another from B, then\\nPATTERN(P1, P2) = X.\\nProof. Consider a graph (X1 ∪X2, E) with vertices x1(a) ∈ X1, a ∈ A, x2(b) ∈ X2, b ∈ B. We also\\nadd 2|A|−2 copies of x2(f) to set X2 corresponding to 2|A|−2 vectors f in sequence P2. Consider\\nan optimal transformation of P1 and a subsequence of P2 into the same sequence according to\\nDefinition 1. We connect two vertices x1(a) and x2(b) if and only if VG1(a) and VG2(b) have an\\nalignment in the transformation.\\nWe want to claim that every vector gadget VG1(a) from P1 contributes a cost of at least\\nEu to the final cost of PATTERN(P1, P2). This will give PATTERN(P1, P2) ≥ X. We consider\\nthe connected components of the graph. We will show that a connected component that has\\nr ≥ 1 vertices from X1, contributes ≥ r · Eu to the final cost of PATTERN(P1, P2). From the\\ncase analysis below we will see that these contributions for different connected components are\\nseparate. Therefore, by summing up the contributions for all the connected components, we get\\nPATTERN(P1, P2) ≥ |A| ·Eu = X.\\nConsider a connected component of the graph with at least one vertex from X1. We examine\\nseveral cases.\\nCase 1. The connected component has only one vertex from X1. Let x1(a) be the vertex.\\nCase 1.1. x1(a) is connected to more than one vertex. In this case, VG1(a) induces a cost of\\nat least 2T > Eu (this cost is induced by symbols of type 5).\\nCase 1.2. x1(a) (corresponding to vector gadget VG1(a)) is connected to only one vertex x2(b)\\n(corresponding to vector gadget VG2(b)). Let x be a contiguous substring of P2 that achieves the\\nminimum of EDIT(P1, x) (see Definition 1).\\nCase 1.2.1. The vector gadget VG2(b) is fully contained in the substring x. We claim that the\\ncontribution from symbols in the sequences VG1(a) and VG2(b) is at least EDIT(VG1(a),VG2(b)).\\nThis is sufficient because we know that EDIT(VG1(a),VG2(b)) ≥ Eu from Theorem 2. If no\\nsymbol in VG1(a) or VG2(b) is aligned with a symbol of type 5, the claim follows directly by\\napplying Theorem 2. Otherwise, every symbol that is aligned with a symbol of type 5 contributes\\ncost 1 to the final cost. The contribution from symbols in the sequences VG1(a) and VG2(b) is at\\nleast EDIT(VG1(a),VG2(b)) because we can transform the sequences VG1(a) and VG2(b) into the\\nsame sequence by first deleting the symbols that are aligned with symbols of type 5 (every such\\nalignment contributes cost 1) and then transforming the remainders of the sequences VG1(a) and\\nVG2(b) into the same sequence.\\nCase 1.2.2. The complement of Case 1.2.1. We need to consider this case because of the\\nfollowing reason. We could potentially achieve a contribution of VG1(a) to PATTERN(P1, P2) that\\nis smaller than Eu by transforming VG1(a) and a contiguous substring of VG2(b) into the same\\nstring (instead of transforming VG1(a) and VG2(b) into the same string). In the next paragraph\\nwe show that this cannot happen.\\n9\\nVG2(b) shares symbols with x and is not fully contained in x. VG2(b) must be the left-most\\n(right-most, resp.) vector gadget in x but then T left-most (right-most, resp.) symbols of type 5\\nof VG′1(a) induce a cost of at least T > Eu since the symbols of type 5 cannot be preserved and\\nmust be substituted or deleted.\\nCase 1.3. x1(a) is connected to no vertex. We get that VG1(a) induces cost of at least\\n|VG1(a)| > Eu.\\nCase 2. The connected component has r > 1 vertices x1(a) from X1. In this case, the cost\\ninduced by the vector gadgets VG1(a) corresponding to the vertices from X1 in the connected\\ncomponent is at least (r − 1) · 2T > r · Eu (this cost is induced by symbols of type 2).\\nThis finishes the argument that PATTERN(P1, P2) ≥ X.\\nIt remains to argue that we can achieve cost X (to show that PATTERN(P1, P2) ≤ X) and it\\ncan be done as in Lemma 3.\\n3.3 Reducing PATTERN to EDIT\\nWe set P ′2 := P2 and P\\n′\\n1 := 6\\n|P ′2|P16\\n|P ′2|. Remember that |A| = |B|.\\nTheorem 4. Let Y := 2·|P ′2|+|A|·Eu. If there are no two orthogonal vectors, then EDIT(P\\n′\\n1, P\\n′\\n2) =\\nY ; otherwise EDIT(P ′1, P\\n′\\n2) ≤ Y − (Eu −Es) = Y − 2.\\nProof. Follows from Lemmas 5 and 6 below.\\nLemma 5. If there are two orthogonal vectors, then\\nEDIT(P ′1, P\\n′\\n2) ≤ Y − (Eu − Es) = Y − 2.\\nProof. We transform P1 and a subsequence of P\\n′\\n2 into the same sequence as in Lemma 3. We replace\\nthe remaining prefix and suffix of P ′2 with symbols of type 6 and delete the excess of symbols of\\ntype 6 from P ′1.\\nLemma 6. If there are no two orthogonal vectors, then\\nEDIT(P ′1, P\\n′\\n2) = Y.\\nProof. We can easily check that EDIT(P ′1, P\\n′\\n2) ≤ Y as in Lemma 5. It remains to prove the opposite\\ninequality.\\nP ′1 contains 2|P\\n′\\n2| symbols of type 6. Those will incur a cost of at least 2|P\\n′\\n2|. P\\n′\\n1 has the\\nremaining subsequence P1, which will incur cost at least PATTERN(P1, P\\n′\\n2). Using Lemma 4, we\\nfinish the proof.\\nAs a result, we get the following theorem.\\nTheorem 5. If EDIT can be computed in time O(n2−δ) for some δ > 0 on two sequences of\\nlength n over an alphabet of size 7, then the Orthogonal Vectors Problem with |A| = |B| = N and\\nA,B ⊆ {0, 1}d can be solved in time dO(1) ·N2−δ.\\nProof. The proof follows immediately from Theorem 4.\\n10\\n4 Acknowledgments\\nThe authors thank Amir Abboud, Karl Bringmann, Sepideh Mahabadi, Ludwig Schmidt and the\\nreviewers for providing helpful comments. This work was supported by an IBM PhD Fellowship,\\ngrants from the NSF, the MADALGO center, and the Simons Investigator award.\\nReferences\\n[ABVW15] Amir Abboud, Arturs Backurs, and Virginia Vassilevska Williams. Tight hardness\\nresults for LCS and other sequence similarity measures. In Foundations of Computer\\nScience (FOCS), 2015 IEEE 56th Annual Symposium on, pages 59–78. IEEE, 2015.\\n[AHVWW16] Amir Abboud, Thomas Dueholm Hansen, Virginia Vassilevska Williams, and Ryan\\nWilliams. Simulating branching programs with edit distance and friends or: A\\npolylog shaved is a lower bound made. STOC, 2016.\\n[AKO10] Alexandr Andoni, Robert Krauthgamer, and Krzysztof Onak. Polylogarithmic ap-\\nproximation for edit distance and the asymmetric query complexity. In FOCS, pages\\n377–386. IEEE, 2010.\\n[AVW14] Amir Abboud and Virginia Vassilevska Williams. Popular conjectures imply strong\\nlower bounds for dynamic problems. In Foundations of Computer Science (FOCS),\\n2014 IEEE 55th Annual Symposium on, pages 434–443. IEEE, 2014.\\n[AVWW14] Amir Abboud, Virginia Vassilevska Williams, and Oren Weimann. Consequences of\\nfaster alignment of sequences. In International Colloquium on Automata, Languages,\\nand Programming, pages 39–51. Springer, 2014.\\n[AWY15] Amir Abboud, Ryan Williams, and Huacheng Yu. More Applications of the Poly-\\nnomial Method to Algorithm Design. In Proceedings of the Twenty-Sixth Annual\\nACM-SIAM Symposium on Discrete Algorithms, pages 218–230. Society for Indus-\\ntrial and Applied Mathematics, 2015.\\n[BK15] Karl Bringmann and Marvin Ku¨nnemann. Quadratic conditional lower bounds for\\nstring problems and dynamic time warping. In Foundations of Computer Science\\n(FOCS), 2015 IEEE 56th Annual Symposium on, pages 79–97. IEEE, 2015.\\n[Bri14] Karl Bringmann. Why walking the dog takes time: Frechet distance has no strongly\\nsubquadratic algorithms unless SETH fails. In Foundations of Computer Science\\n(FOCS), 2014 IEEE 55th Annual Symposium on, pages 661–670. IEEE, 2014.\\n[CW16] Timothy M Chan and Ryan Williams. Deterministic APSP, Orthogonal Vectors, and\\nMore: Quickly Derandomizing Razborov-Smolensky. In Proceedings of the Twenty-\\nSeventh Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1246–1255.\\nSociety for Industrial and Applied Mathematics, 2016.\\n[Fri08] Martin C. Frith. Large-scale sequence comparison: Spaced seeds and suffix arrays.\\nhttp://last.cbrc.jp/mcf-kyoto08.pdf, 2008.\\n11\\n[IP01] Russell Impagliazzo and Ramamohan Paturi. On the Complexity of k-SAT. Journal\\nof Computer and System Sciences, 62(2):367–375, 2001.\\n[IPZ01] Russell Impagliazzo, Ramamohan Paturi, and Francis Zane. Which Problems Have\\nStrongly Exponential Complexity? Journal of Computer and System Sciences,\\n63:512–530, 2001.\\n[MP80] William J Masek and Michael S Paterson. A faster algorithm computing string edit\\ndistances. Journal of Computer and System sciences, 20(1):18–31, 1980.\\n[Nav01] Gonzalo Navarro. A guided tour to approximate string matching. ACM computing\\nsurveys (CSUR), 33(1):31–88, 2001.\\n[PW10] Mihai Pa˘tras¸cu and Ryan Williams. On the possibility of faster SAT algorithms.\\nIn Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algo-\\nrithms, pages 1065–1075. SIAM, 2010.\\n[RVW13] Liam Roditty and Virginia Vassilevska Williams. Fast approximation algorithms for\\nthe diameter and radius of sparse graphs. In Proceedings of the forty-fifth annual\\nACM symposium on Theory of computing, pages 515–524. ACM, 2013.\\n[Wil05] Ryan Williams. A new algorithm for optimal 2-constraint satisfaction and its impli-\\ncations. Theoretical Computer Science, 348(2):357–365, 2005.\\n[Wil15] Virginia Vassilevska Williams. Hardness of easy problems: Basing hardness on popu-\\nlar conjectures such as the strong exponential time hypothesis. In Proc. International\\nSymposium on Parameterized and Exact Computation, pages 16–28, 2015.\\n12\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd12'), 'authors': 'Ahrabian, Hayedeh, Dalini, Abbas Nowzari, Ganjtabesh, Mohammad', 'year': '2019', 'title': 'Molecular solutions for double and partial digest problems in polynomial time', 'full_text': 'Computing and Informatics, Vol. 28, 2009, 599–618\\nMOLECULAR SOLUTIONS FOR DOUBLE AND\\nPARTIAL DIGEST PROBLEMS IN POLYNOMIAL TIME\\nMohammad Ganjtabesh\\nLaboratoir d’Informatique\\nEcole Ploytechnique\\nPalaiseau CEDEX, France\\n&\\nCenter of Excellence in Biomathematics\\nSchool of Mathematics, Statistics, and Computer Science\\nUniversity of Tehran, Tehran, Iran\\ne-mail: mgtabesh@ut.ac.ir\\nHayedeh Ahrabian, Abbas Nowzari-Dalini\\nCenter of Excellence in Biomathematics\\nSchool of Mathematics, Statistics, and Computer Science\\nUniversity of Tehran, Tehran, Iran\\ne-mail: {ahrabian, nowzari}@ut.ac.ir\\nManuscript received 3 October 2008; revised 7 January 2009\\nCommunicated by Ivan Plander\\nAbstract. A fundamental problem in computational biology is the construction of\\nphysical maps of chromosomes from the hybridization experiments between unique\\nprobes and clones of chromosome fragments. Double and partial digest problems\\nare two intractable problems used to construct physical maps of DNA molecules in\\nbioinformatics. Several approaches, including exponential algorithms and heuris-\\ntic algorithms, have been proposed to tackle these problems. In this paper we\\npresent two polynomial time molecular algorithms for both problems. For this rea-\\nson, a molecular model similar to Adleman and Lipton model is presented. The\\npresented operations are simple and performed in polynomial time. Our algorithms\\nare computationally simulated.\\n600 M. Ganjtabesh, H. Ahrabian, A. Nowzari-Dalini\\nKeywords: DNA computing, bioinformatics, digest problem.\\nMathematics Subject Classification 2000: 68Q15, 68W10, 68W25\\n1 INTRODUCTION\\nThe field of DNA computing was pioneered by Adleman [1] who showed the poten-\\ntial of using biomolecules for solving computational problems. He solved an instance\\nof the NP-complete Hamiltonian path problem in linear time. Later, Lipton demon-\\nstrated that Adleman’s experiment could be used to determine the NP-complete\\nsatisfiability problem [17]. The major goal of subsequent research in DNA comput-\\ning area is to develop new techniques to solve NP-complete problems that can not\\nbe solved by current electronic computers in a reasonable amount of time. In this\\nmanner, a large number of DNA algorithms have been proposed for a large class of\\nNP-complete problems [9, 17, 18, 20, 35].\\nThere are a number of feasible operations in DNA computing. Based on the\\nemployed operations, several DNA based models for computing have been intro-\\nduced. The first model is used by Adleman and Lipton, called Adleman-Lipton\\nmodel which contains the simple DNA operations [1, 2, 7, 17]. The other popular\\nmodels are: sticker model [15, 23, 35], surface-based [30], self-assembly [32] and\\nsplicing system [11, 19, 24]. These models are different in molecular operations and\\ncreating the state space of problem.\\nA human chromosome which is a DNA molecule of about 108 base pairs is too\\nlong to be studied entirely and must be broken into fragments or clones. Depending\\non the cloning technology used, the size of the clones may be as small as 3 000 base\\npairs or as large as 2 000 000 base pairs. Information is gathered from the individual\\nclones, and then the DNA is constructed by mathematically determining the posi-\\ntion of the clones. The process of reconstructing the DNA sequence by the broken\\nfragments is called Digest Problem. Digest experiment plays an important role in\\nmolecular biology. In such experiments, enzymes are used to cleave DNA molecules\\nat specific sequence patterns, the restriction sites. The resulting fragments are used\\nin many different ways to study the structure of DNA molecules. Double Digest\\nProblem (DDP ) and Partial Digest Problem (PDP ) are two famous combinatorial\\nproblems that no polynomial time algorithm with electronic computer is given for\\nthem until now [31].\\nIn the DDP , we are given the lengths of DNA fragments arising from digestion\\nexperiments with two enzymes, and we want to find a physical map of the DNA, i.e.\\nthe positions of the restriction sites of the enzymes along the DNA sequence. DDP\\nis known to be NP-Complete [13, 31]. In the PDP , we are given DNA fragment\\nlengths arising from digestion experiment with only one enzyme, and we again ask\\nfor a physical map of the DNA. Neither a proof of NP-completeness nor a polynomial\\ntime algorithm is known for PDP [6, 21, 22].\\nMolecular Solutions for DDP and PDP 601\\nIn this paper, we present two molecular algorithms for solving DDP and PDP\\nproblems in polynomial time complexity. The molecular model employed in these\\nalgorithms is similar to Adleman-Lipton model to construct solution space of DNA\\nstrands for these two combinatorial problems.\\nThe paper is organized as follows: In Section 2, two digestion problems (DDP\\nand PDP ) are defined. Our DNA computational model is given in Section 3. The\\nDNA encoding and molecular algorithm for DDP problem are presented in Sec-\\ntion 4. In Section 5, DNA encoding and molecular algorithm for PDP problem are\\npresented. In Section 6, we present a genetic algorithm for constructing error resis-\\ntance DNA sequences. The computational simulation of two algorithms are given\\nin Section 7. Finally, the conclusion is given in Section 8.\\n2 DIGESTING DNA\\nA DNA molecule is a large molecule that is composed of smaller molecules, the\\nnucleotides. There are four nucleotides, namely Adenine (A), Cytosine (C), Gua-\\nnine (G), and Thymine (T). A nuclease is an enzyme that can cleave DNA molecules\\nat specific restriction sites. This process is called digestion. If the enzyme is ap-\\nplied for long enough time, then it cuts all restriction sites in each clone, yielding\\nfragments between any two adjacent restriction sites. This process is called full or\\ncomplete digestion, in contrast to partial digestion, where only very small amount of\\nenzymes are used, we obtain all fragments between any two restriction sites (that do\\nnot need to be adjacent). Digest experiments can be used to construct physical maps\\nof DNA molecules. A physical map describes the location of markers along the DNA\\nmolecules. For constructing the physical map, we can either use a single digestion\\nby applying one enzyme or double digestion by applying two different enzymes.\\nThe fragment length resulting from a single full digestion experiment can not\\nyield any information about the ordering of the fragments or the positions of the\\nrestriction sites. For this reason, double digestion experiments are performed where\\ntwo different enzymes are used as follows. First a set of clones of the DNA molecules\\nare digested by an enzyme A. Then a second set of clones are digested by an\\nenzyme B. Finally, the third set of clones are digested by a mix of both enzymes A\\nand B, which we refer to as C. All digestions are full digestion. This results in\\nthree multisets of DNA fragments, and in three multisets of distance between all\\nadjacent restriction sites. The objective is to reconstruct the original ordering of\\nthe fragments in the DNA molecules. This is referred to as Double Digest Problem\\n(DDP ). In the following definition of DDP , sum(S) denotes the sum of the elements\\nin a multiset S, and dist(P ) is the multiset of all distances between two neighboring\\npoints in a set P of points on a line.\\nDefinition 1 (Double Digest). Given three multisets A, B and C of positive in-\\ntegers with sum(A) = sum(B) = sum(C), there are three sets PA, PB and PC\\nof points on a line, such that dist(PA) = A, dist(PB) = B, dist(PC) = C and\\nPA ∪ PB = PC (0 is the minimal point in each set).\\n602 M. Ganjtabesh, H. Ahrabian, A. Nowzari-Dalini\\nFor example, given multisets A = {1, 2, 3, 5}, B = {2, 2, 3, 4} and C = {1, 1, 1, 2,\\n2, 2, 2} as an instance of Double Digest. Then PA = {0, 2, 7, 10, 11}, PB = {0, 3, 5,\\n9, 11} and PC = {0, 2, 3, 5, 7, 9, 10, 11} is a feasible solution which is shown in Fi-\\ngure 1.\\nFig. 1. An instance of DDP\\nDDP is an NP-complete problem [13, 31] and several approaches including expo-\\nnential algorithms, heuristic algorithms, and computer assistant interactive strate-\\ngies have been proposed in order to tackle this problem [3, 5, 14, 33]. In Section 4, we\\ngive a molecular polynomial time algorithm for this problem. The other approach\\nfor finding physical maps of DNA molecules is by partial digestion experiment.\\nDefinition 2 (Partial Digest). Given an integer m and a multiset D = {d1, d2, . . . ,\\ndk} of k = (\\nm\\n2\\n) positive integers, there is a set P = {p1, p2, . . . , pm} of m points on\\na line such that {|pi − pj| : i ≤ j ≤ m} = D.\\nFor example, for the distance multiset D = {2, 3, 5, 7, 8, 10}, the point set P =\\n{0, 2, 7, 10} is a feasible solution which is shown in Figure 2.\\nFig. 2. An instance of PDP\\nThe exact computational complexity of PDP is a long standing open prob-\\nlem. For this problem, neither a polynomial time algorithm nor a proof of NP-\\ncompleteness is known [6, 21, 22], but several backtracking algorithm with exponen-\\ntial time complexity are presented in [28, 34]. In Section 5 we also give a polynomial\\ntime molecular algorithm for this problem.\\nMolecular Solutions for DDP and PDP 603\\n3 DNA MODEL OF COMPUTATION\\nPrior to presentation of the molecular algorithms for our problems, we define the\\nDNA operations that we apply in our algorithms [25]. Our DNA operations are\\nsimilar to operations proposed by Adleman and Lipton [1, 17]. A test tube is a set\\nof DNA molecules (i.e. a multiset of finite strings over the alphabet {A, C, G, T}).\\nGiven a test tube, we can perform the following operations:\\ni) Separate(P, P1, P2, s): This operation produces two tubes P1 and P2 where P1\\nis all of the DNA molecules separated from P which contain the strand s as\\na sub-strand and P2 is all of the DNA molecules from P which do not contain\\nthe short strand s as a sub-strand. To implement this operation, the content\\nof tube P is affinity purified with a biotin-avidin magnetic beads system. This\\nis accomplished by incubating the single-stranded DNA in tube P with s con-\\njugated to magnetic beads. Only those single-stranded DNA molecules that\\ncontain the short DNA strand s can be annealed to the bound s. Now these\\nstrands are separated and poured to P1 and the remaining strands are poured\\nto P2.\\nii) Extract(P, P1, P2, i, s): As the above operation, this operation produces two\\ntubes P1 and P2, where P1 is all of the DNA strands separated from the tube P\\nwhich contain the strand s as a sub-strand in a specific position i. An arbitrary\\nlength ℓ can be used for scaling the position, depending on the algorithm. After\\neach separation operation, the strands that have the strand s in the ith posi-\\ntion will be stored in P1, while all the strands that don’t have the strand s in\\nthe ith position will be stored in P2. This is accomplished by incubating the\\nsingle-stranded DNA in the test tube P with s conjugated to magnetic beads.\\nOnly those single-stranded DNA molecules that contain the sequence s are an-\\nnealed to the bound s. In the suitable condition, primer extension occurs in the\\ntest tube. By melting the double-stranded DNA sequences, the strands which\\ncontain s are separated in the test tube P2 and the rest remains in P . Employ-\\ning gel-electrophoresis, the strands with length i× ℓ are detected. Later, these\\nstrands are added to tube P1 in order to distinguish the strands which contain\\ns in position i.\\niii) Append(P, s): Molecularly, this operation appends the strand s into the end\\nof every strand in the tube P . This operation is implemented by pouring the\\nstrands containing the end complementary strands in the tube P (3′-termination\\nof the strands in P ) and the complement of sequence s (s) to the tube P . With\\nadding the strand s and the ligase enzyme to the tube P , s is appended to all\\nthe strands in P .\\niv) Merge(P, P1, P2): This operation combines the strands in P1 and P2 into one\\ntest tube P , without any change in the individual strands. This operation is\\nimplemented easily by pouring the content of two test tubes P1 and P2 in the\\ntest tube P .\\n604 M. Ganjtabesh, H. Ahrabian, A. Nowzari-Dalini\\nv) Primer-Extension(P1, P2): By pouring the contents of the test tube P2 in the\\ntube P1, in the suitable condition, and employing the strands in P2 as probes,\\nprimer extension occurs for each single strand in P1 in the direction 5\\n′ → 3′, i.e.\\nall the strands in P2 are hybridized to their corresponding complement strands\\nin P1 and by adding free nucleotides to the solution in the test tube, primer\\nextension occurs and double strands are constructed.\\nvi) Amplify(P, P1, P2): This operation produces two new tubes P1 and P2 such\\nthat P1 and P2 are two copies of tube P (which are now identical) and tube P\\nbecomes empty tube. To implement this operation, the content of P is amplified\\nby polymerase chain reaction (PCR) using primers 5′-terminate and marked\\n3′-terminate (by magnetic beads ) of the strands in P . The unmarked and\\nmarked sequences are distinguished and assigned to the test tubes P1 and P2,\\nrespectively. The marked sequences in P2 become unmarked later.\\nvii) Length(P, P1, ℓ): This operation separates all DNA strands of length ℓ from\\nP , and pours them into P1. Molecularly, the content of P is run in gel-\\nelectrophoresis, and the strands of length ℓ are separated and assigned to the\\ntest tube P1.\\nviii) Test(P ): This operation produces “true” if P includes at least one DNA strand.\\nThis operation can also be done by amplifying the content of P by polymerase\\nchain reaction and run on a gel-electrophoresis.\\nix) Readout(P ): This operation describes all the stands in the test tube P in a re-\\ncognizable form.\\nx) Discard(P1, P2, . . . , Pn): This operation will discard all the tubes P1, P2, . . . ,\\nPn.\\nNote that in all of the above operations, all the DNA strands can be assumed\\nto be single strands and each operation is performed in O(1) time complexity.\\n4 THE MOLECULAR ALGORITHM FOR DDP\\nThe molecular algorithm given in this section for DDP problem can be simulated\\nby DNA operations in a polynomial time. Our simulation has two stages: DNA\\nencoding and performing DNA operations. As mentioned, in the definition of the\\nDDP problem we have given three multisets A, B and C of positive integers with\\nt = sum(A) = sum(B) = sum(C). Now, before the construction of strands cor-\\nresponding to the encoding of the problem, these three sets are encoded as 0-1\\nsequences and the solution space is constructed with regard to these sequences and\\nsets.\\nEach set, such as S = {s1, s2, . . . , sn} of length n (|S| = n), can be encoded as\\nt-bit binary number where t =\\n∑n\\ni=1 si, with exactly n zeros and k =\\n∑n\\ni=1 (si − 1)\\nones, such that each element of S, say si, is encoded by si − 1 ones followed by one\\nzero. The sets A and B in the DDP problem in our example can be converted to\\nthe following binary numbers:\\nMolecular Solutions for DDP and PDP 605\\nA = [2, 5, 3, 1] =⇒ [10, 11110, 110, 0] =⇒ A′ = [10111101100],\\nB = [3, 2, 4, 2] =⇒ [110, 10, 1110, 10] =⇒ B′ = [11010111010].\\nConsequently, the set C ′ can be obtained by performing the binary AND operation\\non A′ and B′:\\nC ′ = A′ And B′ = [10111101100] And [11010111010] = [10010101000].\\nTherefore, the solution to the DDP problem is to construct all the permuta-\\ntions of binary representation of the sets A and B. The correct mapping is any\\npermutation of A and B that the result of AND operation on them is equal to the\\nbinary representation of one of the permutations on C.\\nFor encoding DDP problem the following sets of DNA sequences are con-\\nstructed:\\n• Two different DNA strands of a fixed length ℓ are constructed for representing\\nthe bit values 0 and 1 which are denoted by 0 and 1, respectively.\\n• A set containing of |C| different DNA strands of fixed length ℓ are constructed\\nfor representing the bit value 0 which are used as delimiter in the multisets A,\\nB, and C and denoted by 0i (1 ≤ i ≤ |C|). These strands help us to easily\\nverify the occurrence of each element of A, B, and C exactly once later in the\\nalgorithm.\\n• For each element ai in A , we construct a strand of length ℓ× ai denoted by αi\\n(1 ≤ i ≤ |A|), such that αi is constructed by ai − 1 strands of 1 representing\\nbit 1 followed by a strand 0i representing bit 0. Clearly, for the elements with\\nequal values in A, different strands are constructed.\\n• Strands corresponding to the elements of B are constructed similarly to the\\nabove definition for the elements of A and denoted by βj (1 ≤ j ≤ |B|).\\n• For each element ci in C, we construct a strand of length ℓ × (ci + 1) denoted\\nby γi, such that γi is constructed by ci − 1 strands of 1 (the complement of 1)\\nfollowed by a strand 0 (the complement of 0) and also followed by a strand 0i\\n(thecomplement of 0i).\\n• We construct a set of sequences that contain all permutations of the strands in\\nA = {a1, a2, . . . , an}. Similar to Adleman’s scheme for encoding the Hamiltonian\\npath problem [1], this can be done easily by considering a complete graph whose\\nvertices are the elements of the set A. For each vertex, we consider the strand αai\\nof length ℓ×ai corresponding to the elements in A and for each edge of the graph\\nthe strand αeij are constructed such that αeij = α\\nR\\nai\\n+ αLaj (the right half end\\nof the complement of αai corresponding to vertex i plus the left half end of the\\ncomplement of αaj corresponding to vertex j). Pouring the edge strands and\\nvertex strands in a test tube, in a suitable condition, all the paths of this graph\\nare constructed. The paths with length equal to ℓ ×\\n∑n\\ni=1 ai are selected such\\n606 M. Ganjtabesh, H. Ahrabian, A. Nowzari-Dalini\\nthat each zero strand 0i (1 ≤ i ≤ n) occurs exactly once in the sequences.\\nThese selected paths represent the permutations of the strands corresponding\\nto all the vertices. For example for A = {2, 4, 4} we construct a graph which is\\nshown in Figure 3. The strands corresponding to all the paths represent all the\\npermutations of the set A. We call this set of strands ∆A, where |∆A| = |A|!.\\nThe ith sequence in each set is denoted by an index i, for example in the set ∆A,\\nthe ith element is denoted by ∆Ai. Considering that the bit values 0i(1 ≤ i ≤ n)\\nfor each element ai in A, are encoded with different strands, therefore elements\\nwith equal values in A are also encoded with different sequences. For this reason,\\nthe verification of the occurrence of exactly n zeros is very simple and the paths\\nwith cycle can not be chosen.\\n1110 1110\\n10\\nFig. 3. Graph corresponding to the multiset A = {2, 4, 4}\\n• Similar to the above discussion, all the permutations of strands in B are con-\\nstructed and we call this set ∆B.\\n• Sequences corresponding to all the permutations of C are also constructed si-\\nmilar to A and are called ∆C . As it is defined, each sequence γi (1 ≤ i ≤\\n|C|) corresponding to the element ci, contains of ci − 1 strands of 1 followed\\nby the complementary of two zero strands 0 and 0i. Clearly, in construction\\nof the permutations of the set C, the subsequences 0i are used as delimiter\\nand this causes the elements with equal values in C are encoded with different\\nsequences. After the construction of all permutations for C, we can delete\\nthe subsequences 0i and in the algorithm we do not need these subsequences.\\nBy using polymerase and ligase, we obtain double stranded molecules, with\\none strand being the original one and the other strand being its complement\\nexcept that the subsequence 0i is removed. After denaturing, the strands which\\ncontain 0i are separated and the remaining strands are the strands with 0i\\nremoved in them. This process should be performed for removing each 0i (1 ≤\\ni ≤ |C|), separately.\\nIt should be noted that, for the convenience in the separation operation, all the\\nstrands in ∆A and ∆B begin with a specific strand αA and βB (tag strands), and all\\nthe strands in ∆C end with the strand γC , respectively, as a marker.\\nNow, assume that all strands of the sets ∆A, ∆B, and ∆C are in the test tubes\\nPα, Pβ , and Pγ , respectively. Note that each strand in ∆A, ∆B, and ∆C is a DNA\\nMolecular Solutions for DDP and PDP 607\\nsequence of length ℓ× (t+1), where t =\\n∑|A|\\ni=1 ai =\\n∑|B|\\ni=1 bi =\\n∑|C|\\ni=1 ci. The molecular\\nalgorithm for DDP is given in Algorithm 1.\\nNow, a brief discussion of the performance of the above algorithm is presented.\\nAfter the construction of all the required sequences, the algorithm DDP −MOL\\nis performed as follows. In phase 1, a set of molecular operations are performed\\nfor the AND operation. This can be done by checking the xth position in both\\nsequences. Depending on the values, the result of AND operation is appended to\\nboth sequences, i.e. for each strand in ∆A namely ∆Ai (1 ≤ i ≤ |A|!) and each\\nstrand in ∆B namely ∆Bj (1 ≤ j ≤ |B|!), we obtain ∆Ai And ∆Bj and append\\nthe result of And operation to the end of ∆Ai and ∆Bj . Later, all the sequences\\ncorresponding to the elements of A and B are separated in two different test tubes,\\nPα and Pβ, respectively.\\nIn phase 2, by employing the operation Primer-Extension on the test tubes Pα\\nand Pβ separately using the content of test tube Pγ as primer, double strands are\\nconstructed. Finally, the sequences which hold all the strands ∆C are extracted as\\na solution of our problem in each test tube.\\nTheorem 1. The molecular algorithm DDP -MOL is performed in O(n) com-\\nplexity for any DDP problem given three multisets A, B, and C of positive in-\\ntegers such that n is a constant proportional to the sum(A) (note that sum(A) =\\nsum(B) = sum(C)).\\nProof. The DDP -MOL has two phases. Phase 1 has a single loop and all the\\noperations in the loop are performed in O(1). Therefore phase 1 is performed in O(n)\\ncomplexity. In phase 2, we have no loop and this is performed in O(1) complexity.\\nHence, the total complexity of the algorithm is O(n). 2\\n5 THE MOLECULAR ALGORITHM FOR PDP\\nIn this section, we present a molecular algorithm for the PDP . The algorithm is\\nsimulated by DNA operations in a polynomial time. The main idea of our simulation\\nis to first generate solution space of DNA sequences. Then, the biological operations\\nare used to remove infeasible solutions and to find feasible solutions from the solution\\nspace.\\nAs mentioned in the previous section, PDP problem can be briefed as follows:\\nGiven an integer m and a multiset D = {d1, d2, . . . , dk} of k = (\\nm\\n2\\n) positive integers,\\nis there a set P = {p1, p2, . . . , pm} of m points on a line such that {|pi − pj| : i ≤\\nj ≤ m} = D.\\nIn order to encode PDP problem as DNA sequences, we construct three different\\nsets of strands:\\n• For the multiset D = {d0, d1, d2, . . . , dk}, k + 1 different strands δd0 , δd1, δd2 ,\\n. . ., δdk of a constant length ℓ are constructed such that each δdi corresponds to\\na di (0 ≤ i ≤ k). It is noted that for elements with equal values in D, different\\nstrands are constructed. Here we assume that d0 is an element with value 0\\n608 M. Ganjtabesh, H. Ahrabian, A. Nowzari-Dalini\\nAlgorithm 1 Double digest molecular algorithm\\nAlgorithm DDP -MOL\\nbegin\\nphase 1:\\nfor x = 1 to t do begin\\nExtract(Pα, P0, P1, x, 1);\\nExtract(Pβ, P2, P3, x, 1);\\nAmplify(P0, P4, P5);\\nAmplify(P1, P6, P7);\\nAmplify(P2, P8, P9);\\nAmplify(P3, P10, P11);\\nif (Test(P4) and Test(P8)) then begin\\nMerge(P12, P4, P8);\\nAppend(P12, 1);\\nend if;\\nif (Test(P5) and Test(P10)) then begin\\nMerge(P13, P5, P10);\\nAppend(P13, 0);\\nend if;\\nif (Test(P6) and Test(P9)) then begin\\nMerge(P14, P6, P9);\\nAppend(P14, 0);\\nend if;\\nif (Test(P7) and Test(P11)) then begin\\nMerge(P15, P7, P11);\\nAppend(P15, 0);\\nend if;\\nExtract(P12, Pα, Pβ , 0, αA);\\nExtract(P13, Pα, Pβ , 0, αA);\\nExtract(P14, Pα, Pβ , 0, αA);\\nExtract(P15, Pα, Pβ , 0, αA);\\nDiscard(P0, P1, . . . , P15);\\nend for;\\nphase 2:\\nPrimer-Extension(Pα, Pγ);\\nPrimer-Extension(Pβ, Pγ);\\nExtract(Pα, P0, P1, 2t + 2, γC);\\nExtract(Pβ, P2, P3, 2t + 2, γC);\\nif Test(Pα) and Test(Pβ) then begin\\nReadout(Pα);\\nReadout(Pβ);\\nend if;\\nend.\\nMolecular Solutions for DDP and PDP 609\\nand without loss of generality, it is included in the set D. We call this set of\\nstrands Γ.\\n• Two different DNA strands of fixed length ℓ are constructed for representing the\\nbit values 0 and 1. We show these strands by 0 and 1, respectively.\\n• A set of strands of length (k + 1)× ℓ corresponding to all 0-1 combinations of\\nbinary numbers of length k + 1 are provided by employing the 0 and 1 strands.\\nThis set is constructed similarly to Lipton’s scheme for encoding the satisfiability\\nproblem. We call this set ∆. Indeed, each strand of length (k +1)× ℓ in this set\\ncorresponds to a binary number of length k + 1. Therefore, the set ∆ contains\\nthe strands corresponding to all (k + 1)-bits binary numbers.\\nAfter construction of the above strands, and assuming that the strands of the\\nset ∆ are in to the tube P0, the molecular algorithm for PDP is presented in\\nAlgorithm 2.\\nA brief discussion of this algorithm is presented here. In phase 1, all the se-\\nquences corresponding to the values in the set D are appended to the sequences in\\nthe initial test tube P0. This process is performed as follows. Consider one of these\\nbinary numbers be x = x1x2 . . . xk. If xi and xj both have value 1 and |di − dj |\\nbelongs to the multiset D, then the sequence δ|di−dj | from the set Γ is appended to\\nthe strands corresponding to this numbers and the value |di − dj | is removed from\\nthe multiset D. Because of this removal, different strands are appended for equal\\nvalues in D.\\nIn phase 2, all the strands in P1 corresponding to the binary numbers with\\nexactly m-bits value 1 that have appended values with length greater or less than\\n2k+1 are discarded. Later, the sequences with the appended strands corresponding\\nto all the values in D are selected. The selected strands are the solution for PDP\\nproblem.\\nIt should be noted that this algorithm obtains all the feasible solutions, but at\\nleast we have two complement solutions because of changing the direction of the\\noriginal sequence.\\nTheorem 2. The molecular algorithm PDP -MOL is performed in O(k2) time com-\\nplexity for any PDP problem given as integer m and a multiset D = {d0, d1, d2, . . . ,\\ndk} of k distances.\\nProof. As we can see, the algorithm has two phases. Phase 1 has two nested loops.\\nSince all the molecular operations employed in this paper are performed in O(1),\\ntherefore phase 1 is performed in O(k2). In phase 2 we have two separated loops.\\nWith regards to the complexity of the operations, phase 2 is also performed in O(k).\\nSo the total complexity of the algorithm is O(k2). 2\\n6 ERROR RESISTANCE DNA SEQUENCE GENERATION\\nAt the time when DNA computing was introduced, a question was raised about how\\nerrors may affect the computing results. Although mature biological operations\\n610 M. Ganjtabesh, H. Ahrabian, A. Nowzari-Dalini\\nAlgorithm 2 Partial digest molecular algorithm.\\nAlgorithm PDP -MOL\\nbegin\\nphase 1:\\nfor i = 0 to k − 1 do begin\\nfor j = i + 1 to k do begin\\nExtract(P0, P1, P2, i, 1);\\nExtract(P1, P3, P4, j, 1);\\nif |di − dj | ∈ D then begin\\nD = D\\\\{|di − dj |}\\nAppend(P3, δ|di−dj |)\\nMerge(P0, P2, P3);\\nMerge(P0, P0, P4);\\nend if;\\nend for;\\nend for;\\nphase 2:\\nLength(P0, P1, 2(k + 1)ℓ);\\nfor i = 1 to k do begin\\nSeparate(P1, P0, P2, δdi);\\nP1 ← P0;\\nend for;\\nfor i = 0 to k do begin\\nExtract(P1, P2, P3, i, 1);\\nif T (P2) then begin\\nprint(i);\\nP1 ← P2;\\nelse\\nMerge(P1, P2, P3);\\nend if;\\nend for;\\nend.\\nhave very low error rates, errors may still accumulate and thus generate incorrect\\nanswers. As hybridization reactions are essential components in the implementa-\\ntion protocol and as under certain circumstances such reactions are prone to errors\\nthat would cause false positive and negative results, care must be taken to avoid\\nthe errors. Errors arise commonly when sequences that generate or tolerate hair-\\npins and internal loops, mismatch hybridization, shifted mismatches, and 3′-end\\nhybridization are used. Application of appropriate sequence considerations, such as\\nbase composition and melting temperature (Tm) of hybrids have proven to be very\\neffective in other experimental contexts where mis-hybridization was a critical con-\\nMolecular Solutions for DDP and PDP 611\\nsideration [10]. Similarly, as ligation reactions are included in the implementation\\nprotocol, care must be taken to avoid mis-ligations. The use of an appropriate lig-\\nase under stringent conditions will prevent mis-ligations [10, 26]. In order to avoid\\nthese errors we should prevent mis-hybridization and undesired secondary structure\\nand also keep uniform chemical characteristics [8, 29]. Such unintended interactions\\namong DNA strands can be minimized by careful sequence design. Random se-\\nquence generation does not satisfy the above properties. Several heuristic methods\\nfor constructing DNA strands set that are robust with respect to the hybridization\\nerrors have been proposed [4, 10, 16, 26].\\nDNA sequence design can be considered as an optimization problem. Therefore,\\nDNA sequences corresponding to any encoding can be generated by a genetic algo-\\nrithm that minimizes the potential of errors in DNA sequences for reliable molecular\\noperations and produces reliable sequences. Based on ideas given in Shin et al. [27],\\nwe discuss a genetic algorithm for constructing DNA sequences with regard to the\\nabove errors for our model. This algorithm is summarized below:\\ni) Generate sequences of four alphabets {A, C, G, T} randomly with length 2d.\\nii) Set counter = 1.\\niii) While (counter <= max count) do\\na) Evaluate the fitness of each sequence.\\nb) Select the sequences with best fitness.\\nc) Apply genetic operators (crossover and mutation) to produce a new popula-\\ntion.\\nd) counter = counter + 1.\\niv) Let the best codes be the fittest encodings.\\nIn this algorithm, we use the conventional genetic operations such as roulette\\nwheel selection, one-cut-point crossover, and single-point mutation [12]. In roulette\\nwheel selection, offsprings with a higher fitness value have a higher probability of\\ncontributing in the next population. Crossover and mutation are applied with prob-\\nability pc and pm, respectively, in offsprings that are selected by roulette wheel\\nselection process. In this algorithm, we have employed a multiobjective fitness func-\\ntion with similar criteria given in Tanaka et al. [29]. Three measures, H-Measure,\\nSimilarity, and Completely complementary at 3′-end, are considered for avoiding\\nmis-hybridization error. To prevent an undesired secondary structure two measures\\nSelf-Complementary and Continuity are considered. To keep a uniform chemical\\ncharacteristics, two other measures, GC-Content and Temperature (Tm), are used.\\nIn our algorithm, a unique fitness function is designed for each of these measures, and\\nthe multiobjective fitness function is the summation of the above fitness functions.\\n612 M. Ganjtabesh, H. Ahrabian, A. Nowzari-Dalini\\n7 COMPUTATIONAL SIMULATION\\nWe developed a tool for simulating our two molecular algorithms for DDP and\\nPDP . Our tool first provides the required DNA sequences for these problems\\nusing the genetic algorithm discussed in the previous section and later performs\\nthe simulated molecular operations on them. The first simulated example that we\\npresent here is a DDP for a given three multisets A = {2, 4, 4}, B = {2, 3, 5}, and\\nC = {1, 2, 2, 2, 3}. The generated sequences for this example by our genetic algo-\\nrithm are also of length ℓ = 6. The strands corresponding to 0, 1, and 0i(1 ≤ i ≤ 3)\\nare constructed and shown in Table 1. The elements in the multisets A, B and C are\\nencoded to 0-1 digits, with respect to the encoding which is defined in Section 5, and\\ntheir corresponding DNA sequences are constructed by the strands corresponding\\nto 0 and 1 as shown in Table 2.\\nBit Symbol Corresponding DNA sequences Complement DNA sequences\\n0 0 GCCATT CGGTAA\\n1 1 CATGAC GTACTG\\n01 01 AGTCAC TCAGTG\\n02 02 CGTACA GCATGT\\n03 03 ATCTCG TAGAGC\\n04 04 TAGAGG ATCTCC\\n05 05 TGAGTC ACTCAG\\nTable 1. 0-1 bits and their corresponding DNA sequences\\nMultiset i Encoded Symbol Corresponding DNA sequences\\n2 101 α1 CATGAC-AGTCAC\\nA 4 11102 α2 CATGAC-CATGAC-CATGAC-CGTACA\\n4 11103 α3 CATGAC-CATGAC-CATGAC-ATCTCG\\n2 101 β1 CATGAC-AGTCAC\\nB 3 1102 β2 CATGAC-CATGAC-CGTACA\\n5 111103 β3 CATGAC-CATGAC-CATGAC-\\nCATGAC-ATCTCG\\n1 001 γ1 GCCATT -TCAGTG\\n2 1002 γ2 GTACTG-GCCATT -GCATGT\\nC 2 1003 γ3 GTACTG-GCCATT -TAGAGC\\n2 1004 γ4 GTACTG-GCCATT -ATCTCC\\n3 11005 γ5 GTACTG-GTACTG-GCCATT -ACTCAG\\nTable 2. Elements of A, B, and C and their corresponding strands\\nThe three sets ∆A, ∆B, and ∆C are all sequences corresponding to the permu-\\ntations of the elements in A, B, and C, which are generated and shown in Tables 3\\nand 4. It should be noted that, for summarizing, in Tables 3 and 4 the DNA se-\\nquences are presented by their corresponding symbols denoted in Table 2. As men-\\ntioned previously, for performing the Primer-Extention operation in the algorithm,\\nMolecular Solutions for DDP and PDP 613\\nthe set ∆C is constructed by using the Watson-Crick complementary sequences. So,\\nfor constructing all permutation for the multiset C we use the complement strand\\nof γi as shown in Table 4.\\nSet Permutation Corresponding DNA sequence\\n101 − 11102 − 11103 αA − α1 − α2 − α3\\n101 − 11103 − 11102 αA − α1 − α3 − α2\\n∆A 11102 − 101 − 11103 αA − α2 − α1 − α3\\n11102 − 11103 − 101 αA − α2 − α3 − α1\\n11103 − 101 − 11102 αA − α3 − α1 − α2\\n11103 − 11102 − 101 αA − α3 − α2 − α1\\n101 − 1102 − 111103 βB − β1 − β2 − β3\\n101 − 111103 − 1102 βB − β1 − β3 − β2\\n∆B 1102 − 101 − 111103 βB − β2 − β1 − β3\\n1102 − 111103 − 101 βB − β2 − β3 − β1\\n111103 − 101 − 1102 βB − β3 − β1 − β2\\n111103 − 1102 − 101 βB − β3 − β2 − β1\\nTable 3. The permutations of the elements in A and B appended by tag strand αA and βB ,\\nrespectively\\nSet Permutation Corresponding DNA sequence\\n0− 10− 10− 10− 110 γ1 − γ2 − γ3 − γ4 − γ5 − γC\\n0− 10− 10− 110− 10 γ1 − γ2 − γ3 − γ5 − γ4 − γC\\n0− 10− 110− 10− 10 γ1 − γ2 − γ5 − γ3 − γ4 − γC\\n0− 110− 10− 10− 10 γ1 − γ5 − γ2 − γ3 − γ4 − γC\\n10− 0− 10− 10− 110 γ2 − γ1 − γ3 − γ4 − γ5 − γC\\n10− 0− 10− 110− 10 γ2 − γ1 − γ3 − γ5 − γ4 − γC\\n10− 0− 110− 10− 10 γ2 − γ1 − γ5 − γ3 − γ4 − γC\\n10− 10− 0− 10− 110 γ2 − γ3 − γ1 − γ4 − γ5 − γC\\n10− 10− 0− 110− 10 γ2 − γ3 − γ1 − γ5 − γ4 − γC\\n∆C 10− 10− 10− 0− 110 γ2 − γ3 − γ4 − γ1 − γ5 − γC\\n10− 10− 10− 110− 0 γ2 − γ3 − γ4 − γ5 − γ1 − γC\\n10− 10− 110− 10− 0 γ2 − γ3 − γ5 − γ4 − γ1 − γC\\n10− 10− 110− 0− 10 γ2 − γ3 − γ5 − γ1 − γ4 − γC\\n10− 110− 10− 10− 0 γ2 − γ5 − γ3 − γ4 − γ1 − γC\\n10− 110− 10− 0− 10 γ2 − γ5 − γ3 − γ1 − γ4 − γC\\n10− 110− 0− 10− 10 γ2 − γ5 − γ1 − γ3 − γ4 − γC\\n110− 10− 10− 10− 0 γ5 − γ2 − γ3 − γ4 − γ1 − γC\\n110− 10− 10− 0− 10 γ5 − γ2 − γ3 − γ1 − γ4 − γC\\n110− 10− 0− 10− 10 γ5 − γ2 − γ1 − γ3 − γ4 − γC\\n110− 0− 10− 10− 10 γ5 − γ1 − γ2 − γ3 − γ4 − γC\\nTable 4. The distinct permutations of the elements in C appended by tag strand γC\\n614 M. Ganjtabesh, H. Ahrabian, A. Nowzari-Dalini\\nAfter the construction of all the required sequences, the algorithm DDP -MOL is\\nsimulated as follows. With respect to the algorithm in phase 1, the molecular opera-\\ntions required for the AND operation are computationally simulated and performed\\non the sequences of the sets ∆A and ∆B in the test tubes Pα and Pβ , respectively.\\nIn this phase, the result of AND operation is appended to the all sequences and\\nseparated in two different test tubes, Pα and Pβ , with respect to the tag strands\\ncalled αA and βB. In phase 2, by pouring the strands of the set ∆C which are in the\\ntest tube Pγ into each tube and allowing the occurrence of primer extension, double\\nstrands are constructed. Finally, the sequences which hold all the strand γC are the\\nsolution in each test tube. Therefore, the solutions for this example are shown in\\nTable 5. As we can see, both solutions in Table 5 are correct for this problem.\\nThe second simulated example that we present here is an example of PDP for\\na given multiset D = {0, 2, 3, 5, 7, 8, 10} where k = 6 and m = 4. Our genetic\\nalgorithm provides DNA sequences of length ℓ = 6 for this example. We consider\\nthat the strands corresponding to 0 and 1 are similar to the 0 and 1 strands generated\\nin the previous example. Similarly, the strands corresponding to the elements of the\\nmultiset D are also constructed and shown in Table 6. Also, the 27 binary numbers\\nof length 7 and corresponding sequences of length 7× 6 are constructed which are\\nshown in Table 7.\\nSolution Number Container set Corresponding DNA sequences\\n1 A αA − α1 − α2 − α3 − γ2 − γ1 − γ5 − γ3 − γ4 − γC\\nB βB − β2 − β3 − β1 − γ2 − γ1 − γ5 − γ3 − γ4 − γC\\n2 A αA − α2 − α3 − α1 − γ2 − γ3 − γ5 − γ1 − γ4 − γC\\nB βB − β1 − β3 − β2 − γ2 − γ3 − γ5 − γ1 − γ4 − γC\\nTable 5. The solutions for the given example\\ndi δdi\\n0 TAGCGA\\n2 TGTACC\\n3 GCTGAA\\n5 TCCATC\\n7 CAATCC\\n8 TGACGA\\n10 CGTGTT\\nTable 6. Strands corresponding to the elements of D\\nBy performing the algorithm and employing simulated molecular operations, in\\nphase 1, for any position i and j with value 1, the sequences corresponding to the\\nvalue |di−dj | that belong to the multiset D, δ|di−dj | are appended to these sequences.\\nIn phase 2, the sequences with length 2(k + 1)ℓ which in this example is equal to 82\\nare separated and illustrated in Table 8. Among these sequences, the sequences\\nwhose appended subsequences are corresponding to the values in the multiset D are\\nMolecular Solutions for DDP and PDP 615\\nselected as a solution. The bolded sequences in Table 8 show the solution of this\\nproblem. One of the solutions is 1−1−0−0−1−0−1− δ2− δ7− δ10− δ5− δ8− δ3,\\nwhich represents the points 0, 2, 7 and 10, therefore the solution is {0, 2, 7, 10}.\\n0-1 combinations corresponding strand\\n0000000 TAGAGG-TAGAGG-TAGAGG-TAGAGG-TAGAGG-\\nTAGAGG-TAGAGG\\n0000001 TAGAGG-TAGAGG-TAGAGG-TAGAGG-TAGAGG-\\nTAGAGG-CATGAC\\n0000010 TAGAGG-TAGAGG-TAGAGG-TAGAGG-TAGAGG-\\nCATGAC-TAGAGG\\n0000011 TAGAGG-TAGAGG-TAGAGG-TAGAGG-TAGAGG-\\nCATGAC-CATGAC\\n0000100 TAGAGG-TAGAGG-TAGAGG-TAGAGG-CATGAC-\\nTAGAGG-TAGAGG\\n...\\n...\\n1111011 CATGAC-CATGAC-CATGAC-CATGAC-TAGAGG-\\nCATGAC-CATGAC\\n1111100 CATGAC-CATGAC-CATGAC-CATGAC-CATGAC-\\nTAGAGG-TAGAGG\\n1111101 CATGAC-CATGAC-CATGAC-CATGAC-CATGAC-\\nTAGAGG-CATGAC\\n1111110 CATGAC-CATGAC-CATGAC-CATGAC-CATGAC-\\nCATGAC-TAGAGG\\n1111111 CATGAC-CATGAC-CATGAC-CATGAC-CATGAC-\\nCATGAC-CATGAC\\nTable 7. Strands corresponding to the binary numbers of length 7\\nConstructed DNA strands\\n0− 2− 3− 5− 7− 8− 10\\n1− 0− 0− 1− 0− 1− 1− δ5 − δ8 − δ10 − δ3 − δ5 − δ2\\n1− 0− 0− 1− 1− 0− 1− δ5 − δ5 − δ2 − δ7 − δ3 − δ5\\n1− 0− 1− 0− 0− 1− 1− δ3 − δ8 − δ10 − δ5 − δ7 − δ2 ← solution1\\n1− 0− 1− 1− 0− 0− 1− δ3 − δ5 − δ10 − δ2 − δ7 − δ5\\n1− 1− 0− 0− 1− 0− 1− δ2 − δ7 − δ10 − δ5 − δ8 − δ3 ← solution2\\n1− 1− 0− 1− 0− 0− 1− δ2 − δ5 − δ10 − δ3 − δ8 − δ5\\nTable 8. Produced strands by performing the algorithm\\n8 CONCLUSION\\nTwo DNA-based algorithms are presented for the solution of PDP and DDP . Our\\nmolecular computational model is similar to Adleman-Lipton model. The algorithms\\n616 M. Ganjtabesh, H. Ahrabian, A. Nowzari-Dalini\\nare proved to have polynomial time complexity. Both algorithms are computation-\\nally simulated for two different examples. The DNA sequences corresponding to the\\nencoding of these problems are generated by a genetic algorithm that minimizes the\\npotential of errors in DNA sequences for reliable molecular operations and produces\\nreliable sequences.\\nAcknowlegment\\nThis research was partially supported by University of Tehran.\\nREFERENCES\\n[1] Adleman, L.M.: Molecular Computation of Solutions to Combinatorial Problems.\\nScience 266, 1994, pp. 1021–1029.\\n[2] Adleman, L.M.: Computing with DNA. Sci. Am. 279, 1998, pp. 54–61.\\n[3] Allison, L.—Yee, C.N.: Restriction Site Mapping Is in Separation Theory. Com-\\nput. Appl. Biosci. 4, 1988, pp. 97–101.\\n[4] Amos, M.—Gibbons, A.—Hodgson, D.: Error-Resistant Implementation of DNA\\nComputations. In DNA Based Computers II, L. Landweber and E. Baum (Eds.),\\nDIMACS Series in Discrete Mathematics and Theoretical Computer Science, Vol. 44\\n(American Mathematical Society, Providence, 1999), pp. 151–162.\\n[5] Bellon, B.: Construction of Restriction Maps. Comput. Appl. Biosci. 4, 1998,\\npp. 111–115.\\n[6] Blažewicz, J.—Formanowicz, P.—Kasprzak, M.—Jaroszewski, M.—\\nMarkiewicz, W.T.: Construction of DNA Restriction Maps Based on a Simplified\\nExperiment. Bioinformatics 17, 2001, pp. 396–404.\\n[7] Boneh, D.—Dunworth, C.—Lipton, R. J.—Sgall, J.: On the Computational\\nPower of DNA. Discrete Appl. Math. 71, 1996, pp. 79–94.\\n[8] Braich, R. S.—Chelyapov, N.—Johnson, C.—Rothemund, P.W.K.—\\nAdleman, L.M.: Solution of a 20-Variable 3-Sat Problem on a DNA Computer.\\nScience 296, 2002, pp. 499–502.\\n[9] Chang, W.L.—Ho, M.—Guo, M.: Molecular Solutions for the Subset-Sum Prob-\\nlem on DNA-Based Supercomputing. BioSystems 73, 2004, pp. 117–130.\\n[10] Deaton, R.—Garzon, M.—Murphy, R.C.—Rose, J.A.—Franceschetti,\\nD.R.—Stevens Jr, S. A.: Reliability and Efficiency of a DNA Based Computation.\\nPhys. Rev. Lett. 80, 1998, pp. 417–420.\\n[11] Freund, R.—Kari, L.—Păun, G.: DNA Computation Based on Splicing: The\\nExistence of Universal Computers. Theory Comput. Syst. 32, 1999, pp. 69–112.\\n[12] Goldberg, D.E.: Genetic Algorithms in Search, Optimization, and Machine Learn-\\ning. Addison-Wesley, Boston 1989.\\n[13] Goldstein, L.—Waterman, M. S.: Mapping DNA by Stochastic Relaxation. Adv.\\nAppl. Math. 8, 1987, pp. 194–207.\\nMolecular Solutions for DDP and PDP 617\\n[14] Inglehart, J.—Nelson, P. C.: On the Limitations of Automated Restriction Map-\\nping. Comput. Appl. Biosci. 10, 1994, pp. 249–261.\\n[15] Kari, L.—Păun, G.—Rozenberg, G.—Salomaa, A.—Yu, S.: DNA Comput-\\ning, Sticker Systems, and Universality. Acta Inform. 35, 1998, pp. 401–420.\\n[16] Kim, D.—Shin, S.—Lee, I.—-Zhang, B.: NACST/Seq: A Sequence Design Sys-\\ntem With Multiobjective Optimization. In Proceedings of 8th International Workshop\\non DNA Based Computers, M. Hagiya and A. Ohuchi (Eds.), Lecture Notes in Com-\\nputer Science, Vol. 2568, Springer-Verlag, London 2003, pp. 242–251.\\n[17] Lipton, R. J.: DNA Solution of Hard Computational Problem. Science 268, 1995,\\npp. 542–545.\\n[18] Liu, J.—Shimohara, K.: Signaling-Pathway-Based Molecular Computing for Effi-\\ncient 3-Sat Problem Solving. Inf. Sci. 161, 2004, pp. 121–137.\\n[19] Mateescu, A.—Păun, G.—Rozenberg, G.—Salomaa, A.: Simple Splicing\\nSystems. Discrete Appl. Math. 84, 1998, pp. 145–163.\\n[20] Ouyang, Q.—Kaplan, P.D.—Liu, S.–Libchaber, A.: DNA Solution of the\\nMaximal Clique Problem. Science 278, 1997, pp. 446–449.\\n[21] Pandurangan, G.—Ramesh, H.: The Restriction Mapping Problem Revisited.\\nJ. Comput. System Sci. 65, 2002, pp. 526–544.\\n[22] Pevzner, P.A.—Waterman, M. S.: Open Combinatorial Problems in Computa-\\ntional Molecular Biology. In Proc. of the 3rd Israel Symposium on Theory of Com-\\nputing and Systems, IEEE Computer Society Press, Piscataway, 1995, pp. 158–173.\\n[23] Păun, G.—Rozenberg, G.: Sticker Systems. Theoret. Comput. Sci. 204, 1998,\\npp. 183–203.\\n[24] Păun, G.—Rozenberg, G.—Salomaa, A.: Computing by Splicing. Theoret.\\nComput. Sci. 168, 1996, pp. 321–336.\\n[25] Păun, G.—Rozenberg, G.—Salomaa, A.: DNA Computing: New Computing\\nParadigms. Springer-Verlag, Heidelberg 1998.\\n[26] Roweis, S.—Winfree, E.: On the Reduction of Errors in DNA Computation.\\nJ. Comput. Biol. 6, 1999, pp. 65–75.\\n[27] Shin, S. Y.—Kim, D.M.—Lee, I. H.—Zhang, B. T.: Evolutionary Sequence\\nGeneration for Reliable DNA Computing. In Proc. of the 2002 Congress on Evolu-\\ntionary Computation, D.B. Fogel, M. A. El-Sharkawi, X. Yao, G. Greenwood, H. Iba,\\nP. Marrow and M. Shackleton (Eds.), IEEE Computer Society Press, New Jersey\\n2002, pp. 79–84.\\n[28] Skiena, S. S.—Sundaram, G.: A Partial Digest Approach to Restriction Site Map-\\nping. Bull. Math. Biology 56, 1994, pp. 275–294.\\n[29] Tanaka, F.—Nakatsugawa, M.—Yamamoto, M.—Shiba, T.—Ohuchi, A.:\\nDeveloping Support System for Sequence Design in DNA Computing. In Proceedings\\nof 7th International Workshop on DNA-Based Computers, N. Jonoska and N.C. See-\\nman (Eds.), Lecture Notes in Computer Science, Vol. 2340, Springer-Verlag, Berlin\\n2001, pp. 129–137.\\n[30] Wang, L.—Liu, Q.—Frutos, A.—Gillmor, S.—Thiel, A.—Strother, T.—\\nCondon, A.—Corn, R.—Lagally, M.—Smith, L.: Surface-Based DNA Com-\\nputing Operations: Destroy and Readout. Biosystems 52, 1999, pp. 189–191.\\n618 M. Ganjtabesh, H. Ahrabian, A. Nowzari-Dalini\\n[31] Waterman, M. S.: Introduction to Computational Biology. CRC Press, New York\\n1995.\\n[32] Winfree, E.: DNA Computing by Self-Assembly. The Bridge 33, 2003, pp. 31–38.\\n[33] Wright, L.W.—Lichter, J. B.—Reinitz, J.—Shifman, J. A.—Kidd, K.K.—\\nMiller, P. L.: Computer-Assisted Restriction Mapping: An Integrated Approach\\nto Handling Experimental Uncertainty. Comput. Appl. Biosci. 10, 1994, pp. 435–442.\\n[34] Zhang, Z.: An Exponential Example for a Partial Digest Mapping Algorithm.\\nJ. Comput. Biol. 1, 1994, pp. 235–239.\\n[35] Zimmermann, K.: Efficient DNA Sticker Algorithms for Graph Theoretic Problems.\\nComput. Phys. Comm. 144, 2002, pp. 297–309.\\nHayadeh Ahrabian is an Associate Professor of School of\\nMathematics, Statistics and Computer Science, University of\\nTehran, and is currently the Director of graduate studies in this\\nschool. Her research interest includes combinatorial algorithms,\\nparallel algorithms, DNA computing, bioinformatics, and ge-\\nnetic algorithms.\\nAbbas Nowzari-Dalini is an Associate Professor of School\\nof Mathematics, Statistics and Computer Science, University of\\nTehran. He is currently the Head of Department of Computer\\nScience in this school. His research interest includes combinato-\\nrial algorithms, parallel algorithms, DNA computing, bioinfor-\\nmatics, neural networks, and computer networks.\\nMohammad Ganjtabesh is a Ph.D. student in the Depart-\\nment of Computer Science, School of Mathematics, Statistics\\nand Computer Science, University of Tehran, under supervision\\nof Dr. H. Ahrabian. His research interest includes combinatorial\\nalgorithms, DNA computing, bioinformatics, and genetic algo-\\nrithms.\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd13'), 'authors': 'Koiliaris, Konstantinos, Xu, Chao', 'year': '2015', 'title': 'A Faster Pseudopolynomial Time Algorithm for Subset Sum', 'full_text': 'ar\\nX\\niv\\n:1\\n50\\n7.\\n02\\n31\\n8v\\n3 \\n [c\\ns.D\\nS]\\n  1\\n2 D\\nec\\n 20\\n16\\nA Faster Pseudopolynomial Time Algorithm for Subset Sum\\nKonstantinos Koiliaris∗ Chao Xu†\\nAbstract\\nGiven a multiset S of n positive integers and a target integer t, the subset sum\\nproblem is to decide if there is a subset of S that sums up to t. We present a new\\ndivide-and-conquer algorithm that computes all the realizable subset sums up to an in-\\nteger u in O˜(min{√nu, u4/3, σ}), where σ is the sum of all elements in S and O˜ hides\\npolylogarithmic factors. This result improves upon the standard dynamic programming\\nalgorithm that runs in O(nu) time. To the best of our knowledge, the new algorithm is\\nthe fastest general deterministic algorithm for this problem. We also present a modified\\nalgorithm for finite cyclic groups, which computes all the realizable subset sums within\\nthe group in O˜(min{√nm,m5/4}) time, where m is the order of the group.\\n1. Introduction\\nGiven a multiset S of n positive integers and an integer target value t, the subset sum problem\\nis to decide if there is a subset of S that sums to t. The subset sum problem is related to the\\nknapsack problem [11] and it is one of Karp’s original NP-complete problems [25]. The subset\\nsum is a fundamental problem used as a standard example of a problem that can be solved in\\nweakly polynomial time in many undergraduate algorithms/complexity classes. As a weakly\\nNP-complete problem, there is a standard pseudopolynomial time algorithm using a dynamic\\nprogramming, due to Bellman, that solves it inO(nt) time [2] (see also [9, Chapter 34.5]). The\\ncurrent state of the art has since been improved by a log t factor using a bit-packing technique\\n[32]. There is extensive work on the subset sum problem, see Table 1.1 for a summary of\\nprevious deterministic pseudopolynomial time results [2, 33, 15, 31, 27, 32, 29, 37, 38].\\nMoreover, there are results on subset sum that depend on properties of the input, as well\\nas data structures that maintain subset sums under standard operations. In particular, when\\nthe maximum value of any integer in S is relatively small compared to the number of elements\\nn, and the target value t lies close to one-half the total sum of the elements, then one can\\nsolve the subset sum problem in almost linear time [16]. This was improved by Chaimovich\\n[7]. Furthermore, Eppstein described a data structure which efficiently maintains all subset\\nsums up to a given value u, under insertion and deletion of elements, in O(u log u log n) time\\nper update, which can be accelerated to O(u log u) when additional information about future\\nupdates is known [14]. The probabilistic convolution tree, by Serang [37, 38], is also able to\\nsolve the subset sum problem in O˜(nmax(S)) time, where O˜ hides polylogarithmic factors.\\nIf randomization is allowed, more algorithms are possible. In particular, Bringmann\\nshowed a randomized algorithm that solves the problem in O˜(nt) time, using only O˜(n log t)\\nspace under the Extended Riemann Hypothesis [4]. Bringmann also provided a random-\\nized near linear time algorithm O˜(n + t) – it remains open whether this algorithm can be\\nderandomized.\\n∗Department of Computer Science, University of Illinois, Urbana - Champaign. E-mail:\\nkoiliar2@illinois.edu\\n†Department of Computer Science, University of Illinois, Urbana - Champaign. E-mail:\\nchaoxu3@illinois.edu\\n1\\nResult Time Space Comments\\nBellman [2] O(nt) O(t) original DP solution\\nPisinger [32] O\\n(\\nnt\\nlog t\\n)\\nO\\n(\\nt\\nlog t\\n)\\nRAM model implementation of\\nBellman\\nPisinger [33] O(nmaxS) O(t) fast if small maxS\\nFaaland [15],\\nPferschy [31]\\nO(n′t) O(t) fast for small n′\\nKlinz et al.\\n[27]\\nO(σ3/2) O(t)\\nfast for small σ, obtainable from\\nabove because n′ = O (\\n√\\nσ)\\nEppstein [14],\\nSerang [37, 38]\\nO˜(nmaxS) O(t log t) data structure\\nLokshtanov et\\nal. [29]\\nO˜(n3t) O˜(n2) polynomial space\\ncurrent work O˜\\n(\\nmin\\n{√\\nn′ t, t4/3, σ\\n})\\nTheorem 2.17\\nO(t) see Section 1.2\\nTable 1.1: Summary of deterministic pseudopolynomial time results on the subset sum problem.\\nThe input is a target number t and a multiset S of n numbers, with n′ distinct values up to t, and σ\\ndenotes the sum of all elements in S.\\nFinally, it is unlikely that any subset sum algorithm runs in time O(t1−ǫ nc), for any\\nconstant c and ǫ > 0, as such an algorithm would imply that there are faster algorithms for\\na wide variety of problems including set cover [4, 10].\\n1.1. Applications of the subset sum problem.\\nThe subset sum problem has a variety of applications including: power indices [42], scheduling\\n[17, 34, 19], set-based queries in databases [41], breaking precise query protocols [12] and\\nvarious other graph problems with cardinality constraints [6, 13, 5, 18, 27, 14] (for a survey\\nof further applications see [26]).\\nA faster pseudopolynomial time algorithm for the subset sum would imply faster poly-\\nnomial time algorithms for a number of problems. The bottleneck graph partition problem\\non weighted graphs is one such example. It asks to split the vertices of the graph into two\\nequal-sized sets such that the value of the bottleneck (maximum-weight) edge, over all edges\\nacross the cut, is minimized. The impact of our results on this problem and other selected\\napplications is highlighted in Section 5.\\n1.2. Our contributions.\\nThe new results are summarized in Table 1.2 – we consider the following all subset sums\\nproblem: Given a multiset S of n elements, with n′ distinct values, with σ being the total\\nsum of its elements, compute all the realizable subset sums up to a prespecified integer u.\\nComputing all subset sums for some u ≥ t also answers the standard subset sum problem\\nwith target value t.\\nOur main contribution is a new algorithm for computing the all subset sums problem\\nin O˜\\n(\\nmin{√nu, u4/3, σ}) time. The new algorithm improves over all previous work (see\\nTable 1.2). To the best of our knowledge, it is the fastest general deterministic pseudopoly-\\n2\\nParameters Previous best Current work\\nn and t O(nt/ log t) O˜\\n(\\nmin\\n{√\\nnt, t4/3\\n})\\nn′ and t O(n′t) O˜\\n(\\nmin\\n{√\\nn′t, t4/3\\n})\\nσ O(σ3/2) O˜(σ)\\nTable 1.2: Our contribution on the subset sum problem compared to the previous best known results.\\nThe input S is a multiset of n numbers with n′ distinct values, σ denotes the sum of all elements in\\nS and t is the target number.\\nnomial time algorithm for the all subset sum problem, and consequently, for the subset sum\\nproblem.\\nOur second contribution is an algorithm that solves the all subset sums problem modulo\\nm, in O\\n(\\nmin{√nm,m5/4} log2m) time. Though the time bound is superficially similar to\\nthe first algorithm, this algorithm uses a significantly different approach.\\nBoth algorithms can be augmented to return the solution; i.e., the subset summing up\\nto each number, with a polylogarithmic slowdown (see Section 4 for details).\\n1.3. Sketch of techniques.\\nThe straightforward divide-and-conquer algorithm for solving the subset sum problem [23],\\npartitions the set of numbers into two sets, recursively computes their subset sums and\\ncombines them together using FFT [14, 37, 38] (Fast Fourier Transform [9, Chapter 30]).\\nThis algorithm has a running time of O(σ log σ log n).\\nSketch of the first algorithm (on integers). Our main new idea is to improve the\\n“conquer” step by taking advantage of the structure of the sets. In particular, if S and T\\nlie in a short interval, then one can combine their subset sums quickly, due to their special\\nstructure. On the other hand, if S and T lie in a long interval, but the smallest number of\\nthe interval is large, then one can combine their subset sums quickly by ignoring most of the\\nsums that exceed the upper bound.\\nThe new algorithm works by first partitioning the interval J0 : uK into a logarithmic\\nnumber of exponentially long intervals. Then computes these partial sums recursively and\\ncombines them together by aggressively deploying the above observation.\\nSketch of the second algorithm (modulo m). Assume m is a prime number. Using\\nknown results from number theory, we show that for any ℓ one can partition the input set into\\nO˜(|S|/ℓ) subsets, such that every such subset is contained in an arithmetic progression of the\\nform x, 2x, . . . , ℓx. The subset sums for such a set can be quickly computed by dividing and\\nlater multiplying the numbers by ℓ. Then combine all these subset sums to get the result.\\nSadly, m is not always prime. Fortunately, all the numbers that are relative prime to m\\ncan be handled in the same way as above. For the remaining numbers we use a recursive\\npartition classifying each number, in a sieve-like process, according to which prime factors\\nit shares with m. In the resulting subproblems all the numbers are coprime to the moduli\\nused, and as such the above algorithm can be used. Finally, the algorithm combines the\\nsubset sums of the subproblems.\\nPaper organization. Section 2 covers the algorithm for positive integers. Section 3 de-\\nscribes the algorithm for the case of modulo m. Section 4 shows how we can recover the\\n3\\nsubsets summing to each set, and Section 5 presents the impact of the results on selected\\napplications of the problem.\\n2. The algorithm for integers\\n2.1. Notations.\\nLet Jx : yK = {x, x+ 1, . . . , y} denote the set of integers in the interval [x, y]. Similarly,\\nJxK = J1 : xK. For two sets X and Y , we denote by X⊕Y the set {x+ y | x ∈ X and y ∈ Y }.\\nIf X and Y are sets of points in the plane, X ⊕ Y is the set {(x1 + y1, x2 + y2) | x1, x2 ∈\\nX and y1, y2 ∈ Y }.\\nFor an element s in a multiset S, its multiplicity in S is denoted by 1S(s). We denote\\nby set(S) the set of distinct elements appearing in the multiset S. The size of a multiset S\\nis the number of distinct elements in S (i.e., |set(S)|). The cardinality of S, is card(S) =∑\\ns∈S 1S(s). We denote that a multiset S has all its elements in the interval Jx : yK by\\nS ⊆ Jx : yK.\\nFor a multiset S of integers, let ΣS =\\n∑\\ns∈S 1S(s) ·s denote the total sum of the elements\\nof S. The set of all subset sums is denoted by∑\\n(S) = {ΣT | T ⊆ S} .\\nThe pair of the set of all subset sums using sets of size at most α along with their associated\\ncardinality is denoted by\\n∑≤α [S] = {(ΣT , |T |) ∣∣ T ⊆ S, |T | ≤ α}. The set of all subset\\nsums of a set S up to a number u is denoted by\\n∑\\n≤u(S) =\\n∑\\n(S) ∩ J0 : uK.\\n2.2. From multisets to sets.\\nHere, we show that the case where the input is a multiset can be reduced to the case of a\\nset. The reduction idea is somewhat standard (see [26, Section 7.1.1]), and first appeared in\\n[28]. We present it here for completeness.\\nLemma 2.1. Given a multiset S of integers, and a number s ∈ S, with 1S(s) ≥ 3. Consider\\nthe multiset S′ resulting from removing two copies of s from S, and adding the number 2s\\nto it. Then,\\n∑\\n≤u(S) =\\n∑\\n≤u(S\\n′). Observe that card(S′) = card(S)− 1.\\nProof: Consider any multiset T ⊆ S. If T contains two or more copies of s, then replace two\\ncopies by a single copy of 2s. The resulting subset is T ′ ⊆ S′, and ΣT = ΣT ′ , establishing\\nthe claim.\\nLemma 2.2. Given a multiset S of integers in JuK of cardinality n with n′ unique values,\\none can compute, in O(n′ log2 u) time, a multiset T , such that: (i)\\n∑\\n≤u(S) =\\n∑\\n≤u(T ),\\n(ii) card(T ) ≤ card(S),\\n(iii) card(T ) = O(n′ log u), and\\n(iv) no element in T has multiplicity exceeding two.\\nProof: Copy the elements of S into a working multiset X. Maintain the elements of set(X)\\nin a heap D, and let T initially be the empty set. In each iteration, extract the minimum\\nelement x from the heap D. If x > u, we stop.\\nIf 1X(x) ≤ 2, then delete x from X, and add x, with its appropriate multiplicity, to the\\noutput multiset T , and continue to the next iteration.\\nIf 1X(x) > 2, then delete x from X, add x to the output set T (with multiplicity one),\\ninsert the number 2x into X with multiplicity m′ = ⌊(1X(x)− 1)/2⌋, (updating also the heap\\n4\\nD – by adding 2x if it is not already in it), and set 1X(x) ← 1X(x) − 2m′. The algorithm\\nnow continues to the next iteration.\\nAt any point in time, we have that\\n∑\\n≤u(S) =\\n∑\\n≤u(X ∪ T ), and every iteration takes\\nO(log u) time, and and as such overall, the running time is O(card(T ) log u), as each iteration\\nincreases card(T ) by at most two. Finally, notice that every element in T is of the form\\n2ix, x ∈ S for some i, where i ≤ log n, and thus card(T ) = O(n′ log u).\\nNote that the following lemma refers to sets.\\nLemma 2.3. Given two sets S, T ⊆ J0 : uK, one can compute S ⊕ T in O(u log u) time.\\nProof: Let fS(x) =\\n∑\\ni∈S x\\ni be the characteristic polynomial of S. Construct, in a similar\\nfashion, the polynomial fT and let g = fS ∗ fT . Observe that the coefficient of xi in g is\\ngreater than 0 if and only if i ∈ S⊕T . As such, using FFT, one can compute the polynomial\\ng in O(u log u) time, and extract S ⊕ T from it.\\nObservation 2.4. If P and Q form a partition of multiset S, then\\n∑\\n(S) =\\n∑\\n(P )⊕∑(Q).\\nCombining all of the above together, we can now state the following lemma which sim-\\nplifies the upcoming analysis.\\nLemma 2.5. Given an algorithm that computes\\n∑\\n≤u(S) in T(n, u) = Ω(u log\\n2 u) time, for\\nany set S ⊆ JuK with n elements, then one can compute ∑≤u(S′) for any multiset S′ ⊆ JuK,\\nwith n′ distinct elements, in O\\n(\\nT(n′ log u, u)\\n)\\ntime.\\nProof: First, from S, compute the multiset T as described in Lemma 2.2, in O(u log2 u)\\ntime. As every element in T appears at most twice, partition it into two sets P and Q.\\nThen\\n∑\\n≤u(T ) =\\n(∑\\n≤u(P )⊕\\n∑\\n≤u(Q)\\n)\\n∩ J0 : uK, which is computed using Lemma 2.3, in\\nO(u log u) time. This reduces all subset sums for multisets of n′ distinct elements to two\\ninstances of all subset sums for sets of size O(n′ log u).\\n2.3. The input is a set of positive integers.\\nIn the previous section it was shown that there is little loss in generality and running time\\nif the input is restricted to sets instead of multisets. For simplicity of exposition, we assume\\nthe input is a set from here on.\\nHere, we present the main algorithm: At a high level it uses a geometric partitioning\\non the input range J0 : uK to split the numbers into groups of exponentially long intervals.\\nEach of these groups is then processed separately abusing their interval range that bounds\\nthe cardinality of the sets from that group.\\nObservation 2.6. Let g be a positive, superadditive (i.e. g(x + y) ≥ g(x) + g(y),∀x, y)\\nfunction. For a function f(n,m) satisfying\\nf(n,m) = max\\nm1+m2=m\\n{\\nf\\n(n\\n2\\n,m1\\n)\\n+ f\\n(n\\n2\\n,m2\\n)\\n+ g(m)\\n}\\n,\\nwe have that f(n,m) = O (g(m) log n).\\nTheorem 2.7. Given a set of positive integers S with total sum σ, one can compute the set\\nof all subset sums\\n∑\\n(S) in O(σ log σ log n) time.\\nProof: Partition S into two sets L,R of (roughly) equal cardinality, and compute recursively\\nL′ =\\n∑\\n(L) and R′ =\\n∑\\n(R). Next, compute\\n∑\\n(S) = L′ ⊕R′ using Lemma 2.3. The recur-\\nrence for the running time is f(n, σ) = maxσ1+σ2=σ{f(n/2, σ1) + f(n/2, σ2) + O(σ log σ)},\\nand the solution to this recurrence, by Observation 2.6, is O(σ log σ log n).\\n5\\nRemark 2.8. The standard divide-and-conquer algorithm of Theorem 2.7 was already known\\nin [38, 14], here we showed a better analysis. Note, that the basic divide-and-conquer algo-\\nrithm without the FFT addition was known much earlier [23].\\nLemma 2.9 ([38, 14]). Given a set S ⊆ J∆K of size n, one can compute the set ∑(S) in\\nO\\n(\\nn∆ log(n∆) log n\\n)\\ntime.\\nProof: Observe that ΣS ≤ ∆n and apply Theorem 2.7.\\nLemma 2.10. Given two sets of points S, T ⊆ J0 : uK × J0 : vK, one can compute S ⊕ T in\\nO\\n(\\nuv log(uv)\\n)\\ntime.\\nProof: Let fS(x, y) =\\n∑\\n(i,j)∈S x\\niyj be the characteristic polynomial of S. Construct, sim-\\nilarly, the polynomial fT , and let g = fS ∗ fT . Note that the coefficient of xiyj is greater\\nthan 0 if and only if (i, j) ∈ S ⊕ T . One can compute the polynomial g by a straightforward\\nreduction to regular FFT (see multidimensional FFT [3, Chapter 12.8]), in O(uv log uv) time,\\nand extract S ⊕ T from it.\\nLemma 2.11. Given two disjoint sets B,C ⊆ Jx : x+ ℓK and ∑≤α [B], ∑≤α [C], one can\\ncompute\\n∑≤α [B ∪ C] in O (ℓα2 log(ℓα)) time.\\nProof: Consider the function f\\n(\\n(i, j)\\n)\\n= (i − xj, j). Let X = f\\n(∑≤α [B]) and Y =\\nf\\n(∑≤α [C]). If (i, j) ∈ ∑≤α [B] ∪∑≤α [C], then i = jx + y for y ∈ J0 : ℓjK. Hence\\nX,Y ⊆ J0 : ℓαK× J0 : αK.\\nComputing X ⊕ Y using the algorithm of Lemma 2.10 can be done in O (ℓα2 log(ℓα))\\ntime. Let Z = (X ⊕Y )∩ (J0 : ℓαK× J0 : αK). The set∑≤α [B ∪ C] is then precisely f−1(Z).\\nProjecting Z back takes an additional O\\n(\\nℓα2 log(ℓα)\\n)\\ntime.\\nLemma 2.12. Given a set S ⊆ Jx : x+ ℓK of size n, computing the set ∑≤α [S] takes\\nO\\n(\\nℓα2 log(ℓα) log n\\n)\\ntime.\\nProof: Compute the median of S, denoted by δ, in linear time. Next, partition S into two\\nsets L = S ∩ JδK and R = S ∩ Jδ + 1 : x+ ℓK. Compute recursively L′ = ∑≤α [L] and\\nR′ =\\n∑≤α [R], and combine them into ∑≤α [L ∪R] using Lemma 2.11. The recurrence for\\nthe running time is:\\nf(n, ℓ) = max\\nℓ1+ℓ2=ℓ\\n{\\nf\\n(n\\n2\\n, ℓ1\\n)\\n+ f\\n(n\\n2\\n, ℓ2\\n)\\n+O\\n(\\nℓα2 log(ℓα)\\n)}\\n,\\nwhich takes O\\n(\\nℓα2 log(ℓα) log n\\n)\\ntime, by Observation 2.6.\\nLemma 2.13. Given a set S ⊆ Jx : x+ ℓK of size n, computing the set ∑≤u(S) takes\\nO\\n(\\n(u/x)2ℓ log(ℓu/x) log n\\n)\\ntime.\\nProof: Apply Lemma 2.12 by setting α = ⌊u/x⌋ to get∑≤α [S]. Projecting down by ignoring\\nthe last coordinate and then intersecting with J0 : uK gives the set\\n∑\\n≤u(S).\\nLemma 2.14. Given a set S ⊆ JuK of size n and a parameter r0 ≥ 1, partition S as follows:\\n• S0 = S ∩ Jr0K, and\\n• for i > 0, Si = S ∩ Jri−1 + 1 : riK, where ri =\\n⌊\\n2ir0\\n⌋\\n.\\nThe resulting partition is composed of ν = O(log u) sets S0, S1, . . . , Sν and can be computed\\nin O(n log n) time.\\n6\\nProof: Sort the numbers in S, and throw them into the sets, in the obvious fashion. As for\\nthe number of sets, observe that 2ir0 > u when i > log u. As such, after log n sets, rν > u.\\nLemma 2.15. Given a set S ⊆ JuK of size n. For i = 0, . . . , ν = O(log u), let Si be the ith\\nset in the above partition and let |Si| = ni. One can compute\\n∑\\n≤u(Si), for all i, in overall\\nO\\n(\\n(u2/r0 +min{r0, n}r0) log2 u\\n)\\ntime.\\nProof: Because S ⊆ JuK, n = O(u). If i = 0, then S0 ⊆ Jr0K, and one can compute\\n∑\\n≤u(S0),\\nin O(n0r0 log(n0r0) log n0) time, using Lemma 2.9. Since n0 ≤ r0 and n0 ≤ n, this simplifies\\nto O\\n(\\nmin{n, r0}r0 log2 u\\n)\\n.\\nFor i > 0, the sets Si contain numbers at least as large as ri−1. Moreover, each set Si is\\ncontained in an interval of length ℓi = ri − ri−1 = ri−1. Now, using Lemma 2.13, one can\\ncompute\\n∑\\n≤u(Si) in O\\n(\\n(u/ri−1)2ℓi log(ℓiu/ri−1) log ni\\n)\\n= O\\n(\\nu2\\nri−1\\nlog2 u\\n)\\ntime. Summing\\nthis bound, for i = 1, . . . , ν, results in O\\n(\\nu2\\nr0\\nlog2 u\\n)\\nrunning time.\\nTheorem 2.16. Let S ⊆ JuK be a set of n elements. Computing the set of all subset sums∑\\n≤u(S) takes O\\n(\\nmin{√nu, u4/3} log2 u) time.\\nProof: Assuming the partition of Lemma 2.14, compute the subset sums Ti =\\n∑\\n≤u(Si), for\\ni = 0, . . . , ν. Let P1 = T1, and let Pi = (Pi−1 ⊕ Ti) ∩ JuK. Each Pi can be computed using\\nthe algorithm of Lemma 2.3. Do this for i = 1, . . . , ν, and observe that the running time to\\ncompute Pν , given all Ti, is O(ν(u log u)) = O(u log\\n2 u).\\nFinally, for all i = 1, . . . , ν calculating the Ti’s:\\n• By setting r0 equal to u2/3 and using Lemma 2.15 takes O\\n(\\nu4/3 log2 u\\n)\\n.\\n• By setting r0 equal to u√n and using Lemma 2.15 takes O\\n(√\\nnu log2 u\\n)\\n.\\nTaking the minimum of these two, proves the theorem.\\nPutting together Theorem 2.7, Theorem 2.16 and Lemma 2.5, results in the following\\nwhen the input is a multiset.\\nTheorem 2.17 (Main theorem). Let S ⊆ JuK be a multiset of n′ distinct elements, with\\ntotal sum σ, computing the set of all subset sums\\n∑\\n≤u(S) takes\\nO\\n(\\nmin\\n{√\\nn′ u log\\n5\\n2 u, u\\n4\\n3 log2 u, σ log σ log\\n(\\nn′ log u\\n)})\\ntime.\\n3. Subset sums for finite cyclic groups\\nIn this section, we demonstrate the robustness of the idea underlying the algorithm of Section\\n2 by showing how to extend it to work for finite cyclic groups. The challenge is that the\\nprevious algorithm throws away many sums that fall outside of JuK during its execution, but\\nthis can no longer be done for finite cyclic groups, since these sums stay in the group and as\\nsuch must be accounted for.\\n3.1. Notations.\\nFor any positive integer m, the set of integers modulo m with the operation of addition forms\\na finite cyclic group, the group Zm = {0, 1, . . . ,m−1} of order m. Every finite cyclic group\\nof order m is isomorphic to the group Zm (as such it is sufficient for our purposes to work\\nwith Zm). Let U(Zm) = {x ∈ Zm | gcd(x,m) = 1} be the set of units of Zm, and let\\nEuler’s totient function ϕ(m) = |U(Zm)| be the number of units of Zm. We remind the\\n7\\nreader that two integers α and β such that gcd(α, β) = 1 are coprime (or relatively prime).\\nThe set\\nxJℓK =\\n{\\nx, 2x, . . . , ℓx\\n}\\nis a finite arithmetic progression, henceforth referred to as a segment of length |xJℓK| = ℓ.\\nFinally, let S/x = {s/x | s ∈ S and x | s} and S%x = {s ∈ S | x ∤ s}, where x | s and x ∤ s\\ndenote that “s divides q” and “s does not divide q”, respectively. For an integer x, let σ0(x)\\ndenote the number of divisors of x and σ1(x) the sum of its divisors.\\n3.2. Subset sums and segments.\\nLemma 3.1. For a set S ⊆ Zm of size n, such that S ⊆ xJℓK, the set\\n∑\\n(S) can be computed\\nin O (nℓ log(nℓ) log n) time.\\nProof: All elements of xJℓK are multiplicities of x, and thus S′ := S/x ⊆ JℓK is a well defined\\nset of integers. Next, compute\\n∑\\n(S′) in O(nℓ log(nℓ) log n) time using the algorithm of\\nLemma 2.9 (over the integers). Finally, compute the set {σx (mod m) | σ ∈∑(S′)} =∑(S)\\nin linear time.\\nLemma 3.2. Let S ⊆ Zm be a set of size n covered by segments x1JℓK, . . . , xkJℓK, formally\\nS ⊆ ⋃ki=1 xiJℓK, then the set ∑(S) can be computed in O(km logm+ nℓ log(nℓ) log n) time.\\nProof: Partition, in O(kn) time, the elements of S into k sets S1, . . . , Sk, such that Si ⊆ xiJℓK,\\nfor i ∈ JkK. Next, compute the subset sums Ti =\\n∑\\n(Si) using the algorithm of Lemma 3.1,\\nfor i ∈ JkK. Then, compute T1 ⊕ T2 ⊕ . . .⊕ Tk =\\n∑\\n(S), by k− 1 applications of Lemma 2.3.\\nThe resulting running time is O\\n(\\n(k−1)m logm+∑i |Si|ℓ log(|Si|ℓ) log |Si|) = O(km logm+\\nnℓ log(nℓ) log n).\\n3.3. Covering a subset of U(Zm) by segments.\\nSomewhat surprisingly, one can always find a short but “heavy” segment.\\nLemma 3.3. Let S ⊆ U = U(Zm), there exists a constant c, for any ℓ such that c2 lnmln lnm ≤\\nℓ ≤ m there exists an element x ∈ U such that |xJℓK ∩ S| = Ω ( ℓm |S|).\\nProof: Fix a β ∈ U . For i ∈ U ∩ JℓK consider the modular equation ix ≡ β (mod m), this\\nequation has a unique solution x ∈ U – here we are using the property that i and β are\\ncoprime to m. Let α = |U |/2m. Let ω(m) be the number of distinct prime factors of m, and\\nθ(m) = 2ω(m) be the number of distinct square-free divisors of m. Then θ(m) ≤ c2 lnmln lnm < αℓ\\n[35]. There are at least 2αℓ− θ(m) ≥ αℓ elements in U ∩ JℓK [40, Equation (1.4)].\\nHence, when β ∈ U is fixed, the number of values of x such that β ∈ xJℓK is at least\\nαℓ. Namely, every element of S ⊆ U is covered by at least αℓ segments {xJℓK | x ∈ U}. As\\nsuch, for a random x ∈ U the expected number of elements of S that are contained in xJℓK\\nis (|S|αℓ) /|U | = ℓ2m |S|. Therefore, there must be a choice of x such that |xJℓK∩S| is larger\\nthan the average, implying the claim.\\nOne can always find a small number of segments of length ℓ that contain all the elements\\nof U(Zm).\\nLemma 3.4. Let S ⊆ U(Zm) of size n, then for any ℓ such that ℓ ≥ m1/2 there is a collection\\nL of O(mℓ lnn) segments, each of length ℓ, such that S ⊆\\n⋃\\nx∈L xJℓK. Furthermore, such a\\ncover can be computed in O\\n(\\n(n+ logm) ℓ\\n)\\ntime.\\n8\\nProof: Consider the set system defined by the ground set Zm and the sets {xJℓK|x ∈ U(Zm)}.\\nNext, consider the standard greedy set cover algorithm [24, 39, 30]: Pick a segment xJℓK such\\nthat |xJℓK∩S| is maximized, remove all elements of S covered by xJℓK, add xJℓK to the cover,\\nand repeat. By Lemma 3.3, there is a choice of x such that the segment xJℓK contains at least\\na cℓ/m fraction of S, for some constant c. After m/cℓ iterations of this process, there will\\nbe at most (1− cℓ/m)m/cℓ n ≤ n/e elements remaining. As such, after O(mℓ lnn) iterations\\nthe original set S is covered.\\nTo implement this efficiently, in the preprocessing stage compute the modular inverses of\\nevery element in JℓK using the extended Euclidean algorithm, in O(ℓ logm) time [9, Section\\n31.2]. Then, for every b ∈ S and every i ∈ JℓK, find the unique x (if it exists) such that\\nix ≡ b (mod m), using the inverse i−1 in O(1) time. This indicates that b is in xJℓK ∩ S.\\nNow, the algorithm computes xJℓK ∩ S, for all x, in time O(nℓ + ℓ logm). Next, feed the\\nsets xJℓK ∩ S, for all x, to a linear time greedy set cover algorithm and return the desired\\nsegments in O(nℓ) time [9, Section 35.3]. The total running time is O\\n(\\n(n + logm) ℓ\\n)\\n.\\n3.4. Subset sums when all numbers are coprime to m.\\nLemma 3.5. Let S ⊆ U(Zm) be a set of size n. Computing the set of all subset sums\\n∑\\n(S)\\ntakes O\\n(\\nmin\\n{√\\nnm,m5/4\\n}\\nlogm log n\\n)\\ntime.\\nProof: If |S| ≥ 2√m, then ∑(S) = Zm [20, Theorem 1.1]. As such, the case where n =\\n|S| ≥ 2√m is immediate.\\nFor the case that n < 2\\n√\\nm we do the following. Apply the algorithm of Lemma 3.4 for\\nℓ = m/\\n√\\nn ≥ m1/2. This results in a cover of S by O(mℓ log n) segments (each of length ℓ),\\nwhich takes O\\n(\\n(n+logm) ℓ\\n)\\n= O(\\n√\\nnm logm) time. Next, apply the algorithm of Lemma 3.2\\nto compute\\n∑\\n(S) in O(nℓ log(nℓ) log n) = O(\\n√\\nnm logm log n) time. Since, n = O(\\n√\\nm) this\\nrunning time is O\\n(\\nmin\\n{√\\nnm,m5/4\\n}\\nlogm log n\\n)\\n.\\n3.5. The algorithm: Input is a subset of Zm.\\nIn this section, we show how to tackle the general case when S is a subset of Zm.\\n3.5.1. Algorithm.\\nThe input instance is a triple (Γ, µ, τ), where Γ is a set, µ its modulus and τ an auxiliary\\nparameter. For such an instance (Γ, µ, τ) the algorithm computes the set of all subset sums\\nof Γ modulo µ. The initial instance is (S,m,m).\\nLet q be the smallest prime factor of τ , referred to as pivot. Partition Γ into the two sets:\\nΓ/q =\\n{\\ns/q\\n∣∣ s ∈ Γ and q | s} and Γ%q = {s ∈ Γ ∣∣ q ∤ s} .\\nRecursively compute the (partial) subset sums\\n∑\\n(Γ/q) and\\n∑\\n(Γ%q), of the instances\\n(Γ/q, µ/q, τ/q) and (Γ%q, µ, τ/q), respectively. Then compute the set of all subset sums∑\\n(Γ) =\\n{\\nqx\\n∣∣ x ∈ ∑(Γ/q)} ⊕∑(Γ%q) by combining them together using Lemma 2.3. At\\nthe bottom of the recursion, when τ = 1, for each set compute its subset sums, using the\\nalgorithm of Lemma 3.5.\\n3.5.2. Handling multiplicities.\\nDuring the execution of the algorithm there is a natural tree formed by the recursion. Con-\\nsider an instance (Γ, µ, τ) such that the pivot q divides τ (and µ) with multiplicity r. The\\ntop level recursion would generate instances with sets Γ/q and Γ%q. In the next level, Γ/q is\\npartitioned into Γ/q2 and (Γ/q)%q. On the other side of the recursion Γ%q gets partitioned\\n9\\n(naively) into (Γ%q)/q (which is an empty set) and (Γ%q)%q = Γ%q. As such, this is a\\nsuperfluous step and can be skipped. Hence, compressing the r levels of the recursion for\\nthis instance results in r + 1 instances:\\nΓ%q, (Γ/q)%q, . . . , (Γ/qr−1)%q, Γ/qr .\\nThe total size of these sets is equal to the size of Γ. In particular, compress this subtree\\ninto a single level of recursion with the original call having r + 1 children. At each such\\nlevel of the tree label the edges by 0, 1, 2, . . . , r, based on the multiplicity of the divisor of\\nthe resulting (node) instance (i.e., an edge between instance sets Γ and (Γ/q2)%q would be\\nlabeled by “2”).\\n3.5.3. Analysis.\\nThe recursion tree formed by the execution of the algorithm has a level for each of the\\nk = O(logm/ log logm) distinct prime factors of m [35] – assume the root level is the 0th\\nlevel.\\nLemma 3.6. Consider running the algorithm on input (S,m,m). Then the values of the\\nmoduli at the leaves of the recursion tree are unique, and are precisely the divisors of m.\\nProof: Let m =\\n∏k\\ni=1 q\\nri\\ni be the prime factorization of m, where qi < qi+1 for all 1 ≤ i < k.\\nThen every vector x = (x1, . . . , xk), with 0 ≤ xi ≤ ri, defines a path from the root to a\\nleaf of modulus m/\\n∏k\\ni=1 q\\nxi\\ni in the natural way: Starting at the root, at each level of the\\ntree follow the edge labeled xi. If for two vectors x and y there is an i ∈ JkK such that\\nxi 6= yi, then the two paths they define will be different (starting at the ith level). And,\\nby the unique factorization of integers, the values of the moduli at the two leaves will also\\nbe different. Finally, note that every divisor of m,\\n∏k\\ni=1 q\\nρi\\ni with 0 ≤ ρi ≤ ri, occurs as a\\nmodulus of a leaf, and can be reached by following the path (r1 − ρ1, . . . , rk − ρk) down the\\ntree.\\nTheorem 3.7. Let S ⊆ Zm be a set of size n. Computing the set of all subset sums\\n∑\\n(S)\\ntakes O\\n(\\nmin\\n{√\\nnm,m5/4\\n}\\nlog2m\\n)\\ntime.\\nProof: The algorithm is described in Section 3.5.1, when the input is (S,m,m). We break\\ndown the running time analysis into two parts: The running time at the leaves, and the\\nrunning time at internal nodes.\\nLet δ be the number of leaves of the recursion tree. Arrange them so the modulus of the\\nith leaf, µi, is the ith largest divisor of m. Note that µi is at most m/i, for all i ∈ JδK. Using\\nLemma 3.5, the running time is bounded by\\nO\\n(\\nδ∑\\ni=1\\nmin\\n{√\\nni µi, µ\\n5/4\\ni\\n}\\nlog ni log µi\\n)\\n= O\\n(\\nlogm log n\\nδ∑\\ni=1\\nmin\\n{√\\nni\\nm\\ni\\n,\\n(m\\ni\\n)5/4})\\n.\\nUsing Cauchy-Schwartz, the first sum of the min is bounded by\\nm\\nδ∑\\ni=1\\n√\\nni\\ni\\n≤ m\\n√√√√( δ∑\\ni=1\\n(\\n√\\nni)\\n2\\n)(\\nδ∑\\ni=1\\n1\\ni2\\n)\\n= O\\n(√\\nnm\\n)\\n,\\nand the second by O(m5/4). Putting it all together, the total work done at the leaves is\\nO\\n(\\nmin\\n{√\\nnm,m5/4\\n}\\nlogm log n\\n)\\n.\\nNext, consider an internal node of modulus µ, pivot q and r+1 children. The algorithm\\ncombines these instances, by applying r times Lemma 2.3. The total running time necessary\\n10\\nfor this process is described next. As the moduli of the instances decrease geometrically, pair\\nup the two smallest instances, combine them together, and in turn combine the result with\\nthe next (third) smallest instance, and so on. This yields a running time of\\nO\\n(\\nr∑\\ni=1\\nµ\\nqi\\nlog\\nµ\\nqi\\n)\\n= O(µ log µ) .\\nAt the leaf level, by Lemma 3.6, the sum of the moduli\\n∑δ\\ni=1 µi equals to σ1(m), and it is\\nknown that σ1(m) = O(m log logm) [21, Theorem 323]. As such, the sum of the moduli of\\nall internal nodes is bounded by O(km log logm) = O(m logm), as the sum of each level is\\nbounded by the sum at the leaf level, and there are k levels. As each internal node, with\\nmodulus µ, takes O(µ log µ) time and x log x is a convex function, the total running time\\nspent on all internal nodes is O\\n(\\nm logm log(m logm)\\n)\\n= O(m log2m).\\nAggregating everything together, the complete running time of the algorithm is bounded\\nby O\\n(\\nmin\\n{√\\nnm,m5/4\\n}\\nlog2m\\n)\\n, implying the theorem.\\nThe results of this section, along with the analysis of the recursion tree above, conclude\\nthe following corollary on covering Zm with a small number of segments. The result is useful\\nfor error correction codes, and improves the recent bound of Chen et al. by a factor of\\n√\\nℓ\\n[8].\\nCorollary 3.8. There exist a constant c, for all ℓ such that c2\\nlnm\\nln lnm ≤ ℓ ≤ m, one can cover\\nZm with O ((σ1(m) lnm)/ℓ) + σ0(m) segments of length ℓ. Furthermore, such a cover can be\\ncomputed in O(mℓ) time.\\nProof: Let Sm/d = {x/(m/d) | x ∈ Zm and gcd(x,m) = m/d}, for all d | m. Note that\\nSm/d = U(Zd), hence by Lemma 3.4, each Sm/d has a cover of O((d ln d)/ℓ) segments. Next,\\n“lift” the segments of each set Sm/d back up to Zm (by multiplying by m/d) forming a cover\\nof Zm. The number of segments in the final cover is bounded by∑\\nd|m\\nℓ≤d\\nO\\n(\\nd\\nℓ\\nlnm\\n)\\n+\\n∑\\nd|m\\nℓ>d\\n1 = O\\n(\\nσ1(m) lnm\\nℓ\\n)\\n+ σ0(m) .\\nThe time to cover each Sm/d, by Lemma 3.4, is O\\n(\\n(n+logm) ℓ\\n)\\n= O\\n(\\n(ϕ(d) + log d) ℓ\\n)\\n, since\\nthere are ϕ(d) elements in Sm/d, and Sm/d ⊆ Zd. Also, ϕ(d) dominates log d, as O\\n(\\nϕ(d)\\n)\\n=\\nΩ(d/ log log d) [21, Theorem 328], therefore the running time simplifies to O\\n(\\nϕ(d)ℓ\\n)\\n. Sum-\\nming over all Sm/d we have\\n∑\\nd|m\\nO\\n(\\nϕ(d)ℓ\\n)\\n= O\\n\\uf8eb\\uf8edℓ∑\\nd|m\\nϕ(d)\\n\\uf8f6\\uf8f8 = O(mℓ) ,\\nsince\\n∑\\nd|m ϕ(d) = m [21, Sec 16.2], implying the corollary.\\nIf ℓ < c2\\nlnm\\nln lnm , then ℓ = mo(1). The corollary above then shows that for all ℓ, there is a\\ncover of Zm with m\\n1+o(1)/ℓ segments.\\n4. Recovering the solution\\nGiven sets X and Y , a number x is a witness for i ∈ X ⊕ Y , if x ∈ X and i − x ∈ Y . A\\nfunction w : X ⊕ Y → X is a witness function , if w(i) is a witness of i.\\nIf one can find a witness function for each X ⊕ Y computation of the algorithm, then\\nwe can traceback the recursion tree and reconstruct the subset that sums up to t in O(n)\\ntime. The problem of finding a witness function quickly can be reduced to the reconstruction\\nproblem defined next.\\n11\\n4.1. Reduction to the reconstruction problem.\\nIn the reconstruction problem, there are hidden sets S1, . . . , Sn ⊆ JmK and we have two\\noracles Size and Sum that take as input a query set Q.\\n• Size(Q) returns the size of each intersection:(|S1 ∩Q|, |S2 ∩Q|, . . . , |Sn ∩Q|)\\n• Sum(Q) returns the sum of elements in each intersection:\\uf8eb\\uf8ed ∑\\ns∈S1∩Q\\ns,\\n∑\\ns∈S2∩Q\\ns, . . . ,\\n∑\\ns∈Sn∩Q\\ns\\n\\uf8f6\\uf8f8\\nThe reconstruction problem asks to find n values x1, . . . , xn such that for all i, if Si is non-\\nempty, xi ∈ Si. Let f be the running time of calling the oracles, and assume f = Ω(m+ n),\\nthen is it known that one can find x1, . . . , xn in O(f log n polylogm) time [1].\\nIf X,Y ⊆ JuK, finding the witness of X ⊕ Y is just a reconstruction problem. Here the\\nhidden sets are W0, . . . ,W2u ⊆ J2uK, where Wi = {x | x + y = i and x ∈ X, y ∈ Y } is the\\nset of witnesses of i. Next, define the polynomials χQ(x) =\\n∑\\ni∈Q x\\ni and IQ(x) =\\n∑\\ni∈Q ix\\ni.\\nThe coefficient for xi in χQχY is |Wi∩Q| and in IQχY is\\n∑\\ns∈Wi∩Q s, which are precisely the\\nith coordinate of Size(Q) and Sum(Q), respectively. Hence, the oracles can be implemented\\nusing polynomial multiplication, in O˜(u) time per call. This yields an O˜(u) time deterministic\\nalgorithm to compute X ⊕ Y with its witness function.\\nHence, with a polylogarithmic slowdown, we can find a witness function every time we\\nperform a ⊕ operation, thus, effectively, maintaining which subsets sum up to which sum.\\n5. Applications and extensions\\nSince every algorithm that uses subset sum as a subroutine can benefit from the new algo-\\nrithm, we only highlight certain selected applications and some interesting extensions. Most\\nof these applications are derived directly from the divide-and-conquer approach.\\n5.1. Bottleneck graph partition.\\nLet G = (V,E) be a graph with n vertices m edges and let w : E → R+ be a weight\\nfunction on the edges. The bottleneck graph partition problem is to split the vertices into\\ntwo equal-sized sets such that the value of the bottleneck (maximum-weight) edge, over\\nall edges across the cut, is minimized. This is the simplest example of a graph partition\\nproblem with cardinality constraints. The standard divide-and-conquer algorithm reduces\\nthis problem to solving O(log n) subset sum problems: Pick a weight, delete all edges with\\nsmaller weight and decide if there exists an arrangement of components that satisfy the size\\nrequirement [22]. The integers being summed are the various sizes of the components, the\\ntarget value is n/2, and the sum of all inputs is n. Previously, using the O(σ3/2) algorithm\\nby Klinz and Woeginger, the best known running time was O(m + n3/2 log n) [27]. Using\\nTheorem 2.7, this is improved to O(m) + O˜(n) time.\\n5.2. All subset sums with cardinality information.\\nLet S = {s1, s2, . . . , sn}. Define\\n∑≤n\\n≤u(S) to be the set of pairs (i, j), such that (i, j) ∈∑≤n\\n≤u(S) if and only if i ≤ u, j ≤ n and there exists a subset of size j in S that sums up to\\ni. We are interested in computing the set\\n∑≤n\\n≤u(S).\\n12\\nWe are only aware of a folklore dynamic programming algorithm for this problem that\\nruns in O(n2u) time. We include it here for completion. Let D[i, j, k] be true if and only if\\nthere exists a subset of size j that sums to i using the first k elements. The recursive relation\\nis\\nD[i, j, k] =\\n\\uf8f1\\uf8f4\\uf8f2\\uf8f4\\uf8f3\\ntrue if i = j = k = 0\\nfalse if i > j = k = 0\\nD[i, j, k − 1] ∨D[i− sk, j − 1, k − 1] otherwise\\nwhere we want to compute D[i, j, n] for all i ≤ u and j ≤ n. In the following we show how\\nto do (significantly) better.\\nTheorem 5.1. Let S ⊆ JuK be a set of size n, then one can compute the set ∑≤n≤u(S) in\\nO\\n(\\nnu log(nu) log n\\n)\\ntime.\\nProof: Partition S into two (roughly) equally sized sets S1 and S2. Find\\n∑≤n/2\\n≤u (S1) and∑≤n/2\\n≤u (S2) recursively, and combine them using Lemma 2.10, in O\\n(\\nnu log(nu)\\n)\\ntime. The\\nfinal running time is then given by Observation 2.6.\\n5.3. Counting and power index.\\nHere we show that the standard divide-and-conquer algorithm can also answer the counting\\nversion of all subset sums. Namely, computing the function Nu,S(x): the number of subsets\\nof S that sum up to x, where x ≤ u.\\nFor two functions f, g : X → Y , define f ⊙ g : X → Y to be\\n(f ⊙ g)(x) =\\n∑\\nt∈X\\nf(x)g(x− t)\\nCorollary 5.2. Given two functions f, g : J0 : uK → N such that f(x), g(x) ≤ b for all x,\\none can compute f ⊙ g in O(u log u log b) time.\\nProof: This is an immediate extension of Lemma 2.10 using the fact that multiplication of\\ntwo degree u polynomials, with coefficient size at most b, takes O(u log u log b) time [36].\\nTheorem 5.3. Let S be a set of n positive integers. One can compute the function Nu,S in\\nO(nu log u log n) time.\\nProof: Partition S into two (roughly) equally sized sets S1 and S2. Compute Nu,S1 and\\nNu,S2 recursively, and combine them into Nu,S = Nu,S1 ⊙ Nu,S1 using Lemma 5.2, in\\nO(u log u log 2n) = O(nu log u) time. The final running time is then given by Observa-\\ntion 2.6.\\n5.3.1. Power indices.\\nThe Banzhaf index of a set S of n voters with cutoff u can be recovered from Nu,S in linear\\ntime. The Theorem 5.3 yields an algorithm for computing the Banzhaf index in O˜(nu)\\ntime. Previous dynamic programming algorithms take O(nu) arithmetic operations, which\\ntranslates to O(n2u) running time [42]. Similar speed-ups (of, roughly, a factor n) can be\\nobtained for the Shapley-Shubik index.\\n13\\nAcknowledgments\\nWe would like to thank Sariel Har-Peled for his invaluable help in the editing of this paper as\\nwell as for various suggestions on improving the presentation of the results. We would also\\nlike to thank Jeff Erickson and Kent Quanrud for their insightful comments and feedback.\\nWe would like to thank Igor Shparlinski and Arne Winterhof for pointing out a problem\\nin a proof. Finally, we also like to thank the anonymous reviewers for their helpful and\\nmeaningful comments on the different versions of this manuscript.\\nReferences\\n[1] Yonatan Aumann, Moshe Lewenstein, Noa Lewenstein, and Dekel Tsur. Finding wit-\\nnesses by peeling. ACM Trans. Algorithms, 7(2):24:1–24:15, March 2011.\\n[2] Richard Bellman. Notes on the theory of dynamic programming iv - maximization over\\ndiscrete sets. Naval Research Logistics Quarterly, 3(1-2):67–70, 1956.\\n[3] Richard E. Blahut. Fast Algorithms for Digital Signal Processing. Addison-Wesley\\nLongman Publishing Co., Inc., Boston, MA, USA, 1st edition, 1985.\\n[4] Karl Bringmann. A near-linear pseudopolynomial time algorithm for subset sum. To\\nappear in SODA ’17, 2017.\\n[5] Leizhen Cai, Siu Man Chan, and Siu On Chan. Random Separation: A New Method\\nfor Solving Fixed-Cardinality Optimization Problems, pages 239–250. Springer Berlin\\nHeidelberg, Berlin, Heidelberg, 2006.\\n[6] Yair Caro and Raphael Yuster. The characterization of zero-sum (mod 2) bipartite\\nramsey numbers. Journal of Graph Theory, 29(3):151–166, 1998.\\n[7] Mark Chaimovich. New algorithm for dense subset-sum problem. Astrisque, (258):363–\\n373, 1999.\\n[8] Z. Chen, I. E. Shparlinski, and A. Winterhof. Covering sets for limited-magnitude errors.\\nIEEE Transactions on Information Theory, 60(9):5315–5321, Sept 2014.\\n[9] T.H. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein. Introduction to Algorithms.\\nThe MIT Press, 3rd edition, 2014.\\n[10] Marek Cygan, Holger Dell, Daniel Lokshtanov, D’niel Marx, Jesper Nederlof, Yoshio\\nOkamoto, Ramamohan Paturi, Saket Saurabh, and Magnus Wahlstrom. On problems\\nas hard as cnf-sat. In Proceedings of the 2012 IEEE Conference on Computational Com-\\nplexity (CCC), CCC ’12, pages 74–84, Washington, DC, USA, 2012. IEEE Computer\\nSociety.\\n[11] George B. Dantzig. Discrete-variable extremum problems. Operations Research,\\n5(2):266–277, 1957.\\n[12] Jonathan L. Dautrich, Jr. and Chinya V. Ravishankar. Compromising privacy in precise\\nquery protocols. In Proceedings of the 16th International Conference on Extending\\nDatabase Technology, EDBT ’13, pages 155–166, New York, NY, USA, 2013. ACM.\\n[13] Josep Diaz, Fabrizio Grandoni, and AlbertoMarchetti Spaccamela. Balanced cut approx-\\nimation in random geometric graphs. In Tetsuo Asano, editor, Algorithms and Com-\\nputation, volume 4288 of Lecture Notes in Computer Science, pages 527–536. Springer\\nBerlin Heidelberg, 2006.\\n14\\n[14] David Eppstein. Minimum range balanced cuts via dynamic subset sums. Journal of\\nAlgorithms, 23(2):375 – 385, 1997.\\n[15] Bruce Faaland. Solution of the value-independent knapsack problem by partitioning.\\nOperations Research, 21(1):pp. 332–337, 1973.\\n[16] Zvi Galil and Oded Margalit. An almost linear-time algorithm for the dense subset-sum\\nproblem. SIAM J. Comput., 20(6):1157–1189, December 1991.\\n[17] Celia A. Glass and Hans Kellerer. Parallel machine scheduling with job assignment\\nrestrictions. Naval Research Logistics (NRL), 54(3):250–257, 2007.\\n[18] Venkatesan Guruswami, Yury Makarychev, Prasad Raghavendra, David Steurer, and\\nYuan Zhou. Finding almost-perfect graph bisections. In Innovations in Computer Sci-\\nence - ICS 2010, Tsinghua University, Beijing, China, January 7-9, 2011. Proceedings,\\npages 321–337, 2011.\\n[19] C. Guret and C. Prins. A new lower bound for the open-shop problem. Annals of\\nOperations Research, 92(0):165–183, 1999.\\n[20] Y.O. Hamidoune, A.S. Llad, and O. Serra. On complete subsets of the cyclic group.\\nJournal of Combinatorial Theory, Series A, 115(7):1279 – 1285, 2008.\\n[21] Godfrey Harold Hardy and Edward Maitland Wright. An introduction to the theory\\nof numbers. Oxford Science Publications. Clarendon Press, Oxford, fifth edition, 1979.\\nAutres tirages : 1983, 1985, 1988 (avec corrections), 1989, 1990, 1992, 1994, 1996, 1998.\\n[22] Dorit S. Hochbaum and Anu Pathria. The bottleneck graph partition problem. Net-\\nworks, 28(4):221–225, 1996.\\n[23] Ellis Horowitz and Sartaj Sahni. Computing partitions with applications to the knapsack\\nproblem. J. ACM, 21(2):277–292, April 1974.\\n[24] David S. Johnson. Approximation algorithms for combinatorial problems. In Proceedings\\nof the Fifth Annual ACM Symposium on Theory of Computing, STOC ’73, pages 38–49,\\nNew York, NY, USA, 1973. ACM.\\n[25] Richard M. Karp. Reducibility among combinatorial problems. In Raymond E. Miller,\\nJames W. Thatcher, and Jean D. Bohlinger, editors, Complexity of Computer Compu-\\ntations, The IBM Research Symposia Series, pages 85–103. Springer US, 1972.\\n[26] H. Kellerer, U. Pferschy, and D. Pisinger. Knapsack Problems. Springer, 2004.\\n[27] Bettina Klinz and Gerhard J. Woeginger. A note on the bottleneck graph partition\\nproblem. Networks, 33(3):189–191, 1999.\\n[28] Eugene L. Lawler. Fast approximation algorithms for knapsack problems. Mathematics\\nof Operations Research, 4(4):339–356, 1979.\\n[29] Daniel Lokshtanov and Jesper Nederlof. Saving space by algebraization. In Proceedings\\nof the Forty-second ACM Symposium on Theory of Computing, STOC ’10, pages 321–\\n330, New York, NY, USA, 2010. ACM.\\n[30] L. Lova´sz. On the ratio of optimal integral and fractional covers. Discrete Math.,\\n13(4):383–390, January 1975.\\n[31] U. Pferschy. Dynamic programming revisited: Improving knapsack algorithms. Com-\\nputing, 63(4):419–430, 1999.\\n15\\n[32] Pisinger. Dynamic programming on the word ram. Algorithmica, 35(2):128–145, 2003.\\n[33] David Pisinger. Linear time algorithms for knapsack problems with bounded weights.\\nJournal of Algorithms, 33(1):1 – 14, 1999.\\n[34] Xiangtong Qi. Coordinated logistics scheduling for in-house production and outsourcing.\\nAutomation Science and Engineering, IEEE Transactions on, 5(1):188–192, Jan 2008.\\n[35] Guy Robin. Estimation de la fonction de Tchebychef θ sur le k-ie`me nombre premier\\net grandes valeurs de la fonction ω(n) nombre de diviseurs premiers de n. Acta Arith.,\\n42(4):367–389, 1983.\\n[36] Arnold Scho¨nhage. Asymptotically fast algorithms for the numerical muitiplication and\\ndivision of polynomials with complex coefficients, pages 3–15. Springer Berlin Heidelberg,\\nBerlin, Heidelberg, 1982.\\n[37] Oliver Serang. The probabilistic convolution tree: Efficient exact bayesian inference for\\nfaster lc-ms/ms protein inference. PLoS ONE, 9(3):e91507, 03 2014.\\n[38] Oliver Serang. A fast numerical method for max-convolution and the application to\\nefficient max-product inference in bayesian networks. Journal of Computational Biology,\\n22(8):770–783, 2015.\\n[39] S.K Stein. Two combinatorial covering theorems. Journal of Combinatorial Theory,\\nSeries A, 16(3):391 – 397, 1974.\\n[40] D. Suryanarayana. On ∆(x, n) = φ(x, n) −xφ(n)/n. Proceedings of the American\\nMathematical Society, 44(1):17–21, jan 1974.\\n[41] Quoc Trung Tran, Chee-Yong Chan, and Guoping Wang. Evaluation of set-based queries\\nwith aggregation constraints. In Proceedings of the 20th ACM Conference on Informa-\\ntion and Knowledge Management, CIKM 2011, Glasgow, United Kingdom, October\\n24-28, 2011, pages 1495–1504, 2011.\\n[42] Takeaki Uno. Efficient computation of power indices for weighted majority games. In\\nKun-Mao Chao, Tsan-sheng Hsu, and Der-Tsai Lee, editors, Algorithms and Computa-\\ntion, volume 7676 of Lecture Notes in Computer Science, pages 679–689. Springer Berlin\\nHeidelberg, 2012.\\n16\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd14'), 'authors': 'Alman, Josh, Williams, Ryan', 'year': '2015', 'title': 'Probabilistic Polynomials and Hamming Nearest Neighbors', 'full_text': 'ar\\nX\\niv\\n:1\\n50\\n7.\\n05\\n10\\n6v\\n1 \\n [c\\ns.D\\nS]\\n  1\\n7 J\\nul \\n20\\n15\\nProbabilistic Polynomials and Hamming Nearest Neighbors\\n(Full Version)\\nJosh Alman∗ Ryan Williams†\\nJuly 21, 2015\\nAbstract\\nWe show how to compute any symmetric Boolean function on n variables over any field (as well as\\nthe integers) with a probabilistic polynomial of degree O(\\n√\\nn log(1/ε)) and error at most ε . The degree\\ndependence on n and ε is optimal, matching a lower bound of Razborov (1987) and Smolensky (1987)\\nfor the MAJORITY function. The proof is constructive: a low-degree polynomial can be efficiently\\nsampled from the distribution.\\nThis polynomial construction is combined with other algebraic ideas to give the first subquadratic\\ntime algorithm for computing a (worst-case) batch of Hamming distances in superlogarithmic dimen-\\nsions, exactly. To illustrate, let c(n) : N → N. Suppose we are given a database D of n vectors in\\n{0,1}c(n) logn and a collection of n query vectors Q in the same dimension. For all u ∈ Q, we wish to\\ncompute a v ∈D with minimum Hamming distance from u. We solve this problem in n2−1/O(c(n) log2 c(n))\\nrandomized time. Hence, the problem is in “truly subquadratic” time for O(logn) dimensions, and in\\nsubquadratic time for d = o((log2 n)/(loglogn)2). We apply the algorithm to computing pairs with max-\\nimum inner product, closest pair in ℓ1 for vectors with bounded integer entries, and pairs with maximum\\nJaccard coefficients.\\n∗Computer Science Department, Stanford University. Supported by NSF CCF-1212372 and NSF DGE-114747\\n†Computer Science Department, Stanford University, rrw@cs.stanford.edu. Supported in part by a David Morgenthaler\\nII Faculty Fellowship, and NSF CCF-1212372. Any opinions, findings and conclusions or recommendations expressed in this\\nmaterial are those of the authors and do not necessarily reflect the views of the National Science Foundation.\\n1 Introduction\\nRecall the Hamming nearest neighbor problem (HNN): given a set D of n database points in the d-\\ndimensional hypercube, we wish to preprocess D to support queries of the form q ∈ {0,1}d , where a query\\nanswer is a point u∈D that differs from q in a minimum number of coordinates. Minsky and Papert ([MP69],\\nChapter 12.7) called this the “Best Match” problem, and it has been widely studied since. Like many\\nsituations where one wants to find points that are “most similar” to query points, HNN is fundamental to\\nmodern computing, especially in search and error correction [Ind04]. However, known exact solutions to the\\nproblem require a data structure of 2Ω(d) size (storing all possible queries) or query time Ω(n/poly(log n))\\n(trying nearly all the points in the database). This is one of many examples of the curse of dimensionality\\nphenomenon in search, with corresponding data structure lower bounds. For instance, Barkol and Rabani\\n[BR00] show a size-query tradeoff for HNN in d dimensions in the cell-probe model: if one uses s cells of\\nsize b to store the database and probes at most t cells in a query, then either s = 2Ω(d/t) or b = nΩ(1)/t.\\nDuring the late 90’s, a new direction opened in the search for better nearest neighbor algorithms. The\\ndriving intuition was that it may be easier to find and generally good enough to have approximate solutions:\\npoints with distance within (1+ ε) of the optimum. Utilizing novel hashing and dimensionality reduction\\ntechniques, this beautiful line of work has had enormous impact [Kle97, IM98, KOR00, Pan06, AI06, Val12,\\nAINR14, AR15]. Still, when turning to approximations, the exponential-in-d dependence generally turns\\ninto an exponential-in-1/ε dependence, leading to a “curse of approximation” [Pat08], with lower bounds\\nmatching this intuition [CCGL99, CR04, AIP06]. For example, Andoni, Indyk, and Patrascu [AIP06] prove\\nthat any data structure for (1+ ε)-approximate HNN using O(1) probes requires nΩ(1/ε2) space.\\nIn this paper, we revisit exact nearest neighbors in the Hamming metric. We study the natural off-line\\nproblem of answering n Hamming nearest neighbor queries at once, on a database of size n. We call this the\\nBATCH HAMMING NEAREST NEIGHBOR problem (BHNN). Here the aforementioned data structure lower\\nbounds no longer apply—there is no information bottleneck. Nevertheless, known algorithms for BHNN\\nstill run in either about n2dΩ(1) time (try all pairs) [GL01, MKZ09] or about n2Ω(d) time (build a table of\\nall possible query answers). We improve over both these bounds for log n ≤ d ≤ o(log2 n/ log log n). Our\\napproach builds on a recently developed framework [Wil14a, Wil14b, AWY15]. In this work, the authors\\nshow how several famous stubborn problems can yield faster algorithms, by constructing low-complexity\\ncircuits for solving simple repeated subparts of the problem. The overall strategy is to convert the simple\\nrepeated pieces into polynomials of a special form, then to evaluate the polynomials on many points fast,\\nvia an algebraic matrix multiplication.\\nFor the problems considered in earlier work, these polynomials can be constructed using 30-year-old\\nideas from circuit complexity. More formally, if f is a Boolean function on n variables and R is a ring, a\\nprobabilistic polynomial over R for f with error ε and degree d is a distribution D of degree-d polynomi-\\nals over R with the property that for all x ∈ {0,1}n, Prp∼D [p(x) = f (x)] ≥ 1− ε . Razborov [Raz87] and\\nSmolensky [Smo87] showed how to construct low-degree probabilistic polynomials for every f computable\\nby a small constant-depth circuit composed of PARITY, AND, and OR gates. They also proved that prob-\\nabilistic polynomials for MAJORITY with constant error require Ω(\\n√\\nn) degree, concluding circuit lower\\nbounds for MAJORITY. Earlier papers [Wil14a, Wil14b, AWY15] used this low-degree construction to de-\\nrive faster algorithms for problems such as dense all-pairs shortest paths, longest common substring with\\nwildcards, and batch partial match queries.\\nDeveloping a faster algorithm for computing Hamming nearest neighbors requires more care than prior\\nwork. In the setting of this paper, the “repeated” computation we need to consider is that of finding a pair of\\nvectors among a small set which have small Hamming distance. But computing Hamming distance requires\\ncounting bits, which means we are implicitly computing a MAJORITY of some kind. This is fundamentally\\nharder than the constant-depth computations handled in prior work. Proceeding anyway, we prove in this\\n1\\npaper that the Razborov-Smolensky\\n√\\nn lower bound is tight up to constant factors: there is a probabilistic\\npolynomial for MAJORITY achieving degree O(\\n√\\nn) with constant error. In fact, we show that this degree\\ncan be achieved for any symmetric Boolean function. We use this to get a subquadratic time algorithm for\\nHamming distance computations up to about log2 n dimensions.\\n1.1 Our Results\\nRecently, Srinivasan [Sri13] gave a probabilistic polynomial for the MAJORITY function of degree√\\nn log(1/ε) · polylog(n) over any field. We construct a probabilistic polynomial for MAJORITY on n\\nvariables with optimal dependence on n and error ε over any field or the integers.\\nTheorem 1.1. Let R be a field, or the integers. There is a probabilistic polynomial over R for MAJORITY on\\nn variables with error ε and degree d(n,ε) = O(\\n√\\nn log(1/ε)). Furthermore, a polynomial can be sampled\\nfrom the probabilistic polynomial distribution in ˜O(∑d(n,ε)i=0\\n(\\nn\\ni\\n)\\n) time.\\nAs mentioned above, Razborov and Smolensky’s famous lower bounds for MAJORITY implies a degree\\nlower bound of precisely Ω(\\n√\\nn) in the case of constant ε . For non-constant ε , an asymptotically lower-\\ndegree polynomial for MAJORITY (in either ε or n) could be used to compute the majority of log(1/ε) bits\\nwith o(log(1/ε)) degree and error ε , which is impossible—the exact degree of MAJORITY on n bits equals\\nn, over any field and Z. Theorem 1.1 can also be applied to derive O(\\n√\\nn log(1/ε)) degree probabilistic\\npolynomials for every symmetric function (again improving on Srinivasan [Sri13]).\\nTheorem 1.2. Let R be a field, or the integers. There is a probabilistic polynomial over R for any symmetric\\nBoolean function on n variables with error ε and degree d(n,ε) = O(√n log(1/ε)).\\nWe use Theorem 1.1 to derive several new algorithms1. The main application is a solution to the BHNN\\nproblem mentioned earlier, where we are given n query points and an n-point database, and wish to answer\\nall n Hamming distance queries in one shot. We show:\\nTheorem 1.3. Let D ⊆ {0,1}c log n be a database of n vectors, where c can be a function of n. Any batch of\\nn Hamming nearest neighbor queries on D can be answered in randomized n2−1/O(c log2 c) time, whp.\\nFor instance, if d = O(logn), then the algorithm runs in truly subquadratic time: n2−ε , for some ε > 0.\\nTo our knowledge, this is the first known improvement over n2 time for the case where d ≥ logn. In general,\\nour algorithm improves over n2 for dimensions up to o(log2 n/(log logn)2).2\\nTheorem 1.3 follows from a similar running time for BICHROMATIC HAMMING CLOSEST PAIR: given\\nk and a collection of “red” and “blue” Boolean vectors, determine if there is a red and blue vector with\\nHamming distance at most k. Such bichromatic problems are central to algorithms over metric spaces.\\nThe versatility of the Hamming metric makes Theorem 1.3 highly applicable. For example, we can also\\nsolve closest pair in ℓ1 norm with bounded integer entries, as well as BICHROMATIC MIN INNER PRODUCT:\\ngiven an integer k and a collection of red and blue Boolean vectors, determine if there is a red and blue vector\\nwith inner product at most k. We show that these problems are in n2−1/O(c log2 c) randomized time, by simple\\nreductions (Theorem 4.5 and Theorem 4.6). As a consequence, closest pair problems in other measures,\\nsuch as the Jaccard distance, can also be solved in subquadratic time.\\nIt is important to keep in mind that sufficiently fast off-line Hamming closest pair algorithms would yield\\na breakthrough in satisfiability algorithms, so there is a potential limit:\\n1We stress that the polynomials of [Sri13] do not seem to imply the algorithms of this paper; removing the extra polylogarithmic\\nfactor is important!\\n2The logarithmic decrease in degree compared to previous results in Theorem 1.1 is crucial for achieving this truly subquadratic\\nruntime: the resulting decrease in the number of monomials in Theorem 4.2 will be necessary to get the runtime in Theorem 4.3 of\\nour algorithm’s analysis.\\n2\\nTheorem 1.4. Suppose there is ε > 0 such that for all constant c, BICHROMATIC HAMMING CLOSEST\\nPAIR can be solved in 2o(d) ·n2−ε time on a set of n points in {0,1}c log n. Then the Strong Exponential Time\\nHypothesis is false.\\nThe proof is actually a reduction from the (harder-looking) ORTHOGONAL VECTORS problem, where it\\nis well-known that n2−ε time would refute SETH [Wil04]. For completeness, the proof is in Section 4.2.\\n1.2 Other Related Work\\nThe “planted” case of Hamming distance has been studied extensively in learning theory and cryptog-\\nraphy. In this setting, all vectors are chosen uniformly at random, except for a planted pair of vectors\\nwith Hamming distance much smaller than the expected distance between two random vectors. Two recent\\nreferences are notable: G. Valiant [Val12] gave a breakthrough O(n1.62) time algorithm, which is indepen-\\ndent of the vector dimension and the Hamming distance of the planted pair. Valiant also gives a (1+ ε)-\\napproximation to the closest pair problem in Hamming distance running in n2−Ω(\\n√\\nε) time. See [MO15] for\\nvery recent work on batch Hamming distance computations in cryptoanalysis.\\nGum and Lipton [GL01] observe that n2 Hamming distances can be computed in O(n2d0.4) time via a di-\\nrect application of fast matrix multiplication. An extension to arbitrary alphabets was obtained by [MKZ09].\\nFor our situation of interest (d ≪ n) this is only a minor improvement over the O(n2d) cost of the obvious\\nalgorithm.\\n2 Preliminaries\\nWe assume basic familiarity with algorithms, complexity theory, and properties of polynomials. It is\\nworth noting that for a weaker notion of approximation, it is not hard to construct low-degree polynomials\\nthat correlate well with MAJORITY, and in fact any symmetric function. In particular, for every symmetric\\nfunction and ε > 0 there is a single degree-O(\\n√\\nn) polynomial that agrees with the function on at least 1− ε\\nof the points in {0,1}n: take a polynomial that outputs the symmetric function’s value on the inputs of\\nHamming weight [n/2−Ω(√n),n/2+O(√n)]. A constant fraction of the n-bit inputs are in this interval,\\nand polynomial interpolation yields an O(\\n√\\nn)-degree polynomial. (See Lemma 3.1.) Our situation is more\\ndifficult: we want all inputs to have a high chance of agreement with our symmetric function, when we\\nsample a polynomial.\\nWe need one lemma from prior work on efficiently evaluating polynomials over a combinatorial rectan-\\ngle of inputs. The lemma was proved and used in earlier work [Wil14a, AWY15] to design randomized\\nalgorithms for many problems.\\nLemma 2.1 ([Wil14a]). Given a polynomial P(x1, . . . ,xd ,y1, . . . ,yd) over a (fixed) finite field with at most\\nn0.17 monomials, and two sets of n inputs A = {a1, . . . ,an} ⊆ {0,1}d , B = {b1, . . . ,bn} ⊆ {0,1}d , we can\\nevaluate P on all pairs (ai,b j) ∈ A×B in ˜O(n2 +d ·n1.17) time.\\nAt the heart of Lemma 2.1 is a rectangular (but not necessarily impractical!) matrix multiplication algo-\\nrithm. For more details, see the references.\\n2.1 Notation\\nIn what follows, for (x1, . . . ,xn) ∈ {0,1}n define |x| := ∑ni=1 xi. For a logical predicate P, we use the\\nnotation [P] to denote the function which outputs 1 when P is true, and 0 when P is false.\\nFor θ ∈ [0,1], define THθ : {0,1}n → {0,1} to be the threshold function THθ (x1, . . . ,xn) := [|x|/n ≥ θ ].\\nIn particular, TH1/2 = MAJORITY. We also define NEARθ ,δ : {0,1}n →{0,1}, such that NEARθ ,δ (x) :=\\n[|x|/n ∈ [θ −δ ,θ +δ ]]. Intuitively, NEARθ ,δ checks whether |x|/n is “near” θ , with error δ .\\n3\\n3 Probabilistic Polynomial for MAJORITY: Proof of Theorem 1.1\\nIn this section, we prove Theorem 1.1. To do so, we construct a probabilistic polynomial for THθ over\\nZ[x1, . . . ,xn] which has degree O(\\n√\\nn log(1/ε)) and on each input is correct with probability at least 1− ε .\\nIntuition for the construction. First, let us suppose |x|/n is not too close to θ : in particular |x|/n is not\\nwithin δ = O(\\n√\\nlog(1/ε)/n) of θ . Then, if we construct a new smaller vector x˜ by sampling 1/10 of the\\nentries of x, it is likely that |x˜|/(n/10) lies on the same side of θ as |x|/n. This suggests a recursive strategy:\\nwe can use our polynomial construction on the sample x˜. Second, if |x|/n is close to θ , then by interpolating,\\nwe can use an exact polynomial of degree O(\\n√\\nn log(1/ε)) (which we call An,θ ,g) that is guaranteed to give\\nthe correct answer. To decide which of the two cases we are in, we will use a probabilistic polynomial for\\nNEAR (on a smaller number of variables), which can itself be written as the product of two probabilistic\\npolynomials for TH. The degree incurred by recursive calls can be adjusted to have tiny overhead, with the\\nright parameters.\\nIn comparison, Srinivasan [Sri13] takes a number theoretic approach. For Ω(log n) different primes p,\\nhis polynomial uses p−1 probabilistic polynomials in order to determine the Hamming weight of the input\\n(mod p). Then, it uses an exact polynomial inspired by the Chinese Remainder Theorem to determine the\\ntrue Hamming weight of the input, and whether it is at least n/2. This approach works on a more general\\nclass of functions than ours, called W -sum determined, which are determined by a weighted sum of the input\\ncoordinates. However, the number of primes being considered inherently means that this type of approach\\nwill incur extra logarithmic degree increases. In fact, we also give a better probabilistic degree for every\\nsymmetric function.\\nInterpolating Polynomial Let An,θ ,g : {0,1}n → Z be an exact polynomial of degree at most 2g\\n√\\nn+ 1\\nwhich gives the correct answer to THθ for any vector x with |x| ∈ [θn− g\\n√\\nn,θn+ g√n], and can give\\narbitrary answers to other vectors. Such a polynomial An,θ ,g can be derived from prior work (at least over\\nfields [Sri13]), but for completeness, we nonetheless prove its existence.3\\nLemma 3.1. For any integers n,r,k with n ≥ k+ r and any integers c1, . . . ,cr, there is a multivariate poly-\\nnomial p : {0,1}n → Z of degree r− 1 with integer coefficients such that p(x) = ci for all x ∈ {0,1}n with\\nHamming weight |x| = k+ i.\\nLemma 3.1 is more general than a result claimed without proof by Srinivasan ([Sri13], Lemma 14). It\\nalso generalizes of a theorem of Bhatnagar et al. ([BGL06], Theorem 2.8).\\nProof. Our polynomial p will have the form\\np(x1, . . . ,xn) =\\nr−1\\n∑\\ni=0\\nai · ∑\\nα∈{0,1}n\\n|α |=i\\n(\\nn\\n∏\\nj=1\\nx\\nα j\\nj\\n)\\nfor some constants a0, . . . ,ar−1. Hence, we will get that for any x ∈ {0,1}n:\\np(x) =\\nr−1\\n∑\\ni=0\\n(|x|\\ni\\n)\\nai.\\n3It is not immediately obvious from univariate polynomial interpolation that An,θ ,g exists as described, since the univariate\\npolynomial p such that An,θ ,g(x) = p(|x|) typically has rational (non-integer) coefficients.\\n4\\nDefine the matrix:\\nM =\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\n(k+1\\n0\\n) (k+1\\n1\\n) · · · (k+1\\nr−1\\n)(k+2\\n0\\n) (k+2\\n1\\n) · · · (k+2\\nr−1\\n)\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.(k+r\\n0\\n) (k+r\\n1\\n) · · · (k+r\\nr−1\\n)\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8 .\\nThe conditions of the stated lemma are that\\nM\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\na0\\na1\\n.\\n.\\n.\\nar−1\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8=\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nc1\\nc2\\n.\\n.\\n.\\ncr\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8 .\\nBy Lemma 3.2 (proved below), M always has determinant 1. Because M is a matrix with integer entries and\\ndeterminant 1, its inverse M−1 is also an integer matrix. Multiplying through by M−1 above gives integer\\nexpressions for the ai, as desired.\\nLemma 3.2. For any univariate polynomials p1, p2, . . . , pr such that pi has degree i−1, and any pairwise\\ndistinct x1,x2, . . . ,xr ∈ Z, the matrix\\nM =\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\np1(x1) p2(x1) · · · pr(x1)\\np1(x2) p2(x2) · · · pr(x2)\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\np1(xr) p2(xr) · · · pr(xr)\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\nhas determinant\\ndet(M) =\\n(\\nr\\n∏\\ni=1\\nci\\n)\\n·\\n(\\n∏\\n1≤i< j≤r\\n(x j − xi)\\n)\\n,\\nwhere ci is the coefficient of xi−1 in pi.\\nProof. For i from 1 up to r−1, we can add multiples of column i of M to the subsequent columns in order\\nto make the coefficient of xi−1 in all the other columns 0. The resulting matrix is\\nM′ =\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nc1 c2x1 · · · crxr−11\\nc1 c2x2 · · · crxr−12\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\nc1 c2xr · · · crxr−1r\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8 .\\nThis is a Vandermonde matrix which has the desired determinant.\\nDefinition. Let n be an integer for which we want to compute T Hθ . Let Mm,θ ,ε : {0,1}m → Z denote the\\nprobabilistic polynomial for THθ with error ≤ ε degree as described above for all m < n. We can assume as\\na base case that when m is constant, we simply use the exact polynomial for THθ .\\nDefine\\nSm,θ ,δ ,ε (x) := (1−Mm,θ+δ ,ε(x)) ·Mm,θ−δ ,ε(x).\\nAssuming Mn,θ ,ε works as prescribed (with ≤ ε error), this is a probabilistic polynomial for NEARθ ,δ with\\nerror at most 2ε . For x ∈ {0,1}n, let x˜ ∈ {0,1}n/10 be a vector of length n/10, where each entry is an\\n5\\nindependent and uniformly random entry of x. Hence, each entry of x˜ is a probabilistic polynomial in x of\\ndegree 1. Let a =\\n√\\n10 ·√ln(1/ε). Our probabilistic polynomial for THθ on n variables is defined to be:\\nMn,θ ,ε(x) := An,θ ,2a(x) ·Sn/10,θ ,a/√n,ε/4(x˜)+Mn/10,θ ,ε/4(x˜) · (1−Sn/10,θ ,a/√n,ε/4(x˜)).\\nNote that x˜ denotes the same randomly chosen vector in each of its appearances, and Sn/10,θ ,a/√n,ε/4\\ndenotes the same draw from the random polynomial distribution in both of its appearances.\\nDegree of Mn,θ ,ε . First we show by induction on n that Mn,θ ,ε has degree ≤ 41\\n√\\nn ln(1/ε). Assume that\\nMm,θ ,ε has degree ≤ 41\\n√\\nm ln(1/ε) for all m < n. We have:\\ndeg(Mn,θ ,ε) = max\\n{\\ndeg\\n[\\nAn,θ ,2a(x) ·Sn/10,θ ,a/√n,ε/4(x˜)\\n]\\n,deg\\n[\\nMn/10,θ ,ε/4(x˜) · (1−Sn/10,θ ,a/√n,ε/4(x˜))\\n]}\\n= deg(Sn/10,θ ,a/√n,ε/4(x˜))+max{deg(An,θ ,2a(x)),deg(Mn/10,θ ,ε/4(x˜))}\\n= 2 ·41\\n√\\nn\\n10\\nln(4/ε)+max\\n{\\n4a\\n√\\nn,41\\n√\\nn\\n10\\nln(4/ε)\\n}\\n= 2 ·41\\n√\\nn\\n10 ln(4/ε)+max\\n{\\n4 · (\\n√\\n10\\n√\\nln(1/ε)) ·√n,41\\n√\\nn\\n10 ln(4/ε)\\n}\\n= 3 ·41\\n√\\nn\\n10 ln(4/ε)≤ 41\\n√\\nn ln(1/ε).\\nTime to compute Mn,θ ,ε Computing An,θ ,2a can be done in poly(n) time as described in Lemma 3.1, as\\ncan sampling x˜ from x. Given the three recursive Mn/10,θ ′,ε/4 polynomials, we can then compute Mn,θ ,ε\\nin three multiplications. Each recursive polynomial has degree at most d(n/10,ε/4), and hence at most\\n∑d(n/10,ε/4)i=0\\n(\\nn\\ni\\n)\\nmonomials. Since the time for these multiplications dominates the time for the recursive\\ncomputations, the total time is ˜O(∑d(n,ε)i=0\\n(\\nn\\ni\\n)\\n) using the fast Fourier transform4, as desired.\\nCorrectness. Now we prove that Mn,θ ,ε correctly simulates THθ with probability at least 1− ε , on all\\npossible inputs. We begin by citing two lemmas explaining our choice of the parameter a.\\nLemma 3.3 (Hoeffding’s Inequality for Binomial Distributions ([Hoe63] Theorem 1)). If m independent\\nrandom draws x1, . . . ,xm ∼ {0,1} are made with Pr[xi = 1] = p for all i, then for any k ≤ mp we have\\nPr\\n[\\nm\\n∑\\ni=1\\nxi ≤ k\\n]\\n≤ exp\\n(\\n−2(mp− k)\\n2\\nm\\n)\\n,\\nwhere exp(x) = ex.\\nLemma 3.4. If x ∈ {0,1}n with |x|/n = w, and x˜ ∈ {0,1}n/10 is a vector each of whose entries is an inde-\\npendent and uniformly random entry of x, with |x˜|/(n/10) = v, then for every ε < 1/4,\\nPr\\n[\\nv≤ w−a/√n]≤ ε\\n4\\n,\\nwhere a =\\n√\\n10 ·√ln(1/ε).\\n4By replacing each variable with increasing powers of a single variable, we can reduce multivariate polynomial multiplication\\nto single variable polynomial multiplication.\\n6\\nProof. Each entry of x˜ is drawn from a binomial distribution with probability w of giving a 1. Hence,\\napplying Lemma 3.3 with p = w, m = n/10, and k = n10 (w−a/\\n√\\nn) = nw10 − a\\n√\\nn\\n10 yields:\\nPr[v≤ w−a/√n] = Pr\\n[\\n|x˜| ≤ nw\\n10 −\\na\\n√\\nn\\n10\\n]\\n≤ exp\\n\\uf8eb\\n\\uf8ec\\uf8ed−2\\n(\\na\\n√\\nn\\n10\\n)2\\nn\\n10\\n\\uf8f6\\n\\uf8f7\\uf8f8 ,\\nwhich simplifies to exp\\n(\\n− a25\\n)\\n= exp(−2ln(1/ε)) = ε2 < ε4 .\\nWe now move on to the main proof of correctness, which proceeds by induction on n. By symmetry, we\\nmay assume we have an input vector x ∈ {0,1}n with |x|/n≥ θ , and we want to show that Mn,θ ,ε(x) outputs\\n1 with probability at least 1− ε . We assume ε < 1/4 so that we may apply Lemma 3.4.\\nFor notational convenience, define the intervals:\\nα0 = [θ −a/\\n√\\nn,θ ], α1 = [θ ,θ +a/\\n√\\nn], β = [θ +a/√n,θ +2a/√n], γ = [θ +2a/√n,1].\\nNote that depending on the values of θ and a, some of these intervals may be empty; this is not a problem\\nfor our proof.\\nLet w = |x|/n. Let x˜ be the random “subvector” of x selected in Mn,θ ,ε (recall we use the same x˜ in each\\nof the three locations it appears in the definition of M). Let v = |x˜|/(n/10). Our proof strategy is to consider\\ndifferent cases depending on the value of w. For each case, we show there are at most four events such that,\\nif all events hold then Mn,θ ,ε outputs the correct answer, and each event does not hold with probability at\\nmost ε4 . By the union bound, this implies that Mn,θ ,ε gives the correct answer with probability at least 1− ε .\\nThe cases are as follows:\\n1. w ∈ α1 (|x|/n is “very close” to θ ). By Lemma 3.4, we know that with probability at least 1− ε4 , we\\nhave v ≥ θ −a/√n. In other words, v ∈ α0∪α1∪β ∪ γ .\\n• v ∈ α0 ∪α1, then with probability at least 1− 2ε4 , we have Sn/10,θ ,a/√n,ε/4(x˜) = 1, by our in-\\nductive assumption that Sn/10,θ ,a/√n,ε/4 is a probabilistic polynomial for NEARθ ,a/√n with error\\nprobability at most 2ε4 . In this case, Mn,θ ,ε(x) = An,θ ,2a(x), which is 1 by definition of A.\\n• v ∈ β ∪ γ , then with probability at least 1− 2ε4 , we have Sn/10,θ ,a/√n,ε/4(x˜) = 0, in which case\\nMn,θ ,ε(x) = Mn/10,θ ,ε/4(x˜). But, by the inductive hypothesis, this is 1 with probability at least\\n1− ε4 , since v > θ in this case.\\nSince we are in one of these two cases with probability ≥ 1− 14ε , and each gives the correct answer\\nwith probability ≥ 1− 3ε4 , the correct answer is given in this case with probability ≥ 1− ε .\\n2. w ∈ β (|x|/n is “close” to θ ). In this case we have w−θ ≤ 2a/√n, therefore An,θ ,2a(x) = 1. Hence,\\nif Sn/10,θ ,a/√n,ε/4(x˜) = 1 then Mn,θ ,ε(x) returns the correct answer. If Sn/10,θ ,a/√n,ε/4(x˜) = 0, then we\\nreturn Mn/10,θ ,ε/4(x˜). By Lemma 3.4, we have v ≥ θ with probability at least 1− ε4 , and in this case,\\nMn/10,θ ,ε/4(x˜) = 1 with probability ≥ 1− ε4 . Hence, M returns the correct value with probability at\\nleast 1− 2ε4 , no matter what the value of Sn/10,θ ,a/√n,ε/4(y) happens to be.\\n3. w ∈ γ (|x|/n is “far” from θ ). By Lemma 3.4, we have v ∈ β ∪ γ with probability at least 1− ε4 . In\\nthis case, v ≥ θ , and so Mn/10,θ ,ε/4(x˜) = 1 with probability ≥ 1− ε4 . Moreover, since v /∈ α0∪α1, it\\nfollows that Sn/10,θ ,a/√n,ε/4(x˜)= 0 with probability ≥ 1− 24 ε , in which case Mn,θ ,ε(x)=Mn/10,θ ,ε/4(x˜).\\nOverall, Mn,θ ,ε(x) = Mn/10,θ ,ε/4(x˜) = 1 with probability ≥ 1− ε .\\nThis completes the proof of correctness, and the proof of Theorem 1.1.\\n7\\n3.1 Symmetric Functions\\nRecall that f : {0,1}→{0,1}n is symmetric if the value of f (x) depends only on |x|, the Hamming weight\\nof x. We now describe how to use the probabilistic polynomial for THθ to derive a probabilistic polynomial\\nfor any symmetric function with the same degree as THθ :\\nReminder of Theorem 1.2 Every symmetric function f : {0,1}→ {0,1}n on n variables has a probabilistic\\npolynomial of O(√n log(1/ε)) degree and error ε .\\nProof. For any 0 ≤ i ≤ n, let fi denote the value of f (x) when x has Hamming weight i. Define:\\nA = {0 < i ≤ n | fi = 1 and fi−1 = 0},\\nB = {0 < i ≤ n | fi = 0 and fi−1 = 1}.\\nThen, f can be written exactly as:\\nf (x) = f0 +∑\\ni∈B\\nT Hi/n(x)− ∑\\nj∈A\\nT H j/n(x). (1)\\nWe replace each T Hθ in (1) with a probabilistic polynomial of Theorem 1.1 with error δ = ε/2. However,\\nwe make sure that in all of the different probabilistic polynomials for T Hθ , we make the same choice for\\nthe sampled vector x˜ at each iteration. We can then apply the proof of Theorem 1.1, to see that every one\\nof the T Hθ probabilistic polynomials will give the correct answer as long as ||x|/n−|x˜|/(n/10)| < a/\\n√\\nn\\nat each of the log10(n) layers of recursion (this is a property only of the sampling, and independent of θ ).\\nRecall that the error parameter at the ith level of the recursion is 14i δ . Hence, by the union bound, the error\\nprobability of the entire probabilistic polynomial is at most\\nδ + 1\\n4\\nδ + 1\\n16δ + · · ·+\\n1\\n4log10(n)\\nδ < 1\\n1−1/4δ < ε ,\\nas desired.\\n4 Closest Pair in Hamming Space, and Batch Nearest Neighbor\\nWe first give a connection between the time complexity of closest pair problems in metric spaces on the\\nhypercube and the existence of certain probabilistic polynomials. Let M be a metric on {0,1}d . We define\\nthe BICHROMATIC M-METRIC CLOSEST PAIR problem to be: given an integer k and a collection of “red”\\nand “blue” vectors in {0,1}d , determine if there is a pair of red and blue vectors with distance at most k\\nunder metric M. This problem arises frequently in algorithms on a metric space M. In what follows, we\\nshall assume that the metric M can be computed on two points of d dimensions in time poly(d). Define the\\nBoolean function\\nM-distk(x1,1, . . . ,x1,d , . . . ,xs,1, . . . ,xs,d ,y1,1, . . . ,y1,d , . . . ,ys,1, . . . ,ys,d)\\n:=\\n∨\\ni, j=1,...,s\\n[M(xi,1, . . . ,xi,d ,y j,1, . . . ,y j,d)≤ k].\\nThat is, M-distk takes two collections of s vectors as input, and outputs 1 if and only if there is a pair of vec-\\ntors (one from each collection) that have distance at most k under metric M. For example, the Hamming-distk\\nfunction tests if there is a pair of vectors with Hamming distance at most k.\\nWe observe that sparse probabilistic polynomials for computing M-distk imply subquadratic time algo-\\nrithms for finding close bichromatic pairs in metric M.\\n8\\nTheorem 4.1. Suppose for all k, d, and n, there is an s = s(d,n) such that M-distk on 2sd variables has\\na probabilistic polynomial with at most n0.17 monomials and error at most 1/3, where each sample can be\\nproduced in ˜O(n2/s2) time. Then BICHROMATIC M-METRIC CLOSEST PAIR on n vectors in d dimensions\\ncan be solved in ˜O(n2/s2 + s2 ·poly(d)) randomized time.\\nProof. We have an integer k and sets R,B ⊆ {0,1}d such that |R|= |B|= n, and wish to determine if there\\nis a u ∈ R and v ∈ B such that M(u,v) ≤ k. First, partition both R and B into ⌈n/s⌉ groups, with at most\\ns vectors in each group. By assumption, for all k, there is a probabilistic polynomial for M-distk with 2sd\\nvariables, n0.17 monomials, and error at most 1/3. Let p be a polynomial sampled from this distribution.\\nOur idea is to efficiently evaluate p on all O(n2/s2) pairs of groups from R and B, by feeding as input to p\\nall s vectors xi from a group of R and all s vectors yi from a group of B.\\nSince the number of monomials m ≤ n0.17, we can apply Lemma 2.1, evaluating p on all pairs of groups\\nin time ˜O(n2/s2). For each pair of groups from R and B, this evaluation determines if the pair of groups\\ncontain a bichromatic pair of distance at most k, with probability at least 2/3.\\nTo obtain a high probability answer, sample ℓ= 10log n polynomials p1, . . . , pℓ for M-distk independently\\nfrom the distribution, in ˜O(n2/s2) time (by assumption). Evaluate each pi on all pairs of groups from R and\\nB in ˜O(n2/s2) time by the above paragraph. Compute the majority value of p1, . . . , pℓ on all pairs of groups,\\nagain in ˜O(n2/s2) time. By Chernoff-Hoeffding bounds, the majority value reported for a pair of groups is\\ncorrect with probability at least 1−n−3. Therefore with probability at least 1−n−1, we correctly determine\\nfor all pairs of groups from R and B whether the pair contains a bichromatic pair of vectors with distance at\\nmost k.\\nGiven a pair of groups R′ and B′ which are reported to contain a bichromatic pair of close vectors,we\\ncan simply brute force to find the closest pair in A′ and B′ in s2 · poly(d) time. (In principle, we could also\\nperform a recursive call, but this doesn’t asymptotically help us in our applications.)\\nNext, we construct a probabilistic polynomial for the Hamming-distk function, using the MAJORITY\\nconstruction of Theorem 1.1.\\nTheorem 4.2. There is a e≥ 1 such that for sufficiently large s and d > e2 logs, the Hamming-distk function\\non 2sd variables has a probabilistic polynomial of degree O(√d logs), error at most 1/3, and at most O(s4 ·( 2d\\nO(\\n√\\nd log s)\\n)\\n) monomials over F2. Moreover, we can sample from the probabilistic polynomial distribution in\\ntime polynomial in the number of monomials.\\nA similar result holds for Z, as well as any field, with minor modifications. (For fields of characteristic p,\\nthe degree increases by a factor of p−1.)\\nProof. Let e≥ 1 be large enough that there is a probabilistic polynomial Dd of degree e\\n√\\nd log(1/ε) for the\\nthreshold function TH(k+1)/d on d inputs, from Theorem 1.1. We construct a probabilistic polynomial H\\nfor Haming-distk over F2, as follows:\\nSet ε = 1/s3, and sample p∼Dd with error ε . Let x1,y1, . . . ,xs,ys be blocks of d Boolean variables, with\\nthe jth variable of xi denoted by xi, j . Choose two uniform random subsets R1,R2 ⊆ [s]2, and form\\nq(x1,y1, . . . ,xs,ys) := 1+\\n2\\n∏\\nk=1\\n(\\n1+ ∑\\n(i, j)∈Rk\\n(1+ p(xi,1 + y j,1, . . . ,xi,d + y j,d))\\n)\\n.\\nFirst, note that since ε = 1/s3, all 2s2 occurrences of the polynomial p in q output the correct answer with\\nprobability at least 1−2/s. Let us suppose this event occurs.\\n9\\nIf there are xi and yi with Hamming distance at most k, then p(xi,1 + y j,1, . . . ,xi,d + y j,d)) = 0 (recall the\\nsummation is modulo 2). Hence the probability that the sum of (1+ p)’s in R1 is odd is 1/2. The same is\\ntrue of R2 independently. Therefore the product of the two sums in the expression for q is 0 with probability\\n3/4, so q outputs 1 with probability 3/4. On the other hand, if every xi and yi has Hamming distance at least\\nk, then 1+ p(xi,1 + y j,1, . . . ,xi,d + y j,d) = 0 for all (i, j) ∈ R1 ∪R2. Therefore the product of the two sums\\n(over R1 and R2) in q is 1, hence q outputs 0 in this case. This shows that q agrees with Hamming-distk on\\nany given input, with probability at least 3/4−2/s > 2/3.\\nNow we prove the monomial bound. Since we are only evaluating q on 0/1 points, we may assume q is\\nmultilinear, and remove all higher powers of the variables. Assuming d > e\\n√\\nd logs, i.e.\\nd > e2 logs, (2)\\nthe number of distinct monomials in the multilinear q is at most O(s4 · ( 2d\\ne\\n√\\nd log(s)\\n)\\n).\\nPutting it all together, we obtain a faster algorithm for BICHROMATIC HAMMING CLOSEST PAIR:\\nTheorem 4.3. For n vectors of dimension d = c(n) log n, BICHROMATIC HAMMING CLOSEST PAIR can be\\nsolved in n2−1/O(c(n) log2 c(n)) time by a randomized algorithm that is correct with high probability.\\nProof. Let d = c logn in the following, with the implicit understanding that c is a function of n. We apply\\nthe reduction of Theorem 4.1 and the probabilistic polynomial for the Hamming-distk of Theorem 4.2.\\nThe reduction of Theorem 4.1 requires that the number of monomials in our probabilistic polynomial is\\nat most n0.17, while the monomial bound for Hamming-distk from Theorem 4.2 is m = O(s2 ·\\n( 2d\\ne\\n√\\nd log s\\n)\\n) for\\nsome universal constant a, provided that d > a2 log s. Therefore our primary task is to maximize the value\\nof s such that m≤ n0.17. This will minimize the final running time of ˜O(n2/s2). With hindsight, let us guess\\ns = n1/(uc log\\n2 c) for a constant u, and focus on the large binomial in the monomial estimate m. Then,(\\n2d\\na\\n√\\nd · logs\\n)\\n=\\n( 2c log n\\na\\n√\\n(c log n) · (log n)/(uc log2 c)\\n)\\n=\\n( 2c log n\\na\\n√\\n(log2 n)/(u log2 c)\\n)\\n=\\n(\\n2c log n\\na log n/(\\n√\\nu logc)\\n)\\n.\\nFor notational convenience, let δ = a/(√u logc). By Stirling’s inequality, we have(\\n2c log n\\nδ logn\\n)\\n≤\\n(\\n2ce\\nδ\\n)δ logn\\n= nδ log(\\n2ce\\nδ ).\\nPlugging δ = a/(√u logc) back into the exponent, we find\\nδ log\\n(\\n2ce\\nδ\\n)\\n=\\na log(2ce\\n√\\nu logc\\na\\n)√\\nu log c . (3)\\nThe quantity (3) can be made arbitrarily small, by setting u sufficiently large. In that case, the number of\\nmonomials m ≤ s2nδ log( 2ceδ ) can be made less than n0.1. Finally, note that a2 logs = a2(log n)/(uc log2 c) <\\nc log n = d, so (2) holds and the reduction of Theorem 4.1 applies. This completes the proof.\\nObserve that the probabilistic polynomials of degree\\n√\\nn log(1/ε)polylog n from prior work [Sri13]\\nwould be insufficient for Theorem 4.3. The extra degree increase would include an extra polylog n fac-\\ntor in expression (3), and hence no constant choice of u would be sufficiently large.\\nNow we show how to solve BATCH HAMMING NEAREST NEIGHBOR (BHNN). In the following theo-\\nrem, we assume for all pairs of vectors in our instance that the maximum metric distance is at most some\\nvalue MAX . (For the Hamming distance, MAX ≤ d.) We reduce the batch nearest neighbor query problem\\nto the bichromatic close pair problem:\\n10\\nTheorem 4.4. Let Ed be some d-dimensional domain supporting a metric space M. If the BICHROMATIC\\nM-METRIC CLOSEST PAIR on n vectors in Ed can be solved in T (n,d) time, then BATCH M-METRIC\\nNEAREST NEIGHBORS on n vectors in Ed can be solved in O(n ·T (√n,d) ·MAX) time.\\nProof. We give an oracle reduction similar to previous work [AWY15]. Initialize an table T of size n, with\\nthe maximum metric value v in each entry. Given n database vectors D and n query vectors Q, color D red\\nand Q blue. Break D into ⌈n/s⌉ groups of size at most s, and do the same for the set Q. For each pair\\n(R′,B′) ⊂ (D×Q) of groups, and for each k = MAX − 1, . . . ,1,0, we initialize Dk := D, Qk := Q, and call\\nBICHROMATIC M-METRIC CLOSEST PAIR on (R′,B′) ⊂ (Dk ×Qk) with integer k. While we continue to\\nfind a pair (xi,y j) ∈ (R′×B′) with M(xi,y j) ≤ k, set T [i] := k and remove y j from Qk and B′. (With a few\\nmore recursive calls, we could also find an explicit vector y j such that M(xi,y j)≤ k.)\\nNow for each call that finds a close bichromatic pair, we remove a vector from Qk; we do this at most MAX\\ntimes for each vector, so there can be at most MAX · n such calls. For each pair of groups, there are MAX\\noracle calls that find no bichromatic pair. Therefore the total running time is O((n+n2/s2) ·T (s,d) ·MAX).\\nSetting s =\\n√\\nn to balance the terms, the running time is O(n ·T (√n,d) ·MAX).\\nThe following is immediate from Theorem 4.4 and Theorem 4.3:\\nReminder of Theorem 1.3 For n vectors of dimension d = c(n) log n, BATCH HAMMING NEAREST\\nNEIGHBORS can be solved in n2−1/O(c(n) log2 c(n)) time by a randomized algorithm, whp.\\n4.1 Some Applications\\nRecall that the ℓ1 norm of two vectors x and y is ∑i |xi− yi|. We can solve BATCH ℓ1 NEAREST NEIGH-\\nBORS on vectors with small integer entries by a simple reduction to BATCH HAMMING NEAREST NEIGH-\\nBORS, (which is probably folklore):\\nTheorem 4.5. For n vectors of dimension d = c(n) log n in {0,1, . . . ,m}d , BATCH L1 NEAREST NEIGHBORS\\ncan be solved in n2−1/O(mc(n) log2(mc(n))) time by a randomized algorithm, whp.\\nProof. Notice that for any x,y ∈ {0, . . . ,m}, the Hamming distance of their unary representations, written as\\nm-dimensional vectors, is equal to |x− y|. Hence, for x ∈ {0, . . . ,m}d , we can transform it into a vector x′ ∈\\n{0,1}md by setting (x′\\nm(i−1)+1,x\\n′\\nm(i−1)+2, . . . ,x\\n′\\nm(i−1)+m) equal to the unary representation of xi, for 1≤ i≤ d.\\nIt is then equivalent to solve the Hamming nearest neighbors problem on these md-dimensional vectors.\\nIt is also easy to extend Theorem 1.3 for vectors over O(1)-sized alphabets using equidistant binary\\ncodes ([MKZ09], Section 5.1). This is useful for applications in biology, such as finding similar DNA\\nsequences. The above algorithms also apply to computing maximum inner products:\\nTheorem 4.6. The BICHROMATIC MINIMUM INNER PRODUCT (and MAXIMUM) problem with n red and\\nblue Boolean vectors in c log n dimensions can be solved in n2−1/O(c log2 c) randomized time.\\nProof. Recall that Theorem 1.4 gives a reduction from BICHROMATIC MINIMUM INNER PRODUCT to\\nBICHROMATIC HAMMING FURTHEST PAIR, and shows that BICHROMATIC HAMMING FURTHEST PAIR\\nis equivalent to BICHROMATIC HAMMING CLOSEST PAIR. The same reduction shows that BICHROMATIC\\nMAXIMUM INNER PRODUCT reduces to the closest pair version. Hence Theorem 1.3 applies, to both\\nminimum and maximum inner products.\\nAs a consequence, we can answer a batch of n minimum inner product queries on a database of size n with\\nthe same time estimate, applying a reduction analogous to that of Theorem 4.4. From there, Theorem 4.6\\n11\\ncan be extended to other important similarity measures, such as finding a pair of sets A,B with maximum\\nJaccard coefficient, defined as |A∩B||A∪B| [Bro97].\\nCorollary 4.1. Given n red and blue sets in {0,1}c log n, we can find the pair of red and blue sets with\\nmaximum Jaccard coefficient in n2−1/O(c log2 c) randomized time.\\nProof. Let S be a given collection of red and blue sets over [d]. We construe the sets in S as vectors, in\\nthe natural way. For all possible values d1,d2 = 1, . . . ,d, we will construct an instance of BICHROMATIC\\nMAXIMUM INNER PRODUCT S′d1,d2 , and take the best pair found, appealing to Theorem 4.6.\\nAs in the proof of Theorem 1.4, we “filter” sets based on their cardinalities. In the instance S′d1,d2 of\\nBICHROMATIC MAXIMUM INNER PRODUCT, we only include red sets with cardinality exactly d1, and\\nblue sets with cardinality exactly d2. For sets R,B, we have\\n|R∩B|\\n|R∪B| =\\n|R∩B|\\nd1 +d2−|R∩B| . (4)\\nSuppose that we choose a red set R and blue set B that maximize |R∩B|. This choice simultaneously max-\\nimizes the numerator and minimizes the denominator of (4), producing the sets R and B with maximum\\nJaccard coefficient over the red sets with cardinality d1 and blue sets with cardinality d2. Finding the max-\\nimum pair R and B over each choice of d1,d2, we will find the overall R and B with maximum Jaccard\\ncoefficient.\\n4.2 Closest Pair in Hamming Space is Hard\\nThe Strong Exponential Time Hypothesis (SETH) states that there is no universal δ < 1 such that for all\\nc, CNF-SAT with n variables and cn clauses can be solved in O(2δn) time.\\nReminder of Theorem 1.4 Suppose there is ε > 0 such that for all constant c, BICHROMATIC HAMMING\\nCLOSEST PAIR can be solved in 2o(d) ·n2−ε time on a set of n points in {0,1}c log n. Then SETH is false.\\nProof. The proof is a reduction from the ORTHOGONAL VECTORS problem with n vectors S ⊂ {0,1}d :\\nare there u,v ∈ S such that 〈u,v〉 = 0? It is well-known that 2o(d) · n2−ε time would refute SETH [Wil04].\\nIndeed, we show that BICHROMATIC MINIMUM INNER PRODUCT (finding a pair of vectors with minimum\\ninner product, not just inner product zero) reduces to BICHROMATIC HAMMING CLOSEST PAIR, as well as\\nthe version for maximum inner product.\\nFirst, we observe that BICHROMATIC HAMMING CLOSEST PAIR is equivalent to BICHROMATIC HAM-\\nMING FURTHEST PAIR: let v be the complement of v (the vector obtained by flipping all the bits of v). Then\\nthe Hamming distance of u and v is H(u,v) = d−H(u,v). Thus by flipping all the bits in the components\\nof the blue vectors, we can reduce from the closest pair problem to furthest pair, and vice versa.\\nNow we reduce ORTHOGONAL VECTORS to BICHROMATIC HAMMING FURTHEST PAIR. Our OR-\\nTHOGONAL VECTORS instance has red vectors Sr and blue vectors Sb, and we wish to find u∈ Sr and v ∈ Sb\\nsuch that 〈u,v〉 = 0.\\nFor every d2 possible choice of I,J = 1, . . . ,d, construct the subset Sr,I of vectors in Sr with exactly I\\nones, and construct the subset Sb,J of vectors in Sb with exactly J ones. We will look for an orthogonal pair\\namong Sr,I and Sb,J for all such I,J separately.\\nRecall that Hamming distance of two vectors equals the ℓ22 norm distance, in {0,1}d . The ℓ22 norm of u\\nand v is\\n||u− v||22 = ||u||2 + ||v||2−2〈u,v〉.\\n12\\nHowever, in Sr,I all vectors have the same norm, and all vectors in Sb,J have the same norm. Therefore,\\nfinding a red-blue pair u ∈ Sr,I and v ∈ Sb,J with minimum inner product is equivalent to finding a pair in\\nSr × Sb with smallest Hamming distance. (Similarly, maximum inner product is equivalent to Hamming\\nclosest pair.)\\nThe reduction only requires O(d2) calls to BICHROMATIC HAMMING FURTHEST PAIR, with no changes\\nto the dimension d nor the number of vectors n.\\n5 Conclusion\\nThere are many interesting further directions. Here are some general questions about the future of this\\napproach for nearest neighbor problems:\\n• Could a similar approach solve the closest pair problem for edit distance in {0,1}d? This is a natural\\nnext step. Reductions from edit distance to Hamming distance are known [BYJKK04] but they yield\\nlarge approximation factors; we think exact solutions should be possible. The main difficulty is that\\nthe circuit complexity (and probabilistic polynomial degree) of edit distance seems much higher than\\nthat of Hamming distance: Hamming distance can be seen as a “threshold of XORs”, but the best\\ncomplexity upper bound for edit distance seems to be NLOGSPACE .\\n• We can solve the off-line “closest pair” version of several data structure problems, by reducing them\\nto problems of evaluating polynomials, and applying matrix multiplication. Is there any way to obtain\\nbetter data structures using this algebraic approach? Of course there are limitations on such data\\nstructures, there are also gaps between known data structures and known lower bounds.\\n• It feels strange to embed multivariate polynomial evaluations into a matrix multiplication, when it is\\nknown that evaluating univariate polynomials on many points can be done even faster than known\\nmatrix multiplication algorithms (using FFTs). Perhaps we can apply other algebraic tools (such as\\nKedlaya and Umans’ multivariate polynomial evaluation algorithms [Uma08, KU11]) directly to these\\nproblems.\\n• Recently, Timothy Chan [Cha15] gave an algorithm for computing dominances among n vectors in\\nR\\nc log n\\n, which has a running time that is very similar to ours: n2−1/O(c log2 c) time. Is this a coincidence?\\n6 Acknowledgements\\nWe thank an anonymous FOCS reviewer for pointing out that our probabilistic polynomial for general\\nsymmetric functions can achieve an O(\\n√\\nn log(1/ε)) degree bound as well.\\nReferences\\n[AI06] Alexandr Andoni and Piotr Indyk. Near-optimal hashing algorithms for approximate nearest\\nneighbor in high dimensions. In FOCS, pages 459–468, 2006.\\n[AINR14] Alexandr Andoni, Piotr Indyk, Huy L Nguyen, and Ilya Razenshteyn. Beyond locality-\\nsensitive hashing. In SODA, pages 1018–1028, 2014.\\n[AIP06] Alexandr Andoni, Piotr Indyk, and Mihai Patrascu. On the optimality of the dimensionality\\nreduction method. In FOCS, pages 449–458, 2006.\\n13\\n[AR15] Alexandr Andoni and Ilya Razenshteyn. Optimal data-dependent hashing for approximate near\\nneighbors. arXiv preprint arXiv:1501.01062, 2015.\\n[AWY15] Amir Abboud, Ryan Williams, and Huacheng Yu. More applications of the polynomial method\\nto algorithm design. In SODA, pages 218–230, 2015.\\n[BGL06] Nayantara Bhatnagar, Parikshit Gopalan, and Richard J. Lipton. Symmetric polynomials over\\nzm and simultaneous communication protocols. J. Comput. Syst. Sci., 72(2):252–285, 2006.\\n[BR00] Omer Barkol and Yuval Rabani. Tighter bounds for nearest neighbor search and related prob-\\nlems in the cell probe model. In STOC, pages 388–396, 2000.\\n[Bro97] Andrei Z Broder. On the resemblance and containment of documents. In Proceedings of\\nCompression and Complexity of Sequences, pages 21–29. IEEE, 1997.\\n[BYJKK04] Ziv Bar-Yossef, TS Jayram, Robert Krauthgamer, and Ravi Kumar. Approximating edit dis-\\ntance efficiently. In FOCS, pages 550–559, 2004.\\n[CCGL99] Amit Chakrabarti, Bernard Chazelle, Benjamin Gum, and Alexey Lvov. A lower bound on the\\ncomplexity of approximate nearest-neighbor searching on the hamming cube. In STOC, pages\\n305–311, 1999.\\n[Cha15] Timothy M Chan. Speeding up the four russians algorithm by about one more logarithmic\\nfactor. In SODA, pages 212–217, 2015.\\n[CR04] Amit Chakrabarti and Oded Regev. An optimal randomised cell probe lower bound for ap-\\nproximate nearest neighbour searching. In FOCS, pages 473–482, 2004.\\n[GL01] Ben Gum and Richard J Lipton. Cheaper by the dozen: Batched algorithms. In SDM, pages\\n1–11. SIAM, 2001.\\n[Hoe63] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of\\nthe American Statistical Association, 58(301):13–30, 1963.\\n[IM98] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the curse\\nof dimensionality. In STOC, STOC ’98, pages 604–613, New York, NY, USA, 1998.\\n[Ind04] Piotr Indyk. Nearest neighbors in high-dimensional spaces. In Handbook of Discrete and\\nComputational Geometry, Second Edition., pages 877–892. 2004.\\n[Kle97] Jon M Kleinberg. Two algorithms for nearest-neighbor search in high dimensions. In STOC,\\npages 599–608, 1997.\\n[KOR00] Eyal Kushilevitz, Rafail Ostrovsky, and Yuval Rabani. Efficient search for approximate nearest\\nneighbor in high dimensional spaces. SIAM Journal on Computing, 30(2):457–474, 2000.\\n[KU11] Kiran S. Kedlaya and Christopher Umans. Fast polynomial factorization and modular compo-\\nsition. SIAM J. Comput., 40(6):1767–1802, 2011.\\n[MKZ09] Kerui Min, Ming-Yang Kao, and Hong Zhu. The closest pair problem under the hamming\\nmetric. In Computing and Combinatorics, pages 205–214. Springer, 2009.\\n14\\n[MO15] Alexander May and Ilya Ozerov. On computing nearest neighbors with applications to decod-\\ning of binary linear codes. In EUROCRYPT, page to appear, 2015.\\n[MP69] Marvin L Minsky and Seymour A Papert. Perceptrons: An Introduction to Computational\\nGeometry. MIT press Boston, MA:, 1969.\\n[Pan06] Rina Panigrahy. Entropy based nearest neighbor search in high dimensions. In SODA, pages\\n1186–1195, 2006.\\n[Pat08] Mihai Patrascu. Lower bound techniques for data structures. PhD thesis, Massachusetts Insti-\\ntute of Technology, 2008.\\n[Raz87] A. A. Razborov. Lower bounds on the size of bounded depth circuits over a complete basis with\\nlogical addition. Mathematical Notes of the Academy of Sciences of the USSR, 41(4):333–338,\\n1987.\\n[Smo87] Roman Smolensky. Algebraic methods in the theory of lower bounds for boolean circuit com-\\nplexity. In STOC, pages 77–82, 1987.\\n[Sri13] Srikanth Srinivasan. On improved degree lower bounds for polynomial approximation. In Con-\\nference on Foundations of Software Technology and Theoretical Computer Science, (FSTTCS),\\npages 201–212, 2013.\\n[Uma08] Christopher Umans. Fast polynomial factorization and modular composition in small charac-\\nteristic. In STOC, pages 481–490, 2008.\\n[Val12] Gregory Valiant. Finding correlations in subquadratic time, with applications to learning pari-\\nties and juntas. In FOCS, pages 11–20, 2012.\\n[Wil14a] Ryan Williams. Faster all-pairs shortest paths via circuit complexity. In STOC, pages 664–673,\\n2014.\\n[Wil14b] Ryan Williams. The polynomial method in circuit complexity applied to algorithm design\\n(invited talk). In Conference on Foundation of Software Technology and Theoretical Computer\\nScience, (FSTTCS), pages 47–60, 2014.\\n[Wil04] Ryan Williams. A new algorithm for optimal 2-constraint satisfaction and its implications.\\nTheor. Comput. Sci., 348(2-3):357–365, 2005. See also ICALP’04.\\n15\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd15'), 'authors': 'Dementiev, Roman, Maier, Tobias, Sanders, Peter', 'year': '2016', 'title': 'Concurrent Hash Tables: Fast and General?(!)', 'full_text': 'Concurrent Hash Tables: Fast and General(?)!\\nTobias Maier1, Peter Sanders1, and Roman Dementiev2\\n1 Karlsruhe Institute of Technology, Karlsruhe, Germany {t.maier,sanders}@kit.edu\\n2 Intel Deutschland GmbH roman.dementiev@intel.com\\nAbstract\\nConcurrent hash tables are one of the most impor-\\ntant concurrent data structures which is used in\\nnumerous applications. Since hash table accesses\\ncan dominate the execution time of whole applica-\\ntions, we need implementations that achieve good\\nspeedup even in these cases. Unfortunately, cur-\\nrently available concurrent hashing libraries turn\\nout to be far away from this requirement in partic-\\nular when adaptively sized tables are necessary or\\ncontention on some elements occurs.\\nOur starting point for better performing data\\nstructures is a fast and simple lock-free concurrent\\nhash table based on linear probing that is however\\nlimited to word sized key-value types and does not\\nsupport dynamic size adaptation. We explain how\\nto lift these limitations in a provably scalable way\\nand demonstrate that dynamic growing has a per-\\nformance overhead comparable to the same gener-\\nalization in sequential hash tables.\\nWe perform extensive experiments comparing\\nthe performance of our implementations with six of\\nthe most widely used concurrent hash tables. Ours\\nare considerably faster than the best algorithms\\nwith similar restrictions and an order of magnitude\\nfaster than the best more general tables. In some\\nextreme cases, the difference even approaches four\\norders of magnitude.\\nCategory: [D.1.3] Programming Techniques\\nConcurrent Programming [E.1] Data Structures\\nTables [E.2] Data Storage Representation Hash-\\ntable representations\\nTerms: Performance, Experimentation, Mea-\\nsurement, Design, Algorithms\\nKeywords: Concurrency, dynamic data struc-\\ntures, experimental analysis, hash table, lock-\\nfreedom, transactional memory\\n1 Introduction\\nA hash table is a dynamic data structure which\\nstores a set of elements that are accessible by their\\nkey. It supports insertion, deletion, find and update\\nin constant expected time. In a concurrent hash ta-\\nble, multiple threads have access to the same table.\\nThis allows threads to share information in a flex-\\nible and efficient way. Therefore, concurrent hash\\ntables are one of the most important concurrent\\ndata structures. See Section 4 for a more detailed\\ndiscussion of concurrent hash table functionality.\\nTo show the ubiquity of hash tables we give a\\nshort list of example applications: A very sim-\\nple use case is storing sparse sets of precom-\\nputed solutions (e.g. [27], [3]). A more compli-\\ncated one is aggregation as it is frequently used\\nin analytical data base queries of the form SELECT\\nFROM. . . COUNT. . . GROUP BY x [25]. Such a query se-\\nlects rows from one or several relations and counts\\nfor every key x how many rows have been found\\n(similar queries work with SUM, MIN, or MAX). Hash-\\ning can also be used for a data-base join [5]. An-\\nother group of examples is the exploration of a large\\ncombinatorial search space where a hash table is\\nused to remember the already explored elements\\n(e.g., in dynamic programming [36], itemset mining\\n[28], a chess program, or when exploring an implic-\\nitly defined graph in model checking [37]). Simi-\\nlarly, a hash table can maintain a set of cached ob-\\njects to save I/Os [26]. Further examples are dupli-\\ncate removal, storing the edge set of a sparse graph\\nin order to support edge queries [23], maintaining\\nthe set of nonempty cells in a grid-data structure\\nused in geometry processing (e.g. [7]), or maintain-\\ning the children in tree data structures such as van\\nEmde-Boas search trees [6] or suffix trees [21].\\nMany of these applications have in common that\\n– even in the sequential version of the program –\\nhash table accesses constitute a significant fraction\\n1\\nar\\nX\\niv\\n:1\\n60\\n1.\\n04\\n01\\n7v\\n2 \\n [c\\ns.D\\nS]\\n  6\\n Se\\np 2\\n01\\n6\\nof the running time. Thus, it is essential to have\\nhighly scalable concurrent hash tables that actually\\ndeliver significant speedups in order to parallelize\\nthese applications. Unfortunately, currently avail-\\nable general purpose concurrent hash tables do not\\noffer the needed scalability (see Section 8 for con-\\ncrete numbers). On the other hand, it seems to be\\nfolklore that a lock-free linear probing hash table\\nwhere keys and values are machine words, which is\\npreallocated to a bounded size, and which supports\\nno true deletion operation can be implemented us-\\ning atomic compare-and-swap (CAS) instructions\\n[36]. Find-operations can even proceed naively and\\nwithout any write operations. In Section 4 we ex-\\nplain our own implementation (folklore) in detail,\\nafter elaborating on some related work, and intro-\\nducing the necessary notation (in Section 2 and\\n3 respectively). To see the potential big perfor-\\nmance differences, consider an exemplary situation\\nwith mostly read only access to the table and heavy\\ncontention for a small number of elements that are\\naccessed again and again by all threads. folklore\\nactually profits from this situation because the con-\\ntended elements are likely to be replicated into lo-\\ncal caches. On the other hand, any implementa-\\ntion that needs locks or CAS instructions for find-\\noperations, will become much slower than the se-\\nquential code on current machines. The purpose of\\nour paper is to document and explain performance\\ndifferences, and, more importantly, to explore to\\nwhat extent we can make folklore more general with\\nan acceptable deterioration in performance.\\nThese generalizations are discussed in Section 5.\\nWe explain how to grow (and shrink) such a table,\\nand how to support deletions and more general data\\ntypes. In Section 6 we explain how hardware trans-\\nactional memory can be used to speed up insertions\\nand updates and how it may help to handle more\\ngeneral data types.\\nAfter describing implementation details in Sec-\\ntion 7, Section 8 experimentally compares our hash\\ntables with six of the most widely used concurrent\\nhash tables for microbenchmarks including inser-\\ntion, finding, and aggregating data. We look at\\nboth uniformly distributed and skewed input dis-\\ntributions. Section 9 summarizes the results and\\ndiscusses possible lines of future research.\\n2 Related Work\\nThis publication follows up on our previous findings\\nabout generalizing fast concurrent hash tables [18].\\nIn addition to describing how to generalize a fast\\nlinear probing hash table, we offer an extensive\\nexperimental analysis comparing many concurrent\\nhash tables from several libraries.\\nThere has been extensive previous work on con-\\ncurrent hashing. The widely used textbook “The\\nArt of Multiprocessor Programming” [12] by Her-\\nlihy and Shavit devotes an entire chapter to concur-\\nrent hashing and gives an overview over previous\\nwork. However, it seems to us that a lot of previ-\\nous work focuses more on concepts and correctness\\nbut surprisingly little on scalability. For example,\\nmost of the discussed growing mechanisms assume\\nthat the size of the hash table is known exactly\\nwithout a discussion that this introduces a perfor-\\nmance bottleneck limiting the speedup to a con-\\nstant. Similarly, the actual migration is often done\\nsequentially.\\nStivala et al. [36] describe a bounded concurrent\\nlinear probing hash table specialized for dynamic\\nprogramming that only support insert and find.\\nTheir insert operation starts from scratch when the\\nCAS fails which seems suboptimal in the presence\\nof contention. An interesting point is that they\\nneed only word size CAS instructions at the price\\nof reserving a special empty value. This technique\\ncould also be adapted to port our code to machines\\nwithout 128-bit CAS.\\nKim and Kim [14] compare this table with a\\ncache-optimized lockless implementation of hashing\\nwith chaining and with hopscotch hashing [13]. The\\nexperiments use only uniformly distributed keys,\\ni.e., there is little contention. Both linear prob-\\ning and hashing with chaining perform well in that\\ncase. The evaluation of find-performance is a bit\\ninconclusive: chaining wins but using more space\\nthan linear probing. Moreover it is not specified\\nwhether this is for successful (use key of inserted\\nelements) or mostly unsuccessful (generate fresh\\nkeys) accesses. We suspect that varying these pa-\\nrameters could reverse the result.\\nGao et al. [10] present a theoretical dynamic lin-\\near probing hash table, that is lock-free. The main\\ncontribution is a formal correctness proof. Not all\\ndetails of the algorithm or even an implementation\\nis given. There is also no analysis of the complexity\\n2\\nof the growing procedure.\\nShun and Blelloch [34] propose phase concurrent\\nhash tables which are allowed to use only a sin-\\ngle operation within a globally synchronized phase.\\nThey show how phase concurrency helps to im-\\nplement some operations more efficiently and even\\ndeterministically in a linear probing context. For\\nexample, deletions can adapt the approach from\\n[15] and rearrange elements. This is not possible\\nin a general hash table since this might cause find-\\noperations to report false negatives. They also out-\\nline an elegant growing mechanism albeit without\\nimplementing it and without filling in all the detail\\nlike how to initialize newly allocated tables. They\\npropose to trigger a growing operation when any\\noperation has to scan more than k log n elements\\nwhere k is a tuning parameter. This approach is\\ntempting since it is somewhat faster than the ap-\\nproximate size estimator we use. We actually tried\\nthat but found that this trigger has a very high\\nvariance – sometimes it triggers late making opera-\\ntions rather slow, sometimes it triggers early wast-\\ning a lot of space. We also have theoretical concerns\\nsince the bound k log n on the length of the longest\\nprobe sequence implies strong assumptions on cer-\\ntain properties of the hash function. Shun and Blel-\\nloch make extensive experiments including applica-\\ntions from the problem based benchmark suite [35].\\nLi et al. [17] use the bucket cuckoo-hashing\\nmethod by Dietzfelbinger and Weidling [8] and de-\\nvelop a concurrent implementation. They exploit\\nthat using a BFS-based insertion algorithm, the\\nnumber of element moves for an insertion is very\\nsmall. They use fine grained locks which can some-\\ntimes be avoided using transactional memory (Intel\\nTSX). As a result of their work, they implemented\\nthe small open source library libcuckoo, which we\\nmeasure against (which does not use TSX). This\\napproach has the potential to achieve very good\\nspace efficiency. However, our measurements indi-\\ncate that the performance penalty is high.\\nThe practical importance of concurrent hash ta-\\nbles also leads to new and innovative implementa-\\ntions outside of the scientific community. A good\\nexample of this is the Junction library, that was\\npublished by Preshing [31] in the beginning of 2016,\\nshortly after our initial publication [19].\\n3 Preliminaries\\nWe assume that each application thread has its own\\ndesignated hardware thread or processing core and\\ndenote the number of these threads with p. A data\\nstructure is non-blocking if no blocked thread cur-\\nrently accessing this data structure can block an\\noperation on the data structure by another thread.\\nA data structure is lock-free if it is non-blocking\\nand guarantees global progress, i.e., there must al-\\nways be at least one thread finishing its operation\\nin a finite number of steps.\\nHash Tables store a set of 〈Key,Value〉 pairs (ele-\\nments).1 A hash function h maps each key to a cell\\nof a table (an array). The number of elements in\\nthe hash table is denoted n and the number of oper-\\nations is m. For the purpose of algorithm analysis,\\nwe assume that n and m are \\x1d p2 – this allows us\\nto simplify algorithm complexities by hiding O(p)\\nterms that are independent of n and m in the over-\\nall cost. Sequential hash tables support the inser-\\ntion of elements, and finding, updating, or delet-\\ning an element with given key – all of this in con-\\nstant expected time. Further operations compute n\\n(size), build a table with a given number of initial\\nelements, and iterate over all elements (forall).\\nLinear Probing is one of the most popular se-\\nquential hash table schemes used in practice. An\\nelement 〈x, a〉 is stored at the first free table entry\\nfollowing position h(x) (wrapping around when the\\nend of the table is reached). Linear probing is at\\nthe same time simple and efficient – if the table is\\nnot too full, a single cache line access will be enough\\nmost of the time. Deletion can be implemented by\\nrearranging the elements locally [15] to avoid holes\\nviolating the invariant mentioned above. When the\\ntable becomes too full or too empty, the elements\\ncan be migrated to a larger or smaller table re-\\nspectively. The migration cost can be charged to\\ninsertions and deletions causing amortized constant\\noverhead.\\n1Much of what is said here can be generalized to the\\ncase when Elements are black boxes from which keys are\\nextracted by an accessor function.\\n3\\n4 Concurrent Hash Table In-\\nterface and Folklore Imple-\\nmentation\\nAlthough it seems quite clear what a hash table is\\nand how this generalizes to concurrent hash tables,\\nthere is a surprising number of details to consider.\\nTherefore, we will quickly go over some of our inter-\\nface decisions, and detail how this interface can be\\nimplemented in a simple, fast, lock-free concurrent\\nlinear probing hash table.\\nThis hash table will have a bounded capacity\\nc that has to be specified when the table is con-\\nstructed. It is the basis for all other hash table\\nvariants presented in this publication. We call this\\ntable the folklore solution, because variations of it\\nare used in many publications and it is not clear to\\nus by whom it was first published.\\nThe most important requirement for concurrent\\ndata structures is, that they should be linearizable,\\ni.e., it must be possible to order the hash table op-\\nerations in some sequence – without reordering two\\nopperations of the same thread – so that executing\\nthem sequentially in that order yields the same re-\\nsults as the concurrent processing. For a hash ta-\\nble data structure, this basically means that all op-\\nerations should be executed atomically some time\\nbetween their invokation and their return. For ex-\\nample, it has to be avoided, that a find returns\\nan inconsistent state, e.g. a half-updated data field\\nthat was never actually stored at the corresponding\\nkey.\\nOur variant of the folklore solution ensures the\\natomicity of operations using 2-word atomic CAS\\noperations for all changes of the table. As long as\\nthe key and the value each only use one machine\\nword, we can use 2-word CAS opearations to atom-\\nically manipulate a stored key together with the\\ncorresponding value. There are other variants that\\navoid need 2-word compare and swap operations,\\nbut they often need a designated empty value (see\\n[31]) . Since, the corresponding machine instruc-\\ntions are widely available on modern hardware, us-\\ning them should not be a problem. If the target\\narchitecture does not support the needed instruc-\\ntions, the implementation can easily be switched\\nto use a variant of the folklore solution which does\\nnot use 2-word CAS. As it can easily be deduced\\nby the context, we will usually omit the “2-word”\\nprefix and use the abbreviation CAS for both single\\nand double word CAS operations.\\nInitialization The constructor allocates an array\\nof size c consisting of 128-bit aligned cells whose key\\nis initialized to the empty values.\\nModifications We propose, to categorize all\\nchanges to the hash table content into one of the\\nfollowing three functions, that can be implemented\\nvery similarly (does not cover deletions).\\ninsert(k, d): Returns false if an element with\\nthe specified key is already present. Only one op-\\neration should succeed if multiple threads are in-\\nserting the same key at the same time.\\nupdate(k, d, up): Returns false, if there is no\\nvalue stored at the specified key, otherwise this\\nfunction atomically updates the stored value to\\nnew = up(current, d). Notice, that the resulting\\nvalue can be dependent on both the current value\\nand the input parameter d.\\ninsertOrUpdate(k, d, up): This operation updates\\nthe current value, if one is present, otherwise the\\ngiven data element is inserted as the new value.\\nThe function returns true, if insertOrUpdate per-\\nformed an insert (key was not present), and false\\nif an update was executed.\\nWe choose this interface for two main reasons.\\nIt allows applications to quickly differentiate be-\\ntween inserting and changing an element – this is\\nespecially usefull since the thread who first inserted\\na key can be identified uniquely. Additionally it\\nallows transparent, lockless updates that can be\\nmore complex, than just replacing the current value\\n(think of CAS or Fetch-and-Add).\\nThe update interface using an update function\\ndeserves some special attention, as it is a novel ap-\\nproach compared to most interfaces we encountered\\nduring our research. Most implementations fall into\\none of two categories: They return mutable refer-\\nences to table elements – forcing the user to imple-\\nment atomic operations on the data type; or they\\noffer an update function which usually replaces the\\ncurrent value with a new one – making it very hard\\nto implement atomic changes like a simple counter\\n(find + increment + overwrite not necessarily\\natomic).\\nIn Algorithm 1 we show the pseudocode of the\\ninsertOrUpdate function. The operation com-\\n4\\nALGORITHM 1: Pseudocode for the insertOrUpdate operation\\nInput: Key k, Data Element d, Update Function up : Key×Val×Val→ Val\\nOutput: Boolean true when a new key was inserted, false if an update occurred\\n1 i = h(k);\\n2 while true do\\n3 i = i % c;\\n4 current = table[i];\\n5 if current.key == empty key then // Key is not present yet ...\\n6 if table[i].CAS(current, 〈k, d 〉) then\\n7 return true\\n8 else\\n9 i--;\\n10 else if current.key == k then // Same key already present ...\\n11 if table[i].atomicUpdate(current, d, up) then\\n// default: atomicUpdate(·) = CAS( current, up( k, current.data, d))\\n12 return false\\n13 else\\n14 i--;\\n15 i++;\\nputes the hash value of the key and proceeds to\\nlook for an element with the appropriate key (be-\\nginning at the corresponding position). If no ele-\\nment matching the key is found (when an empty\\nspace is encountered), the new element has to be\\ninserted. This is done using a CAS operation. A\\nfailed swap can only be caused by another inser-\\ntion into the same cell. In this case, we have to\\nrevisit the same cell, to check if the inserted el-\\nement matches the current key. If a cell storing\\nthe same key is found, it will be updated using the\\natomicUpdate function. This function is usually\\nimplemented by evaluating the passed update func-\\ntion (up) and using a CAS operation, to change the\\ncell. In the case of multiple concurrent updates, at\\nleast one will be successful.\\nIn our (C++) implementation, partial template\\nspecialization can be used to implement more ef-\\nficient atomicUpdate variants using atomic opera-\\ntions – changing the default line 11, e.g. overwrite\\n(using single word store), increment (using fetch\\nand add).\\nThe code presented in Algorithm 1 can easily be\\nmodified to implement the insert (return false\\nwhen the key is already present – line 10) and\\nupdate (return true after a successful update –\\nline 12 and false when the key is not found –\\nline 5) functions. All modification functions have a\\nconstant expected running time.\\nLookup Since this folklore implementation does\\nnot move elements within the table, it would be\\npossible for find(k) to return a reference to the\\ncorresponding element. In our experience, return-\\ning references directly tempts inexperienced pro-\\ngrammers to opperate on these references in a way\\nthat is not necessarily threadsafe. Therefore, our\\nimplementation returns a copy of the corresponding\\ncell (〈k, d 〉), if one is found (〈empty key, ·〉 other-\\nwise). The find operation has a constant expected\\nrunning time.\\nOur implementation of find somewhat non-\\ntrivial, because it is not possible to read two ma-\\nchine words at once using an atomic instruction2.\\nTherefore it is possible for a cell to be changed in-\\nbetween reading its key and its value – this is called\\na torn read. We have to make sure, that torn reads\\ncannot lead to any wrong behavior. There are two\\nkinds of interesting torn reads: First an empty key\\nis read while the searched key is inserted into the\\nsame cell, in this case the element is not found (con-\\nsistent since it has not been fully inserted); Second\\n2The element is not read atomically, because x86 does\\nnot support that. One could use a 2-word CAS to achieve\\nthe same effect but this would have disastrous effects on per-\\nformance when many threads try to find the same element.\\n5\\nthe element is updated between the key being read\\nand the data being read, since the data is read sec-\\nond, only the newer data is read (consistent with a\\nfinished update).\\nDeletions The folklore solution can only han-\\ndle deletions using dummy elements – called tomb-\\nstones. Usually the key stored in a cell is replaced\\nwith del key. Afterwards the cell cannot be used\\nanymore. This method of handling deleted ele-\\nments is usually not feasible, as it does not increase\\nthe capacity for new elements. In Section 5.4 We\\nwill show, how our generalizations can be used to\\nhandle tombstones more efficiently.\\nBulk Operations While not often used in prac-\\ntice, the folklore table can be modified to sup-\\nport operations like buildFrom(·) (see Section 5.5)\\n– using a bulk insertion which can be more effi-\\ncient than element-wise insertion – or forall(f) –\\nwhich can be implemented embarrassingly parallel\\nby splitting the table between threads.\\nSize Keeping track of the number of contained el-\\nements deserves special notice here because it turns\\nout to be significantly harder in concurrent hash ta-\\nbles. In sequential hash tables, it is trivial to count\\nthe number of contained elements – using a single\\ncounter. This same method is possible in parallel\\ntables using atomic fetch and add operations, but it\\nintroduces a massive amount of contention on one\\nsingle counter creating a performance bottleneck.\\nBecause of this we did not include a counting\\nmethod in folklore implementation. In Section 5.2\\nwe show how this can be alleviated using an ap-\\nproximate count.\\n5 Generalizations and Exten-\\nsions\\nIn this section, we detail how to adapt the concur-\\nrent hash table implementation – described in the\\nprevious section – to be universally applicable to\\nall hash table workloads. Most of our efforts have\\ngone into a scalable migration method that is used\\nto move all elements stored in one table into an-\\nother table. It turns out that a fast migration can\\nsolve most shortcomings of the folklore implemen-\\ntation (especially deletions and adaptable size).\\n5.1 Storing Thread-Local Data\\nBy itself, storing thread specific data connected to\\na hash table does not offer additional functionality,\\nbut it is necessary to efficiently implement some of\\nour other extensions. Per-thread data can be used\\nin many different ways, from counting the number\\nof insertions to caching shared resources.\\nFrom a theoretical point of view, it is easy to\\nstore thread specific data. The additional space is\\nusually only dependent on the number of threads\\n(O(p) additional space), since the stored data is\\noften constant sized. Compared to the hash table\\nthis is usually negligible (p\\x1c n < c).\\nStoring thread specific data is challenging from\\na software design and performance perspective.\\nSome of our competitors use a register(·) func-\\ntion that each thread has to call before accessing\\nthe table. This allocates some memory, that can\\nbe accessed using the global hash table object.\\nOur solution uses explicit handles. Each thread\\nhas to create a handle, before accessing the hash ta-\\nble. These handles can store thread specific data,\\nsince they are not shared between threads. This is\\nnot only in line with the RAII idiom (resource ac-\\nquisition is initialization [24]), it also protects our\\nimplementation from some performance pitfalls like\\nunnecessary indirections and false sharing3. More-\\nover, the data can easily be deleted once the thread\\ndoes not use the hash table anymore (delete the\\nhandle).\\n5.2 Approximating the Size\\nKeeping an exact count of the elements stored in\\nthe hash table can often lead to contention on one\\ncount variable. Therefore, we propose to support\\nonly an approximative size operation.\\nTo keep an approximate count of all elements,\\neach thread maintains a local counter of its success-\\nful insertions (using the method desribed in Sec-\\ntion 5.1). Every Θ(p) such insertions this counter\\nis atomically added to a global insertion counter\\nI and then reset. Contention at I can be provably\\n3Significant slow down created by the cache coherency\\nprotocol due to multiple threads repeatedly changing dis-\\ntinct values within the same cache line.\\n6\\nmade small by randomizing the exact number of lo-\\ncal insertions accepted before adding to the global\\ncounter, e.g., between 1 and p. I underestimates\\nthe size by at most O\\n(\\np2\\n)\\n. Since we assume the\\nsize to be \\x1d p2 this still means a small relative er-\\nror. By adding the maximal error, we also get an\\nupper bound for the table size.\\nIf deletions are also allowed, we maintain a global\\ncounter D in a similar way. S = I − D is then a\\ngood estimate of the total size as long as S \\x1d p2.\\nWhen a table is migrated for growing or shrink-\\ning (see Section 5.3.1), each migration thread lo-\\ncally counts the elements it moves. At the end of\\nthe migration, local counters are added to create\\nthe initial count for I (D is set to 0).\\nThis method can also be extended to give an\\nexact count – in absence of concurrent inser-\\ntions/deletions. To do this, a list of all handles\\nhas to be stored at the global hash table object. A\\nthread can now iterate over all handles computing\\nthe actual element size.\\n5.3 Table Migration\\nWhile Gao et al. [10] have shown that lock-free dy-\\nnamic linear probing hash tables are possible, there\\nis no result on their practical feasibility. Our focus\\nis geared more towards engineering the fastest mi-\\ngration possible, therefore, we are fine with small\\namounts of locking, as long as it improves the over-\\nall performance.\\n5.3.1 Eliminating Unnecessary Contention\\nfrom the Migration\\nIf the table size is not fixed, it makes sense to as-\\nsume that the hash function h yields a large pseu-\\ndorandom integer which is then mapped to a cell\\nposition in 0..c− 1 where c is the current capacity\\nc.4 We will discuss a way to do this by scaling. If\\nh yields values in the global range 0..U − 1 we map\\nkey x to cell hc(x) := bh(x) cU c. Note that when\\nboth c and U are powers of two, the mapping can\\nbe implemented by a simple shift operation.\\nGrowing Now suppose that we want to migrate\\nthe table into a table that has at least the same size\\n(growing factor γ ≥ 1). Exploiting the properties\\n4We use x..y as a shorthand for {x, . . . , y} in this paper.\\nof linear probing and our scaling function, there is\\na surprisingly simple way to migrate the elements\\nfrom the old table to the new table in parallel which\\nresults in exactly the same order a sequential algo-\\nrithm would take and that completely avoids syn-\\nchronization between threads.\\nLemma 1. Consider a range a..b of nonempty cells\\nin the old table with the property that the cells\\na− 1 mod c and b+ 1 mod c are both empty – call\\nsuch a range a cluster (see Figure 1a). When\\nmigrating a table, sequential migration will map\\nthe elements stored in that cluster into the range\\nbγac .. bγ(b+ 1)c in the target table, regardless of\\nthe rest of the source array.\\nProof. Let x be an element stored in the cluster a..b\\nat position p(x) = hc(x) + d(x). Then hc(x) has to\\nbe in the cluster a..b, because linear probing does\\nnot displace elements over empty cells (hc(x) =\\nbh(x) cU c ≥ a), and therefore, h(x) c\\n′\\nU ≥ a c\\n′\\nc ≥ γa.\\nSimilarly, from bh(x) cU c ≤ b follows h(x) cU <\\nb+ 1, and therefore, h(x) c\\n′\\nU < γ(b+ 1).\\nTherefore, two distinct clusters in the source ta-\\nble cannot overlap in the target table. We can ex-\\nploit this lemma by assigning entire clusters to mi-\\ngrating threads which can then process each cluster\\ncompletely independently. Distributing clusters be-\\ntween threads can easily be achieved by first split-\\nting the table into blocks (regardless of the tables\\ncontents) which we assign to threads for parallel mi-\\ngration. A thread assigned block d..e will migrate\\nthose clusters that start within this range – implic-\\nitly moving the block borders to free cells as seen in\\nFigure 1b). Since the average cluster length is short\\nand c = Ω\\n(\\np2\\n)\\n, it is sufficient to deal out blocks\\nof size Ω(p) using a single shared global variable\\nand atomic fetch-and-add operations. Additionally\\neach thread is responsible for initializing all cells in\\nits region of the target table. This is important,\\nbecause sequentially initializing the hash table can\\nquickly become infeasible.\\nNote that waiting for the last thread at the end\\nof the migration introduces some waiting (locking),\\nbut this does not create significant work imbalance,\\nsince the block/cluster migration is really fast and\\nclusters are expected to be short.\\nShrinking Unfortunately, the nice structural\\nLemma 1 no longer applies. We can still parallelize\\n7\\nab\\nγa\\nγ(b+ 1)\\na′\\nb′\\nγa′\\nγ(b′ + 1)\\n(a) Two neighboring clusters and their non-\\noverlapping target areas (γ = 2).\\n(b) Left: table split into even blocks. Right: resulting\\ncluster distribution (moved implicit block borders).\\nFigure 1: Cluster migration and work distribution\\nthe migration with little synchronization. Once\\nmore, we cut the source table into blocks that we\\nassign to threads for migration. The scaling func-\\ntion maps each block a..b in the source table to a\\nblock a′..b′ in the target table. We have to be care-\\nful with rounding issues so that the blocks in the\\ntarget table are non-overlapping. We can then pro-\\nceed in two phases. First, a migrating thread mi-\\ngrates those elements that move from a..b to a′..b′.\\nThese migrations can be done in a sequential man-\\nner, since target blocks are disjoint. The majority\\nof elements will fit into the target block. Then, af-\\nter a barrier synchronization, all elements that did\\nnot fit into their respective target blocks are mi-\\ngrated using concurrent insertion i.e., using atomic\\noperations. This has negligible overhead since el-\\nements like this only exist at the boundaries of\\nblocks. The resulting allocation of elements in the\\ntarget table will no longer be the same as for a\\nsequential migration but as long as the data struc-\\nture invariants of a linear probing hash table are\\nfulfilled, this is not a problem.\\n5.3.2 Hiding the Migration from the Un-\\nderlying Application\\nTo make the concurrent hash table more general\\nand easy to use, we would like to avoid all explicit\\nsynchronization. The growing (and shrinking) op-\\nerations should be performed asynchronously when\\nneeded, without involvement of the underlying ap-\\nplication. The migration is triggered once the ta-\\nble is filled to a factor ≥ α (e.g. 50 %), this is\\nestimated using the approximate count from Sec-\\ntion 5.2, and checked whenever the global count is\\nupdated. When a growing operation is triggered,\\nthe capacity will be increased by a factor of γ ≥ 1\\n(Usually γ = 2). The difficulty is ensuring that this\\noperation is done in a transparent way without in-\\ntroducing any inconsistent behavior and without\\nincurring undue overheads.\\nTo hide the migration process from the user, two\\nproblems have to be solved. First, we have to find\\nthreads to grow the table, and second, we have to\\nensure, that changing elements in the source table\\nwill not lead to any inconsistent states in the target\\ntable (possibly reverting changes made during the\\nmigration). Each of these problems can be solved\\nin multiple ways. We implemented two strategies\\nfor each of them resulting in four different variants\\nof the hash table (mix and match).\\nRecruiting User-Threads A simple approach\\nto dynamically allocate threads to growing the ta-\\nble, is to “enslave” threads that try to perform\\ntable accesses that would otherwise have to wait\\nfor the completion of the growing process anyway.\\nThis works really well when the table is regularly\\naccessed by all user-threads, but is inefficient in the\\nworst case when most threads stop accessing the ta-\\nble at some point, e.g., waiting for the completion of\\na global computation phase at a barrier. The few\\nthreads still accessing the table at this point will\\nneed a lot of time for growing (up to Ω(n)) while\\nmost threads are waiting for them. One could try\\nto also enslave waiting threads but it looks difficult\\nto do this in a sufficiently general and portable way.\\nUsing a Dedicated Thread Pool A provably\\nefficient approach is to maintain a pool of p threads\\ndedicated to growing the table. They are blocked\\n8\\nuntil a growing operation is triggered. This is when\\nthey are awoken to collectively perform the migra-\\ntion in time O(n/p) and then get back to sleep.\\nDuring a migration, application threads might have\\nto sleep until the migration threads are finished.\\nThis will increase the CPU time of our migration\\nthreads making this method nearly as efficient as\\nthe enslavement variant. Using a reasonable com-\\nputation model, one can show that using thread\\npools for migration increases the cost of each table\\naccess by at most a constant in a globally amortized\\nsense (over the non-growing folklore solution). We\\nomit the relatively simple proof.\\nTo remain fair to all competitors, we used ex-\\nactly as many threads for the thread pool as there\\nwere application threads accessing the table. Ad-\\nditionally each migration thread was bound to a\\ncore, that was also used by one corresponding ap-\\nplication thread.\\nMarking Moved Elements for Consistency\\n(asynchronous) During the migration it is im-\\nportant that no element can be changed in the old\\ntable after it has been copied to the new table. Oth-\\nerwise, it would be hard to guarantee that changes\\nare correctly applied to the new table. The easiest\\nsolution to this problem is, to mark each cell before\\nit is copied. Marking each cell can be done using\\na CAS operation to set a special marked bit which\\nis stored in the key. In practice this reduces the\\npossible key space. If this reduction is a problem,\\nsee Section 5.6 on how to circumvent it. To ensure\\nthat no copied cell can be changed, it suffices to\\nensure that no marked cell can be changed. This\\ncan easily be done by checking the bit before each\\nwriting operation, and by using CAS operations for\\neach update. This prohibits the use of fast atomic\\noperations to change element values.\\nAfter the migration, the old hash table has to\\nbe deallocated. Before deallocating an old table,\\nwe have to make sure that no thread is currently\\nusing it anymore. This problem can generally be\\nsolved by using reference counting. Instead of stor-\\ning the table with a usual pointer, we use a ref-\\nerence counted pointer (e.g. std::shared ptr) to\\nensure that the table is eventually freed.\\nThe main disadvantage of counting pointers\\nis that acquiring a counting pointer requires an\\natomic increment on a shared counter. Therefore, it\\nis not feasible to acquire a counting pointer for each\\noperation. Instead a copy of the shared pointer can\\nbe stored locally, together with the increasing ver-\\nsion number of the corresponding hash table (using\\nthe method from Section 5.1). At the beginning of\\neach operation, we can use the local version number\\nto make sure that the local counting pointer still\\npoints to the newest table version. If this is not the\\ncase, a new pointer will be acquired. This happens\\nonly once per version of the hash table. The old\\ntable will automatically be freed once every thread\\nhas updated its local pointer. Note that counting\\npointers cannot be exchanged in a lock-free man-\\nner increasing the cost of changing the current table\\n(using a lock). This lock could be avoided by using\\na hazard pointer. We did not do this\\nPrevent Concurrent Updates to ensure Con-\\nsistency (synchronized) We propose a simple\\nprotocol inspired by read-copy-update protocols\\n[22]. The thread t triggering the growing operation\\nsets some global growing flag using a CAS instruc-\\ntion. A thread t performing a table access sets a\\nlocal busy flag when starting an operation. Then\\nit inspects the growing flag, if the flag is set, the\\nlocal flag is unset. Then the local thread waits for\\nthe completion of the growing operation, or helps\\nwith migrating the table depending on the current\\ngrowing strategy. Thread t waits until all busy\\nflags have been unset at least once before starting\\nthe migration. When the migration is completed,\\nthe growing flag is reset, signaling to the waiting\\nthreads that they can safely continue their table-\\noperations. Because this protocol ensures that no\\nthread is accessing the previous table after the be-\\nginning of the migration, it can be freed without\\nusing reference counting.\\nWe call this method (semi-)synchronized, be-\\ncause grow and update operations are disjoint.\\nThreads participating in one growing step still ar-\\nrive asynchronously, e.g. when the parent applica-\\ntion called a hash table operation. Compared to\\nthe marking based protocol, we save cost during\\nmigration by avoiding CAS operations. However,\\nthis is at the expense of setting the busy flags for\\nevery operation. Our experiments indicates that\\noverall this is only advantageous for updates using\\natomic operations like fetch-and-add that cannot\\ncoexist with the marker flags.\\n9\\n5.4 Deletions\\nFor concurrent linear probing, we combine tomb-\\nstoning (see Section 4) with our migration algo-\\nrithm to clean the table once it is filled with too\\nmany tombstones.\\nA tombstone is an element, that has a del key in\\nplace of its key. The key x of a deleted entry 〈x, a〉\\nis atomically changed to 〈del key, a〉. Other ta-\\nble operations scan over these deleted elements like\\nover any other nonempty entry. No inconsistencies\\ncan arise from deletions. In particular, a concurrent\\nfind-operations with a torn read will return the ele-\\nment before the deletion since the delete-operation\\nwill leave the value-slot a untouched. A concurrent\\ninsert 〈x, b〉 might read the key x before it is over-\\nwritten by the deletion and return false because\\nit concludes that an element with key x is already\\npresent. This is consistent with the outcome when\\nthe insertion is performed before the deletion in a\\nlinearization.\\nThis method of deletion can easily be imple-\\nmented in the folklore solution from Section 4. But\\nthe starting capacity has to be set dependent on\\nthe number of overall insertions, since this form of\\ndeletion does not free up any of the deleted cells.\\nEven worse, tombstones will fill up the table and\\nslow down find queries.\\nBoth of these problems can be solved by migrat-\\ning all non-tombstone elements into a new table.\\nThe decision when to migrate the table should be\\nmade solely based on the number of insertions I\\n(= number of nonempty cells). The count of all\\nnon-deleted elements I −D is then used to decide\\nwhether the table should grow, keep the same size\\n(notice γ = 1 is a special case for our optimized mi-\\ngration), or shrink. Either way, all tombstones can\\nbe removed in the course of the element migration.\\n5.5 Bulk Operations\\nBuilding a hash table for n elements passed to the\\nconstructor can be parallelized using integer sorting\\nby the hash function value. This works in time\\nO(n/p) regardless how many times an element is\\ninserted, i.e., sorting circumvents contention. See\\nthe work of Mller et al.[25] for a discussion of this\\nphenomenon in the context of aggregation.\\nThis can be generalized for processing batches of\\nsizem = Ω(n) that may even contain a mix of inser-\\ntions, deletions, and updates. We outline a simple\\nalgorithm for bulk-insertion that works without ex-\\nplicit sorting albeit does not avoid contention. Let\\na denote the old size of the hash table and b the\\nnumber of insertions. Then a+b is an upper bound\\nfor the new table size. If necessary, grow the table\\nto that size or larger (see below). Finally, in paral-\\nlel, insert the new elements.\\nMore generally, processing batches of size m =\\nΩ(n) in a globally synchronized way can use the\\nsame strategy. We outline it for the case of bulk\\ninsertions. Generalization to deletions, updates,\\nor mixed batches is possible: Integer sort the ele-\\nments to be inserted by their hash key in expected\\ntime O(m/p). Among elements with the same hash\\nvalue, remove all but the last. Then “merge” the\\nbatch and the hash table into a new hash table\\n(that may have to be larger to provide space for the\\nnew elements). We can adapt ideas from parallel\\nmerging [11]. We co-partition the sorted insertion\\narray and the hash table into corresponding pieces\\nof size O(m/p). Most of the work can now be done\\non these pieces in an embarrassingly parallel way –\\neach piece of the insertion array is scanned sequen-\\ntially by one thread. Consider an element 〈x, a〉 and\\nprevious insertion position i in the table. Then we\\nstart looking for a free cell at position max(h(x), i)\\n5.6 Restoring the Full Key Space\\nOur table uses special keys, like the empty key\\n(empty key) and the deleted key (del key). El-\\nements that actually have these keys cannot be\\nstored in the hash table. This can easily be fixed\\nby using two special slots in the global hash table\\ndata structure. This makes some case distinction\\nnecessary but should have rather low impact on the\\noverall performance.\\nOne of our growing variants (asynchronous) uses\\na marker bit in its key field. This halves the possi-\\nble key space from 264 to 263. To regain the lost key\\nspace, we can store the lost bit implicitly. Instead\\nof using one hash table that holds all elements, we\\nuse the two subtables t0 and t1. The subtable t0\\nholds all elements whose key does not have its top-\\nmost bit set. While t1 stores all elements whose\\nkey does have the topmost bit set, but instead of\\nstoring the topmost bit explicitly it is removed.\\nEach element can still be found in constant time,\\nbecause when looking for a certain key, it is imme-\\n10\\ndiately obvious in which table the corresponding\\nelement will be stored. After choosing the right\\ntable, comparing the 63 explicitly stored bits can\\nuniquely identify the correct element. Notice that\\nboth empty keys have to be stored distinctly (as\\ndescribed above).\\n5.7 Complex Key and Value Types\\nUsing CAS instructions to change the content of\\nhash table cells makes our data structure fast but\\nlimits its use to cases where keys and values fit into\\nmemory words. Lifting this restriction is bound\\nto have some impact on performance but we want\\nto outline ways to keep this penalty small. The\\ngeneral idea is to replace the keys and or values by\\nreferences to the actual data.\\nComplex Keys To make things more concrete\\nwe outline a way where the keys are strings and\\nthe hash table data structure itself manages space\\nfor the keys. When an element 〈s, a〉 is inserted,\\nspace for string s is allocated. The hash table\\nstores 〈r, a〉 where r is a pointer to s. Unfortu-\\nnately, we get a considerable performance penalty\\nduring table operations because looking for an el-\\nement with a given key now has to follow this in-\\ndirection for every key comparison – effectively de-\\nstroying the advantage of linear probing over other\\nhashing schemes with respect to cache efficiency.\\nThis overhead can be reduced by two measures:\\nFirst, we can make the table bigger thus reducing\\nthe necessary search distance – considering that the\\nkeys are large anyway, this has a relatively small\\nimpact on overall storage consumption. A more\\nsophisticated idea is to store a signature of the key\\nin some unused bits of the reference to the key (on\\nmodern machines keys actually only use 48 bits).\\nThis signature can be obtained from the master\\nhash function h extracting bits that were not used\\nfor finding the position in the table (i.e. the least\\nsignificant digits). While searching for a key y one\\ncan then first compare the signatures before actu-\\nally making a full key comparison that involves a\\ncostly pointer dereference.\\nDeletions do not immediately deallocate the\\nspace for the key because concurrent operations\\nmight still be scanning through them. The space\\nfor deleted keys can be reclaimed when the array\\ngrows. At that time, our migration protocols make\\nsure that no concurrent table operations are going\\non.\\nThe memory management is challenging since\\nwe need high throughput allocation for very fine\\ngrained variable sized objects and a kind of garbage\\ncollection. On the positive side, we can find all the\\npointers to the strings using the hash function. All\\nin all, these properties might be sufficiently unique\\nthat a carefully designed special purpose imple-\\nmentation is faster than currently available general\\npurpose allocators. We outline one such approach:\\nNew strings are allocated into memory pages of size\\nΩ(p). Each thread has one current page that is\\nonly used locally for allocating short strings. Long\\nstrings are allocated using a general purpose allo-\\ncator. When the local page of a thread is full, the\\nthread allocates a fresh page and remembers the\\nold one on a stack. During a shrinking phase, a\\ngarbage collection is done on the string memory.\\nThis can be parallelized on a page by page basis.\\nEach thread works on two pages A and B at a time\\nwhere A is a partially filled page. B is scanned and\\nthe strings stored there are moved to A (updating\\ntheir pointer in the hash table). When A runs full,\\nB replaces A. When B runs empty, it is freed. In\\neither case, an unprocessed page is obtained to be-\\ncome B.\\nComplex Values We can take a similar ap-\\nproach as for complex keys – the hash table data\\nstructure itself allocates space for complex val-\\nues. This space is only deallocated during migra-\\ntion/cleanup phases that make sure that no con-\\ncurrent table operations are affected. The find-\\noperation only hands out copies of the values so\\nthat there is no danger of stale data. There are\\nnow two types of update operations. One that mod-\\nifies part of a complex value using an atomic CAS\\noperation and one that allocates an entirely new\\nvalue object and performs the update by atomi-\\ncally setting the value-reference to the new object.\\nUnfortunately it is not possible to use both types\\nconcurrently.\\nComplex Keys and Values Of course we can\\ncombine the two approaches described above. How-\\never in that case, it will be more efficient to store\\na single reference to a combined key-value object\\ntogether with a signature.\\n11\\n6 Using Hardware Memory\\nTransactions\\nThe biggest difference between a concurrent table,\\nand a sequential hash table is the use of atomic\\nprocessor instructions. We use them for access-\\ning and modifying data which is shared between\\nthreads. An additional way to achieve atomicity is\\nthe use of hardware transactional memory synchro-\\nnization introduced recently by Intel and IBM. The\\nnew instruction extensions can group many mem-\\nory accesses into a single transaction. All changes\\nfrom one transaction are committed at the same\\ntime. For other threads they appear to be atomic.\\nGeneral purpose memory transactions do not have\\nprogress guarantees (i.e. can always be aborted),\\ntherefore they require a fall-back path implement-\\ning atomicity (a lock or an implementation using\\ntraditional atomic instructions).\\nWe believe that transactional memory synchro-\\nnization is an important opportunity for concurrent\\ndata structures. Therefore, we analyze how to ef-\\nficiently use memory transactions for our concur-\\nrent linear probing hash tables. In the following,\\nwe discuss which aspects of our hash table can be\\nimproved by using restricted transactional mem-\\nory implemented in Intel Transactional Synchro-\\nnization Extensions (Intel TSX).\\nWe use Intel TSX by wrapping sequential code\\ninto a memory transaction. Since the sequential\\ncode is simpler (e.g. less branches, more freedom\\nfor compiler optimizations) it can outperform in-\\nherently more complex code based on (expensive\\n128-bit CAS) atomic instructions. As a transac-\\ntion fall-back mechanism, we employ our atomic\\nvariants of hash table operations. Replacing the\\ninsert and update functions of our specialized grow-\\ning hash table with Intel TSX variants increases the\\nthroughput of our hash table by up to 28 % (see\\nSection 8.4). Speedups like this are easy to ob-\\ntain on workloads without contentious accesses (si-\\nmultaneous write accesses on the same cell). Con-\\ntentious write accesses lead to transaction aborts\\nwhich have a higher latency than the failure of a\\nCAS. Our atomic fall-back minimizes the penalty\\nfor such scenarios compared to the classic lock-\\nbased fall-back that causes more overhead and se-\\nrialization.\\nAnother aspect that can be improved through\\nthe use of memory transactions is the key and\\nvalue size. On current x86 hardware, there is no\\natomic instruction that can change words bigger\\nthan 128 bits at once. The amount of memory that\\ncan be manipulated during one memory transac-\\ntion can be far greater than 128 bits. Therefore,\\none could easily implement hash tables with com-\\nplex keys and values using transactional memory\\nsynchronization. However, using atomic functions\\nas fall-back will not be possible. Solutions with\\nfine-grained locks that are only needed when the\\ntransactions actually fail, are still possible.\\nWith general purpose memory transactions it is\\neven possible to atomically change multiple values\\nthat are not stored consecutively. Therefore, it is\\npossible to implement a hash table that separates\\nthe keys from the values storing each in a separate\\ntable. In theory this could improve the cache local-\\nity of linear probing.\\nOverall, transactional memory synchronization\\ncan be used to improve performance and to make\\nthe data structure more flexible.\\n7 Implementation Details\\nBounded Hash Tables. All of our implemen-\\ntations are constructed around a highly optimized\\nvariant of the circular bounded folklore hash table\\nthat was describe in Section 4. The main perfor-\\nmance optimizations were to restrict the table size\\nto powers of two – replacing expensive modulo op-\\nerations with fast bit operations When initializing\\nthe capacity c, we compute the lowest power of two,\\nthat is still at least twice as large as the expected\\nnumber of insertions (2n ≤ size ≤ 4n).\\nWe also built a second non growing hash table\\nvariant called tsxfolklore, this variant forgoes the\\nusual CAS-operations that are used to change cells.\\nInstead tsxfolklore uses TSX transactions to change\\nelements in the table atomically. As described in\\nSection 6, we use our usual atomic operations as\\nfallback in case a TSX transaction is aborted.\\nGrowing Hash Tables. All of our growing hash\\ntables use folklore or tsxfolklore to represent the\\ncurrent status of the hash table. When the table\\nis approximately 60% filled, a migration is started.\\nWith each migration, we double the capacity. The\\nmigration works in cell-blocks of the size 4096.\\n12\\nBlocks are migrated with a minimum amount of\\natomics by using the cluster migration described in\\nSection 5.3.1.\\nWe use a user-space memory pool from Intel’s\\nTBB library to prevent a slow down due to the re-\\nmapping of virtual to physical memory (protected\\nby a coarse lock in the Linux kernel). This improves\\nthe performance of our growing variants, especially\\nwhen using more than 24 threads. By allocating\\nmemory from this memory pool, we ensure that the\\nvirtual memory that we receive is already mapped\\nto physical memory, bypassing the kernel lock.\\nIn Section 5.3.1 we identified two orthogonal\\nproblems that have to be solved to migrate hash\\ntables: which threads execute the migration? and\\nhow can we make sure that copied elements cannot\\nbe changed? For each of these problems we formu-\\nlated two strategies. The table can either be mi-\\ngrated by user-threads that execute operations on\\nthe table (u), or by using a pool of threads which\\nis only responsible for the migration (p). To en-\\nsure that copied elements cannot be changed, we\\nproposed to wait for each currently running opera-\\ntion synchronizing update and growing phases (s),\\nor to mark elements before they are copied, thus\\nproceeding fully asynchronously (a).\\nAll strategies can be combined – creating the fol-\\nlowing four growing hash table variants: uaGrow\\nuses enslavement of user threads and asynchronous\\nmarking for consistency; usGrow also uses user\\nthreads threads, but ensures consistency by syn-\\nchronizing updates and growing routines; paGrow\\nuses a pool of dedicated migration threads for the\\nmigration and asynchronous marking of migrated\\nentries for consistency; and psGrow combines the\\nuse of a dedicated thread pool for migration with\\nthe synchronized exclusion mechanism.\\nAll of these versions can also be instantiated us-\\ning the TSX based non-growing table tsxfolklore as\\na basis.\\n8 Experimental Evaluation\\nWe performed a large number of experiments to\\ninvestigate the performance of different concur-\\nrent hash tables in a variety of circumstances (an\\noverview over all tested hash tables can be found in\\nTable 1). We begin by describing the tested com-\\npetitors (Section 8.1, our variants are introduced in\\nSection 7), the test instances (Section 8.3), and the\\ntest environment (Section 8.2). Then Section 8.4\\ndiscusses the actual measurements. In Section 8.5,\\nwe conclude the section by summarizing our exper-\\niments and reflecting how different generalizations\\naffect the performance of hash tables.\\n8.1 Competitors\\nTo compare our implementation to the current\\nstate of the art we use a broad selection of other\\nconcurrent hash tables. These tables were chosen\\non the basis of their popularity in applications and\\nacademic publications. We split these hash table\\nimplementations into the following three groups de-\\npending on their growing functionality.\\n8.1.1 Efficiently Growing Hash Tables\\nThis group contains all hash tables, that are able\\nto grow efficiently from a very small initial size.\\nThey are used in our growing benchmarks, where\\nwe initialize tables with an initial size of 4096 thus\\nmaking growing necessary.\\nJunction Linear , Junction Grampa , and\\nJunction Leapfrog The junction library con-\\nsists of three different variants of a dynamic con-\\ncurrent hash table. It was published by Jeff Presh-\\ning over github [31], after our first publication on\\nthe subject ([19]). There are no scientific publi-\\ncations, but on his blog [32] Preshing writes some\\ninsightful posts on his implementation. In theory,\\njunction’s hash tables use an approach to growing\\nwhich is similar to ours. A filled bounded hash\\ntable is migrated into a newly allocated bigger ta-\\nble. Although they are constructed from a simi-\\nlar idea, the execution seems to differ quite signif-\\nicantly. The junction hash tables use a quiescent-\\nstate based reclamation (QSBR) protocol, for mem-\\nory reclamation. Using this protocol, in order to\\nreclaim freed hash table memory, the user has to\\nregularly call a designated function.\\nContrary to other hash tables, we used the pro-\\nvided standard hash function (avalanche), because\\njunction assumes its hash function, to be invertible.\\nTherefore, the hash function which is used for all\\nother tables (see Section 8.3) is not usable.\\nThe different hash tables within junction all per-\\nform different variants of open addressing. These\\n13\\nvariants are described in more detail, in one of\\nPreshing’s blogposts (see [32]).\\ntbbHM F and tbbUM F (correspond to\\nthe TBB maps tbb::concurrent hash map and\\ntbb::concurrent unordered map respectively)\\nThe Threading Building Blocks [30] (TBB) library\\n(Version 4.3 Update 6) developed by Intel is one of\\nthe most widely used libraries for shared memory\\nconcurrent programming. The two different con-\\ncurrent hash tables it contains behave relatively\\nsimilar in our tests. Therefore, we sometimes\\nonly plot the results of tbbHM F. But they have\\nsome differences concerning the locking of accessed\\nelements. Therefore, they behave very differently\\nunder contention.\\n8.1.2 Hash Tables with Limited Growing\\nCapabilities\\nThis group contains all hash tables that can only\\ngrow by a limited amount (constant factor of the\\ninitial size) or become very slow when growing is\\nrequired. When testing their growing capabilities,\\nwe usually initialize these tables with half their tar-\\nget size. This is comparable to a workload where\\nthe approximate number of elements is known but\\ncannot be bound strictly.\\nfolly + (folly::AtomicHashMap) This hash ta-\\nble was developed at facebook as a part of their\\nopen source library folly [9] (Version 57:0). It uses\\nrestrictions on key and data types similar to our\\nfolklore implementation. In contrast to our grow-\\ning procedure, the folly table grows by allocating\\nadditional hash tables. This increases the cost of\\nfuture queries and it bounds the total growing fac-\\ntor to ≈ 18 (×initial size).\\ncuckoo (cuckoohash map) This hash table us-\\ning (bucket) cuckoo hashing as its collision resolu-\\ntion method, is part of the small libcuckoo library\\n(Version 1.0). It uses a fine grained locking ap-\\nproach presented by Li et al. [17] to ensure consis-\\ntency. Cuckoo is mentionable for their interesting\\ninterface, which combines easy container style ac-\\ncess with an update routine similar to our update\\ninterface.\\nRCU ×/RCU QSBR × This hash table is\\npart of the Userspace RCU library (Version 0.8.7)\\n[29], that brings the read copy update principle to\\nuserspace applications. Read copy update is a set\\nof protocols for concurrent programming, that are\\npopular in the Linux kernel community [22]. The\\nhash table uses split-ordered lists to grow in a lock-\\nfree manner. This approach has been proposed by\\nShalev and Shavit [33].\\nRCU uses the recommended read-copy-update\\nvariant called urcu. RCU QSBR uses a QSBR\\nbased protocol that is comparable to the one used\\nby junction hash tables. It forces the user to repeat-\\nedly call a function with each participating thread.\\nWe tested both variants, but in many plots we show\\nonly RCU × because both variants behaved very\\nsimilarly in our tests.\\n8.1.3 Non-Growing Hash Tables\\nOne of the most important subjects of this publica-\\ntion is offering a scalable asynchronous migration\\nfor the simple folklore hash table. While this makes\\nit usable in circumstances where bounded tables\\ncannot be used, we want to show that even when\\nno growing is necessary we can compete against\\nbounded hash tables. Therefore, it is reasonable\\nto use our growing hash table even in applications\\nwhere the number of elements can be bounded in a\\nreasonable manner, offering a graceful degradation\\nin edge cases and allowing improved memory usage\\nif the bound is not reached.\\nFolklore Our implementation of the folklore\\nsolution described in Section 4. Notice that this\\nhash table is the core of our growing variants.\\nTherefore, we can immediately determine the over-\\nhead that the ability for growing places on this im-\\nplementation (Overhead for approximate counting\\nand shared pointers).\\nPhase Concurrent \\x07 This hash table imple-\\nmentation proposed by Shun and Blelloch [34] is\\ndesigned to support only phase concurrent accesses,\\ni.e. no reads can occur concurrently with writes.\\nWe tested this table anyway, because several of our\\ntest instances satisfy this constraint and it showed\\npromising running times.\\n14\\nHopscotch Hash N Hopscotch hashing (ver 2.0)\\nis one of the more popular variants of open ad-\\ndressing. The version we tested, was published\\nby Herlihy et al. [13] connected to their origi-\\nnal publication proposing the technique. Inter-\\nestingly, the provided implementation only imple-\\nments the functionality of a hash set (unable to\\nretrieve/update stored data). Therefore, we had to\\nadapt some tests to account for that (insert∼=put\\nand find∼=contains).\\nLeaHash H This hash table is designed by Lea\\n[16] as part of Java’s Concurrency Package. We\\nhave obtained a C++ implementation which was\\npublished together with the hopscotch table. It was\\npreviously used during for experiments by Herlihy\\net al. [13] and Shun and Blelloch [34]. LeaHash uses\\nhashing with chaining and the implementation that\\nwe use has the same hash set interface as hopscotch.\\nAs previously described, we used hash set imple-\\nmentations for Hopscotch hashing, as well as Lea-\\nHash (they were published like this). They should\\neasily be convertible into common hash map imple-\\nmentations, without loosing too much performance,\\nbut probably using quite a bit more memory.\\n8.1.4 Sequential Variants\\nTo report absolute speedup numbers, we imple-\\nmented sequential variants of growing and fixed size\\ntables. They do not use any atomic instructions\\nor similar slowdowns. They outperform popular\\nchoices like google’s dense hash map significantly\\n(80% increased insert throughput), making them a\\nreasonable approximation for the optimal sequen-\\ntial performance.\\n8.1.5 Color/Marker Choice\\nFor practicality reasons, we chose not to print a\\nlegend with all of our figures. Instead, we use this\\nsection to explain the color and marker choices for\\nour plots (see Section 8.1 and Table 1), hopefully\\nmaking them more readable.\\nSome of the tested hash tables are part of the\\nsame library. In these cases, we use the same\\nmarker, for all hash tables within that library. The\\ndifferent variants of the hash table are then dif-\\nferentiated using the line color (and filling of the\\nmarker).\\nFor our own tables, we mostly use and for\\nuaGrow and usGrow respectively.\\n8.2 Hardware Overview\\nMost of our Experiments were run on a two socket\\nmachine, with Intel Xeon E5-2670 v3 processors\\n(previously codenamed Haswell-EP). Each proces-\\nsor has 12 cores running at 2.3 Ghz base frequency.\\nThe two sockets are connected by two Intel QPI-\\nlinks. Distributed to the two sockets there are\\n128 GB of main memory (64 GB each). The pro-\\ncessors support Intel Hyper-Threading, AVX2, and\\nTSX technologies.\\nThis system runs a Ubuntu distribution with the\\nkernel number 3.13.0-91-generic. We compiled all\\nour tests with gcc 5.2.0 – using optimization level\\n-O3 and the necessary compiler flags (e.g. -mcx16,\\n-msse4.2 among others).\\nAdditionally we executed some experiments on a\\n32-core 4-socket Intel Xeon E5-4640 (SandyBridge-\\nEP) machine, with 512 GB main memory (using\\nthe same operating system and compiler), to verify\\nour findings, and show improved scalability even on\\n4-socket machines.\\n8.3 Test Methodology\\nEach test measures the time it takes, to execute 108\\nhash table operations (strong scaling). Each data\\npoint was computed by taking the average, of five\\nseparate execution times. Different tests use dif-\\nferent hash table operations and key distributions.\\nThe used keys are pre-computed before the bench-\\nmark is started. Each speedup given in this section\\nis computed as the absolute speedup over our hand-\\noptimized sequential hash table.\\nThe work is distributed between threads dynam-\\nically. While there is work to do, threads re-\\nserve blocks of 4096 operations to execute (using an\\natomic counter). This ensures a minimal amount\\nof work imbalance, making the measurements less\\nprone to variance.\\nTwo executions of the same test will always use\\nthe same input keys. Most experiments are per-\\nformed with uniformly random generated keys (us-\\ning the Mersenne twister random number generator\\n[20]). Since real world inputs may have recurring\\nelements, there can be contention which can po-\\ntentially lead to performance issues. To test hash\\n15\\nTable 1: Overview over Table Functionalities.\\nname plot std. interface growing atomic updates deletion arbitrary types\\nxyGrow\\nuaGrow using handles X X X\\nusGrow ” X X X\\npaGrow ” X X X\\npsGrow ” X X X\\nJunction\\nlinear qsbr function X only overwrite X\\ngrampa ” X ” X\\nleapfrog ” X ” X\\nTBB\\nhash map F X X X X X\\nunordered F X X X unsafe X\\nFolly + X const factor X\\nCuckoo X slow X X X\\nRCU\\nurcu × register thread very slow X X X\\nqsbr × qsbr function ” X X X\\nFolklore X X\\nPhase \\x07 sync phases only overwrite X\\nHopscotch N X set interface X\\nLea Hash H X set interface X\\ntable performance under contention, we use Zipf’s\\ndistribution to create skewed key sequences. Using\\nthe Zipf distribution, the probability for any given\\nkey k is P (k) = 1ks·HN,s , where HN,s is the N -th\\ngeneralized harmonic number\\n∑N\\nk=1\\n1\\nks (normaliza-\\ntion factor) and N is the universe size (N = 108).\\nThe exponent s can be altered to regulate the con-\\ntention. We use the Zipf distribution, because it\\nclosely models some real world inputs like natural\\nlanguage, natural size distributions (e.g. of firms\\nor internet pages), and even user behavior ([2], [4],\\n[1]). Notice that key generation is done prior to\\nthe benchmark execution as to not influence the\\nmeasurements unnecessarily (this is especially nec-\\nessary, for skewed inputs).\\nAs a hash function, we use two CRC32C x86 in-\\nstructions with different seeds, to generate the up-\\nper and lower 32 bits of each hash value. Their\\nhardware implementation minimizes the computa-\\ntional overhead.\\n8.4 Experiments\\nThe most basic functionality of each hash table is\\ninserting and finding elements. The performance of\\nmany parallel algorithms depends on the scalability\\nof parallel insertions and finds. Therefore, we begin\\nour experiments with a thorough investigation into\\nthe scalability of these basic hash table operations.\\nInsert Performance We begin with the very ba-\\nsic test of inserting 108 different uniformly random\\nkeys, into a previously empty hash table. For this\\nfirst test, all hash tables have been initialized to\\nthe final size making growing unnecessary. The re-\\nsults presented in Figure 2a show clearly, that the\\nfolklore solution is optimal in this case. Since\\nthere is no migration necessary, and the table can\\nbe initialized large enough, such that long search\\ndistances become very improbable. The large dis-\\ncrepancy between the folklore solution, and all\\nprevious growable hash tables is what motivated\\nus, to work with growable hash tables in the first\\nplace. As shown in the plot, our growing hash table\\nuaGrow looses about 10% of performance over\\nfolklore (9.6× Speedup vs. 8.7×). This perfor-\\nmance loss can be explained with some overheads\\nthat are necessary for eventually growing the table\\n(e.g. estimating the number of elements). All hash\\ntables that have a reasonable performance (> 50%\\n16\\n1 4 8 12 16 24 36 48\\nnumber of threads p\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nabsolute speedup\\n(a) Insert into pre-initialized table\\n1 4 8 12 16 24 36 48\\nnumber of threads p\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nabsolute speedup\\n(b) Insert into growing table\\nFigure 2: Throughput while inserting 108 elements into a previously empty table (Legend see Table 1).\\nof folklore performance), are variants of open ad-\\ndressing (junction leapfrog 4.4 at p = 12, folly +\\n5.1, phase \\x07 8.3) that have similar restrictions on\\nkey and value types. All hash tables that can han-\\ndle generic data types are severely outclassed (F,\\nF, , ×, and ×).\\nAfter this introductory experiment, we take a\\nlook at the growing capability of each table. We\\nagain insert 108 elements into a previously empty\\ntable. This time, the table has only been initial-\\nized, to hold 4092 elements (5 · 107 for all semi\\ngrowing tables). We can clearly see from the plots\\nin Figure 2b, that our hash table variants are signif-\\nicantly faster than any comparable tables. The dif-\\nference becomes especially obvious once two sock-\\nets are used (> 12 cores). With more than one\\nsocket, none of our competitors could achieve any\\nsignificant speedups. On the contrary, many tables\\nbecome slower when executed on more cores. This\\neffect, does not happen for our table.\\nJunction grampa is the only growing hash table\\n– apart from our growing variants – which achieves\\nabsolute speedups higher than 2. Overall, it is still\\nseverely outperformed by our hash table uaGrow\\n(factor 2.5×). Compared to all other tables, we\\nachieve at least seven times the performance (de-\\nscending order; using 48 threads) folly + (7.4×),\\njunction leapfrog (7.7×), tbb hm F (9.6×), tbb\\num F (10.7×), junction linear (22.6×), cuckoo\\n(61.3×), rcu × (63.2×), and rcu with qsbr ×\\n(64.5×).\\nThe speedup in this growing instance is even\\nbetter than the speedup in our non-growing tests.\\nOverall we reach absolute speedups of > 9× com-\\npared to the sequential version (also with growing).\\nThis is slightly better then the absolute speedup in\\nthe non-growing test (≈ 8.5), suggesting that our\\nmigration is at least as scalable as hash table ac-\\ncesses. Overall the insert performance of our im-\\nplementation behaves as one would have hoped. It\\nperforms similar to folklore in the non-growing\\ncase, while performing similarly well in tests where\\ngrowing is necessary.\\nFind Performance When looking for a key in a\\nhash table there are two possible outcomes, either\\nit is in the table or it is not. For most hash tables\\nnot finding an element takes longer than finding\\nsaid element. Therefore, we present two distinct\\nmeasurements for both cases Figure 3a and Fig-\\nure 3b. The measurement for successful finds has\\nbeen made by looking for 108 elements, that have\\npreviously been inserted into a hash table. For the\\nunsuccessful measurement, 108 uniformly random\\nkeys are searched in this same hash table.\\nAll the measurements made for these plots were\\ndone on a preinitialized table (preinitialized before\\ninsertion). This does not make a difference for our\\nimplementation, but it has an influence on some of\\nour competitors. All tables that grow by allocat-\\ning additional tables (namely cuckoo and folly\\n+) have significantly worse find performance on a\\n17\\n1 4 8 12 16 24 36 48\\nnumber of threads p\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\nabsolute speedup\\n(a) Successful finds\\n1 4 8 12 16 24 36 48\\nnumber of threads p\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n800\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\n0\\n3\\n6\\n9\\n12\\n15\\n18\\n21\\n24\\nabsolute speedup\\n(b) Unsuccessful finds\\nFigure 3: Performance and scalability calling 108 unique find operations, on a table, containing 108\\nunique keys (Legend see Table 1).\\ngrown table, as they can have multiple active tables\\nat the same time (all of them have to be checked).\\nObviously, find workloads achieve bigger\\nthroughputs than insert heavy workloads – no\\nmemory is changed and no coordination is neces-\\nsary between processors (i.e. atomic operations).\\nIt is interesting that find operations seem to\\nscale better with multiple processors. Here, our\\ngrowable implementations achieve speedups of 12.8\\ncompared to 9 in the insertion case.\\nWhen comparing the find performance between\\ndifferent tables, we can see that other implemen-\\ntations with open addressing narrow the gap to-\\nwards our implementation. Especially, the hop-\\nscotch hashing N and the phase concurrent ap-\\nproach \\x07 seem to perform well when finding ele-\\nments. Hopscotch hashing N performs especially\\nwell in the unsuccessful case, here it outperforms\\nall other hash tables, by a significant margin. How-\\never, this has to be taken with a grain of salt,\\nbecause the tested implementation only offers the\\nfunctionality of a hash set (contains instead of find).\\nTherefore, less memory is needed per element and\\nmore elements can be hashed into one cache line,\\nmaking lookups significantly more cache efficient.\\nFor our hash tables, the performance reduction\\nbetween successful and unsuccessful finds is around\\n20 to 23 % The difference of absolute speedups be-\\ntween both cases is relatively small – suggesting\\nthat sequential hash tables suffer from the same\\nperformance penalties. The biggest difference has\\nbeen measured for folly + (51 to 55 % reduced per-\\nformance). Later we see that the reason for this\\nis likely that folly + is configured to use only rela-\\ntively little memory (see Figure 10). When initial-\\nized with more memory, its performance gets closer\\nto the performance of other hash tables using open\\naddressing.\\nPerformance under Contention Up to this\\npoint, all data sets we looked at contained uni-\\nformly random keys sampled from the whole key\\nspace. This is not necessarily the case in real world\\ndata sets. For some data sets one keys might ap-\\npear many times. In some sets one key might even\\ndominate the input. Access to this key’s element\\ncan slow down the global progress significantly, es-\\npecially if hash table operations use (fine grained)\\nlocking, to protect hash table accesses.\\nTo benchmark the robustness of the compared\\nhash tables onthese degenerated inputs, we con-\\nstruct the following test setup. Before the execu-\\ntion, we compute a sequence of skewed keys using\\nthe Zipf distribution described in Section 8.3 (108\\nkeys from the range 1..108). Then the table is filled\\nwith all keys from the same range 1..108.\\nFor the first benchmark we execute an update\\noperation for each key of the skewed key se-\\nquence, overwriting its previously stored element\\n(Figure 4a). These update operations will create\\n18\\ncontending write accesses to the hash table. Note\\nthat updates perform simple overwrites, i.e., the re-\\nsulting value of the element is not dependent on the\\nprevious value. The hash table will remain at a con-\\nstant size for the whole execution, making it easy\\nto compare different implementations independent\\nof effects introduced through growing. In the sec-\\nond benchmark, we execute find operations instead\\nof updates, thus creating contending read accesses.\\nFor sequential hash tables, contention on some\\nelements can have very positive effects. When one\\ncell is visited repeatedly, its contents will be cached\\nand future accesses will be faster. The sequential\\nperformance is shown in our figures using a dashed\\nblack line. For concurrent hash tables, contention\\nhas very different effects.\\nUnsurprisingly, the effects experienced from con-\\ntention are different between writing and reading\\noperations. The reason is that multiple threads\\ncan read the same value simultaneously, but only\\none thread at a time can change a value (on cur-\\nrent CPU architecture). Therefore, read accesses\\ncan profit from cache effects – much like a sequen-\\ntial hash table, while write accesses are hindered by\\nthe contention. This goes so far, that for workloads\\nwith high contention no concurrent hash table can\\nachieve the performance of a sequential table.\\nAppart from slowdown because of exclusive write\\naccesses, there is also the additional problem of\\ncache invalidation. When a value is repeatedly\\nchanged by different cores of a multi-socket archi-\\ntecture, then cached copies have to be invalidated\\nwhenever this value is changed. This leads to bad\\ncache efficiency and also to high traffic on QPI\\nLinks (connections between sockets).\\nFrom the update measurement shown in Fig-\\nure 4a it is clearly visible, that the serious impact\\nthrough contention begins between s = 0.85 and\\n0.95. Up until that point contention has a positive\\neffect even on update operations. For a skew be-\\ntween s = 0.85 and 0.95, about 1 % to 3 % of all\\naccesses go to the most common element (key k1).\\nThis is exactly the point where 1/p ≈ P (k1), there-\\nfore, on average there will be one thread changing\\nthe value of k1.\\nIt is noteworthy that the usGrow version of\\nour hash table is more efficient when updating than\\nthe uaGrow version. The reason for this is that\\nusGrow uses 128 bit CAS operations to update el-\\nements while simultaneously making sure, that the\\nmarked bit of the element has not been set before\\nthe change. This can be avoided using the usGrow\\nvariant by specializing the update method to use\\natomic operations on the data part of the element.\\nThis is possible because updates and grow routines\\ncannot overlap in this variant.\\nThe plot in Figure 4b shows that concurrent hash\\ntables achieve performance improvements similar\\nto sequential ones when repeatedly accessing the\\nsame elements. Our hash table can even increase\\nits speedups over uniform access patterns, the high-\\nest speedup of uaGrow is 17.9 at s = 1.25. Since\\nthe speedup is this high, we also included scaled\\nplots showing 5× and 10× the throughput of the se-\\nquential variant. Unfortunately, our growable vari-\\nants cannot improve as much, with contention as\\nthe non-growing folklore and phase concurrent \\x07\\ntables (both 23.2 at s = 1.25). This is probably\\ndue to minor overheads compared to the folklore\\nimplementation which get pronounced since the\\noverall function execution time is reduced.\\nOverall, we see that our folklore implementa-\\ntion which our growable variants are based upon,\\noutperforms all other competitors. Our growable\\nvariant usGrow is consistently close to folklore’s\\nperformance – outperforming all hash tables that\\nhave the ability to grow.\\nAggregation – a common Use Case Hash ta-\\nbles are often used for key aggregation. The idea\\nis that all data elements connected to the same\\nkey are aggregated using a commutative and as-\\nsociative function. For our test, we implemented\\na simple key count program. To implement the\\nkey count routine with a concurrent hash table,\\nan insert-or-increment function is necessary. For\\nsome tables, we were not able to implement an up-\\ndate function, where the resulting value depends\\non the previous value, within the given interface\\n(junction tables, rcu tables, phase concurrent, hop-\\nscotch, and leahash). This was mainly a problem of\\nthe used interfaces, therefore, it could probably be\\nsolved by reimplementing a more functional inter-\\nface. For our table this can easily be achieved with\\nthe insertOrUpdate interface using an increment\\nas update function (see Section 4).\\nThe aggregation benchmark uses the same Zipf\\nkey distribution as the other contention tests. For\\n108 skewed keys, the insert-or-increment function\\n19\\n0.25 0.50 0.75 1.00 1.25 1.50 2.00\\ncontention parameter s\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\n(a) Update (Overwrite)\\n0.25 0.50 0.75 1.00 1.25 1.50 2.00\\ncontention parameter s\\n0k\\n1k\\n2k\\n3k\\n4k\\n5k\\n6k\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\n(b) Successful Finds (1×, 5× and 10× seq.)\\nFigure 4: Throughput executing 108 operations using a varying amount of skew in the key sequence\\n(all keys were previously inserted; using 48 threads; Legend see Table 1). The sequential performance is\\nindicated using dashed lines.\\nis called. Contrary to the previous contention test,\\nthere is no pre-initialization. Therefore, the num-\\nber of distinct elements in the hash table is depen-\\ndent on the contention of the key sequence (given\\nby s). This makes growable hash tables even more\\ndesirable, because the final size can only be guessed\\nbefore the execution.\\nLike in previous tests, we make two distinct mea-\\nsurements. One with growing (Figure 5a) and one\\nwithout (Figure 5b). In the test without growing,\\nwe initialize the table with a size of 108 to ensure\\nthat there is enough room for all keys, even if they\\nare distinct. We excluded the semi-growing tables\\nfrom Figure 5b as approximating the number of\\nunique keys can be difficult. To set the growing per-\\nformance into relation, we show some non-growing\\ntests. Growing actually costs less in the presence\\nof contentious updates, because the resulting table\\nwill be smaller than without contention, therefore,\\nfewer growing steps can be amortized over the same\\nnumber of operations.\\nThe result of this measurement is clearly related\\nto the result of the contentious overwrite test shown\\nin Figure 4a. However, changing a value by incre-\\nment has some slight differences to overwriting it,\\nsince the updated value of an insert-or-increment is\\ndependent on its previous value. In the best case,\\nthis increment can be implemented using an atomic\\nfetch-and-add operation (i.e. usGrow , folklore ,\\nand folly +). However this is not possible for in\\nall hash tables, sometimes dependent updates are\\nimplemented using a read-modify-CAS cycle (i.e.\\nuaGrow ) or fine grained locking (i.e. tbb hash\\nmap F or cuckoo ).\\nUntil s = 0.85, uaGrow seems to be the more\\nefficient option, since it has an increased writing\\nperformance and the update cycle will be success-\\nful most of the time. From that point on, usGrow\\nis clearly more efficient because fetch-and-add be-\\nhaves better under contention. For highly skewed\\nworkloads, it comes really close to the performance\\nof our folklore implementation which again per-\\nforms the best out of all implementations.\\nDeletion Tests As described in Section 5.4, we\\nuse migration, not only to implement an efficiently\\ngrowing hash table, but also to clean up the ta-\\nble after deletions. This way all tombstones are\\nremoved, and thus freed cells are reclaimed. But\\nhow does this fare against different ways of remov-\\ning elements. This is what we investigate with the\\nfollowing benchmark.\\nThe test starts on a prefilled table (107 elements)\\nand consists of 108 insertions – each immediately\\nfollowed by a deletion. Therefore, the table remains\\nat approximately the same size throughout the test\\n(±p elements). All keys used in the test are gener-\\nated before the benchmark execution (uniform dis-\\n20\\n0.25 0.50 0.75 1.00 1.25 1.50 2.00\\ncontention parameter s\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\n400\\n450\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\n(a) Aggregation using a pre-initialized size of 108 (⇒\\nsize = |operations|).\\n0.25 0.50 0.75 1.00 1.25 1.50 2.00\\ncontention parameter s\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\n(b) Aggregation with growing. Dashed plots ( and )\\nindicate non-growing performance.\\nFigure 5: Throughput of an aggregation executing 108 insert-or-increment operations using skewed\\nkey distributions (using 48 threads; Legend see Table 1). The dashed black line indicates sequential\\nperformance. Some tables are not shown because their interface does not support insert-or-increment in\\na convenient way.\\ntribution). As described in Section 8.3, all keys\\nare stored in one array. Each insert uses an entry\\nfrom this array distributed in blocks of 4096 from\\nthe beginning. The corresponding deletion uses the\\nkey that is 107 elements prior to the corresponding\\ninsert. The keys stored within the hash table are\\ncontained in a sliding window of the key array.\\nWe constructed the test to keep a constant ta-\\nble size, because this allows us to test non-growing\\ntables without significantly overestimating the nec-\\nessary capacity. All hash tables are initialized\\nwith 1.5 × 107 capacity, therefore, it is necessary\\nto reclaim deleted cells to successfully execute the\\nbenchmark.\\nThe measurements shown in Figure 6 indicate,\\nthat only the phase concurrent hash table \\x07 by\\nShun and Blelloch [34] can outperform our table.\\nThe reason for this is pretty simple. Their table\\nperforms linear probing comparable to our tech-\\nnique, but it does not use any tombstones for dele-\\ntion. Instead, deleted cells are reclaimed immedi-\\nately (possibly moving elements). This is only pos-\\nsible, because the table does not allow concurrent\\nlookup operations, thus, removing the possibility\\nfor the so called ABA problem (a lookup of an ele-\\nment while it is deleted returns wrong data, if there\\nis also a concurrent insert into the newly freed cell).\\nFrom all remaining hash tables that support fully\\nconcurrent access, ours is clearly the fastest, even\\nthough there are other hash tables like cuckoo\\nand hopscotch N that also get around full table\\nmigrations.\\nMixed Insertions and Finds It can be argued\\nthat some of our tests are just micro-benchmarks\\nwhich are not representative of real world work-\\nloads that often mix insertions with lookups. To\\naddress these concerns, we want to show that mixed\\nfunction workloads (i. e. combined find and insert\\nworkloads) behave similarly.\\nAs in previous tests, we generate a key sequence\\nfor our test. Each key of this sequence is used for\\nan insert or a find operation. Overall, we generate\\n108 keys for our benchmark. For each key, insert\\nor find is chosen at random according to the write\\npercentage wp. In addition to the keys used in the\\nbenchmark, we generate a small number of keys\\n(pre = 8192 ·p = 2 blocks ·p) that are inserted prior\\nto the benchmark. This ensures that the table is\\nnot empty and there are keys that can be found\\nwith lookups.\\nThe keys used for insertions are drawn uniformly\\nfrom the key space. Our goal for find keys is to pre-\\nconstruct the find keys in a way that makes find op-\\nerations successful and is also fair to all data struc-\\ntures. If all find operations were executed to the\\npre-inserted keys then linear probing hash tables\\nwould have an unfair advantage, because elements\\nthat are inserted early have very short probing dis-\\n21\\n12 24 48\\nnumber of threads p\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\nFigure 6: Throughput in a test using deletion.\\nSome tables have been left out of this test, because\\nthey do not support deletion with memory reclama-\\ntion (Legend see Table 1). By alternating between\\ninsertions and deletions, we keep the number of el-\\nements in the table at approximately 107 elements.\\nFor the purpose of computing the throughput, 108\\nsuch alternations are executed, each counting as\\none operation (1 Op = insert + delete).\\ntances, while later elements can take much longer\\nto find. Therefore any find will look for a random\\nkey, that is inserted at least 8192 ·p elements earlier\\nin the key sequence. This key is usually already in\\nthe table when the find operation is called. Look-\\ning for a random inserted element is representative\\nof the overall distribution of probing distances in\\nthe table.\\nNotice that this method does not strictly en-\\nsure that all search keys are already inserted. In\\nour practical tests we found, that the number of\\nkeys which were not found was negligible for per-\\nformance purposes (usually below 1000).\\nComparable to previous tests, we test all hash\\ntables with and without the necessity to grow the\\ntable. In the non-growing test the size of each table\\nis pre-initialized to be c = pre+ (wp · 108). In the\\ngrowing tests semi-growing hash tables are initial-\\nized with half that capacity.\\nSimilar to previous tests it, is obvious that our\\nnon-growing linear probing hash table folklore\\noutperforms most other tables especially on find-\\nheavy workloads. Overall, our hash tables behave\\nsimilar to the sequential solution with a constant\\nspeedup around a factor of 10×. Interestingly, the\\nrunning time does not seem to be a linear function\\n(over wp). Instead, performance decreases super-\\nlinearly. One reason for this could be that for find-\\nheavy workloads, the table remains relatively small\\nfor most of the execution. Therefore, cache ef-\\nfects and similar influences could play a role, since\\nlookups only look for a small sample of elements\\nthat is already in the table.\\nUsing Dedicated Growing Threads In Sec-\\ntion 5.3.2 and 7 we describe the possibility, to use\\na pool of dedicated migration threads which grow\\nthe table cooperatively. Usually the performance of\\nthis method does not differ greatly from the perfor-\\nmance of the enslavement variant used throughout\\nour testing. This can be seen in Figure 8. There-\\nfore, we omitted these variants from most plots.\\nIn Figure 8a one can clearly see the similarities\\nbetween the the variants using a thread pool and\\ntheir counterparts (uaGrow ∼= paGrow and us-\\nGrow ∼= psGrow ). The biggest consistent dif-\\nference we found between the two options has been\\nmeasured during the deletion benchmark in Fig-\\nure 8b. During this benchmark, insert and delete\\nare called alternately. This keeps the actual table\\nsize constant. For our implementation, this means\\nthat there are frequent migrations on a relatively\\nsmall table size. This is difficult when using ad-\\nditional migration threads, since the threads have\\nto be awoken regularly, introducing some operating\\nsystem overhead (scheduling and notification).\\nUsing Intel TSX Technology As described in\\nSection 6, concurrent linear probing hash tables can\\nbe implemented using Intel TSX technology to re-\\nduce the number of atomic operations. Figure 9\\nshows some of the results using this approach.\\nThe implementation used in these tests changes\\nonly the operations within our bounded hash table\\n(folklore) to use TSX-transactions. Atomic fallback\\nimplementations are used, when a transaction fails.\\nWe also instantiated our growing hash table vari-\\nants, to use the TSX-optimized table as underlying\\nhash table implementation.\\nWe tested this variant with a uniform insert\\nworkload (see “Insert Performance”), because the\\nlookup implementation does not actually need a\\ntransaction. We also show the non-TSX variant,\\nusing dashed lines, to indicate the relative perfor-\\nmance benefits.\\nIn Figure 9a one can clearly see that TSX-\\n22\\n0 10 20 30 40 50 60 70 80\\npercentage of insertions wp in [%]\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n800\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\n(a) Mixed insertions and finds on a pre-initialized table\\n(wp · 108 + pre).\\n0 10 20 30 40 50 60 70 80\\npercentage of Insertions wp in [%]\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\n(b) Mixed insertions and finds on a growing table.\\nFigure 7: Executing 108 operations mixed between insertions and finds (using 48 threads; Legend see\\nTable 1). Dashed lines indicate sequential performance (1× and 10×). Find keys are generated in a fair\\nway, that ensures that most find operations are successful.\\noptimized hash tables offer improved performance\\nas long, as growing is not necessary. Unfortunately,\\nFigure 9b paints a different picture for instances\\nwhere growing is necessary. While TSX can be used\\nto improve the usGrow variant of our hash table\\nespecially when using hyperthreading, it offers no\\nperformance benefits in the uaGrow variant. The\\nreason for this is that the running time in these\\nmeasurements is dominated by the table migration\\nwhich is not optimized for TSX-transactions.\\nIn theory, the migration algorithm can make use\\nof transactions similarly to single operations. It\\nwould be interesting whether an optimized migra-\\ntion could further improve the growing instances of\\nthis test. We have not implemented such a migra-\\ntion, as it introduces the need for some complex\\nparameter optimizations – partitioning the migra-\\ntion into smaller blocks or executing each block-\\nmigration into multiple transactions. We estimate\\nthat a well optimized TSX-migration can gain per-\\nformance increases on the order of those witnessed\\nin the non-growing case.\\nMemory Consumption One aspect of paral-\\nlel hash tables, that we did not talk about until\\nnow is memory consumption. Overall, a low mem-\\nory consumption is preferable, but having less cells\\nmeans that there will be more hash collisions. This\\nleads to longer running times especially for non-\\nsuccessful find operations.\\nMost hash tables do not allow the user to set a\\nspecific table size directly. Instead they are initial-\\nized using the expected number of elements. We\\nuse this mechanism to create tables of different\\nsizes. Using these different hash tables with differ-\\nent sizes, we find out how well any one hash table\\nscales when it is given more memory. This is inter-\\nesting for applications where the hash table speed is\\nmore important than its memory footprint (lookups\\nto a small or medium sized hash table within an ap-\\nplication’s inner loop).\\nThe values presented in the follow-\\ning plot are aquired by initializing the\\nhash tables with different table capacities\\n(4096, 0.5×, 1.0×, 1.25×, 1.5×, 2.0×, 2.5×, 3.0×108\\nexpected elements; semi- and non-growing hash\\ntables start at 0.5× and 1× respectively). During\\nthe test, the memory consumption is measured\\nby logging the size of each allocation, and deal-\\nlocation during the execution (done by replacing\\nallocation methods, e.g. malloc and memalign).\\nMeasurements with growing (initial capacity\\n< 108) are marked with dashed lines. Afterwards\\nthe table is filled with 108 elements. The plotted\\nmeasurements show the throughput that can be\\nachieved when doing 108 unsuccessful lookups on\\n23\\n1 4 8 12 16 24 36 48\\nnumber of threads p\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nabsolute speedup\\n(a) Insertions into Growing Table.\\n12 24 48\\nnumber of threads p\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\n(b) Alternating Insertions and Deletions.\\nFigure 8: Comparison between our regular implementation and the variant using a dedicated migration\\nthread pool (dashed lines mark variants enslaving user-threads; Legend see Table 1).\\n1 4 8 12 16 24 36 48\\nnumber of threads p\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\nabsolute speedup\\n(a) Without growing (pre-initialized).\\n1 4 8 12 16 24 36 48\\nnumber of threads p\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nabsolute speedup\\n(b) Insertions with growing.\\nFigure 9: Comparison between our regular implementation and the variant using TSX-transactions in\\nplace of atomics (dashed lines are regular variants without TSX).\\nthe preinitialized table. This throughput is plotted\\nover the amount of allocated memory each hash\\ntable used.\\nThe minimum size for any hash table should be\\naround 1.53 GiB ≈ 108 · (8 B + 8 B) (Key and Value\\neach have 8 B). Our hash table uses a number of\\ncells equal to the smallest power of 2 that is at\\nleast two times as large as the expected number\\nof elements. In this case this means we use 228 ≈\\n2.7 ·108, therefore, the table will be filled to ≈ 37 %\\nand use exactly 4GiB. We believe that this memory\\nusage is reasonable, especially for heavily accessed\\ntables where the performance is important. This is\\nsupported by our measurements as all hash tables\\nthat use less memory have bad performance.\\nMost hash tables round the number of cells in\\nsome convenient way. Therefore, there are of-\\nten multiple measurement points using the same\\namount of memory. As expected, using the same\\namount of memory will usually achieve a compa-\\nrable performance. Out of the tested hash tables\\nonly the folly + hash table grows linearly with the\\nexpected final size. It is also the hash table, that\\ngains the most performance by increasing its mem-\\nory. This makes a lot of sense considering that it\\nuses linear probing and is by default configured to\\nuse more than 50 % of its cells.\\nThe plot also shows that some hash tables do not\\ngain any performance benefits from the increased\\nsize. Most notable for this are cuckoo , all varia-\\ntions of junction and the urcu hash tables ×.\\nThe TBB hash tables F and F seem to use a con-\\n24\\nstant amount of memory, independently from the\\npreinitialized number of elements. This might be\\na measurement error, caused by the fact that they\\nuse different memory allocation methods (not not\\nlogged in our test).\\nThere are also some things that can be learned\\nabout growing hash tables from this plot. Our mi-\\ngration technique ensures, that our hash table has\\nthe exact same size when growing is required as\\nwhen it is preinitialized using the same number\\nof elements. Therefore, lookup operations on the\\ngrown table take the same time as they would on\\na preinitialized table. This is not true, for many of\\nour competitors. All Junction tables and RCU pro-\\nduce smaller tables when growing was used, they\\nalso suffer from a minor slowdown, when using\\nlookups on these smaller tables. Using Folly is even\\nworse, it produces a bigger table – when growing is\\nneeded – and still suffers from significantly worse\\nperformance.\\nScalability on a 4-Socket Machine Bad per-\\nformance on multi-socket workloads is recurring\\ntheme throughout our testing. This is especially\\ntrue for some of our competitors where 2-Socket\\nrunning times are often worse than 1-Socket run-\\nning times. To further expand the understanding\\nof this problem we made some tests on the 4-Socket\\nmachine described in Section 8.2.\\nThe used test instances are generated similar\\nto the insert/find tests described in the beginning\\nof this section (108 executed operations with uni-\\nformly random keys). The results can be seen in\\nFigure 11a (Insertions) and Figure 11b (unsuccess-\\nful finds).\\nOur competitor’s hash tables seem to be a lot\\nmore effective when using only one of the four sock-\\nets (compared to one of two sockets on the two-\\nsocket machine). This is especially true for the\\nLookup workload where the junction hash tables\\nstart out more efficient than our implementation.\\nHowever this effect seems to invert once multiple\\nsockets are used.\\nIn the test using lookups, there seems to be a per-\\nformance problem using our hash table. It seems to\\nscale sub-optimally on one socket. On two sockets\\nhowever, the hash table seems to scale significantly\\nbetter.\\nOverall the four-socket machine reconfirms our\\nobservations. None of our competitors scale well\\nwhen a growing hash table is used over multiple\\nsockets. On the contrary, using multiple sockets\\nwill generally reduce the throughput. This is not\\nthe case for our hash table. The efficiency is re-\\nduced when using more then two sockets but the\\nabsolute throughput at least remains stable.\\n8.5 The Price of Generality\\nHaving looked at many detailed measurements, let\\nus now try to get a bigger picture by asking which\\nhash table performs well for specific requirements\\nand how much performance has to be sacrificed for\\nadditional flexibility. This will give us an intuition,\\nwhere performance is sacrificed on our way to a\\nfully general hash table. Seeing that all tested hash\\ntables fail to scale linearly on multi-socket machines\\nwe try to answer the question if concurrent hash\\ntables are worth their overhead at all.\\nAt the most restricted level – no grow-\\ning/deletions and word sized key and value types\\n– we have shown that common linear probing hash\\ntables offer the best performance (over a number of\\noperations). Our implementation of this “folklore”\\nsolution outperforms different approaches consis-\\ntently, and performs at least as good as other simi-\\nlar implementations (i.e. the phase concurrent ap-\\nproach). We also showed, that this performance\\ncan be improved by using Intel TSX technology.\\nFurthermore, we have shown that our approach to\\ngrowing hash tables does not affect the performance\\non known input sizes significantly (preinitialized ta-\\nble to the correct size).\\nSticking to fixed data types but allowing dynamic\\ngrowing, the best data structures are our growing\\nvariants ({ua,us,pa,ps}Grow). The difference in\\nour measurements between pool growing (pxGrow)\\nand the corresponding variants with enslavement\\n(uxGrow) are not very big. Growing with marking\\nperforms better than globally synchronized grow-\\ning except for update heavy workloads. The price\\nof growing compared to a fixed size is less than\\na factor of two for insertions and updates (aggre-\\ngation) and negligible for find-operations. More-\\nover, this slowdown is comparable to the slowdown\\nexperienced in sequential hash tables when grow-\\ning is necessary. None of the other data structures\\nthat support growing comes even close to our data\\nstructures. For insertions and updates we are an or-\\n25\\n0 2 4 6 8 10 12 14 16 18 20 22 24 26 28\\nsize in GiB\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\n(a) Performance of unsuccessful\\nfind operations over the size of\\nthe data structure.\\nFigure 10: For these tests 108 keys are searched (unsuccessfully) on a hash table containing 108 elements.\\nPrior to the setup of the benchmark, the tables were initialized with different sizes (there can be many\\npoints on one (x-)coordinate) (Legend see Table 1).\\n1 4 8 16 24 32 64\\nnumber of threads p\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nabsolute speedup\\n(a) Insertions into a growing hash table.\\n1 4 8 16 24 32 64\\nnumber of threads p\\n0\\n50\\n100\\n150\\n200\\n250\\nth\\nro\\nug\\nhp\\nut\\n in\\n [M\\nO\\nps\\n/s\\n]\\n0\\n3\\n6\\n9\\n12\\n15\\n18\\n21\\nabsolute speedup\\n(b) Unsuccessful Lookups into the filled table.\\nFigure 11: Basic tests made on our 4-Socket machine, consisting of four eight core E5-4640 processors\\nwith 2.4 GHz each (codenamed Sandybridge) and 512 GB main memory (Legend see Table 1).\\nder of magnitude faster then many of our competi-\\ntors. Furthermore, only one competitor achieves\\nspeedups above one when inserting into a growing\\ntable (junction grampa).\\nAmong the tested hash tables, only TBB,\\nCuckoo, and RCU have the ability to store arbi-\\ntrary key-/value-type combinations. Therefore, us-\\ning arbitrary data objects with one of these hash\\ntables can be considered to cost at least an order\\nof magnitude in performance (TBB[arbitrary] ≤\\nTBB[word sized] ≈ 1/10 · xyGrow). In our opin-\\nion, this restricts the use of these data structures to\\nsituations where hash table accesses are not a com-\\nputational bottleneck. For more demanding appli-\\ncations the only way to go is to get rid of the general\\ndata types or the need for concurrent hash tables\\naltogether. We believe that the generalizations we\\nhave outlined in Section 5.7 will be able to close\\nthis gap. Actual implementations and experiments\\nare therefore interesting future work.\\nFinally let us consider the situation where we\\nneed general data types but no growing. Again, all\\nthe competitors are an order of magnitude slower\\nfor insertion than our bounded hash tables. The\\nsingle exception is cuckoo, which is only five times\\nslower for insertion and six times slower for suc-\\ncessful reads. However, it severely suffers from con-\\ntention being an almost record breaking factor of\\n26\\n5 600 slower under find-operations with contention.\\nAgain, it seems that better data structures should\\nbe possible.\\n9 Conclusion\\nWe demonstrate that a bounded linear probing\\nhash table specialized to pairs of machine words\\nhas much higher performance than currently avail-\\nable general purpose hash tables like Intel TBB,\\nLeahash, or RCU based implementations. This\\nis not surprising from a qualitative point of view\\ngiven previous publications [36, 14, 34]. However,\\nwe found it surprising how big the differences can\\nbe in particular in the presence of contention. For\\nexample, the simple decision to require a lock for\\nreading can decrease performance by almost four\\norders of magnitude.\\nPerhaps our main contribution is to show that\\nintegrating an adaptive growing mechanism into\\nthat data structure has only a moderate perfor-\\nmance penalty. Furthermore, the used migration\\nalgorithm can also be used to implement deletions\\nin a way that reclaims freed memory. We also ex-\\nplain how to further generalize the data structure\\nto allowing more general data types.\\nThe next logical steps are to implement these\\nfurther generalizations efficiently and to integrate\\nthem into an easy to use library that hides most of\\nthe variants from the user, e.g., using programming\\ntechniques like partial template specialization.\\nFurther directions of research could be to look\\nfor a practical growable lock-free hash table.\\nAcknowledgments We would like to thank\\nMarkus Armbruster, Ingo Mu¨ller, and Julian Shun\\nfor fruitful discussions.\\nReferences\\n[1] Lada A. Adamic and Bernardo A. Huberman.\\nZipf’s law and the internet. Glottometrics,\\n3(1):143–150, 2002.\\n[2] Robert L. Axtell. Zipf distribution of us firm\\nsizes. Science, 293(5536):1818–1820, 2001.\\n[3] Holger Bast, Stefan Funke, Domagoj Matije-\\nvic, Peter Sanders, and Dominink Schultes. In\\ntransit to constant time shortest-path queries\\nin road networks. In Proceedings of the Meet-\\ning on Algorithm Engineering & Expermi-\\nments (ALENEX), pages 46–59, 2007.\\n[4] Lee Breslau, Pei Cao, Li Fan, Graham Phillips,\\nand Scott Shenker. Web caching and zipf-\\nlike distributions: evidence and implications.\\nIn INFOCOM ’99. Eighteenth Annual Joint\\nConference of the IEEE Computer and Com-\\nmunications Societies. Proceedings. IEEE, vol-\\nume 1, pages 126–134 vol.1, Mar 1999.\\n[5] Shimin Chen, Anastassia Ailamaki, Phillip B\\nGibbons, and Todd C Mowry. Improving hash\\njoin performance through prefetching. ACM\\nTransactions on Database Systems (TODS),\\n32(3):17, 2007.\\n[6] Roman Dementiev, Lutz Kettner, Jens Mehn-\\nert, and Peter Sanders. Engineering a sorted\\nlist data structure for 32 bit keys. In 6th\\nWorkshop on Algorithm Engineering & Exper-\\niments, pages 142–151, New Orleans, 2004.\\n[7] Martin Dietzfelbinger, Torben Hagerup, Jyrki\\nKatajainen, and Martti Penttonen. A reli-\\nable randomized algorithm for the closest-pair\\nproblem. Journal of Algorithms, 25(1):19–51,\\n1997.\\n[8] Martin Dietzfelbinger and Christoph Wei-\\ndling. Balanced allocation and dictionar-\\nies with tightly packed constant size bins.\\nTheoretical Computer Science, 380(1–2):47–\\n68, 2007.\\n[9] Facebook. folly version 57:0. https://\\ngithub.com/facebook/folly, 2016.\\n[10] Hui Gao, Jan Friso Groote, and Wim H.\\nHesselink. Lock-free dynamic hash tables\\nwith open addressing. Distributed Computing,\\n18(1), 2005.\\n[11] Torben Hagerup and Christine Ru¨b. Optimal\\nmerging and sorting on the EREW-PRAM.\\nInformation Processing Letters, 33:181–185,\\n1989.\\n[12] Maurice Herlihy and Nir Shavit. The Art of\\nMultiprocessor Programming, Revised Reprint.\\nElsevier, 2012.\\n27\\n[13] Maurice Herlihy, Nir Shavit, and Moran\\nTzafrir. Hopscotch hashing. In Distributed\\nComputing, pages 350–364. Springer, 2008.\\n[14] Euihyeok Kim and Min-Soo Kim. Performance\\nanalysis of cache-conscious hashing techniques\\nfor multi-core CPUs. International Journal of\\nControl & Automation (IJCA), 6(2), 2013.\\n[15] Donald E. Knuth. The Art of Computer\\nProgramming—Sorting and Searching, vol-\\nume 3. Addison Wesley, 2nd edition, 1998.\\n[16] Doug Lea. Hash table util. concurrent.\\nconcurrenthashmap, revision 1.3. JSR-166,\\nthe proposed Java Concurrency Package.\\nhttp://gee. cs. oswego. edu/cgi-bin/viewcvs.\\ncgi/jsr166/src/main/java/util/concurrent,\\n2003.\\n[17] Xiaozhou Li, David G. Andersen, Michael\\nKaminsky, and Michael J. Freedman. Al-\\ngorithmic improvements for fast concurrent\\ncuckoo hashing. In Proceedings of the Ninth\\nEuropean Conference on Computer Systems,\\nEuroSys ’14. ACM, 2014.\\n[18] Tobias Maier, Peter Sanders, and Roman\\nDementiev. Concurrent hash tables: Fast\\nand general?(!). In Proceedings of the 21st\\nACM SIGPLAN Symposium on Principles\\nand Practice of Parallel Programming, PPoPP\\n’16, pages 34:1–34:2, New York, NY, USA,\\n2016. ACM.\\n[19] Tobias Maier, Peter Sanders, and Roman De-\\nmentiev. Concurrent hash tables: Fast and\\ngeneral?(!). CoRR, abs/1601.04017, 2016.\\n[20] Makoto Matsumoto and Takuji Nishimura.\\nMersenne twister: A 623-dimensionally\\nequidistributed uniform pseudo-random\\nnumber generator. ACMTMCS: ACM\\nTransactions on Modeling and Computer\\nSimulation, 8:3–30, 1998. http://www.math.\\nkeio.ac.jp/~matumoto/emt.html.\\n[21] Edward M. McCreight. A space-economical\\nsuffix tree construction algorithm. Journal of\\nthe ACM, 23(2):262–272, April 1976.\\n[22] Paul E. McKenney and John D. Slingwine.\\nRead-copy update: Using execution history to\\nsolve concurrency problems. Parallel and Dis-\\ntributed Computing and Systems, pages 509–\\n518, 1998.\\n[23] Kurt Mehlhorn and Peter Sanders. Algorithms\\nand Data Structures — The Basic Toolbox.\\nSpringer, 2008.\\n[24] Scott Meyers. Effective C++: 55 specific ways\\nto improve your programs and designs. Pear-\\nson Education, 2005.\\n[25] Ingo Mu¨ller, Peter Sanders, Arnaud Lacurie,\\nWolfgang Lehner, and Franz Fa¨rber. Cache-\\nefficient aggregation: Hashing is sorting. In\\nProceedings of the 2015 ACM SIGMOD Inter-\\nnational Conference on Management of Data,\\npages 1123–1136. ACM, 2015.\\n[26] Rajesh Nishtala, Hans Fugal, Steven Grimm,\\nMarc Kwiatkowski, Herman Lee, Harry C Li,\\nRyan McElroy, Mike Paleczny, Daniel Peek,\\nPaul Saab, et al. Scaling memcache at face-\\nbook. In 10th USENIX Symposium on Net-\\nworked Systems Design and Implementation\\n(NSDI), volume 13, pages 385–398, 2013.\\n[27] Philippe Oechslin. Making a faster cryptana-\\nlytic time-memory trade-off. In Dan Boneh,\\neditor, Advances in Cryptology - CRYPTO\\n2003: 23rd Annual International Cryptology\\nConference, pages 617–630, Berlin, Heidel-\\nberg, 2003. Springer Berlin Heidelberg.\\n[28] Jong Soo Park, Ming-Syan Chen, and Philip S.\\nYu. An effective hash-based algorithm for min-\\ning association rules. In ACM SIGMOD Con-\\nference on Management of Data, pages 175–\\n186, 1995.\\n[29] Mathieu Desnoyers Paul E. McKenney and Lai\\nJiangshan. LWN: URCU-protected hash ta-\\nbles. http://lwn.net/Articles/573431/, 2013.\\n[30] Chuck Pheatt. Intel R©; Threading Building\\nBlocks. J. Comput. Sci. Coll., 23(4):298–298,\\nApril 2008.\\n[31] Jeff Preshing. Junction. https://github.\\ncom/preshing/junction, 2016.\\n[32] Jeff Preshing. New concurrent hash maps\\nfor c++. http://preshing.com/20160201/\\n28\\nnew-concurrent-hash-maps-for-cpp/,\\n2016.\\n[33] Ori Shalev and Nir Shavit. Split-ordered lists:\\nLock-free extensible hash tables. J. ACM,\\n53(3):379–405, May 2006.\\n[34] Julian Shun and Guy E. Blelloch. Phase-\\nconcurrent hash tables for determinism. In\\n26th ACM Symposium on Parallelism in Al-\\ngorithms and Architectures (SPAA), pages 96–\\n107. ACM, 2014.\\n[35] Julian Shun, Guy E. Blelloch, Jeremy T.\\nFineman, Phillip B. Gibbons, Aapo Kyrola,\\nHarsha Vardhan Simhadri, and Kanat Tang-\\nwongsan. Brief announcement: the problem\\nbased benchmark suite. In 24th ACM Sympo-\\nsium on Parallelism in Algorithms and Archi-\\ntectures (SPAA), pages 68–70. ACM, 2012.\\n[36] Alex Stivala, Peter J. Stuckey, Maria Garcia\\nde la Banda, Manuel Hermenegildo, and An-\\nthony Wirth. Lock-free parallel dynamic pro-\\ngramming. Journal of Parallel and Distributed\\nComputing, 70(8), 2010.\\n[37] Tony Stornetta and Forrest Brewer. Imple-\\nmentation of an efficient parallel bdd package.\\nIn 33rd Design Automation Conference, pages\\n641–644. ACM, 1996.\\n29\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd16'), 'authors': 'Axelsen H. B., Frank M. P., Koomey J. G., Li M., Maroney O. J. E., Vieri C., Vieri C. J., Yates S.', 'year': '2016', 'title': 'Energy-Efficient Algorithms', 'full_text': 'Energy-Efficient Algorithms\\nErik D. Demaine∗ Jayson Lynch∗ Geronimo J. Mirano∗ Nirvan Tyagi∗\\nMay 30, 2016\\nAbstract\\nWe initiate the systematic study of the energy complexity of algorithms (in addition to time\\nand space complexity) based on Landauer’s Principle in physics, which gives a lower bound on\\nthe amount of energy a system must dissipate if it destroys information. We propose energy-\\naware variations of three standard models of computation: circuit RAM, word RAM, and trans-\\ndichotomous RAM. On top of these models, we build familiar high-level primitives such as\\ncontrol logic, memory allocation, and garbage collection with zero energy complexity and only\\nconstant-factor overheads in space and time complexity, enabling simple expression of energy-\\nefficient algorithms. We analyze several classic algorithms in our models and develop low-energy\\nvariations: comparison sort, insertion sort, counting sort, breadth-first search, Bellman-Ford,\\nFloyd-Warshall, matrix all-pairs shortest paths, AVL trees, binary heaps, and dynamic arrays.\\nWe explore the time/space/energy trade-off and develop several general techniques for analyzing\\nalgorithms and reducing their energy complexity. These results lay a theoretical foundation for\\na new field of semi-reversible computing and provide a new framework for the investigation of\\nalgorithms.\\nKeywords: Reversible Computing, Landauer’s Principle, Algorithms, Models of Computation\\n∗MIT Computer Science and Artificial Intelligence Laboratory, 32 Vassar Street, Cambridge, MA 02139, USA,\\n{edemaine,jaysonl,geronm,ntyagi}@mit.edu. Supported in part by the MIT Energy Initiative and by MADALGO —\\nCenter for Massive Data Algorithmics — a Center of the Danish National Research Foundation.\\nar\\nX\\niv\\n:1\\n60\\n5.\\n08\\n44\\n8v\\n1 \\n [c\\ns.D\\nS]\\n  2\\n6 M\\nay\\n 20\\n16\\nContents\\n1 Introduction 1\\n2 Energy Models 4\\n2.1 Energy Circuit Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 Energy Word RAM Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2.1 Irreversible Word Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.2.2 Reversible Word Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.3 Energy Transdichotomous RAM Model . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.4 High-level Pseudocode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.4.1 Logging and Unrolling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.4.2 Promise Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n3 Reversible Primitives 11\\n3.1 Control Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n3.1.1 Jumps and Branches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n3.1.2 Conditional Statements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.1.3 For Loops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n3.1.4 Function calls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n3.2 Memory Management and Garbage Collection . . . . . . . . . . . . . . . . . . . . . . 18\\n4 Energy Reduction Techniques 20\\n4.1 Complete Logging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n4.2 Reversible Subroutine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n4.3 Pointer Swapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n4.4 Data Structure Rebuilding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n5 Data Structures 22\\n5.1 Stacks and Linked Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n5.2 Dynamic Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n5.3 AVL Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n5.4 Binary Heaps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n6 Algorithms 27\\n6.1 Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n6.1.1 Comparison Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n6.1.2 Counting Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n6.2 Graph Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n6.3 Bellman-Ford . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n6.4 Floyd-Warshall . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n6.5 All Pairs Shortest Path via (min,+) Matrix Multiplication . . . . . . . . . . . . . . . 34\\n7 Future Directions 35\\n0\\n1 Introduction\\nLandauer limit. CPU power efficiency (number of computations per kilowatt hour of energy) has\\ndoubled every 1.57 years from 1946 to 2009 [KBSW11]. Within the next 15–60 years, however, this\\ntrend will hit a fundamental limit in physics, known as Landauer’s Principle [Lan61]. This principle\\nstates that discarding one bit of information (increasing the entropy of the environment by one bit)\\nrequires kT ln 2 energy, where k is Boltzmann’s constant and T is ambient temperature, which is\\nabout 2.8 · 10−21 joules or 7.8 · 10−28 kilowatt hours at room temperature (20◦C). (Even at liquid\\nnitrogen temperatures, this requirement goes down by less than a factor of 5.) Physics has proved\\nthis principle under a variety of different assumptions [Lan61,Pie00,LPSG07,Mar09,LCB11], and\\nit has also been observed experimentally [BAP+12].\\nMost CPUs discard many bits of information per clock cycle, as much as one per gate; for\\nexample, an AND gate with output 0 or an OR gate with output 1 “forgets” the exact values of its\\ninputs. To see how this relates to Landauer’s principle, consider the state-of-the-art 15-core Intel\\nXeon E7-4890 v2 2.8GHz CPU. In a 4-processor configuration, it achieves 1.2 · 1012 computations\\nper second at 620 watts,1 for a ratio of 7.4 ·1015 computations per kilowatt hour. At the pessimistic\\nextreme, if every one of the 4.3 · 109 transistors discards a bit, then the product 3.2 · 1025 is only\\nthree orders of magnitude greater than Landauer limit. If CPUs continue to double in energy\\nefficiency every 1.57 years, this gap will close in less than 18 years. At the more optimistic extreme,\\nif a 64-bit computation discards only 64 bits (to overwrite one register), the gap will close within\\n59 years. The truth is probably somewhere in between these extremes.\\nReversible computing. The only way to circumvent the Landauer limit is to do logically re-\\nversible computations, whose inputs can be reconstructed from their outputs, using physically\\nadiabatic circuits. According to current knowledge, such computations have no classical funda-\\nmental limitations on energy consumption. General-purpose CPUs with adiabatic circuits were\\nconstructed by Frank and Knight at MIT [Fra99]. The design of reversible computers is still being\\nactively studied, with papers on designs for adders [NTR11], multipliers [NTR10], ALUs [MR11],\\nclocks [SAI+13], and processors [TAG12] being published within the last five years. AMD’s CPUs\\nsince Oct. 2012 (Piledriver) use “resonant clock mesh technology” (essentially, an adiabatic clock\\ncircuit) to reduce overall energy consumption by 24% [Cyc12]. Thus the ideas from reversible\\ncomputing are already creating energy savings today.\\nBut what can be done by reversible computation? Reversible computation is an old idea, with\\nreversible Turing machines being proved universal by Lecerf in 1963 [Lec63] and ten years later by\\nBennett [Ben73]. Early complexity results showed that any computation can be made reversible,\\nbut with a quadratic space overhead [Ben89] or an exponential time overhead [LMT97, Wil00], in\\nparticular models of computation. More recent results give a trade-off with subquadratic space\\nand subexponential time [BTV01]. These general transforms are too expensive; in particular, in a\\nbounded-space system, consuming extra space to make computations reversible is just delaying the\\ninevitable destruction of bits.\\nThe relationship between thermodynamics and information theory is described by Zurek [Zur89].\\nIn a series of papers, Li, Tromp, and Vitanyi discuss irreversible operations as a useful metric for\\nenergy dissipation in computers and study the trade-off between time, space, and irreversible op-\\nerations. An energy cost based on Kolmogorov complexity [LV08], a precise but uncomputable\\nmeasure of the information content of a string, is introduced in [LV92] and further explored\\n1We follow Koomey et al.’s [KBSW11] definitions, using Cisco’s measured SPECint rate base2006 of 2,320 to\\nestimate millions of computations per second (MCPS).\\n1\\nin [BGL+98, Vit05, LV96]. These papers study algorithms for Bennett’s pebble game as well as\\nsimulating Turing machines; however, they still focus on universal results, eshrew RAM models,\\nand analyze problems more from a complexity than an algorithms perspective.\\nIrreversibility is just one source of energy consumption in current chips, and several other models\\nof computation attempt to capture them individually: switching energy of VLSI circuits [Kis91],\\ndynamic and leakage power loss in CMOS circuits [KA10, KAG11], and I/O or memory access\\n[JMR05]. Albers [Alb10] surveys many algorithmic techniques for reducing energy consumption\\nof current computers, including techniques like sleep states and power-down mechanisms, dynamic\\nspeed scaling, temperature management, and energy-minimizing scheduling. Ultimately, however,\\nwe believe that irreversibility will become a critical energy cost shaping the future of computing,\\nand a topic now ripe for algorithmic analysis.\\nOur results. This paper is the first to perform a thorough algorithmic study of partially reversible\\ncomputing, and to analyze realistic time/space/energy trade-offs. We define the (Landauer/irre-\\nversibility) energy cost, and use it to explore reversible computing in a novel manner. Although\\nthere are many other sources of energy inefficiency in a computer we believe the Landauer energy\\ncost is a fundamental and useful measure. A key perspective shift from most of the reversible\\ncomputing literature (except [LV96]) is that we allow algorithms to destroy bits, and measure\\nthe number of destroyed bits as the energy cost. This approach enables the unification of classic\\ntime/space measures with a new energy measure. In particular, it enables us to require algorithms\\nto properly clean up all additional space by the end of their execution, and data structures to be\\nproperly charged for their total space allocation.\\nWe introduce three basic models for analyzing the energy cost of word-level operations, similar\\nto the standard word models used in most algorithms today: the word RAM, the more general\\ntransdichotomous RAM, and the realistically grounded circuit RAM. Our models allow arbitrary\\ncomputation to be performed, but define a spectrum of “irreversibility”, from reversible (free)\\ncomputation to completely destructive (expensive) computation. On top of these basic models\\n(akin to assembly language), we build a high-level pseudocode for easy algorithm specification, by\\nshowing how to implement many of the familiar high-level programming structures, as well as some\\nnew structures, with zero energy overhead and only constant-factor overheads in time and space:\\n1. Control logic: If/then/else, for/while loops, jumps, function calls, stack-allocated variables.\\n2. Memory allocation: Dynamic allocation and deallocation of fixed-size or variable-size\\nblocks, in particular implementing pointer-machine algorithms.\\n3. Garbage collection: Reference-counting and mark-and-sweep algorithms for finding no-\\nlonger-used memory blocks for automatic deallocation.\\n4. Logging and unrolling: Specific to energy-efficient computation, we describe a new programming-\\nlanguage feature that makes it easy to turn energy into space overhead, and later remove that\\nspace overhead by playing it backwards.\\nThese models open up an entire research field, which we call energy-efficient algorithms, to find\\nthe minimum energy required to solve a desired computational problem within given time and space\\nbounds. We launch this field with several initial results about classic algorithmic problems, first\\nanalyzing the energy cost of existing algorithms, and then modifying or designing new algorithms to\\nreduce the energy cost without significantly increasing the time and space costs. Table 2 summarizes\\nthese results.\\nAlthough there are many practical papers about minimizing energy in computation (favoring\\ninstructions that use somewhat less energy than others), the algorithms community has not made\\nit a standard measure to complement time and space because, without the idea of reversibility,\\n2\\nPrimitive Time Space in Log Energy Thm.\\n(ops) (bits) (bits)\\nControl Logic\\nPaired Jump Θ(1) 1 0 3.1\\nVariable Jump Θ(1) 1 + w 0 3.1\\nProtected If Θ(1) 0 0 3.2\\nGeneral If Θ(1) 1 0 3.2\\nSimple For loop Θ(l) 0 0 3.3\\nProtected For loop Θ(l) 0 0 3.4\\nGeneral For loop Θ(l) lg l 0 3.5\\nFunction call Θ(1) 0 0 3.6\\nMemory Management\\nFree lists Θ(N) Θ(wN) 0 3.7\\nReference Counting Θ(N) Θ(wN) 0 3.8\\nMark & Sweep Θ(N) Θ(wN) 0 3.9\\nTable 1: Summary of our reversible primitives analyses and results including control logic, memory\\nmanagement, and garbage collection. In this table, w is the word size, l is the number of loop\\niteration, and N represents number of memory objects.\\nenergy is simply within a constant-factor of time. By contrast, in our model, the energy cost can\\nbe anywhere between 0 (for reversible computation) and t ·w where t is the running time (number\\nof word operations) and w is the number of bits in a word.\\nConsequences. Reducing the energy consumption of many computations by several orders of\\nmagnitude (n) will have tremendous impact on practice. Computer servers alone constitute 23–\\n31 gigawatts of power consumption, which translates to $14–18 billion annually and 1.1–1.5% of\\nworldwide electricity use [Koo11]; there are roughly 50 times as many PCs with an annual growth\\nrate of 12% [YDG+07]; and there are about as many smartphones as PCs [eMa14]. Improved\\nenergy efficiency would save both environmental impact and money. Reducing energy consumption\\nwould also improve the longevity of batteries in portable devices (laptops, phones, watches, etc.), or\\nenable the use of smaller and lighter batteries for similar performance. Perhaps most interestingly,\\nlower energy consumption would lead to faster CPUs, as cooling is the main bottleneck in increasing\\nclock speeds; reducing the energy consumption by a factor of α, we expect to be able to run the\\nCPU roughly α times faster. For example, the world record for CPU clock speed of 8.429 GHz was\\nset by AMD with liquid nitrogen cooling [Vel11].\\nOur approach is ambitious in that it requires rethinking both software (algorithms) and hard-\\nware. Our belief is that building a rich algorithmic theory for (partially) reversible computation,\\nand showing the orders of magnitude in possible energy reduction for important problems, will\\nprove to hardware makers that reversibility is a lucrative feature worth exploring intensely, even\\nbefore it becomes inevitable by hitting the Landauer Limit.\\nGuide. This paper has several sections and does not necessarily need to be read in order or in full,\\ndepending on the reader’s interest. We recommend reading Sections 2.2 and 2.4 before continuing\\nonto later parts of the paper, to set up the model which is used extensively in the rest of the paper.\\nThe remainder of Section 2 further explores our energy models and useful variations. The remaining\\nsections of the paper can be read in any preferred order. Parts of Sections 3–6 use results from\\n3\\nAlgorithm Time Space (words) Energy (bits) Thm.\\nGraph Algorithms\\nBreadth-first Search Θ(V + E) Θ(V + E) Θ(wV + E) 6.9\\nReversible BFS [Fra99] Θ(V + E) Θ(V + E) 0 6.10\\nBellman-Ford Θ(V E) Θ(V ) Θ(V Ew) 6.12\\nReversible Bellman-Ford Θ(V E) Θ(V E) 0 6.13\\nFloyd-Warshall Θ(V 3) Θ(V 2) Θ(wV 3) 6.14\\nReversible Floyd-Warshall [Fra99] Θ(V 3) Θ(V 3) 0 6.15\\nMatrix APSP Θ(V 3 lg V ) Θ(V 2) Θ(wV 3 lg V ) 6.17\\nReversible Matrix APSP [Fra99] Θ(V 3 lg V ) Θ(V 2 lg V ) 0 6.16\\nSemi-reversible Matrix APSP Θ(V 3 lg V ) Θ(V 2) Θ(wV 2 lg V ) 6.16\\nData Structures\\nStandard AVL Trees (build) O(n lg n) O(n) O(w · n lg n)\\n(search) O(lg n) O(1) O(lg n) 5.4\\n(insert) O(lg n) O(1) O(w lg n) 5.5\\n(k deletes) O(k lg n) O(1) O(w lg n) 5.6\\nReversible AVL Trees (build) O(n lg n) O(n) 0\\n(search) O(lg n) O(1) 0 5.7\\n(insert) O(lg n) O(1) 0 5.8\\n(k deletes) O(k lg n) O(k) 0 5.9\\nStandard Binary Heap (insert) O(lg n) O(1) O(lg n) 5.10\\n(delete max) O(lg n) O(lg n) O(w lg n) 5.11\\nReversible Binary Heap (insert) O(lg n) O(1) 0 5.10\\n(delete max) O(lg n) O(lg n) 0 5.12\\nDynamic Array (build) O(n) O(n) 0\\n(query) O(1) O(1) 0 5.3\\n(add) O(1) O(1) 0 5.3\\n(delete) O(1) O(1) 0 5.3\\nTable 2: Summary of our algorithmic analyses and results. In this table, n is the problem size or\\nnumber of elements in the data-structure, w is the word size, lg is log2, and in graph algorithms,\\nV is the number of vertices, and E is the number of edges.\\nprevious sections, but these should remain understandable without having seen the prior proofs.\\nSection 3 constructs and analyzes basic control logic and memory management, to enable high-\\nlevel pseudocode for algorithm specification. Section 4 provides some general techniques we have\\ndeveloped for constructing (semi-)reversible algorithms. Sections 5 and 6 analyze several classic\\nalgorithms and data structures, and construct new algorithms and data structures that are more\\nenergy efficient. Section 7 poses open problems.\\n2 Energy Models\\nIn the following sections we present three different models of computation which define an energy\\ncomplexity that attempts to capture the energy loss from Landauer’s Principle. We begin with\\na circuit model due to its intuitiveness and similarity to early work done on reversible logic and\\ncomputation. We then build up RAM models which bear far more similarity to those used for the\\nanalysis of algorithms.\\n4\\n2.1 Energy Circuit Model\\nAt the lowest level we will consider logical gates. Every gate is a Boolean function g : x→ y. The\\nenergy cost of a gate is defined as the log of the size ratio of the input space, X, to the output\\nspace, Y = g(X). Thus, energy E = lg\\n(\\nX\\nY\\n)\\n, whose units are bits. The energy cost cannot be\\nnegative because a given input cannot map to more than one output. Here we forbid randomized\\ncomputation. Alternatively, one could allow the creation of b random bits at an energy cost of b.\\nAlso, the energy cost is zero exactly when the function is bijective in which case we call the gate\\nreversible. A circuit is a directed acyclic graph of gates with input nodes of in-degree zero whose\\noutputs are the bits corresponding to the input for a problem and whose output is the collection of\\noutputs from the gates which do not go into another gate. The energy cost of a circuit is defined\\nto be the sum of the energy costs of every gate. If the energy cost of a circuit is zero we call it\\nreversible.\\nLemma 2.1. A reversible circuit must compute a bijective function.\\nProof. For a circuit to be reversible, all of its gates must be reversible, and thus bijective. The\\ncomposition of bijective functions is also bijective.\\nWe now have a framework for understanding the energy complexity of circuits. Although we\\nimagine there are very interesting things that can be said about the energy complexity of circuits,\\nwe primarily want this infrastructure to help justify pieces of further models. For that purpose,\\ngeneral circuits are too powerful and we wish to specify a class of circuits which are simple enough\\nthey may reasonably make up the logic in a computer. Normally one restricts to AND, OR, and\\nNOT gates; however, this would make it impossible to construct most circuits reversibly. Luckily,\\nthere are small, universal, reversible logic gates. Thus, we will also permit the use of Fredkin gates,\\nalso known as controlled-swap gates, and Toffoli gates, also known as controlled-controlled-not\\ngates. Further, we allow the (reversible) zero-input gates true and false whose outputs are 1\\nand 0 respectively; as well as endtrue and endfalse whose inputs are restricted to be 1 and\\n0 respectively and whose output is the null set. Now for a given function f , we can define a\\nnew circuit-size complexity as the minimum number of and, or, not, true, false, endtrue,\\nendfalse , Toffoli, and Fredkin gates needed to create a circuit which computes f . Similarly\\neach circuit has a circuit-depth, corresponding to the longest path in the circuit, and thus we can\\nsimilarly define a circuit-depth complexity for f .\\nFredkin and Toffoli give a universality result, showing that any reversible function can be\\ncomputed by reversible circuits, given extra or ancillary bits. Further, only a constant-factor\\nincrease in circuit-depth and complexity is needed over a similar circuit constructed from and, or,\\nand not gates [FT82]. In a recent paper, Aaronson, Grier, and Schaeffer go further classifying all\\nreversible logic gates on bits. They also prove any reversible function can be implemented with only\\na constant number of ancillary bits [AGS15]. To give some intuition, Figure 1 shows how Toffoli\\ngates can be used to create other logical operations.\\nBecause Fredkin and Toffoli gates can similarly be simulated by a constant number of AND,\\nOR, and NOT gates, this should not impact circuit complexity beyond constant-factors; however,\\nit is necessary to be able to capture our notion of energy complexity in a useful way.\\n2.2 Energy Word RAM Model\\nThe Energy Word RAM model allows any contiguous segment of memory of size w to be accessed\\nin constant time and defines a fixed set of operations that can take in O(1) word sized inputs\\nin constant time. We also assume memory allocation is handled in a reversible manner. This\\n5\\nAB\\nC\\nA\\nB\\nC⊕\\n(A ∧ B)\\n(a) Toffoli gate\\nA\\nB\\n0\\nA\\nB\\nA ∧ B\\n(b) And\\n1\\n1\\nC\\n1\\n1\\n¬C\\n(c) Not\\nA\\n1\\n0\\nA\\n1\\nA\\n(d) Copy\\nA\\n1\\nA\\nA\\n1\\n0\\n(e) DeleteCopy\\nFigure 1: Standard logic implemented using a Toffoli gate.\\nwill become a more reasonable assumption later, when we show linked-lists and stacks can be\\nimplemented reversibly. The program and operations have the following restrictions. First, we\\nrestrict ourselves to the operations typically found in high-level languages as well as their reversible\\nanalogues. Second, the operation’s energy costs should be calculated based off of what can be\\nconstructed in the circuit model. Third, all reversible operations must come paired with their\\ninverse operation. Finally, all Energy Word RAM programs must return the machine to its original\\nstate, with the exception of a copy of the output living somewhere in memory. This can be done\\nsimply but expensively by irreversibly zeroing out every bit and paying the associated energy cost.\\nThe reversible operations we allow include in-place addition and subtraction (e.g., a += b),\\nincrement and decrement (e.g., a += 1), swapping two variables, testing for equality or less-than\\nrelation, copying a variable into an initially empty variable\\n(Copy(a, b) ≡ b += a)\\nand destroying a known copy of a variable\\n(DestroyCopy(a, b) ≡ b −= a)\\nWe have introduced here a useful notation, that of underlining variables whose values are empty,\\nwhich shall serve us in writing pseudocode as well. The irreversible operations we allow include\\noverwriting one variable with another, and computing the bitwise And or Or of two variables.\\n2.2.1 Irreversible Word Operations\\n• And(a, b): Returns bitwise And. Costs 1 unit of energy.\\n• Or(a, b): Returns bitwise Or. Costs 1 unit of energy.\\n• Set(a, b): Sets the value of a equal to the value of b. Costs w energy.\\n2.2.2 Reversible Word Operations\\n• Add(a, b): Returns a+ b and b. Inverted by Sub(a, b)\\n• Sub(a, b): Returns a− b and b. Inverted by Add(a, b)\\n• Inc(a): Returns a+ 1. Inverted by Dec(a)\\n• Dec(a): Returns a− 1. Inverted by Inc(a)\\n• Not(a): Assumes a is 0 or 1. Returns the logical negation of a. Inverted by Not(a).\\n• Swap(a, b): Swaps the values stored in the memory locations of a and b. Inverted by\\nSwap(a, b).\\n6\\n• LessThanOrEqualTo(a, b): Returns the inputs and 1 if a ≤ b and 0 otherwise. Note, if\\nthe extra output bit is deleted after it is used, it will cost 1 energy.\\n• InverseLessThanOrEqualTo(a, b, c): Returns a, b, and c− 1 if a ≤ b. Returns a, b, and\\nc if a > b.\\n• Copy(a, b): Returns a and b + a. If b is known to be zero, this results in a successful copy.\\nInverted by DestroyCopy(a, b).\\n• DestroyCopy(a, b): Returns a and b− a. If b is known to be equal to a, then b is now zero.\\nInverted by Copy(a, b).\\nIn this model, we intend that our lowest level pseudocode correspond to an assembly-like lan-\\nguage. For simplicity we will continue to work with variables and locations in memory as though\\nthey are all stored in RAM, rather than deal with registers, paging, and other complications that\\nmay arise depending on the computer architecture. At this level we also explicitly number every\\nline of our program and grant the code access to the program counter, PC, which is the location\\nin memory of the current instruction. At every instruction the PC is incremented, but it can also\\nbe manipulated manually, allowing jumps among other operations. It is very easy to make code\\nirreversible by manipulating the PC, as this is implicitly adding control logic to the program. The\\ninstruction set we will be using in this paper is the same as the one with which we defined our Word\\nRAM model. For an instruction set for a reversible computer that has been built see Appendix B\\nof Frank’s Thesis [Fra99] or [TAG12].\\n2.3 Energy Transdichotomous RAM Model\\nThe Energy Transdichotomous RAM model is computationally the most powerful and flexible. As\\nwith the Word RAM model, we allow access to memory segments of size w in constant time and\\nassume memory allocation is done reversibly. Generally we will assume that w = Ω(lg n), making\\nthe word size capable of indexing the entire input of the problem. We also allow any operations\\non O(1) words to be performed in constant time; however, every algorithm can only use a constant\\nnumber of different operations. The energy cost of an operation is simply the log of the ratio of\\nthe input space to the output space, as in the circuit model. Note that this is a lower bound on\\nthe energy cost of the operation in the circuit model, and thus a lower bound in the Energy Word\\nRAM model. Finally, we still need to leave the computer in its initial state, except for a copy of\\nthe output.\\nThis model is convenient to work in because it is relatively easy to calculate the energy cost\\nof many operations and the flexibility of choosing operations allows us to exploit information in\\nthe system without having to work out the details of how it would be implemented. For example,\\nwhen dividing an integer by four would generally incur two bits of energy loss or two bits of\\ngarbage; however, if we happen to know that the number is even, there is really only a single bit\\nof information being lost. Instead of having to worry about how to perform shifts and additions to\\nsave this bit, the Transdichotomous RAM model allows us to have a ‘divide by four when evenly\\ndivisible by 2’ operation with the restriction that it only takes even inputs.\\nWe now develop some conventions for writing programs in the Transdichotomous RAM Model.\\nAll lines are of the form TUPLE = TUPLE. Both tuples must contain the same number of\\nelements, and the number of elements must be O(1). The left tuple is a list of all of the values in\\nmemory which are used in the computation being performed on this line, including those simply\\nbeing overwritten. The right tuple contains expressions representing the values that will be in the\\ncorresponding variables on the left. These expressions must contain no more than O(1) constant\\n7\\ntime operations. One interesting convention about this language is every variable implicitly serves\\ntwo purposes depending on its location. On the left, all variables refer to the memory location\\nwhere they are stored, and on the right they refer to the values being represented at those memory\\nlocations.\\nAs we did above, here we shall annotate variables whose value is known to be zero (often new,\\nunassigned variables) with an underline. This information is often critical to the energy cost of an\\nexpression. For example, (a, b, c) = (a, b, a+ b) would cost w units of energy because we are erasing\\nevery bit in c before replacing it with the value a+ b. However, (a, b, c) = (a, b, a+ b) has no energy\\ncost because the input has the value of c assumed to be zero, thus reducing the input space by a\\nfactor of 2w and making the number of inputs and outputs the same.\\nThe following are some examples of common operations written in the format. All operations\\nare assumed to be integer operations with reasonable overflow and rounding conventions. The\\nfollowing examples cost zero energy:\\n• Copy: (a, b) = (a, a)\\n• DestroyCopy: (a, b) = (a, b− a)\\n• Add: (a, b) = (a+ b, b)\\n• Add a constant: (a) = (a+ 5)\\n• Quotient and remainder of two numbers: (a, b, c) = (a/b, a mod b, b)\\n• CNOT (aka reversible XOR): (a, b) = (a, a⊕ b)\\n• AND: (a, b, c) = (a, b, a ∧ b)\\n• Multiplication: (a, b, c) = (a, a ∗ b >> w, a ∗ b mod 2w)\\n• LessThan: (a, b, c) = (a, b, a < b)\\nThe following examples cost w units of energy:\\n• Delete: (a) = (0)\\n• AND: (a, b) = (a ∧ b, 0)\\n• XOR: (a, b) = (a⊕ b, 0)\\n• Add: (a, b) = (a+ b, 0)\\nHere are some examples of common operations that cost between 0 and w units of energy:\\n• AND: (a, b) = (a ∧ b, b). This has an energy cost of 1\\n• Right shift by 3: (a) = (a << 3). This has an energy cost of 3.\\n• Remainder: (a, b) = (a mod b, b). This has an energy cost of w − lg b\\nWe introduce another convention to avoid writing down every variable that is not changing: If a\\nvariable appears in the right-hand side of a line and not the left it is short-hand for that value\\nbeing assigned to the same memory location it originally came from. We will also omit parenthesis\\nif there is only one variable or expression. Thus, (a, b, c) = (a + b + c, b, c) becomes a = a + b + c\\nand (a, b) = (b, b) becomes a = b. One may initially worry that this will obscure the energy cost of\\nthe operation; however, it actually makes no difference. If a variable appears as a single element of\\nthe input and output tuple, it contributes the same factor to the input space as the output space.\\nThese will cancel when calculating the entropy of the operation and thus omitting variables which\\nare assigned back to themselves will not change the calculated energy cost. We encourage the use\\nof this convention to make the code easier to understand.\\n8\\n2.4 High-level Pseudocode\\nAlthough the previous section provides a nice, clean way to analyze the energy, space, and time\\ncomplexity of an algorithm; we may want a more concise and C-like language. Past research\\non reversible programming languages has focused on fully reversible programming languages and\\narchitectures. The first high-level reversible programming languages developed were Janus [LD82]\\n[Yok10] and R [Fra99]. The first reversible architecture, Pendulum, was developed by Vieri [Vie99]\\n[VAF+98]. Along with Pendulum, Vieri introduced a reversible low-level instruction set, PISA,\\nwhich is used as a basic reversible instruction set for many future works. Most recently, this\\narchitecture has been further improved with the development of Bob [TAG12] using a slightly\\nmodified version of PISA known as BobISA, providing more efficient branch handling and address\\ncalculation. Axelsen [Axe11] presented the first compilation techniques to translate high-level\\nJanus to low-level PISA, two independently developed reversible languages, and showed that his\\ntechniques can be extended for use in any high-level reversible language.\\nWe modeled our pseudocode off of these previous high and low level reversible languages while\\nalso adding a few new commands to allow for partial reversibility. We now allow lines of the form\\nVARIABLE = EXPRESSION as well as for loops, while loops, if/else statements, and subroutine\\ncalls. We also introduce log blocks and unroll statements in Section 2.4.1. On lines where we are\\nassigning a variable, we assume that every input in the expression will remain unchanged in its\\nmemory location after the computer performs the operation and that the variable on the left-hand\\nside will have its value replaced by the value of the expression. If this is a reversible operation, the\\nvariable will merely be changed as appropriate; if it is an irreversible operation, then the variable\\nwill be changed and an additional energy cost will be incurred based on the model being used.\\nFigures 2 and 3 give some simple examples of equivalent code in the three different levels of\\npseudocode conventions we’ve developed (high, intermediate, low). The high level is our C-like\\nlanguage. The intermediate language converts high level control logic to jumps and labels. The\\nlow level breaks it down further to an assembly-like language. Future sections will use one or more\\nconventions as needed for clarity.\\n2.4.1 Logging and Unrolling\\nDealing with garbage data tends to become tedious when writing reversible computer code. For\\nexample, suppose that we were comparing two variables, a and b, and that we wanted to use the\\nresult of this comparison to increase some counter; see Figure 4a.\\nIn a normal computer, by the function’s end, a would be garbage-collected automatically; how-\\never, in our reversible computer a naive garbage collection algorithm would destroy the information\\nstored in a, clearing whatever value it held and costing a word of energy. Thus, the reversible al-\\ngorithm programmer must handle the task of deallocating a manually.\\nWe call the process of using a series of commands to directly reverse some portion of the code\\nunrolling. Manually writing all such commands can be tedious and is prone to error. To expedite\\nthe process, we introduce the high-level keywords log and unroll:\\nIn Figure 4b, the line a = x > y is included inside the log indentation block, and so is to\\nbe reversed at the call to unroll. For much longer programs, this extra syntax can save the\\nprogrammer a great deal of effort that would otherwise be spent writing reverse code. Note that\\nthe log and unroll commands only exist in the highest-level language, and are translated into their\\nmanual equivalent at compile-time. The above program, therefore, would compile to the low-level\\nprogram seen in Figure 4c.\\nThe rules for unrolling are straightforward. Reversible commands can be unrolled simply by\\n9\\nx = x+ y + z high\\n(x, y, z) = (x+ y + z, x, z) intermediate\\n101 tempx = x low\\n102 x += y\\n103 y −= x\\n104 x += z\\n105 y += tempx\\n106 y += tempx\\n107 tempx −= y\\nFigure 2: Simple example of code in high,\\nintermediate, and low-level pseudocode.\\nlog x = 5\\nlog x = 10\\nunroll\\nx = 5\\n(tempx, x) = (x, 0)\\nx = 10\\nx = x− 10\\n(tempx, x) = (0, tempx)\\nx = x− 5\\n101 x += 5\\n102 mem[lp] += x\\n103 x −= mem[lp]\\n104 lp += 1\\n105 x += 10\\n106 x −= 10\\n107 lp −= 1\\n108 x += mem[lp]\\n109 mem[lp] −= x\\n110 x −= 5\\nFigure 3: Simple example of logging in high,\\nintermediate, and low-level pseudocode.\\nincluding their inverse commands in reverse calling order. Unrolling reversible control logic is\\ndiscussed in Section 3.1.\\n10\\na = x > y\\ncounter += a\\n(a) garbage data not\\nunrolled\\nlog:\\na = x > y\\ncounter += a\\nunroll\\n(b) logged high-level\\na = x > y\\ncounter += a\\na −= x > y\\ndealloc(a)\\n(c) logged low-level /\\nautomatic unroll\\nFigure 4: Three examples detailing the mechanics of logged code.\\nTo allow our model to unroll semi-reversible programs, which may include irreversible com-\\nmands, we introduce the log stack, a data structure onto which the program can push extra bits of\\ninformation to be used later to invert the otherwise-irreversible operations. We keep track of our\\nposition in the log stack with the log pointer, lp. In the Transdichotomous model, every operation\\nmust have its inverse and the process for logging that operation explicitly specified. Furthermore,\\nwe assume that this garbage is encoded as efficiently as possible and thus only requires as many\\nbits of space as are needed to distinguish the input space from the output space. Once again,\\nwhen we log lines with operations that were previously irreversible, we are implicitly defining new\\noperations and should take appropriate precautions. In our Word RAM model, these operations,\\ntheir inverses, and operations capable of interfacing with lp and memory must be specified.\\n2.4.2 Promise Notation\\nWe introduce another notational convention that will assist in writing low-energy pseudocode for\\nthe Transdichotomous RAM model. At the end of a standard line of code, one may add a comma,\\nthe keyword “assert”, and then a claimed Boolean expression restricting the values of the involved\\nvariables. Some useful examples include:\\nIsTrue = 0, assert 0 ≤ IsTrue ≤ 1\\nx = x/y, assert 6 | x\\nHere IsTrue may have been the result of a comparison and is known to be either 0 or 1. Thus\\nthe energy cost of destroying it is only 1 bit instead of w bits. In the second example, we might\\nknow that the problem being computed has some symmetries that a compiler might not see which\\nrestrict the values x can take on. Asserts allow us to implicitly define functions which have a\\nrestricted input space and thus reduce energy costs. Given the convenience of defining functions in\\nthis manner, we must be very careful that we are still using only O(1) different operations in our\\nalgorithm.\\n3 Reversible Primitives\\nIn this section, we develop many high-level primitives commonly used throughout algorithms, but\\nwhich need special care to be done in an energy-efficient manner. Before proceeding, we should\\ndiscuss in slightly more detail the architecture of our theoretical semi-reversible computer. Our\\ncomputer only has a single mode of operation, always incrementing the program counter with\\n11\\nevery instruction. Reversing operations comes from writing the inverse operation in a later section\\nof code, rather than having a separate reversal mode which travels backward along the program\\ncounter, inverting those operations. This gives us more flexibility in how to handle irreversible\\nsections of code, and the manner in which we reverse operations which are not dependent upon\\neach other. However, it comes at the cost that we cannot recover the value of the program counter.\\nThus this design will have to incur an energy cost of w every time the computer is reset. Because\\nwe can run many programs between restarts, we do not consider this to be of major consequence.\\n3.1 Control Logic\\nFollowing Frank [Fra99], we can make branching logic reversible with constant space overhead\\nusing paired branching with the destination of a branch being a branch that points back. Thus,\\nwe have symmetry, and when running backward, we can just follow the branch we arrived on.\\nHowever, because we are not working in a fully reversible model, there are some caveats we must\\npay attention to: all reversible control logic in this section depends on all of the code within the\\ncontrol sequence being reversible. If this is not the case, we can make no guarantees about the\\ncorrectness when irreversible operations are being performed within some control logic, especially\\nif they are manipulating the variables the control logic depends on.\\nIn Section 2.2 we noted that we can do comparisons reversibly with a single bit of extra space.\\nIn this section, we look at jumps, branches, conditionals, for loops, and function calls.\\n3.1.1 Jumps and Branches\\nHere we consider the most basic building blocks of control logic, alterations to the program counter\\nin the form of jumps and branches (conditional jumps). Jumps can be performed by a reversible\\naddition to the program counter, we use notation goto, gotoifeq, and gotoifneq. However, if\\nthe program counter is allowed to change, we can no longer assume every line was reached by an\\nincrement to the program counter, thus creating an irreversible situation. To deal with this, all\\nprogram counter jumps must be paired with a comefrom, comefromifeq, or comefromifneq\\nstatement. In our pseudocode, we allow for goto to direct an absoloute or relative jump and note\\nthat a compiler can transform absolute to relative, as is used in most reversible architectures.\\nTheorem 3.1. Jumps can be implemented reversibly with constant-factor increases in time and\\nspace and up to an extra word of space per jump.\\nProof. All jumps must be paired with comefrom statements. In the case of a regular comefrom\\nstatement, the program knows that it reached this location via a jump and thus will jump back in\\nthe reverse. However, it is rare that an unconditional jump such as this exist. In the more general\\ncase, the program must decide whether the comefrom was reached via a jump or from the line above\\nby an increment in the program counter. To address this, we log two things upon jumping: (1)\\nthe length of the jump and (2) a bit indicating that a jump occurred. We then use a comefromif\\nto check this bit upon reversing. At the corresponding reverse comefromif we’ll pop the value\\noff the log stack and use it to either change the program counter or not depending on whether the\\ncode jumped to that location. A general example is shown in Figure 6.\\nWe can make an additional optimization if a comefrom statement only has one corresponding\\ngoto. Because we know the jump location corresponding with the comefrom, this can be imple-\\nmented reversibly by noting the jump length directly in the source code and not logging. In this\\ncase we only have a single bit of storage for logging whether the jump was taken. This is illustrated\\nin Figure 5.\\n12\\nlog:\\n[code1]\\ngoto label1\\n[code2, k lines long]\\nlabel label1\\n[code3]\\n[code4]\\nunroll\\n[code1]\\nmem[lp] += 1\\nlp += 1\\ngoto k\\n[code2]\\nlp += 1 //adds 0 to log indicating no jump\\n[code3]\\n[code4] //unroll begins\\n[reverse of code3]\\ncomefromifeq(mem[lp], k)\\nlp -= 1 //removes 0 bit from log\\n[reverse of code2]\\nlp -= 1\\nmem[lp] -= 1 //removes 1 bit from log\\n[reverse of code1]\\nFigure 5: The intermediate-level and compiled low-level code for a reversible jump. The jump and\\ncomefrom statements are paired uniquely so the jump distance can be placed directly in the low\\nlevel code.\\nWe make the distinction above between two flavors of jumps. The first, paired jump, requires\\nthat goto and comefrom be in a 1 : 1 pairing, meaning the jump length is pre-determined and\\nonly a single bit of storage is required. The second, variable jump, occurs when the jump length is\\ndetermined is determined at run-time. This can occur when, for example, multiple jumps land at\\nthe same destination(6). This requires pulling the destination off the log and takes an additional\\nword of space.\\n3.1.2 Conditional Statements\\nWe distinguish between two different types of conditionals, a protected if statement and a general\\nif statement. A protected if statement is one in which the conditional is not modified within the\\nif statement.\\nTheorem 3.2. Protected If statements can be implemented reversibly with constant-factor increases\\nin time and space, and general If statements with an extra bit of overhead in space.\\nProof. Let a represent the variable which holds the conditional expression of the if statement. First\\n13\\nlog:\\n[code1]\\ngoto label1 //call this jump1\\n[code2]\\ngoto label1 //call this jump2\\n[code3]\\nlabel label1\\n[code4]\\n[code5]\\nunroll\\n[code1]\\nmem[lp] += [distance from jump1 to label1]\\nlp += 1\\nmem[lp] += 1\\nlp += 1\\ngoto(mem[lp-2 ]) //this is jump1\\n[code2]\\nmem[lp] += [distance from jump2 to label1]\\nlp += 1\\nmem[lp] += 1\\nlp += 1\\ngoto(mem[lp-2 ]) //this is jump2\\n[code3]\\nlp += 1 //adds 0 to log indicating no jump occurred\\ncomefromifeq(mem[lp-1 ], mem[lp-2 ]) //jump occurred, jump length\\n[code4]\\n[code5] //unroll begins\\n[reverse of code4]\\ngotoifeq(mem[lp-1 ], mem[lp-2 ]) //reverse of comefromifeq\\nlp -= 1\\n[reverse of code3]\\nmem[lp] −= 1 //reverse of jump2\\nlp −= 1\\nmem[lp] -= [distance from jump2 to label1]\\nlp −= 1\\n[reverse of code2]\\nmem[lp] −= 1 //reverse of jump1\\nlp −= 1\\nmem[lp] -= [distance from jump1 to label1]\\nlp −= 1\\n[reverse of code1]\\nFigure 6: High-level and compiled low-level code for a general reversible jump. The jump length is\\nstored so that the reverse jump can be made.\\n14\\nifp(x > y):\\nz += 5\\nx −= 6\\n101 a = x > y\\n102 gotoifneq(a, 12)\\n103 z += 5\\n. . .\\n114 comefromifneq(a, 12)\\n115 a −= x > y\\n116 x −= 6\\nFigure 7: A reversible if statement in high-\\nlevel and low-level pseudocode. The come-\\nfrom statement on line 114 is a marker\\nwhich, though it does not explicitly cause\\nany change to the state of the program dur-\\ning forward execution, is essential to ensure\\nthe instantaneous reversibility of the code.\\nlog:\\nifr(x > y):\\nz += 5\\nx −= 6\\ny += x\\nunroll\\n101 a = x > y\\n102 gotoifneq(a, 2)\\n103 z += 5\\n104 comefromifneq(a, 2)\\n105 a −= x > y\\n106 x −= 6\\n107 y += x //unroll begins\\n108 x += 6\\n109 a += x > y\\n110 gotoifneq(a, 2) //reverse of comefrom\\n111 z −= 5\\n112 comefromifneq(a, 2)\\n113 a −= x > y\\nFigure 8: Logged reversible if statement in\\nhigh-level code and compiled low-level code\\nconsider a protected statement. Regardless of whether a was false and we jumped or a was true\\nand we entered the if statement body, a’s value will be preserved at the end of the if statement. As\\ndescribed in [Fra99], two-way branching may then be employed to make the if statement reversible.\\nTo briefly summarize Frank’s results, in the forward direction, the value of a determines whether\\nwe enter the body of the if statement or jump to the end. In a similar way, when reversing the\\noperation, if we leave a marker at the end of the if statement indicating that this location may have\\nbeen reached via two different paths — either by jumping or by flowing through the if statement\\nbody — depending on the value of a. Because a will be maintained, we can use a comefromif\\nconditioned on a to determine if the if statement body should be undone. Because this if statement\\nis reversible, it does not incur any cost in our logging space (Figure 8).\\nIn a general if statement, because a is not protected and is subject to change, we must log the\\nvalue of a upon initial execution and use the logged value in the comefromif to determine whether\\nto jump in the reverse direction. This gives an extra bit of storage.\\n3.1.3 For Loops\\nWe now examine a special case of for loops. A simple for loop is one in which a variable i iterates\\nover the values 1 through k, each time executing some piece of code which does not alter i or k.\\n15\\nlog:\\nfor(i=1 ; i ≥ 1 ; i < x ; i++):\\n[code1]\\n[code2]\\nunroll\\nlog:\\ni = 0\\nlabel beginFor\\ni += 1\\ngotoifneq ( i < x, endFor):\\n[code1]\\ngoto beginFor\\nlabel endFor\\n[code2]\\nunroll\\nFigure 9: An example of a simple for loop being translated from high-level to intermediate-level\\ncode consisting of jumps and branches.\\nTheorem 3.3. Simple for loops can be implemented reversibly with constant-factor increases in\\ntime and space.\\nProof. At the end of the body of the for loop we can simply increment i, check if it is less than or\\nequal to k, and if not use a branch to jump back to the beginning of the body of the for loop (Figure\\n9). Incrementation can be done reversibly and we have shown branching can be done reversibly.\\nIt is easy to see one can unroll one of these loops by simply running the inverse code for i ranging\\nfrom k to 1. Because i only exists within the for loop and we know the loop was executed, we\\ndo not need to worry about whether or not to execute the inverse code, or how many loops were\\nperformed.\\nWe consider an extension of the simple for loop. A for loop has internal conditions if all\\nvariables used in the condition of the for loop only exist within the scope of the for loop. That\\nvalue is protected if it is never changed irreversibly. We define a protected for loop as a for loop\\nwith protected, internal conditions(Figure 10).\\nTheorem 3.4. A protected for loop can be performed reversibly with constant factor overhead in\\ntime and space.\\nProof. This follows the same reasoning as the proof of Theorem 3.2. We can construct the for loop\\nout of a paired branch statement. Since we know the terminal value of the iterator is not changed\\nafter the loop ends, then when reversing, we can check it’s value to see if the for loop ever executed.\\nThe update function on the iterator is also promised to be reversible. Thus to determine if one is\\ndone unwinding the loop, one can apply the inverse update to the iterator and then check whether\\nit has reached its initial value.\\n16\\nlog:\\n[code1]\\nfor (i = a; g(i, b); f(i)):\\n[code2]\\n[code3]\\nunroll\\nlog:\\n[code1]\\ni = f−1(a)\\nlabel beginFor\\nf(i) //f is a reversible function\\ngotoifneq ( g(i, b), endFor):\\n[code2]\\ngoto beginFor\\nlabel endFor\\n[code3]\\nunroll\\nFigure 10: Example of high-level to intermediate-level code for a protected for loop\\nIn general, loops cannot be performed reversibly without overhead. For example, we may not\\nbe able to distinguish whether or not a loop ever executed because the variable checked in the\\ncondition may have gone outside the range during the loop or have been set that way before the\\nloop ever executed. Even if we know a loop has executed, it is possible for the variables considered\\nby the loop to reach the same initial value multiple times throughout the execution of the loop. If\\nthis is the case, then we cannot rely on these initial values to tell us to terminate the loop, as was\\nthe case with simple for loops.\\nHowever, this issue can be addressed with a small amount of overhead. We will simply create\\nan extra variable at the start of the loop and increment it whenever we loop. Once the loop\\nterminates, we will push this variable onto the log stack, so we can recover the number of times\\nour loop executed. Assuming the internals of the loop are reversible, then the entirety of the loop\\nis now reversible. We call this a general for loop.\\nTheorem 3.5. A general for loop can be performed reversibly with constant factor overhead in\\ntime and an extra word of space representing the number of loop executions.\\n3.1.4 Function calls\\nTheorem 3.6. Function calls can be implemented reversibly with constant-factor increases in time\\nand space.\\nProof. Reversible function calls work in a similar manner to normal ones, involving the stack pointer\\nbeing pushed onto the stack, following the new pointer to the function, and then returning. There\\nare two major changes needed. First, every function that might be logged will create a second\\nfunction, which is the unrolling version, and appropriate pointers to move to this function when\\n17\\nunrolling will be used. Second, we need to be careful to not overwrite anything when moving\\naround. In the case of functions, we know where the function is written in memory, and we know\\nwhere the function ends. Thus it is easy to specify a jump in our code whose comefrom is the\\nbeginning of the function. At the end of the function, we create another jump which returns us\\nto our original place in the code, reading the location which was pushed onto the stack (Figure\\n11).\\n3.2 Memory Management and Garbage Collection\\nAllocating and reclaiming free memory are critical tasks to the function of any modern computer.\\nHappily, we show that some basic forms of memory allocation and garbage collection can be done\\nreversibly with only constant factor overhead in time and space. Free lists are a simple type of\\nmemory allocation which involves a list of pointers to blocks of free memory. The pointer to the next\\nblock of memory is stored in the previous block of memory. Allocation is performed by removing\\nthe pointer from the free list and handing it to the program requesting the memory. To deallocate\\na block of memory, the pointer to that block is put on the end of the free list. We assume all\\ndeallocated memory blocks have been returned to their zeroed out state.\\nTheorem 3.7. Memory allocation using free lists can be done reversibly with constant-factor over-\\nheads in time and space.\\nProof. Free lists can be handled in a similar manner to doubly-linked lists 5.1. Passing a pointer\\nfrom the memory manager to the program or back are reversible operations. To append a block of\\nmemory to a list, we simply point the new block to the former head of the list, then point the head\\npointer to the new block, an operation which is straightforward to reverse.\\nGarbage collection often uses a technique known as reference counting. Reference counting\\nkeeps track of the number of references to an object or resource and deallocates the space when\\nit is no longer referenced. In our analysis, we do not charge the cost of freeing or destroying the\\nobjects to the algorithm. Since this destruction would need to happen regardless of what sort\\nof garbage collection, if any, was performed we believe the energy costs involved are not a fair\\nrepresentation of the work done by the garbage collector itself.\\nTheorem 3.8. Reference Counting can be done reversibly with constant-factor overhead in space\\nand time.\\nProof. The reference counter requires lg k bits where k is the number of references to the object. We\\ncan augment the creation of a reference to increment the counter, which is a reversible operation.\\nIn addition, we can add a simple conditional check on the destruction of a reference to see if the\\nreference counter is at 1. Since this meets the condition of a protected if, it takes no auxiliary\\nspace. We can now decrement the reference, destroy the object, and continue on with removing the\\nreference. Thus, maintaining the counter only incurs a constant factor increase in time and space.\\nThe destruction of the object would have occurred regardless, and thus it is not fair to charge its\\nenergy cost to the reference counting algorithm.\\nMark and Sweep is another garbage collection algorithm which walks through every object\\nreachable from the heap and marks them. It then goes through all objects in memory and destroys\\nany which are unmarked, and unmarks the rest. Example code can be found below.\\n18\\ndef function(x):\\n[code1]\\nreturn y\\ndef reverseFunction(y):\\n[reverse of code1]\\nreturn x\\n[code2]\\nz += function(x)\\n[code3]\\nunroll\\ndef function(x):\\ncomefrom(mem[sp-1 ]) //indicates where fxn was called from\\nx += mem[sp-2 ] //pulls input from stack\\n[code1] //body of function\\nr0+ = y //special register for return value\\n[reverse of code1]\\nx -= mem[sp-2 ]\\ngoto(mem[sp-1 ]) //returns to program\\ndef reverseFunction(x):\\ncomefrom(mem[sp-1 ]) //indicates where fxn was called from\\nx += mem[sp-2 ] //pulls input from stack\\n[code1] //body of function\\nr0− = y //special register for return value\\n[reverse of code1]\\nx -= mem[sp-2 ]\\ngoto(mem[sp-1 ]) //returns to program\\n[code2]\\npush(x)\\npush(pc)\\ngoto([function start]) //jump to function\\ncomefrom([function end])\\nswap(z, r0) //caller sets r0 back to 0\\npop(pc-4 )\\npop(x)\\n[code3] //begin unroll\\n[reverse of code3]\\npush(x)\\npush(pc)\\nswap(z, r0) //put output back in r0 to be reversed\\ngoto([reverseFunction start]) //jump to function\\ncomefrom([reverseFunction end])\\npop(pc-4 )\\npop(x)\\n[reverse of code2]\\nFigure 11: High-level and low-level language example of function call and unroll.\\n19\\nMarkAndSweep:\\nfor each root variable r:\\nMark(r)\\nSweep()\\nMark(Objectp):\\nif !p.marked:\\np.marked = True;\\nfor each Object q\\nreferenced by p′:\\nMark(q);\\nSweep():\\nfor each Object p\\nin the heap:\\nif p.marked:\\np.marked = False\\nelse:\\nheap.release(p)\\nTheorem 3.9. Mark and Sweep can be done reversibly with constant-factor overhead in space and\\ntime.\\nProof. The reversible Mark and Sweep will be a slight variation on the standard code above. First,\\neach assignment to p.marked can be replaced by (reversibly) toggling that bit because we just\\nchecked that it was in the opposite state. At this point the remaining operations are (1) following\\npointers and control logic, which can be logged and done reversibly with constant-factor overhead,\\nand (2) destroying unmarked objects. Destroying these objects does not impact any of the variables\\nin the control logic and thus can safely stay unlogged.\\n4 Energy Reduction Techniques\\nThis section overviews some general techniques that have been helpful in constructing reversible\\nalgorithms and proves some general theorems about algorithms sharing certain properties.\\n4.1 Complete Logging\\nOne very simple, yet surprisingly useful technique is to simply log every step of an algorithm. This\\nincurs a space cost of O(t(n)) words where t(n) is the runtime of the algorithm. Although this\\nseems wasteful, the prevalence of linear time algorithms or linear time sub-routines in algorithms\\nmakes this important to remember.\\n4.2 Reversible Subroutine\\nEarlier we saw that function calls can be implemented reversibly. We now give a stronger result\\nfor being able to use some reversible subroutines efficiently.\\nTheorem 4.1. If we have a fully reversible subroutine whose only effect on the program is through\\nits return value, one need only store the inputs and outputs to this subroutine to later unroll it with\\nonly a constant-factor overhead in time.\\nProof. To do this, we use a slightly more complicated, two-step unrolling process. First, after\\nthe subroutine has initially run, we copy out the output and immediately unroll the subroutine.\\nThis copy of the output looks no different to the rest of the program from what would normally\\nbe computed, and we’ve already stipulated that the subroutine cannot alter the program through\\nany other means. When it comes time to unroll the subroutine, we may have lost important\\nlogged information needed to take us from the output back to the input. At this point, we run\\nthe subroutine forward, recovering all of that needed information. Next we delete the copy of the\\noutput and unroll the subroutine normally.\\n20\\n4.3 Pointer Swapping\\nDepending on how it is managed, pointer destruction will generally take energy. Many data struc-\\ntures liberally create and destroy pointers to various nodes or other pieces of data. In many cases,\\none can keep back-pointers or self-referential pointers which would allow them to simply be moved\\naround. In general, moving pointers around has zero energy cost, and thus this is one simple way\\nto reduce the energy complexity of many data structures.\\n4.4 Data Structure Rebuilding\\nWhen attempting to implement data structures which support insert and delete operations re-\\nversibly, we run into a new challenge. Often the insertion or deletion operation will create some\\namount of garbage data which is necessary to reverse it in the future. We also need the result of the\\noperation to remain in place, so we cannot immediately reverse the operation. Thus over the life of\\nthe data structure, its size will depend on the total number of insert and delete operations, rather\\nthan just the number of elements in the data structure. To circumvent this we can use a technique\\nwe call periodic unwinding. Note, this technique depends exceedingly on what is considered the\\ndata-structure and what is an algorithm that uses the data-structure. This is discussed more below\\nand in Section 5.\\nTheorem 4.2. If a data structure which allows reversible insertions, deletions, and traversals can be\\nconstructed reversibly from k insertions in O(k) time and space, and its operations can be performed\\nreversibly with constant-factor overhead in time and space, then it can be maintained reversibly in\\namortized time with constant-factor overheads in space and time via periodic unwinding.\\nProof. If there are only O(n) deletions, then we can simply log and unroll all of the operations. If\\nnot, we need to keep track of the number of insertions and deletions that have occurred because\\nthe last rebuilding. We will keep track of these counts and increment them as part of the insertion\\nor deletion routine. We also track the number of elements in the data structure. Whenever a delete\\nis called, we then check to see whether the number of deletions is more than twice the number\\nof nodes in the data structure. If this is true, we will proceeded to rebuild the data structure.\\nWe perform a reversible traversal of the tree, with the addition that we make an extra copy of\\nthe inserted data at every node. Now that we have this copy, we can proceed to unroll the data\\nstructure, clearing the log. Once this is done we construct a new data structure with the same\\nvalues as before the reversing, but with none of the accumulated garbage. Construction of the new\\ndata structure takes O(n) time. To trigger a rebuilding, we must have called delete a larger number\\nof times than the size of the data structure we are building. We charge the amortized constant cost\\nper element being added to the new data structure to the number of deletes performed, giving us\\nconstant amortized time. Our counters and copies of the data all require O(n) space, meaning we\\nnever use more than a constant-factor overhead in space.\\nWith this method, after rebuilding and clearing the log, the data structure can no longer provide\\nany information about past items which were deleted from the data structure. This is covered by\\nthe assumption that the algorithm interacting with the data structure makes copies of all of its\\ninputs and if it needs them to reverse itself, it is responsible for maintaining that information.\\nThus, depending on how the data structure is being used, this technique can be superfluous or very\\npowerful.\\n21\\n5 Data Structures\\nData structures are meant to be used in the context of an algorithm. In the standard model for\\nalgorithms, we can draw a nice abstraction between these two and analyze their properties sepa-\\nrately. We also wish to do so in the energy complexity model; however, we need to be more careful\\nabout the responsibilities of the data structure and the algorithm for maintaining information and\\nreversibility. First, we assume the data structure is only accessed through the prescribed opera-\\ntions; we don’t want the algorithm irreversibly altering stored elements or manually altering the\\ndata structure in an unknown way. Second, if it is a reversible data structure, every operation\\nmust have a reverse operation. Third, when inserting, the algorithm gives a copy of the data to\\nthe data structure and is responsible for maintaining its own reversibility after an insert has been\\nreversed. Fourth, the algorithm will handle zeroing of the bits of elements removed from the data\\nstructure. For this purpose, the common delete operation will be replaced with Extract(x) which\\nremoves an element from a data structure and returns it; however, we will generally still call this\\noperation delete. Fifth, for a reversible algorithm, it is responsible for making the correct calls to\\nreverse functions to reset the data structure.\\nThis is certainly not the only way to treat this interface. We could just as well require the data\\nstructure to remember all of the calls performed on it so it could reverse itself upon command.\\nSimilarly, we could imagine that a deleted item cannot be handed back to the algorithm, but must\\nin some way be removed by the data structure, most likely when unrolling. We’ve chosen our\\nconventions because it more closely matches our idea of how subroutines should work and because\\nit is clearer to us how to analyze such cases.\\n5.1 Stacks and Linked Lists\\nTheorem 5.1. Doubly-linked lists can be implemented reversibly with constant-factor overheads in\\ntime and space.\\nProof. Doubly linked lists need to support adding, removing, and finding items. To search for an\\nitem x, one begins at one end of the list, performs a comparison of x and the next value in the\\ndata structure, and then either returns a match or follows a pointer. Match-checking and following\\npointers can be done reversibly. In this case, we change neither x nor the values in the data\\nstructure, so we can use a Protected If to perform our comparisons. This leaves us with a constant\\nfactor overhead in time and space.\\nTo add an element in position i, we follow pointers in the list until we reach the element at\\nposition i − 1. Our counter can be kept reversibly as was done for For Loops 3.1.3. We move the\\nforward pointer from element i− 1 into our new element, and then similarly move the back-pointer\\nfrom element i into our new element. Next we copy the pointer to our new element into the forward\\npointer slot of element i− 1 and into the back-pointer slot of element i. We then destroy our copy\\nof the pointer to the new element. Since moves, copies, and destroy copies are all reversible, we\\nincur only constant factor overheads in insertion of a new item.\\nRemoving an element from a data-structure is very similar to adding one, and also only requires\\nmoves, copies, and delete-copies. To remove element i, we delete the copies of it’s pointer which\\nelements i− 1 and i+ 1 contain. We then copy the forward and back pointers from elements i− 1\\nand i+ 1 into the appropriate positions. The pointer to our removed element is then handed back\\nto the algorithm which called the remove item method on our data structure.\\n22\\nCorollary 5.2. Stacks, queues, and dequeues can be implemented reversibly with constant-factor\\noverheads in time and space.\\n5.2 Dynamic Arrays\\nDynamic arrays store elements in an ordered array that grows and shrinks depending on the number\\nof elements currently being stored. They support the basic operations add(e), delete(), and\\nquery(i). add and delete append or remove an element from the end of the array respectively.\\nquery returns the element stored at a specific index of the array. Dynamic arrays can also support\\nthe optional operations add(i, e) and delete(i) which specify at which index to add or delete an\\nelement, adjusting all elements stored after that index accordingly.\\nA dynamic array stores a length attribute and a size attribute. The length attribute represents\\nthe number of active elements are in the array. The size attribute represents the amount of space\\nallocated for the array. length is incremented and decremented corresponding to add and delete\\noperations. Upon an add, if length = size, we double the array size by allocating new memory,\\ncopying over the elements and deallocating the old array. Similarly upon a delete, if length =\\nsize/4, we halve the array size by deallocating the second half.\\nTheorem 5.3. Dynamic arrays can be implemented reversibly with constant time and space over-\\nhead with an extra bit of space for add/delete operations. Size of the structure grows with the\\nnumber of add/delete operations.\\nProof. We now consider how to handle these operations reversibly. Because both add/delete\\noperations work at the end of the array, we check the length attribute to find where to perform\\nthe reverse operation. For reverse-add, we remove the element and decrement length. And for\\nreverse-delete, we must log the deleted element to add it back and increment length. We must\\nalso consider how to handle table doubling. On an add/delete operation, table doubling (or\\nhalving) occurs based on the result of a single comparison of length and size. We can log a single\\nbit representing the result of this comparison for each add/delete operation that will indicate\\nwhether a table doubling (or halving) needs to be reversed.\\nThis bit is necessary in order to undo table doubling because we can not determine whether a\\ntable doubling operation occurred just by looking at the resulting length and size attributes. For\\nexample, consider a table with length n and size 2n where the last operation was an add. This\\nstate could have been reached in two ways. (1) length n− 1, size n incurring a table doubling; (2)\\nlength n− 1, size 2n.\\nWe maintain the dynamic array to preserve the order and length of its elements in the reverse\\ndirection, thus a reverse-query operation can be run in the same way as a query operation\\nby simply making the same query again. Periodic rebuilding of this data structure follows from\\nTheorem 4.2 because all operations are reversible with constant factor overhead and rebuild can\\nbe done in linear time.\\n5.3 AVL Trees\\nUsing and maintaining standard AVL trees incurs an energy cost proportional to the time of the\\nassociated operations.\\nTheorem 5.4. Search(x) can be performed on standard AVL trees in Θ(lg n) time, O(1) auxiliary\\nspace and O(lg n) energy.\\n23\\nProof. To search an AVL tree we follow a pointer to the root of the tree and compare our value to\\nthe value in the current node. If it is the same we have found the value. If it is smaller we follow\\nthe left child pointer in the tree and repeat the comparison, if it is larger we follow the right child\\npointer. We must check that those pointers are not null, and if they are we terminate knowing the\\nitem is not in the tree. Following pointers incurs no energy cost, but each irreversible compassion\\ntakes one bit of energy. AVL trees are guaranteed to have a max depth of O(lg n) and thus we\\nuse O(lg n) time and energy. Since we need only maintain the pointer to the current node and the\\nvalue we are searching for there is only a constant amount of space needed.\\nTheorem 5.5. Insert(x) can be performed on standard AVL trees in Θ(lg n) time, O(1) auxiliary\\nspace and O(w lg n) energy.\\nProof. To insert a new element, we perform comparisons and follow child pointers, as with a search,\\nuntil we reach a null pointer. We replace this with our item to be inserted, potentially deleting\\nthe value previously in the memory address holding the pointer. The tree may now need to be\\nrebalanced. Every insertion may incur up to O(lg n) rotations. The details of the four different\\ncases for rotation can be seen in [CLRS01] and the time analysis remains the same. For energy, we\\nnote that all rotations in the standard AVL algorithm involve a constant number of comparisons,\\neach costing 1 unit of energy, and the creation and deletion of at least one pointer, each costing\\nO(w) energy. This leads to a total of O(w lg n) energy. If we are slightly more clever and use\\npointer swapping techniques, the energy drops to a constant amount per rotation and comparison,\\nresulting in O(lg n) energy.\\nTheorem 5.6. Delete(x) can be performed on standard AVL trees in Θ(lg n) time, O(1) auxiliary\\nspace and O(w lg n) energy.\\nProof. To delete an item we first perform a search on the tree for the element to be deleted,\\nincurring the associated costs. The element and the pointer to it are then deleted at a cost of w\\nunits of energy. We may then need to rebalanced the tree. Just as with insertion, re-balancing after\\na deletion may incur up to O(lg n) rotations. The standard AVL algorithm involves the creation\\nand deletion of a pointer during a rotation, each costing O(w) energy. If we are slightly more\\nclever and use pointer swapping techniques, the energy drops to a constant amount per rotation\\nand comparison, resulting in O(lg n).\\nWe will show that, provided only Search and Insert operations are invoked, reversible AVL\\ntrees can be maintained with only constant-factor auxiliary space consumption. If Delete is to be\\ninvoked, then the structure will accumulate an extra Θ(k) words of space for k Delete operations\\ninvoked over the lifetime of the tree. Such space consumption can still be made reasonable within\\nthe context of a larger algorithm, provided that runs of Insert and Delete form a small part of\\nthe algorithm, and are unwound periodically to refresh log space.\\nNote that because these algorithms employ only conditional branches which do not modify their\\nconditions (for example, in Search when comparing a value against a node of the tree to choose\\na branch, we leave the value of the comparison intact post-search), they are completely reversible\\nwith no logging penalty.\\nTheorem 5.7. Search(x) can be performed on reversible AVL trees in Θ(lg n) time, O(1) auxiliary\\nspace and 0 energy.\\nProof. Provided that our reversible AVL tree is constructed using two-way nodes, performing\\nSearch(x) reversibly is straightforward. Upon reaching a node v in the tree, we compare its\\n24\\nvalue with x and use the resulting bit to determine whether to jump left or right. After jumping, as\\nwe have maintained in memory a pointer back to v, we can compare x to v again to destroy this bit\\nand free the space gain. Once the final node is reached and our answer is found or determined to\\nbe absent, we can log our result somewhere and reverse our computation to destroy any remaining\\ngarbage bits. This procedure uses constant auxiliary space, and produces our answer reversibly in\\nO(lg n) time.\\nInsert is the next operation we address. It includes the task of reversibly rebalancing the tree,\\na slightly more complicated task than that of Search.\\nFrom a given tree, there may be multiple legal trees that underwent a different rotation to\\nproduce it. Thus, if we didn’t store any auxiliary information about the AVL rotations as we\\nperformed them, it would not be possible to immediately reverse a tree’s configuration. A key\\ninsight into the space consumption of this process is to note that, for a tree containing n unique\\nelements, each of those elements must occupy at least Ω(lg n) bits of space each on average (in\\nthe word model in particular, each element takes a constant w = Θ(lg n) bits of space). Thus, we\\nmust store a Θ(lg n)-sized entry to a rotation log for each inserted element. The space cost of this\\nlogging is absorbed into the space cost of the element’s value in the tree itself. This is the premise\\nof the following theorem:\\nTheorem 5.8. Insert(x) for x not yet present in the reversible AVL tree can be performed in\\nΘ(lg n) time, preserving the Θ(n) space cost of the tree and using 0 energy.\\nProof. Insertion consists of traversing the reversible AVL tree, adding the new element to the tree,\\nand making any rotations that are necessary to balance the tree.\\nTraversing the tree can be done reversibly as in Search(x), and we refer to its proof for\\nreversibility. Once we know where x is to be added into the tree, we create a new node for it and\\nproceed to rebalance the tree.\\nDuring the balancing step, rotations begin at the lowest level and proceed upward. To perform\\nour operations reversibly, we will keep a log of every rotation performed at each of the lg n levels\\nof the tree. For each level, we will store 01 for a right-rotation, 10 for a left-rotation, and 00 for no\\nrotation. By keeping this log, we have enough information to go in the reverse direction, proceeding\\nfrom the top of the tree to its bottom and checking x’s value against those of the encountered nodes\\nto progress. In this way, we keep our Insert(x) action reversible.\\nEach log entry need only store a number of bits proportional in size to the maximum height\\nof the tree. Because there are n unique entries in the tree at any given time, each call to Insert\\nincurs only O(lg n) bits of space cost. Thus our log, which stores Θ(lg n) additional bits per element,\\nresults in only a constant-factor increase in the space consumption of the tree. This holds even if\\ndeletions from the tree have also been performed, as inserting into a tree will always grow the tree’s\\nspace consumption asymptotically by Ω(lg n) bits per insertion, while the log size will grow by\\nO(lg n) bits per insertion. The space consumption of the tree is thus preserved to within constant\\nfactors.\\nTheorem 5.9. Delete(x) can be incorporated into reversible AVL trees, taking O(lg n) time and\\nincurring an additional Θ(k lg n) = O(kw) bits or O(k) words of space for k delete operations.\\nProof. Delete(x) simply requires the same traversal operations as Search(x), followed by the\\nextraction of the desired node. Given that it takes no more than O(lg n) bits to specify a location\\nin the tree, and because we will need Θ(lg n) bits to store the subsequent rotation information, we\\nmust, as is the case for Insert(x), store additional information in the log proportional to the size\\nof the original elements. This incurs a cost while simultaneously reducing the size of the tree, and\\n25\\nMaxHeapify(A, i):\\n(i, left) = (i, 2 ∗ i)\\n(i, right) = (i, 2 ∗ i+ 1)\\n(i, largest) = (i, i)\\nif left ≤ A.length()\\nand A[left] > A[largest]:\\nlargest = left\\nif right ≤ A.length()\\nand A[right] > A[largest]:\\nlargest = right\\nif largest 6= i:\\nSwap(A[largest], A[i])\\nMaxHeapify(A, largest)\\nInsert(a):\\nheapSize = heapSize+1\\nA[heapSize] = a\\nfor i = 0 to blg(heapSize)c:\\nif A[bheapSize/2i+1c]\\n< A[bheapSize/2ic]:\\nswap(A[b heapSize/2i+1c],\\nA[b heapSize/2ic])\\nDeleteMax():\\nSwap(A[1], A[heapSize])\\n(A[heapSize],maxValue) = (0, A[heapSize])\\nMaxHeapify(A, 1)\\nReturn maxValue\\nFigure 12: Binary Heap Code\\nReversibleMaxHeapify(A, i):\\n(i, left) = (i, 2 ∗ i)\\n(i, right) = (i, 2 ∗ i+ 1)\\nif right ≤ A.length\\nand A[right] > A[largest]:\\nlargest = right\\nelse if left ≤ len(A)\\nand A[left] > A[largest] :\\nlargest = left\\nif largest 6= 0:\\nswap(A[i], A[largest])\\nReversibleMaxHeapify(A, largest)\\nleft = left− 2 ∗ i\\nright = right− 2 ∗ i− 1\\nReversibleDeleteMax():\\ndeleteCount = deleteCount + 1\\nif deleteCount > heapSize :\\nRebuild(A)\\n(A[1], A[heapSize]) = (A[heapSize], A[1])\\n(A[heapSize],MaxV alue) = (0, A[heapSize])\\nReversibleMaxHeapify(A, 1)\\nReturnMaxValue\\nFigure 13: Reversible Binary Heap\\nthus for k deletions we require Θ(k lg n) = O(kw) extra bits or O(k) words of space beyond that\\noccupied by the tree itself, for a total space consumption of O(n+ k) words.\\n5.4 Binary Heaps\\nBinary heaps are binary trees with the property that all children are either less than their parent,\\nin the case of max-heaps or greater in the case of min-heaps. They support inserting an arbitrary\\nelement into the data-structure as well as popping the max/min element. We give a brief energy\\nanalysis as well as an efficient reversible algorithm.\\nTheorem 5.10. Binary Heaps can have items inserted irreversibly with Θ(lg n) time, Θ(1) space,\\nand Θ(lg n) energy; or reversibly with Θ(lg n) time, Θ(1) space, and 0 energy.\\nProof. Insert begins by incrementing the heap size and assigning an empty entry in the array to\\nthe value to be inserted. These are both reversible operations. We then proceed to fix the heap by\\nchecking whether the current node is smaller than the parent, and if so swapping them. Normally\\nthe comparison and loop would destroy bits, leading to a O(lg n) energy cost, however this is a\\nprotected if and simple for loop so Lemmas 3.2 and 3.3 allow us to do so reversibly with no energy\\ncost.\\n26\\nTheorem 5.11. Binary Heaps can have the root node deleted irreversibly with Θ(lg n) time, Θ(lg n)\\nspace, and Θ(w lg n) energy.\\nProof. DeleteMax moves out the largest element of the heap and then restructures it with a recursive\\ncall to MaxHeapify. This can be called up to O(lg n) times. Every call to MaxHeapify looks at the\\nchildren of the current node, determines if it is larger than the value in the current node, and if so\\nswaps them. The maintenance of the current largest value as well as those of the children involves\\noverwriting those values. The conditionals additionally take a constant amount of energy, leading to\\nan energy complexity of O(w lg n). The standard time and space analysis remains unchanged.\\nTo implement Reversible Binary Heaps, we make two changes: MaxHeapify is written reversibly,\\nand we use periodic unwinding to keep O(n) space. The Rebuild function is not given here, but\\ncan be derived from Theorem 4.2\\nTheorem 5.12. Binary Heaps can have the root node deleted reversibly with Θ(lg n) time, Θ(lg n)\\nspace, and 0 energy.\\nProof. MaxHeapify has been rewritten to ensure that no values are ever overwritten during the\\nfunction call by reordering the logic and subtracting away known values from variables. Figure 13\\nshows the substitutions to ensure no values are overwritten. We then note that the conditionals\\nare protected if statements, and thus reversible by Theorem 3.2.\\nAs with AVL trees, binary heaps subject to k insertions and deletions will accumulate an extra\\nO(k) space to be maintained. In some cases this can be resolved by periodic unwinding.\\n6 Algorithms\\nThis section includes the analysis for the time, space, and energy complexity of several standard\\nalgorithms in our model. We also give a number of improved algorithms. Some of our results for\\nalgorithms with zero energy complexity are similar to results claimed or proved in [Fra99] about\\nreversible algorithms. However, we prove these results within our own model, which differers slightly\\nfrom [Fra99].\\n6.1 Sorting\\nSorting is among most fundamental and well understood algorithmic problems. In this section\\nwe give reversible algorithms for comparison and counting sorts which match the time and space\\ncomplexities of know irreversible algorithms. It is especially interesting to see this is achievable\\ndespite the known entropy change during comparison sorts which give us a lower bound on their\\ntime complexity.\\n6.1.1 Comparison Sort\\nTheorem 6.1. A comparison sort destroying its input must consume Ω(lg n!) energy.\\nProof. Given a list of n unique items, there are n! permutations of that list, only one of which is\\nin sorted order. Thus the end-to-end entropy of a sorting function is lg(n!), resulting in an energy\\ncomplexity of Θ(lg n!). This was noted in [Fra99].\\n27\\nWe can achieve this energy bound with Merge Sort. Merge Sort takes in an array of numbers,\\nrecursively calls itself on half of the array until it reaches the sorted array of size 1. It then merges\\nthe returned arrays by iteratively comparing the smallest values in each array and moving it to the\\nbeginning of a new sorted array.\\nTheorem 6.2. Comparison sort destroying its input can be done in Θ(n lg n) time, Θ(n) space,\\nand Θ(n lg n) energy.\\nProof. Here we look at the standard merge sort which runs in Θ(n lg n) time and Θ(n) space. This\\nsorting algorithm recursively divides up the items to be sorted, then sorts increasingly larger groups\\nduring the merge step. For the merge step, one takes two sorted lists and starts comparing the\\nsmallest element. The smaller one is put into the new sorted list and the process continues. The\\nΘ(n lg n) comparisons will consume Θ(n lg n) energy. We also need to ensure that all of the entries\\nare moved into empty arrays, so no overwriting of values occurs. If one copies and deletes during\\nthis process there will be an extra factor of w in the energy cost.\\nIf we do not destroy the input and are careful with our algorithm, we can do better:\\nTheorem 6.3. Comparison sort, not destroying its input, if performed on an array of n w-bit\\nelements with lg n = O(w), can be done reversibly in Θ(n lg n) time, Θ(n) auxiliary space, and 0\\nenergy.\\nProof. This algorithm is a modification of Merge Sort. In summary, we will we augment each\\nelement of the array with its index in the array, and so-equipped shall reversibly merge sort the\\nelements in Θ(n lg n) time. After we remove these indices from the sorted array, the output of the\\nalgorithm will be the original array L and a sorted copy of the array Lsorted.\\nDuring a traditional Merge Sort, there are three main steps: dividing the array in two, recursing\\non each half of the array, and merging the two resultant arrays into a complete, sorted array.\\nThe first step, dividing an array in two, is reversed trivially: Given the two resultant lists of\\nsuch an operation L[r : s] and L[s+ 1 : t], the original subarray L[r : t] is their concatenation.\\nThe second step, recursing, will be reversible if our entire algorithm is reversible. We know the\\nbase case of sorting a size-1 array is reversible, and thus if steps 1 and 3 of our algorithm are also\\nreversible, then this recursive step will be as well.\\nThe third step, in contrast to the first two, presents us with some difficulties. Given a resultant,\\nfully-merged subarray Lsorted[r : t], it is not at all obvious how to go backwards, i.e. how to\\nreproduce the input subarrays Lsorted[r : s] and Lsorted[s + 1 : t]. Information will be lost in this\\nmerge step, and to allow our algorithm to be done reversibly, we must find some way to preserve\\nit.\\nOur augmentation makes this step possible. Before sorting, we transform L into a new array L′\\nof twice the size, which consists of the elements of L each augmented with their index in L. Thus,\\nthe elements of our array are 2-tuples (v, i) of each element’s value and original location in L. The\\nabove transformation is sufficient to make the merge step reversible. To see why, consider any step\\nin the algorithm in which we are trying to merge two sorted subarrays L[r :s] and L[s+1:t]. Denote\\nthe merge subroutine that we are trying to compute as Ms+ 1\\n2\\n(L[r : s], L[s + 1 : t]); that is, we are\\nmerging around a pivot s+ 12 , determined by which step of the algorithm we are presently carrying\\nout. All elements (v, i) with i < s + 12 must have come from L[r : s], and elements with i > s +\\n1\\n2\\nfrom L[s + 1 : t]. Given a resultant array Lsorted[r : t] = Ms+ 1\\n2\\n(Lsorted[r : s], Lsorted[s + 1 : t]), we\\ncan reverse the merge operation step-by-step simply by checking each element’s index against the\\npivot s+ 12 to determine where it came from. This enables us to construct two-way branches that\\n28\\nperform the merge in a way that is instantaneously reversible. Because the pivot is fixed for each\\nstep in the algorithm, no information is lost in computing and decomputing it, and thus this step\\nof the algorithm may be implemented reversibly with only constant additional auxiliary space.\\nThe output of the above algorithm is a list L′sorted of (v, i) tuples sorted in the v keys (whereas\\nour original list L′ was “sorted” in the i keys). Now we need only remove the auxiliary indices\\nfrom the elements (v, i) to produce our unaugmented sorted list Lsorted. This step must be handled\\nwith care to ensure that every step is reversible. We begin by reproducing the original array L\\nvia a single pass over L′sorted, a simple operation that has not yet destroyed any data. Next, we\\ncopy out Lsorted, the final, sorted array that we care about. What remains is to dismantle L\\n′\\nsorted,\\nand here we shall employ a special trick: We will perform a single pass over the original array L,\\nand for every value v encountered we will perform a logged binary search for this element in the\\nunaugmented sorted list Lsorted. When the element is found, we will know the value v, index i,\\nand the location of the element (v, i) in the augmented sorted array L′sorted. This is sufficient to\\ndestroy this element, setting its entry to zero before unrolling the log of our binary search. The\\ncomplete dismantling operation uses only Θ(lg n) additional logging space total, and only takes\\ntime Θ(n lg n), so our runtime and space consumption are preserved.\\nThis algorithm is instantaneously reversible at every step, and could be implemented using only\\nsimple for loops and two-way conditional branches. Thus, the algorithm is completely reversible\\nunder our model. Given an array of size n = O(2w) which occupies nw space in memory, we can\\nreversibly comparison-sort the array using Θ(nw) bits of auxiliary space in Θ(n lg n) time, matching\\nthe best irreversible algorithm to within constant-factors of space and time.\\nTheorem 6.4. Comparison sort, not destroying its input, can be done reversibly on an array of n\\nd-bit elements which require nd space in Θ(n lg n) time, Θ(nd) auxiliary space, and 0 energy.\\nProof. As we saw in the preceding theorem, reversible comparison sort is straightforward to perform\\nif we first augment each element in the array with a number corresponding to its index in the original\\nlist. When the size of the values d is Ω(lg n), then we attain the optimum space bound as the lg n-\\nsized indices get absorbed into the space cost of the d-bit elements. However, we are faced with a\\nconundrum when d is o(lg n).\\nTo handle this case, we shall employ counting sort in order to reduce the problem of sorting\\nL to the problem of sorting the unique keys of L. We utilize a reversible AVL tree (described in\\nSection 5.3) to achieve this.\\nThis algorithm works by reducing the array L only to its unique elements, to sort those elements,\\nand finally to perform a Counting Sort of the original array, consulting our sorted elements to\\ndetermine the final order. Let k be the number of distinct elements of L. We employ a reversible\\nAVL tree, with actions carefully specified so as to keep them reversible. First, we read the distinct\\nelements of L into the tree, bringing it to a size O(kd), and keeping an O(n)-bit uniqueness log and\\nan O(nd)-bit rotation log (see reversible AVL trees discussion) as we go. This step takes O(n lg n)\\ntime. Next, we apply Counting Sort on the original array (see Section 6.1.2), consulting the static\\ntree in O(lg n) time for each element and achieving the O(n lg n) runtime in this step as well. The\\noutput array may be copied and the entire algorithm reversed (Note: not an unrolling of a log, but\\nrather an execution of a reversed version of the algorithm) to leave us with our desired arrays L\\nand Lsorted.\\nThis algorithm works by reducing the array L only to its unique elements, to sort those elements,\\nand finally to perform a Counting Sort of the original array, consulting our sorted elements to\\ndetermine the final order. Let k be the number of distinct elements of L. We employ a reversible\\nAVL tree, with actions carefully specified so as to keep them reversible. First, we read the distinct\\n29\\nelements of L into the tree, bringing it to a size O(kd), and keeping an O(n)-bit uniqueness log and\\nan O(nd)-bit rotation log (see reversible AVL trees discussion) as we go. This step takes O(n lg n)\\ntime. Next, we apply Counting Sort on the original array (see Section 6.1.2), consulting the static\\ntree in O(lg n) time for each element and achieving the O(n lg n) runtime in this step as well. The\\noutput array may be copied and the entire algorithm reversed (Note: not an unrolling of a log, but\\nrather an execution of a reversed version of the algorithm) to leave us with our desired arrays L\\nand Lsorted.\\nIn terms of the intricate details glossed over above, the most involved are in the first step:\\nreversibly constructing a reversible AVL tree out of the unique elements of L. We proceed as\\nfollows: making a single pass over our array L, we add every element into the tree. If an element is\\nthe first of its exact value to be encountered, we store a corresponding uniqueness bit as true and\\nadd the element to the tree. If an element’s value already exists in our tree, we store its uniqueness\\nbit as false and move on (never adding duplicates to the tree). These n uniqueness bits allow us to\\nreverse the algorithm, as we know for which elements we modified the tree and on which ones we\\ndid not. By theorem 5.8, these insertions take Θ(nd) space and O(n lg n) time.\\nOnce the AVL tree is constructed, the rest of the algorithm is a straightforward Counting Sort\\nwith a slower Θ(lg k) lookup time, yielding the O(n lg k) time which is optimal for Comparison-\\nbased Sort. In addition to the fully-reversible AVL tree data structures, our algorithm employs only\\nsimple for-loop passes over the input and reversible two-way branching in the AVL tree, ensuring\\nits reversibility. The entire algorithm takes Θ(nd) space and O(n lg n) time, matching the best\\nirreversible comparison sorts up to constant-factors of space and time.\\nWe can also create reversible version of other sorting algorithms.\\nDuplicated-Insertion-Sort(A)\\nfori = 1 to A.Length:\\ncount = 0\\nforj = 1 to A.Length:\\nifA[i] > A[j]:\\ncount += 1\\nB[count] = A[i]\\nTheorem 6.5. Reversible Duplicated Insertion Sort runs in Θ(n2) time, Θ(n) space, and 0 energy.\\nProof. The primary adaption we will make to the algorithm is unrolling after each iteration of the\\nouter for loop. The for loops are simple for loops and thus incur only constant-factor overheads\\nwhen done reversibly. During each inner loop, we perform n comparisons, O(n) increments, and\\na single addition, because we know the output array, B, will be empty. This takes Θ(n) space in\\nthe log and after every iteration we will unroll all operations except for the addition to B. This\\nunrolling returns our counter back to zero. The counter and log are empty after each unrolling, so\\nwe are finished once we perform the final insertion into B.\\n6.1.2 Counting Sort\\nCounting sort involves counting the number of elements at or below a specific value, and then\\nrunning through them and adding them to an array based on how many elements are below them.\\nThis achieves Θ(n + k) time and space where k is the size of the maximum integer to be sorted.\\nPseudocode is given below.\\n30\\nCounting-Sort(A, k) :\\nfor j=1 to length[A]: //simple for loop, lg(length[A]) energy\\nC[A[j]]=C[A[j]]+1 //inc is free\\nfor i=1 to k: //simple for loop, lg k energy\\nC[i]=C[i]+C[i− 1] //addition is free\\nfor j=length[A] to 1: //simple for loop, lg(length[A]) energy\\nB[C[A[j]]]=A[j] //free if we assume the sorted elements are unique\\n// otherwise unconstrained replacement costs w energy\\nC[A[j]]=C[A[j]]-1 //dec is free\\ndelete(C) //free delete because we promised C is now empty\\nreturn(B)\\nlog Counting-Sort(A, k) :\\nfor j=1 to length[A]: //simple for loop, lg(length[A]) space\\nC[A[j]]=C[A[j]] + 1 //inc is free\\nfor i=1 to k: //simple for loop, lg k space\\nC[i]=C[i]+C[i− 1] //reversible and the answer is always available,\\nfor j=length[A] to 1: //simple for loop, lg(length[A]) space\\nB[C[A[j]]]=A[j] //free if we assume the sorted elements are unique\\n//otherwise unconstrained replacement costs w space\\nC[A[j]]=C[A[j]]-1 //dec is free\\ndelete(C) //free delete because we promised C is now empty\\nreturn(B)\\nTheorem 6.6. Counting Sort can be done in Θ(n + k) time, Θ(n + k) space, and Θ(wn + lg k)\\nenergy.\\nProof. The time and space complexity comes from maintaining and iterating over the input as well\\nas an array of size k which represents the range of the numbers. The unconstrained replacement\\nwhen sorting the numbers results in an O(wn) energy cost and irreversibly running the for loop\\nover the range requires Θ(lg k) energy, because we lose the index which ranged up to k.\\nTheorem 6.7. If all entries are unique, then Counting Sort has an energy complexity of Θ(lg n+\\nlg k) energy.\\nProof. If all the entries in the Counting Sort are unique, then the final for loop never overwrites a\\nprevious value, thus not incurring that cost. However, we still must pay the cost for the irreversible\\nfor loop, yielding an energy complexity of Θ(lg n+ lg k).\\nTheorem 6.8. Reversible Counting Sort can be done in Θ(n + k) time, Θ(n + k) space, and 0\\nenergy.\\nProof. Increments, addition, and simple for loops can all be done reversibly with constant-factor\\noverheads. The assignment in the third loop is the only potential problem; however, we cannot have\\nmore than n items overwritten, so if we log these values we suffer no more than a constant-factor\\noverhead in space.\\n31\\n6.2 Graph Algorithms\\nFrank [Fra99] argues that Breadth-first Search and Depth-first search can be done reversibly. We\\nreproduce this result in our model and give a different analysis. Breadth-first search requires\\nkeeping a list of visited nodes and a queue of nodes to be visited. The visited node is selected from\\nthe queue and then checked to see if it is the target node. If not, its neighbors are checked to see\\nif they are in the list of visited nodes, and if not they are added to the queue.\\nTheorem 6.9. Breadth-first Search runs in Θ(V +E) time, Θ(V ) space, and Θ(wV +E) energy.\\nProof. In a BFS we visit every node at most once and we check to see if a node should be queued\\nup no more than once for every edge. We also have this number of comparisons which each require\\none bit of energy if done irreversibly. Further, an irreversible queue requires the creation and\\ndestruction of pointers, and the list of visited nodes must be cleared, leading to Θ(V ) words being\\noverwritten.\\nTheorem 6.10. Reversible Breadth-first Search can runs in Θ(V ) time, Θ(V + E) space, and 0\\nenergy.\\nProof. Comparison and pointer following can be done reversibly. If the queue maintained as a\\ndoubly-linked list, then we can add and remove things reversibly via pointer swapping. Copying\\nvalues into a list is also a reversible operation.\\nCorollary 6.11. Reversible Depth First Search can runs in Θ(V + E) time, Θ(V ) space, and 0\\nenergy.\\nProof. A DFS works exactly the same as a BFS except a stack of nodes to be visited is used instead\\nof a queue. Stacks can be maintained reversibly with the same time, space, and energy complexity\\nas queues, so the resulting DFS algorithm will have the same time, space and energy complexity\\nas the BFS.\\n6.3 Bellman-Ford\\nBellman-Ford() :\\nfor each v in vertices:\\nif v 6= source then v.distance =∞ //rest are 0\\nv.predecessor = null\\nfor i = 1 to vertices.size −1 :\\nfor each edge (u, v) with weight w in edges:\\n(w, u.distance, tempDist) = (w, u.distance, u.distance+w)\\nif tempDist < v.distance:\\nv.distance = tempDist\\n(u, v.predecessor) = (u, u) //overwriting costs w energy\\nfor each edge (u, v) with weight w in edges:\\n(w, u.distance, tempDist) = (w, u.distance, u.distance +w)\\nif tempDist > v.distance:\\nprint “negative weight cycle detected”\\nfail\\n32\\nTheorem 6.12. Bellman-Ford runs in Θ(V E) time, Θ(V + E) space, and Θ(V Ew) energy.\\nProof. In the primary nested for loops which iterate over the verticies and edges, we potentially\\noverwrite the distance between two nodes with increasingly smaller distances, and overwrite the\\nedges making up the shortest path. These each require w energy to perform and are potentially\\ncalled Θ(V E) times. This for loop is also what dominates the time complexity, as can be seen in\\nany standard analysis. The algorithm requires storing the graph, as well as two arrays of size V ,\\nthus running in Θ(V + E) space.\\nTheorem 6.13. Reversible Bellman-Ford runs in Θ(V E) time, Θ(V E) space, and 0 energy.\\nProof. First, at the end of each for loop involving the variable tempDist, we can subtract distance[u]\\nand w from the value to ensure it is zero before we next need to set it, thus avoiding the energy\\ncost of zeroing it out. Rewriting v.distance and v.predecessor do not appear to have an obvious\\nway of making them reversible. We can; however, simply log the entire algorithm and pay the cost\\nin writing their values to our log sheet whenever they are overwritten. This potentially requires\\nΘ(V E) values to be stored in the log, resulting in a total space complexity of Θ(V E).\\n6.4 Floyd-Warshall\\nThe Floyd-Warshall algorithm calculates the shortest paths between all pairs of nodes in Θ(V 3) time\\nand Θ(V 2) space. Aside from initializing the path matrix to contain the edge weights, pseudocode\\nfor the algorithm is as follows [?]:\\nFloydWarshall():\\nfor k = 1 to n:\\n//simple for loop adds lg n extra energy or space\\nfor i = 1 to n: //another lg n\\nfor j = 1 to n : //one last lg n giving a total of lg3 n after nesting\\npath[i][j] = min(path[i][j], path[i][k] + path[k][j])\\n//unconstrained replacement takes w energy\\nFrank [Fra99] argues that the Floyd-Warshall algorithm can be adapted to run reversibly with\\nΘ(V 3) space. This is a substantial increase in space to make the program reversible and thus save\\nenergy.\\nTheorem 6.14. Floyd-Warshall runs in Θ(V 3) time, Θ(V 2) space, and Θ(V 3w) energy.\\nProof. The algorithm acts on a matrix of size v2 and performs Θ(V 3) updates which consist of a\\ncomparison, addition, and possible replacement. Each replacement requires w energy, yielding a\\ntotal energy complexity of Θ(V 3w).\\nTheorem 6.15. Reversible Floyd-Warshall runs in Θ(V 3) time, O(V 3) space, and 0 energy.\\nProof. Simple for loops, as well as addition and minimum can all be performed reversibly; however,\\nwe will need to log the replacements done during the updates for the algorithm. Thus running the\\nalgorithm reversibly will require O(V 3) space, despite constant-factor overheads in time.\\n33\\n6.5 All Pairs Shortest Path via (min,+) Matrix Multiplication\\nAnother algorithm for solving APSP involves using the adjacency matrix representation of a graph\\nA and noticing that the relaxations over the edges can be expressed by calculating a new matrix,\\nC, whose entries are given by cij =\\nk\\nmin(aik +akj). Further, this operation is associative, so we can\\nspeed up the calculation by using repeated squaring. Thus we have O(lg V ) iterations over (V 2)\\nelements which take O(V ) time to compute. Frank [Fra99] claims without proof that this leads to\\na reversible algorithm that runs in Θ(V 3 lg V ) time and Θ(V 2 lg V ) space. We give a proof of this\\nresult.\\nTheorem 6.16. Reversible APSP using repeated squaring with (min,+) matrix multiplication runs\\nin O(V 3 lg V ) time, O(V 2 lg V ) space, and 0 energy.\\nProof. In this algorithm, information is lost when we overwrite our previous entries after calculating\\nthe minimum, an irreversible operation. To make this reversible, we can simply store a log of all\\nintermediate matrices, using O(V 2 lg V ) space. From each intermediate state, we can recompute\\nthe values needed to unroll the last step of the matrix squaring. We need only recompute these\\nvalues a single time, leading to a constant-factor increase in time, giving the desired result. Let\\nus take a closer look at an individual matrix multiply. Here we can loop over every new entry in\\nour matrix and calculate each term individually. Given a graph represented by adjacency matrix\\nW = (wi,j), and a matrix L\\n(m) = (l\\n(m)\\ni,j ) representing the shortest paths between two vertices with\\npath length at most m. Each entry is updated as\\nl\\n(m+1)\\ni,j =1≤k≤|V |\\nmin\\n(\\nl\\n(m)\\ni,j + wk,j\\n)\\nThis section of the algorithm perform Θ(V ) reversible additions but also O(V ) irreversible oper-\\nations to perform the min over all of the entries. We can log all of these operations with O(wV )\\nspace overhead and then proceed to unwind leaving our new matrix entry. Now to unwind a whole\\nmatrix multiply, we will simply recalculate each individual entry, delete the copy, and unwind. Now\\nthat we know we can reverse each matrix multiply given the previous one, we will simply store all\\nO(lg n) intermediate matrices which are calculated. This leads to O(V 2 lg V ) space and a constant\\nfactor overhead in time.\\nTheorem 6.17. APSP using repeated squaring with (min,+) matrix multiplication runs in O(V 3 lg V )\\ntime, O(V 2) space, and O(wV 3 lg V ) energy.\\nProof. The time analysis is the same as with the reversible case. In terms of space, we only\\nneed to store the current matrix, and thus only need O(V 2) space. At every calculation of a\\nnew matrix element, we overwrite the previous one, expending a word of energy for each of these\\ncomputations.\\nWe now present a new variation on APSP which demonstrates a non-trivial trade-off between\\nenergy and space. By exploiting reversible subroutines, we’re able to reach the APSP with repeated\\nsquaring bounds on time and space, but beat it in energy cost. The reversible, semi-reversible, and\\nstandard APSP using repeated squaring demonstrate there are semi-reversible algorithms that\\nactually achieve bounds not reached by the fully reversible or fully irreversible counterparts.\\nTheorem 6.18. Semi-reversible APSP using repeated squaring with (min,+) matrix multiplication\\nruns in O(V 3 lg V ) time, O(V 2) space, and O(wV 2 lg V ) energy.\\n34\\nProof. To begin, we will examine how each individual entry in the matrix is updated. Say we have\\na graph represented by adjacency matrix W = (wi,j), and a matrix L\\n(m) = (l\\n(m)\\ni,j ) representing the\\nshortest paths between two vertices with path length at most m. Each entry is updated as\\nl\\n(m+1)\\ni,j =1≤k≤|V |\\nmin\\n(\\nl\\n(m)\\ni,j + wk,j\\n)\\nThis subroutine runs in O(V ) time and O(wV ) energy and can thus be trivially made reversible\\nby logging everything, using O(V ) time and space. We replace our normal update function with\\nthe new reversible one, and by Theorem 3.6 we have a new, more energy efficient algorithm. The\\nsubroutine does not use asymptotically more time than before, the temporary use of O(V ) space is\\nmuch smaller than that needed to store the matrices and is freed upon completion of the subroutine,\\nand the energy cost drops by a factor of V which reduces the algorithms total energy cost by a\\nfactor of V .\\n7 Future Directions\\nThis paper built up a framework for designing and analyzing the energy cost of algorithms caused\\nby irreversibility, and started the quest for positive results for basic algorithms and data structures.\\nIn many cases, we obtained fully reversible versions of algorithms, but other problems seem more\\nresistant. For example, is there a reversible all-pairs shortest path algorithm with only constant\\nfactor overheads in time and space? We managed to give a reduced-energy semi-reversible algorithm\\nfor the problem, but a fully reversible algorithm still seems elusive. Shortest-path algorithms more\\ngenerally seem like a category that are difficult to make reversible, as they use very little space\\nand make frequent use of rewriting old values. We anticipate other graph problems such as max-\\nflow/min-cut may also be challenging and interesting for similar reasons.\\nThere are more fundamental algorithms that should be given high priority given their use\\nin many other results: hashing, predecessor data structures (e.g., van Emde Boas trees), max-\\nflow/min-cut, Fast Fourier Transforms, and dynamic programming. Geometric algorithms offer\\nmore nontrivial challenges to attain reversibility, such as line intersection, orthogonal range finding,\\nconvex hull, and Delaunay triangulations. We also see the field of machine learning being an\\ninteresting target for analysis in the semi-reversible model: these algorithms often have significantly\\nhigher time complexities than space complexities, fundamental updates (such as Bayes’ rule) which\\nappear reversible, and many conditional updates or data overwrites.\\nOne important question for any practical application is how to deal with long-running programs.\\nAlthough we are perfectly happy to log some auxiliary information during the execution of a specific\\nprogram, it may be more problematic to maintain reversibility for the entire operating system of\\na computer or a long-lived database. This is an area we believe ideas like semi-reversibility and\\nperiodic rebuilding will become particularly important.\\nThere are some areas where we see slight extensions of the model opening up interesting ques-\\ntions. First, incorporating randomness seems a practical necessity and carries interesting thermo-\\ndynamic implications depending on how it is modeled. Assuming there is an energy cost associated\\nwith the production of randomness (say, equal to the number of random bits), this may give fur-\\nther reason to investigate exactly how much randomness is needed for an algorithm’s correctness.\\nStreaming algorithms and other models where the working space is much smaller than the problem\\ninput seem like a rich source of problems. Because we now use sublinear space, our trivial transform\\nis no longer applicable. Further, the larger the gap in space and time, the less ability we have to\\naccrue garbage. Finally, succinct data structures, which try to minimize the bits of space used up\\n35\\nto sublinear factors, seem like another challenge: many of our transforms double or triple the space\\nbeing used by an algorithm, while in the succinct setting, this overhead must be considered.\\nFinally, a major open direction is to obtain lower bounds. The additional constraints on semi-\\nreversible algorithm design might allow showing algorithms cannot be obtained without some min-\\nimum time-space-energy trade-off.\\nAcknowledgments\\nWe thank Martin Demaine and Kevin Kelley for helpful early discussions about this project, in\\nparticular early formulations of the models of computation. We also thank Maria L. Messick and\\nLicheng Rao for useful discussion and help clarifying our pseudocode and model.\\nReferences\\n[AGS15] Scott Aaronson, Daniel Grier, and Luke Schaeffer. The classification of reversible bit\\noperations. CoRR, abs/1504.05155, 2015.\\n[Alb10] Susanne Albers. Energy-efficient algorithms. Commun. ACM, 53(5):86–96, May 2010.\\n[Axe11] Holger Bock Axelsen. Clean translation of an imperative reversible programming lan-\\nguage. In Compiler Construction, pages 144–163. Springer, 2011.\\n[BAP+12] Antoine Berut, Artak Arakelyan, Artyom Petrosyan, Sergio Ciliberto, Raoul Dillen-\\nschneider, and Eric Lutz. Experimental verification of Landauer’s principle linking\\ninformation and thermodynamics. Nature, 483(7388):187–189, 2012.\\n[Ben73] Charles H. Bennett. Logical reversibility of computation. IBM Journal of Research and\\nDevelopment, 17(6):525–532, 1973.\\n[Ben89] Charles H. Bennett. Time/space trade-offs for reversible computation. SIAM Journal\\non Computing, 18(4):766–776, August 1989.\\n[BGL+98] Charles H. Bennett, Pe´ter Ga´cs, Ming Li, Paul MB Vita´nyi, and Wojciech H. Zurek.\\nInformation distance. IEEE Transactions on Information Theory, 44(4):1407–1423,\\n1998.\\n[BTV01] Harry Buhrman, John Tromp, and Paul Vitnyi. Time and space bounds for reversible\\nsimulation. In Fernando Orejas, Paul G. Spirakis, and Jan van Leeuwen, editors, Pro-\\nceedings of the 28th International Colloquium on Automata, Languages and Program-\\nming, volume 2076 of Lecture Notes in Computer Science, pages 1017–1027. Springer\\nBerlin Heidelberg, 2001.\\n[CLRS01] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Intro-\\nduction to Algorithms. The MIT Press, 2 edition, 2001.\\n[Cyc12] Cyclos Semiconductor. Cyclos Semiconductor announces first commercial imple-\\nmentation of resonant clock mesh technology. http://www.cyclos-semi.com/news/\\nfirst commercial implementation.html, February 2012.\\n[eMa14] eMarketer. Smartphone users worldwide will total 1.75 billion in 2014. http://www.\\nemarketer.com/Article/Smartphone-Users-Worldwide-Will-Total-175-Billion-2014/\\n1010536, January 2014.\\n[Fra99] Michael Patrick Frank. Reversibility for efficient computing. PhD thesis, Massachusetts\\nInstitute of Technology, 1999.\\n[FT82] E. Fredkin and T. Toffoli. Conservative logic. International Journal of Theoretical\\nPhysics, 21, 1982.\\n36\\n[JMR05] Ravi Jain, David Molnar, and Zulfikar Ramzan. Towards a model of energy complex-\\nity for algorithms. In Proceedings of the 2005 IEEE Wireless Communications and\\nNetworking Conference, volume 3, pages 1884–1890, 2005.\\n[KA10] Vijay Anand Korthikanti and Gul Agha. Towards optimizing energy costs of algorithms\\nfor shared memory architectures. In Proceedings of the 22nd Annual ACM Symposium\\non Parallelism in Algorithms and Architectures, pages 157–165, New York, NY, USA,\\n2010. ACM.\\n[KAG11] Vijay Anand Korthikanti, Gul Agha, and Mark R. Greenstreet. On the energy complex-\\nity of parallel algorithms. In Guang R. Gao and Yu-Chee Tseng, editors, Proceedings\\nof the 2011 International Conference on Parallel Processing, pages 562–570, 2011.\\n[KBSW11] Jonathan G. Koomey, Stephen Berard, Maria Sanchez, and Henry Wong. Implications\\nof historical trends in the electrical efficiency of computing. IEEE Annals of the History\\nof Computing, 33(3):46–54, March 2011.\\n[Kis91] Gloria Kissin. Upper and lower bounds on switching energy in vlsi. J. ACM, 38(1):222–\\n254, January 1991.\\n[Koo11] Jonathan G. Koomey. Growth in data center electricity use 2005 to 2010. Analytics\\nPress, August 2011.\\n[Lan61] R. Landauer. Irreversibility and heat generation in the computing process. IBM Journal\\nof Research and Development, 5(3):261–269, 1961.\\n[LCB11] B. Lambson, D. Carlton, and J. Bokor. Exploring the thermodynamic limits of compu-\\ntation in integrated systems: magnetic memory, nanomagnetic logic, and the Landauer\\nlimit. Phys Rev Lett, 107(1):010604, 2011.\\n[LD82] Christopher Lutz and Howard Derby. Janus: a time-reversible language. Caltech class\\nproject, 1982.\\n[Lec63] Yves Lecerf. Logique mathematique. machines de Turing re´versibles. re´cursive insol-\\nubilite´ en n ∈ N de l’e´quation u = θnu, ou` θ est un “isomorphisme de codes”. note.\\nComptes rendus hebdomadaires des se´ances de l’Academie des sciences, 257(18):2597–\\n2600, October 1963.\\n[LMT97] Klaus-Jo¨rn Lange, Pierre Mckenzie, and Alain Tapp. Reversible space equals deter-\\nministic space. In Proceedings of the 12th Annual IEEE Conference on Computational\\nComplexity, pages 45–50, 1997.\\n[LPSG07] James Ladyman, Stuart Presnell, Anthony J Short, and Berry Groisman. The connec-\\ntion between logical and thermodynamic irreversibility. Studies In History and Philoso-\\nphy of Science Part B: Studies In History and Philosophy of Modern Physics, 38(1):58–\\n79, 2007.\\n[LV92] Ming Li and Paul Vita´nyi. Theory of thermodynamics of computation. In Proceedings\\nof the IEEE Physics of Computation Workshop, pages 42–46, 1992.\\n[LV96] Ming Li and Paul Vita´nyi. Reversibility and adiabatic computation: trading time and\\nspace for energy. Proceedings of the Royal Society of London A: Mathematical, Physical\\nand Engineering Sciences, 452(1947):769–789, 1996.\\n[LV08] Ming Li and Paul Vita´nyi. An introduction to Kolmogorov Complexity and its Applica-\\ntions. Springer, 3rd edition, 2008.\\n[Mar09] O. J. E. Maroney. Generalizing Landauer’s principle. Phys. Rev. E, 79:031105, Mar\\n2009.\\n[MR11] Matthew Morrison and Nagarajan Ranganathan. Design of a reversible alu based on\\nnovel programmable reversible logic gate structures. In Proceedings of the 2011 IEEE\\nComputer Society Annual Symposium on VLSI, pages 126–131, 2011.\\n37\\n[NTR10] M. Nachtigal, H. Thapliyal, and N. Ranganathan. Design of a reversible single precision\\nfloating point multiplier based on operand decomposition. In Proceedings of the 10th\\nIEEE Conference on Nanotechnology, pages 233–237, Aug 2010.\\n[NTR11] M. Nachtigal, H. Thapliyal, and N. Ranganathan. Design of a reversible floating-point\\nadder architecture. In Proceedings of the 11th IEEE Conference on Nanotechnology,\\npages 451–456, Aug 2011.\\n[Pie00] Barbara Piechocinska. Information erasure. Phys. Rev. A, 61:062314, May 2000.\\n[SAI+13] Visvesh S. Sathe, Srikanth Arekapudi, Alexander T. Ishii, Charles Ouyang, Marios C.\\nPapaefthymiou, and Samuel Naffziger. Resonant-clock design for a power-efficient, high-\\nvolume x86-64 microprocessor. IEEE Journal of Solid-State Circuits, 48(1):140–149,\\n2013.\\n[TAG12] Michael Kirkedal Thomsen, Holger Bock Axelsen, and Robert Glu¨ck. A reversible pro-\\ncessor architecture and its reversible logic design. In Proceedings of the 3rd International\\nConference on Reversible Computation, pages 30–42, Gent, Belgium, 2012.\\n[VAF+98] Carlin Vieri, M Josephine Ammer, Michael Frank, Norman Margolus, and Tom Knight.\\nA fully reversible asymptotically zero energy microprocessor. In Proceedings of the\\nPower Driven Microarchitecture Workshop, pages 138–142, 1998.\\n[Vel11] Chris Velazco. AMDs new FX processor reaches world\\nrecord clock speed. http://techcrunch.com/2011/09/13/\\namds-new-fx-processor-reaches-world-record-clock-speed/, September 2011.\\n[Vie99] Carlin James Vieri. Reversible computer engineering and architecture. PhD thesis,\\nMassachusetts Institute of Technology, 1999.\\n[Vit05] Paul Vita´nyi. Time, space, and energy in reversible computing. In Proceedings of the\\n2Nd Conference on Computing Frontiers, CF ’05, pages 435–444, New York, NY, USA,\\n2005. ACM.\\n[Wil00] Ryan Williams. Space-efficient reversible simulations. Technical report, DIMACS REU\\nreport, 2000. http://web.stanford.edu/∼rrwill/spacesim9 22.pdf.\\n[YDG+07] Simon Yates, Ellen Daley, Benjamin Gray, J. P. Gownder, and Rachel Batiancila.\\nWorldwide PC Adoption Forecast, 2007 To 2015. Forrester, June 2007. http://www.\\nforesightfordevelopment.org/sobipro/download-file/46-724/54.\\n[Yok10] Tetsuo Yokoyama. Reversible computation and reversible programming languages. Elec-\\ntronic Notes in Theoretical Computer Science, 253(6):71–81, 2010.\\n[Zur89] W. H. Zurek. Thermodynamic cost of computation, algorithmic complexity and the\\ninformation metric. Nature, 341:119–124, September 1989.\\n38\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd17'), 'authors': 'Barratt, Jeffrey, Zhang, Brian', 'year': '2020', 'title': 'Cache-Friendly Search Trees; or, In Which Everything Beats std::set', 'full_text': 'Cache-Friendly Search Trees;\\nor, In Which Everything Beats std::set\\nJeffrey Barratt\\njbarratt\\nBrian Zhang\\nbhz\\nJuly 4, 2019\\n1 Introduction\\nWhile a lot of work in theoretical computer science has gone into optimizing the runtime and\\nspace usage of data structures, such work very often neglects a very important component\\nof modern computers: the cache. In doing so, very often, data structures are developed that\\nachieve theoretically-good runtimes but are slow in practice due to a large number of cache\\nmisses. In 1999, Frigo et al. [1999] introduced the notion of a cache-oblivious algorithm:\\nan algorithm that uses the cache to its advantage, regardless of the size or structure of\\nsaid cache. Since then, various authors have designed cache-oblivious algorithms and data\\nstructures for problems from matrix multiplication to array sorting [Demaine, 2002]. We\\nfocus in this work on cache-oblivious search trees; i.e. implementing an ordered dictionary\\nin a cache-friendly manner. We will start by presenting an overview of cache-oblivious\\ndata structures, especially cache-oblivious search trees. We then give practical results using\\nthese cache-oblivious structures on modern-day machinery, comparing them to the standard\\nstd::set and other cache-friendly dictionaries such as B-trees.\\n2 The Ideal-Cache Model\\nTo discuss caches theoretically, we first need to give a theoretical model that makes use of\\na cache. The ideal-cache model was first introduced by Frigo et al. [1999]. In this model, a\\ncomputer’s memory is modeled as a two-level hierarchy with a disk and a cache. The cache\\nhas size M and is split into blocks of size B each1. The cache is assumed to be:\\n(1) fully-associative: any block in the disk can be moved into any block of the cache.\\n(2) ideal: the cache “knows” which blocks of the disk will be accessed in the future, and\\nthus always evicts blocks from the cache optimally.\\n1The letter B will conflict later on with the B in B-tree. This will be annoying, but we continue anyway\\nand hope the reader is not too confused.\\n1\\nar\\nX\\niv\\n:1\\n90\\n7.\\n01\\n63\\n1v\\n1 \\n [c\\ns.D\\nS]\\n  2\\n Ju\\nl 2\\n01\\n9\\n(3) tall: M = Ω(B2).\\nThe running time of an algorithm is then measured by the number of memory transfers that\\nit must perform, with the idea that, in practice, transferring blocks of memory is significantly\\nslower than actually running an algorithm that counting memory transfers is a reasonable\\nmetric. Some justification of the assumptions is necessary—in practice, caches do not know\\nthe future; instead, a heuristic is used. Two common heuristics used are the LRU (Least\\nRecently Used) and FIFO (First-In First-Out) heuristics: in the former, the least-recently\\naccessed block is evicted when the cache is full; in the latter, the least-recently added block\\nis evicted. Further, practical caches are not fully associative. In the other extreme, a 1-way\\nassociative cache is one in which each block of the disk can only appear in one location in\\nthe cache. At first glance, neither of these seem close to the ideal-cache model. However,\\nDemaine [2002] gives the following results:\\nTheorem 1 (Demaine [2002], Corollary 1). Suppose that the number of memory transfers\\ndepends only polynomially on the size M of the cache; in particular, that halving M will\\nonly increase the complexity by a constant factor. Then an LRU or FIFO cache only uses a\\nconstant factor more memory transfers than an ideal cache.\\nTheorem 2 (Demaine [2002], Lemma 2). For some α > 0, a fully associative LRU cache of\\nsize αM and block size B can be simulated in M space with O(1) expected memory access\\ntime to any block.\\nIn particular, such a cache would fit completely into our ideal cache of size M and be 1-way\\nassociative (since we have O(1) expected memory access); thus, assumption (1) is reasonable.\\nFrigo et al. [1999] also show that an algorithm that is optimal in this two-level ideal cache\\nmodel is also optimal in a memory hierarchy with more than two levels of caches. For\\nsimplicity, we will omit the formalism and refer readers to the original paper for the details\\nof this result.\\nThe results above, combined, show the power of the ideal-cache model: an algorithm that\\nis optimal in the ideal-cache model is also optimal (to within a constant factor) in any LRU\\nmemory hierarchy.\\n3 Cache-Oblivious Model and Algorithms\\nThe cache-oblivious model was also first introduced by Frigo et al. [1999]. The idea behind\\nthe model is simple: assume we do not know the block size B or cache size M of our computer\\nand want to extend our algorithms to work on arbitrary architectures. We perform analyses\\non our algorithms using the above ideal-cache model and an unspecified block size B, which\\nwill become a variable in our memory transfer equations. The overall goal of designing\\ncache-oblivious algorithms is to match the time bound of an algorithm designed with the\\nblock size B in mind; for instance, a B-tree with nodes limited to size B − 1 and 2B − 1, a\\nconstant factor times the size of B, results in ideal bounds on memory transfers to complete\\n2\\nrelevant operations. We will go deeper into this analysis with examples later in this section\\nas well as in the following sections.\\nWe will begin by exploring the process of scanning an array in Section 3.1 followed by an\\nattempt at binary search with sub-optimal memory transfer bounds in Section 3.2. This\\nsection will give a point of reference for analyses later in the paper.\\n3.1 Cache-Oblivious Analysis of Array Traversal\\nWhile this is a simple problem with a simple solution which performs optimally regardless\\nof the cache size, the analysis of this algorithm is useful. A traversal of a contiguous block\\nof memory, usually expressed as an array, is an example of a cache-oblivious algorithm; it\\nperforms optimally regardless of the cache size and structure. We will now prove bounds on\\nthe memory usage of this simple case.\\nTheorem 3 (Demaine [2002], Theorem 1). Scanning N elements stored in a contiguous\\nsegment of memory costs at most ⌈N/B⌉ + 1 memory transfers.\\nThe proof of this theorem is obvious; to read all the elements in the array, we must scan\\neach element, which are held in N separate memory positions. However, since these memory\\npositions are adjacent in memory, we can access blocks of B elements with only one memory\\ntransfer, meaning that we only need ⌈N/B⌉+1 memory transfers; the 1 comes from overflow\\nof the end of the array onto the next block.\\nEven with this trivial example, we see the power of designing cache-oblivious algorithms:\\nwith the same data structure or algorithm, we can produce an algorithm that uses the\\nasymptotically-minimal number of memory transfers to accomplish a task without tuning\\nparameters and regardless of the block size B.\\n3.2 Cache-Oblivious Analysis of Binary Search\\nWhen we perform a standard binary search on a sorted array, we can find elements in the\\noptimal O(logn) runtime. However, performing optimally with the number of comparisons\\nor computer instructions does not necessarily guarantee optimal memory usage.\\nTheorem 4 (Demaine [2002], Theorem 3). Binary search on a sorted array incurs O(logN−\\nlogB) memory transfers.\\nThe proof of this theorem is again easy to see; imagine splitting the sorted array into blocks\\nof size B. We access logN elements during our search, with the distance between element\\naccesses within the array halving each time. For the final logB elements, all accesses in\\nthe search will be made within that same block. Thus we need O(logN − logB) memory\\ntransfers to complete a binary search of a sorted array.\\nAs we will see later in the paper, the lower bound of finding an element in a sorted set is\\nO(logBN), meaning that the naive binary search algorithm is suboptimal in its total number\\n3\\nof memory transfers. We will improve on the naive binary search asymptotic memory transfer\\ncost later in this paper.\\n4 Cache-Oblivious Search Trees\\nIn this section, we will present a design of a cache-oblivious search tree, as given by Prokop\\n[1999] and Brodal et al. [2002].\\n4.1 Static Search Trees: The van Emde Boas Layout\\nWe begin by supposing that we had access to all the elements that we wish to put in our\\nbinary search tree. In this section, we will present a binary search tree layout that achieves an\\noptimal average-case number of memory transfers. This structure was first given by Prokop\\n[1999] as part of his master’s thesis, but he apparently considered the proofs so trivial that\\nthey were omitted. We thus present the proofs as given by Demaine [2002].\\nTheorem 5 (Demaine [2002], Theorem 7). In a search tree of size N , starting from an\\nempty cache, Ω(logBN) memory transfers are required to search for an element.\\nProof. The sorted position of one of N elements contains Θ(logN) bits of information. A\\nsingle block of size B can be used to recover only Θ(logB) bits of information; in particular,\\nthe sorted location of our desired element among the Θ(B) elements in the block. Thus, to\\nrecover the Θ(logN) bits of information required to determine a given element’s location in\\nthe sorted order, we require Ω(logN/ logB) = Ω(logBN) memory transfers.\\nAssume WLOG that N is a power of 2; if not, pad the list of elements up to a power of 2.\\nThis incurs at most constant-factor loss. The van Emde Boas (vEB) layout2 for a complete\\nbinary search on N = 2H nodes is defined recursively as follows.\\n(1) If H = 1 then there is only one node. Put it in memory.\\n(2) If H > 1 then let 2` be the largest power of 2 strictly less than H, and let m = H − 2`.\\nLay out the top m layers of the tree in the van Emde Boas layout, followed, in order, by\\nthe 2m subtrees of the top m layers (each of which has 22\\n` −1 nodes), also in vEB layout.\\nA visualization of the vEB layout is given in Demaine [2002] on page 16. Notice that, not\\ncounting the root node, a vEB layout stores a “tree block” of size B (i.e. induced subtrees\\nof a subset of nodes) contiguously in memory, except at the root.\\nTheorem 6 (Demaine [2002], Theorem 8). Searching in a tree in vEB layout takes O(logBN)\\nmemory transfers.\\n2As far as we are aware, the van Emde Boas layout was not discovered by van Emde Boas. The name,\\nto our knowledge, comes because of its similarity to the van Emde Boas tree, which looks similar but has\\nnothing to do with cache locality.\\n4\\nProof. After the root, reading one block of memory will allow one to descend through\\nΘ(logB) layers of the complete binary search tree, since a memory block of size B will\\nstore a subtree of size Θ(logB) in the layout. Thus, descending through O(logN) layers\\ntakes O(logN/ logB) = O(logBN).\\nIt is also useful to note that a vEB layout enables fast inorder traversal of nodes. In particular,\\nwe can state the following result, which Brodal et al. [2002] does not even bother giving a\\nnumber, and instead simply writes in the body of the text:\\nTheorem 7 (Brodal et al. [2002], with a simplifying assumption). Assuming that M > H,\\nan inorder traversal starting at any node v in a vEB layout takes time O(logBN +S(v)/B),\\nwhere S(v) is the number of elements in the subtree at v.\\nProof. A block of size B, again, stores Θ(logB) layers of the search tree. Suppose that our\\ninorder traversal is currently at some node u. Suppose we always use the cache to store the\\nblocks that include the current path from the root down to u. Adding these blocks to the\\ncache at the beginning of the traversal takes time O(logBN). Note that this takes at most\\nH blocks of memory, and guarantees that a block that is evicted from the cache will not ever\\nbe needed again, since we’re doing an in-order traversal. Further, during the traversal, a\\nsubtree of size B nodes will be accessed contiguously; therefore, we will only need to load new\\nblocks into memory every Θ(B) nodes. Adding everything up gives the desired bound.\\nThe assumption M >H only breaks down for trees with 2M nodes, which won’t even fit into\\nthe disk of any reasonable machine, so we consider it reasonable enough.\\n4.2 Dynamic Search Trees\\nThe difficulty of using the above vEB layout to make a dynamic search tree is that we\\ncannot rebalance the tree in the same way we would rebalance, say, a red-black or splay\\ntree: a rotation would cause entire subtrees to be moved around, and in the vEB layout,\\nthis would require physically moving the elements instead of merely rewiring pointers. In\\nthis section, we present the dynamic cache-oblivious search tree of Brodal et al. [2002].\\nThis search tree does not quite give the ideal runtime of O(logBN) memory transfers on\\nall operations, but it comes somewhat close: searches take O(logBN), and insertions take\\namortized O(logBN + (logN)2/B) memory transfers. Although this is not information-\\ntheoretically ideal, this is, to our knowledge, the best-known result for a BST that has the\\nideal O(N/B) memory transfers for a full in-order traversal. (There are other, more complex\\ndata structures that have O(logBN) insertions and deletions, but lose this O(N/B) traversal\\nruntime). For simplicity and brevity, we neglect deletions; they can be managed by a similar\\nrebalancing technique that is given in Brodal et al. [2002].\\nThe data structure works by storing all the elements in a complete vEB layout with 2H > N\\nnodes, some of which may be empty, and periodically rebalancing subtrees. Pick a density\\nparameter τ1 ∈ [1/2,1), and for i = 2, . . . ,H define τi = τi−1 +∆ where ∆ = (1− τ1)/(H − 1), so\\nthat the τi are evenly spaced in the interval [τ1,1]. Let N(v) be the number of nodes in the\\n5\\nsubtree rooted at v, d(v) be the depth of node v, and S(v) = 2H−d(v)+1 − 1 denote the size of\\nthe complete subtree rooted at v, including layers of the tree that may not yet be occupied\\nby any nodes.\\nInsertion works as follows: compute the location u at which the new key should be inserted.\\nIf u is within the tree (i.e. at depth at most H), then place u in the correct spot. Otherwise,\\nlet v be the nearest ancestor of u such that N(v) < τd(v)S(v), or the root of the tree if no such\\nancestor v exists. This is essentially the closest ancestor to u that has a smaller percentage\\nof the nodes below it filled than its height τd(v) allows.\\nWe will make space for u by complete rebalancing: perform an inorder traversal of the subtree\\nrooted at v, storing the elements in a separate array, and then re-inserting them, together\\nwith u, as compactly as possible into the subtree v (i.e. taking log2N(v) + O(1) layers),\\nusing another inorder traversal. Notice that since N(v) < S(v), there must now be space for\\nthe additional node u to be added in while the elements are being inserted.\\nIf we ever find that N(root) ≥ τ1S(root) before an insertion, then we perform a complete\\nrebalance of the root node itself, incrementing H (and re-allocating the array) between the\\ntwo traversals. We now claim that this insertion method does indeed achieve the desired\\ntime bound. We will give a slightly simplified version of the analysis in Brodal et al. [2002].\\nWe start with a simple lemma.\\nLemma 1 (Brodal et al. [2002], Lemma 3.1, modified slightly). Immediately after a rebal-\\nancing of any node v, for any descendant w of v, we have N(w) < N(v)(1 + S(w))/S(v).\\nProof. Observe that (1+S(w))/S(v) > (1+S(w))/(1+S(v)) = 2d(v)−d(w), and that v is now\\nperfectly balanced, so each descendant w of v has size at most N(v)2d(v)−d(w). Substituting\\ngives the desired result.\\nTheorem 8 (Brodal et al. [2002], Theorem 3.1). Inserting a key as described above takes\\namortized O(logBN + (logN)2/B) memory transfers.\\nProof. Traversing the tree to find the insertion location takes time O(logBN). It only\\nremains to bound the work done by rebalancinng.\\nSuppose we rebalance node v, due to an insertion below v. Then there is some child w of v for\\nwhich N(w) ≥ τd(w)S(w) (otherwise, we would not be rebalancing v). Consider the last time\\nnode v or any of its ancestors was rebalanced. Immediately after this rebalancing, Lemma\\n1 gives that N(w) < τd(v)(1 + S(w)) ≤ τd(v)S(w) + 1. Thus, there must have been at least\\nτd(w)S(w)−τd(v)S(v)−1 = ∆S(w)−1 insertions at node w or its descendants. The rebalance\\ntakes time O(logBN + S(v)/B) by Theorem 7, since it just consists of inorder traversals.\\nThe O(logBN) term is accounted for by the tree traversal, and the O(S(v)/B) term can\\nbe accounted for using the banker’s method by charging O(S(v)/B∆S(w)) = O(2/(B∆) =\\nO(H/B) to each of the insertions under w (noting that S(v)/S(w) ≈ 2 = O(1)). Each\\ninsertion is charged at most H times (once for each ancestor) between rebalances; thus,\\nrebalancing amortizes to O(H2/B) = O((logN)2/B).\\n6\\nIncrementing H is only done every O(N) insertions and consists of two tree traversals. The\\ntree traversals, as before, take time O(logBN +N/B), where the logBN term is accounted\\nfor in the search, and the N/B term can be (more than) accounted for by placing 1 additional\\ncredit on each insertion. Recomputing the indices of the vEB tree can be done during the in-\\norder traversal by computing parent and child indices at each node. As argued in Section 5.2,\\ncomputing parent and child indices takes time O(logH) = O(log logN); thus, this amortizes\\nto O((log logN)/B) and does not affect the overall runtime, since that runtime already\\ncontains an O((logN)2/B) term. Adding everything up, the overall amortized runtime is\\nO(logBN + (logN)2/B), as desired.\\nWe note that the van Emde Boas layout is not special in the above data structure. Although\\nwe will lose the memory access bounds given by Theorem 8, we may replace the van Emde\\nBoas layout by any other layout. In particular, in Section 5, we consider the ramifications of\\nusing a breadth-first layout instead of a vEB layout. We also notice that this data structure\\nneeds very little memory: a single array suffices to store the keys. Beyond that, we only\\nneed to store as much memory as necessary to compute the parent and child indices of any\\ngiven node efficiently. For a BFS layout, this is a simple multiplication or division by 2; for\\nthe vEB layout, this can get more complex; see Section 5.2.\\n5 Experimental Results\\n5.1 Speed Comparison\\nWe implement ordered dictionaries that support adding and searching for elements using\\nsplay trees, B-trees of different orders, and the structure seen in Section 4.2 (which we’ll\\ncall the “small tree”, since it doesn’t use much memory), with both a BFS layout and a\\nvEB layout. We find that, in practice, performance seems to be best for τ1 = 1/2, so we\\nonly include that case here. The B-tree of order 16 performed best, so we include that case\\nin our tests, and we include the B-tree of order 2 for comparison. The B-tree of order 16\\nshould be thought of as a cache-friendly structure that has been tuned for the block size of\\nour particular machine. We also include std::set, but we find that this is largely hopeless:\\nstd::set is easily beaten in just about all tasks by just about all our structures.\\nWe run four experiments on each data structure:\\n(1) In-order insertion: N elements are inserted into the structure in sorted order.\\n(2) Random insertion: N elements are inserted into the structure in random order.\\n(3) In-order traversal: After a random insertion, every element is accessed, in sorted order.\\n(4) Random traversal: After a random insertion, every element is accessed, in a random\\norder different from the insertion order.\\nFigures 1 through 4 show the experimental results. We make several observations:\\n7\\nFigure 1: In-order insertion Figure 2: In-order traversal\\nFigure 3: Random insertion Figure 4: Random traversal\\n8\\n(1) The in-order insertion plot clearly shows the O((logN)2) dependence of the small tree\\ninsertion in both BFS and vEB layouts.\\n(2) The B-tree of order 16 is, unsurprisingly, all-around one of the fastest trees. This is\\nunsurprising, since, as mentioned before, we can think of this tree as the explicitly-tuned\\n“cache-friendly” data structure.\\n(3) Also unsurprisingly, the performance of the vEB layout, especially on larger sizes, is\\nusually between that of the B-tree of order 2 (our “standard”, unoptimized, not-cache-\\nfriendly search tree) and the B-tree of order 16 (our explicitly-tuned “cache-friendly”\\ntree).\\n(4) The vEB layout seems to be generally slower than the BFS layout, only matching the\\nlatter’s performance in traversal for very large input sizes. It is likely that there may\\nbe a crossover point past 226 elements, but we stopped our experiments here as the run-\\ntime was becoming prohibitively expensive. The BFS layout benefits from the memory-\\nfriendliness of the small tree and the instantaneous computation of child and parent\\nindices of any given node, but not the cache-friendless of the vEB layout. The gap can\\nonly be due to the difficulty of computing vEB indices, since the data structures are\\notherwise identical. See Section 5.2 for an expanded discussion of this.\\n(5) The splay tree performs surprisingly poorly across the board, even in inorder traversal.\\nWe suspect that this is due to poor memory locality and usage. It should be mentioned,\\nthough, that the poor inorder traversal performance disappears if the elements are in-\\nserted in sorted order; in this case, the splay tree suddenly becomes the fastest at inorder\\ntraversal. For brevity, we have not included these plots, since they look otherwise quite\\nsimilar to the shown ones.\\n(6) std::set performs all-around poorly. This deficiency is harder to explain.\\n5.2 Calculating Indices in the van Emde Boas Layout\\nCalculating the parent and child indices of an element in a van Emde Boas layout is nontrivial.\\nSimply storing parent and child pointers/indices wastes space if it is possible to efficiently\\ncalculate these values on an on-demand basis. Common approaches involve implementing a\\nconversion between the easy-to-compute breadth first indexing, where children are calculated\\nwith c1 = 2i and c2 = 2i + 1 and parents with p = i/2, to van Emde Boas indexing. Many\\nattempts to quickly calculate this conversion from breadth-first notation to van Emde Boas\\ntake time O(logh), where h is the height of the tree. No fast constant-time algorithm seems\\nto exist as of yet, and the time to compute this conversion takes longer than a cache miss\\ndoes with these existing algorithms. Thus, keeping track of parent and child pointers still\\nremains to be the quickest method, although it takes extra space within the data structure.\\nHowever, in our exploration of this topic, we attempted to improve on existing methods,\\nseen in Brodal et al. [2002] and implemented in Coriolan [2011] and Copeland [2011]. Both\\nof these methods are sub-optimal because they either use O(logn) persistent memory, as\\n9\\nFigure 5: Runtimes of different methods of converting BFS notation to vEB notation\\nseen in Coriolan [2011], or O(logn) memory at runtime in the form of stack frames, as seen\\nin Copeland [2011].\\nWhile both of these methods both use O(logh) time, their use of external memory is un-\\nnecessary. We introduce a constant-memory algorithm for converting indices in breadth-first\\nnotation to the corresponding position which uses O(logw) time, where w is the word size of\\nthe machine. If memory usage comes at a steep premium and cache misses are very expen-\\nsive, using this algorithm with the vEB tree could be useful. The algorithm can be found in\\nAppendix A, implemented in C++.\\nAs seen in Figure 5, our algorithm for converting BFS indices to vEB indices performs the\\nfastest out of the three when the height is greater than seven, which corresponds to no more\\nthan 127 nodes in the tree, smaller than what is seen in a majority of applications. Even\\nthen, as discussed above, simply using a BFS layout outperforms the vEB layout due to the\\ncost of calculating indices. Until a more efficient algorithm for calculating indices is found,\\nit is probably best to not use the vEB layout in practice.\\n6 Conclusion\\nCache-oblivious algorithms and more specifically cache-oblivious search trees are an exciting\\narea to explore because of their relevance both in theory and in practice. In theory, using the\\ncache-oblivious model allows us to think about how a computer uses its cache, and optimize\\nmemory usage accordingly through some very nice techniques such as the van Emde Boas\\nlayout of a binary search tree. In practice, thinking about the cache or reducing memory\\n10\\nusage is an excellent way to improve the performance of any program; in our case, if the\\nsize of the cache is not known in advance, we find that the cache-oblivious structure (or its\\nsimpler cousin, the BFS-layout small tree) outperforms naive implementations such as the\\nB-tree of order 2 or the splay tree.\\nFinally, it is important to note the poor performance of the std::set class, which was in\\nevery way outperformed by almost every data structure studied in this paper. Why this is\\nand how to possibly improve on its implementation could be an area of further study.\\nReferences\\nGerth Stølting Brodal, Rolf Fagerberg, and Riko Jacob. Cache oblivious search trees via\\nbinary trees of small height. In Proceedings of the thirteenth annual ACM-SIAM symposium\\non Discrete algorithms, pages 39–48. Society for Industrial and Applied Mathematics, 2002.\\nBob Copeland. External memory data structure playground.\\nhttps://github.com/bcopeland/em misc, 2011.\\nCoriolan. Van emde boas layout. https://bitbucket.org/coriolan/van-emde-boas-\\nlayout/src/default, 2011.\\nErik D Demaine. Cache-oblivious algorithms and data structures. Lecture Notes from the\\nEEF Summer School on Massive Data Sets, 8(4):1–249, 2002.\\nMatteo Frigo, Charles E Leiserson, Harald Prokop, and Sridhar Ramachandran. Cache-\\noblivious algorithms. In Foundations of Computer Science, 1999. 40th Annual Symposium\\non, pages 285–297. IEEE, 1999.\\nHarald Prokop. Cache-Oblivious Algorithms. PhD thesis, Massachusetts Institute of Tech-\\nnology, 1999.\\n11\\nAppendix A\\n// BFS Index to vEB index conversion algorithm\\nint BFStovEB(int index, int h) {\\nif (index <= 1) {\\nreturn index - 1;\\n}\\nint currh = height(index);\\nint remainder = index & ~(1 << currh);\\n// calculate offset when h is not a power of 2\\nint disttobottom = h - currh - 1;\\nint thing = disttobottom ^ h;\\nint afterthing = height(thing);\\nint topxor = 1 << afterthing;\\nint habove = h & (topxor - 1);\\nint tothehabove = (1 << (h - habove));\\nindex = (index & (tothehabove - 1)) + tothehabove;\\ncurrh -= habove;\\nint offset = (1 << habove) - 1 +\\n(remainder >> currh) * ((1 << topxor) - 1);\\n// calculate spot in h = power of 2\\nif (currh <= 0) {\\nreturn offset;\\n}\\nint leftspine = (currh & 1) + ((currh & 2) >> 1) * 3 +\\n((currh & 4) >> 2) * 15 + ((currh & 8) >> 3) * 255 +\\n((currh & 16) >> 4) * ((1 << 16) - 1);\\nremainder = index & ~(1 << currh);\\nint shift = (remainder & 1) * (currh & 1) +\\n((remainder >> (currh & 1)) & 3) * ((1 << (currh & 2)) - 1) +\\n((remainder >> (currh & 3)) & 15) * ((1 << (currh & 4)) - 1) +\\n((remainder >> (currh & 7)) & 255) * ((1 << (currh & 8)) - 1) +\\n((remainder >> (currh & 15)) & 0xFFFF) * ((1 << (currh & 16)) - 1);\\nreturn offset + leftspine + shift;\\n}\\n12\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd18'), 'authors': 'Kopelowitz, Tsvi, Pettie, Seth, Porat, Ely', 'year': '2014', 'title': 'Higher Lower Bounds from the 3SUM Conjecture', 'full_text': 'Higher Lower Bounds from the 3SUM Conjecture∗\\nTsvi Kopelowitz†\\nBar-Ilan University\\nSeth Pettie ‡\\nUniversity of Michigan\\nEly Porat §\\nBar-Ilan University\\nAbstract\\nThe 3SUM conjecture has proven to be a valuable tool for proving conditional lower bounds\\non dynamic data structures and graph problems. This line of work was initiated by Paˇtras¸cu\\n(STOC 2010) who reduced 3SUM to an offline SetDisjointness problem. However, the reduction\\nintroduced by Paˇtras¸cu suffers from several inefficiencies, making it difficult to obtain tight\\nconditional lower bounds from the 3SUM conjecture.\\nIn this paper we address many of the deficiencies of Paˇtras¸cu’s framework. We give new and\\nefficient reductions from 3SUM to offline SetDisjointness and offline SetIntersection (the reporting\\nversion of SetDisjointness) which leads to polynomially higher lower bounds for several problems.\\nUsing our reductions, we are able to show the essential optimality of several classic algorithms,\\nassuming the 3SUM conjecture.\\n• Chiba and Nishizeki’s O(mα)-time algorithm (SICOMP 1985) for enumerating all triangles\\nin a graph with arboricity/degeneracy α is essentially optimal, for any α.\\n• Bjørklund, Pagh, Williams, and Zwick’s algorithm (ICALP 2014) for listing t triangles is\\nessentially optimal, assuming the matrix multiplication exponent is ω = 2.\\n• Any static data structure for SetDisjointness that answers queries in constant time must\\nspend Ω(N2−o(1)) time in preprocessing, and any such data structure that spends near-\\nlinear preprocessing must spend Ω(N1/2−o(1)) time per query. Here N is the size of the\\nset system. Since answering two keyword searches is at least as hard as SetDisjointness,\\nthese lower bounds imply that any substitute for the inverted index used by search engines\\ncannot simultaneously have ideal preprocessing and query times.\\nThese statements were unattainable via Paˇtras¸cu’s reductions.\\nWe also introduce several new reductions from 3SUM to pattern matching problems and\\ndynamic graph problems. Of particular interest is a new conditional lower bound on Dynamic\\nMaximum Cardinality Matching, which uses a new technique for obtaining amortized lower\\nbounds.\\nThis paper is dedicated to the memory of Mihai Paˇtras¸cu\\n∗Supported by NSF grants CCF-1217338, CNS-1318294, CCF-1514383, CCF-1637546, CCF-1815316, and a grant\\nfrom the US-Israel Binational Science Foundation. This research was performed in part at the Center for Massive Data\\nAlgorithmics (MADALGO) at Aarhus University, which is supported by the Danish National Research Foundation\\ngrant DNRF84.\\n†Email: kopelot@gmail.com\\n‡Email: pettie@umich.edu.\\n§Email: porately@gmail.com\\n1\\nar\\nX\\niv\\n:1\\n40\\n7.\\n67\\n56\\nv3\\n  [\\ncs\\n.D\\nS]\\n  1\\n2 J\\nan\\n 20\\n19\\n1 Introduction\\nData structure lower bounds come in two varieties: conditional and unconditional. The strongest\\nunconditional lower bounds (in the cell probe model) are either poly-logarithmic [52, 43, 60, 45]\\nor on extreme tradeoffs between sub-logarithmic update time and large query time; see [54, 4].\\nPaˇtras¸cu [55] proposed an approach for proving polynomial conditional lower bounds (CLBs) based\\non the conjectured hardness of the 3SUM problem, via a new intermediate problem he called\\nConvolution3SUM. The integer versions of the 3SUM and Convolution3SUM problems are defined\\nas follows. Given a set A ⊂ Z, the 3SUM problem is to decide if there is a triple (a, b, c) ∈ A3 of\\ndistinct elements such that a+ b = c. The Convolution3SUM problem is, given a vector A ∈ Zn, to\\ndecide if there is a pair (i, j) ∈ [n]2 for which A(i) + A(j) = A(i+ j). Here [n] = {0, 1, . . . , n− 1}\\nis the first n naturals.\\nIt was originally conjectured [26] that 3SUM requires Ω(n2) time. However, there are now\\nknown to be O(n2/polylog(n)) 3SUM algorithms for both integer [7] and real [30, 31, 27, 23, 12]\\ninputs. The modern 3SUM Conjecture is that the time complexity of the problem is Ω(n2−o(1)),\\neven in expectation.\\n3SUM and Set Disjointness. Paˇtras¸cu [55] gave a reduction from 3SUM to Convolution3SUM\\nshowing that the 3SUM conjecture implies that Convolution3SUM also requires Ω(n2−o(1)) time.\\nThis reduction is summarized in the following theorem. Observe that the optimum value for\\nk = k(n) depends on the actual complexities of 3SUM and Convolution3SUM.\\nTheorem 1.1 (Paˇtras¸cu [55]). Define T3S(n) and TC3S(n) to be the randomized (Las Vegas) com-\\nplexities of 3SUM and Convolution3SUM on instances of size n. For any parameter k, T3S(n) =\\nO(n2/k + (k3 + k2 log n) · TC3S(n/k)).\\nPaˇtras¸cu then reduced Convolution3SUM to the offline SetDisjointness problem. The input to\\nthis problem consists of a universe U of elements, a family F ⊂ 2U of subsets of U , and q pairs\\nof subsets (S, S′) ∈ F × F . For each query pair we are interested in whether S ∩ S′ = ∅ or not.\\nThe following two Theorems summarize Paˇtras¸cu’s reductions. (The second Theorem is implicit in\\nSection 2.3 of [55].)\\nTheorem 1.2 (Paˇtras¸cu [55]). Let g(n) be such that Convolution3SUM requires Ω( n\\n2\\ng(n)) expected\\ntime. For any\\n√\\nn · g(n) \\x1c n\\x0f \\x1c n/g(n) let A be an algorithm for the offline SetDisjointness\\nproblem where |U | = n, |F| = Θ(n1/2+\\x0f), each set in F has at most O(n1−\\x0f) elements from U , each\\nelement in U appears in Θ(\\n√\\nn) sets from F , and q = Θ(n1+\\x0f). Then A requires Ω( n2g(n)) expected\\ntime.\\nTheorem 1.3 (Paˇtras¸cu [55]). Let g(n) be such that Convolution3SUM requires Ω( n\\n2\\ng(n)) expected\\ntime. For any\\n√\\nn · g(n) \\x1c n\\x0f \\x1c n/g(n) let A be an algorithm for the offline SetDisjointness\\nproblem where |U | = Θ(n2−2\\x0f), |F| = Θ(n1/2+\\x0f log n), each set in F has at most O(n1−\\x0f) elements\\nfrom U , each element in U appears in Θ(n2\\x0f−1/2) sets from F , and q = Θ(n1+\\x0f log n). Then A\\nrequires Ω( n\\n2\\ng(n)) expected time.\\nThe 3SUM conjecture implies that g(n) = no(1), and so \\x0f ∈ (1/2, 1). Using this case, Paˇtras¸cu\\nprovided CLBs for triangle enumeration, set-disjointness, and various other dynamic graph and\\ndata structure problems.\\n2\\nRelated work. Paˇtras¸cu’s results led to a surge of research on CLBs that assume the 3SUM\\nconjecture. Vassilevska Williams and Williams [59] showed that finding a triangle of zero weight in\\na weighted graph requires Ω(n3−o(1)) time. Abboud, Vassilevska Williams, and Weimann [2] proved\\nthat the Local Alignment problem, which is of great importance in computational biology, cannot\\nbe solved in truly sub-quadratic time. Amir, Chan, Lewenstein, and Lewenstein [5] proved that for\\nthe Jumbled Indexing problem on a text with n integers (from a large enough alphabet), either the\\npreprocessing time needs to be truly quadratic or the query time needs to be truly linear. Abboud\\nand Vassilevska Williams [1] showed numerous CLBs conditioned on various popular conjectures.\\nThey showed, in particular, that conditioned on the 3SUM conjecture, data structure versions of\\n(s, t)-Reachability, Strong Connectivity, Subgraph Connectivity, Bipartite Perfect Matching, and\\n“Pagh’s problem” all require non-trivial polynomial preprocessing, query, or update time. Abboud,\\nVassilevska Williams, and Yu [3] introduced a different approach for reducing 3SUM to dynamic\\ngraph problems via intermediate problems called ∆-Matching Triangles and Triangle Collection.\\nAfter the conference version of this paper was published there has been plenty of followup work\\nthat uses our reductions or extends our results in new directions. Amir et al. [6] proved CLBs for\\npattern matching with gaps using our triangle enumeration results. Kopelowitz and Krauthgamer\\nproved CLBs for various distance oracles on colored points [39]. Goldstein, Kopelowitz, Lewenstein,\\nand Porat [28] proved CLBs for partial matrix multiplication and problems related to finding\\nwitnesses for convolutions. In [29], the same authors considered a data structural version of the\\n3SUM conjecture, and derived time/space tradeoffs from it. Dahlgaard [16] extended our lower\\nbounds on incremental maximum cardinality matching to other problems, and other hardness\\nassumptions.\\n1.1 Limitations of Paˇtras¸cu’s Reductions\\nPaˇtras¸cu’s reductions are ingenious but suffer from a few limitations, namely:\\n• The number of SetDisjointness queries is rather large (at least ω(n3/2)) making it impossible\\nto obtain any ω(n1/2) time lower bound per query.\\n• The size N of the set systems in the offline SetDisjointness problem is also rather large (N =\\nΩ(n3/2)) so it is impossible to get lower bounds of Ω(N4/3) time as a function of N .\\n• The sets in the offline SetDisjointness instances are very sparse. In Paˇtras¸cu’s framework all\\nsets have size at most O(\\n√|U |), thereby limiting the possibility of obtaining meaningful CLBs\\nfor the dense case of many graph problems, e.g., triangle enumeration.\\n• Finally, Paˇtras¸cu’s reduction from 3SUM to offline SetDisjointness is only meaningful if the\\ntrue complexity of 3SUM is ω(n13/7).1\\nRegarding the first two limitations, we would like to have much more control over the size of\\nthe set system and the number of queries. The third limitation seems intrinsic to any reduction\\nto SetDisjointness where the sets are essentially random, since, by the birthday paradox, random\\n1To see this, note that Theorem 1.2 is only applicable if g(n) = O(n1/3), i.e., Convolution3SUM requires Ω(n5/3)\\ntime. It is consistent with Theorem 1.1 that Convolution3SUM can be solved in O(n5/3) time while 3SUM can be\\nsolved in O(n13/7) time. Furthermore, even if 3SUM were proved to be Ω(n5/3) unconditionally, this would imply no\\nsuperlinear lower bound on Convolution3SUM (via Theorem 1.1) or to any of the problems that Convolution3SUM is\\nreduced to.\\n3\\nReductions to Set Disjointness/Intersection\\nReduction |F| |U | q Remarks\\nConv3SUM → SetDisjointness n1/2+\\x0f n n1+\\x0f √ng(n)\\x1c n\\x0f \\x1c n/g(n)\\nConv3SUM → SetDisjointness n1/2+\\x0f log n n2−2\\x0f n1+\\x0f log n\\n3SUM → SetDisjointness n log n n2−2γ n1+γ log n γ ∈ [0, 1) and δ ∈ (0, 1)\\n3SUM → SetIntersection n 12 (1+δ+γ) n1+δ−γ n1+γ Total output size: O(n2−δ)\\nFigure 1: The first two reductions from Convolution3SUM are from Paˇtras¸cu [55]. The third and\\nfourth reductions from 3SUM are new.\\nω(\\n√|U |)-size sets will almost surely intersect, giving no useful information per query. The fourth\\nlimitation gets at the issue of robustness: how valuable are reductions from 3SUM if the 3SUM\\nconjecture turns out to be false—but not by much? The possibility of a truly subquadratic 3SUM\\nalgorithm seemed remote a few years ago but given recent developments [58, 30, 31, 13, 37] it is not\\nso absurd. Chan and Lewenstein [13] showed that numerous special cases of 3SUM can be solved\\nin truly subquadratic time.\\n1.2 New Result: A More Versatile Lower Bound Framework\\nWe overcome the limitations of Paˇtras¸cu’s framework by giving efficient reductions directly from\\n3SUM to SetDisjointness and from 3SUM to SetIntersection, which we define shortly. By avoiding\\nConvolution3SUM as an intermediate problem, our reductions imply non-trivial lower bounds, even\\nif the true complexity of 3SUM is just Ω(n3/2+\\x0f), for any \\x0f > 0.\\nFor clarity’s sake, we present our new framework in terms of a new constant γ instead of the\\nconstant \\x0f of Theorem 1.2. Notice that in Paˇtras¸cu’s frameworks 1/2 < \\x0f < 1 while here 0 < γ < 1.\\nThe first theorem reduces 3SUM to offline SetDisjointness.\\nTheorem 1.4. Let f(n) be such that 3SUM requires expected time Ω( n\\n2\\nf(n)). For any constant\\n0 < γ < 1 let A be an algorithm for offline SetDisjointness where |U | = Θ(n2−2γ), |F| = Θ(n log n),\\neach set in F has at most O(n1−γ) elements from U , and q = Θ(n1+γ log n). Then A requires\\nΩ( n\\n2\\nf(n)) expected time.\\nWe also consider the offline SetIntersection problem where the input is the same as in SetDis-\\njointness, except that we are required to enumerate a large enough subset of all elements in the q\\nintersections. Note that in this case we allow γ = 0. Refer to Section 2 for proofs of Theorems 1.4\\nand 1.5.\\nTheorem 1.5. Let f(n) be such that 3SUM requires expected time Ω( n\\n2\\nf(n)). For any constants\\n0 ≤ γ < 1 and δ > 0, let A be an algorithm for offline SetIntersection where |U | = Θ(n1+δ−γ),\\n|F| = Θ(\\n√\\nn1+δ+γ), each set in |F| has at most O(n1−γ) elements from U , q = Θ(n1+γ), and\\nthe algorithm is required to enumerate at most O(n2−δ) elements from the intersections. Then A\\nrequires Ω( n\\n2\\nf(n)) expected time.\\nThese theorems eliminate the need to use Convolution3SUM as a stepping stone (Theorem 1.1)\\nwhen proving lower bounds via SetDisjointness/SetIntersection. Nonetheless, the Convolution3SUM\\n4\\nproblem is still useful and its exact relationship to the 3SUM problem is an interesting open question.\\nThe more constrained structure of Convolution3SUM makes it easier to use in some CLBs; see [5, 59].\\nWe are also able to prove that the randomized complexities of 3SUM and Convolution3SUM differ\\nby at most a logarithmic factor. Refer to Section 6 for proof of Theorem 1.6.\\nTheorem 1.6. Define T3S(n) and TC3S(n) to be the randomized (Las Vegas) complexities of 3SUM\\nand Convolution3SUM on instances of size n. Then T3S(n) = O(log n · TC3S(n)).\\n1.3 Techniques and Implications\\nOur new framework borrows liberally from the techniques of Paˇtras¸cu’s framework, particularly the\\nuse of almost linear hash functions. However, in order to obtain our improvements we assemble the\\nbuilding blocks in a new and simpler way. The implications of our new framework are significant.\\nTo start off, since q can be made arbitrarily close to n (by having γ approach 0), it is now possible\\nto obtain much higher CLBs for the query time of SetDisjointness. Similarly, since the size of the\\noffline SetDisjointness can be made small (by having γ approach 0), it is now possible to obtain\\nhigher CLBs in terms of the size of the offline SetDisjointness instance.\\nFinally, as is illustrated in Section 3, using the new framework it is now possible to obtain CLBs\\nfor graph problems which apply to all edge densities. Such reductions make use of Theorem 1.5.\\nFor example, in the case of triangle enumeration our CLBs imply that clever techniques, such as\\nfast matrix multiplication, cannot lead to faster enumeration algorithms, even in very dense graphs.\\nTo illustrate the advantages of the new framework, we briefly show how we are able to obtain\\nhigher CLBs for the fundamental problem of online SetDisjointness, and then continue in the body\\nof this paper to describe new and better CLBs for many old and new problems. A corresponding\\ndiscussion on CLBs for the online SetIntersection problem is given in Section 5.1.\\n1.4 A Higher Conditional Lower Bound for Online SetDisjointness\\nIt is straightforward to see that online SetDisjointness solves offline SetDisjointness. We phrase the\\nCLBs in terms of N : the sum of set sizes. Define tp to be the preprocessing time of an online\\nSetDisjointness structure and tq its query time.\\nUsing Paˇtras¸cu’s reduction from Convolution3SUM to SetDisjointness we have N = Θ(n1.5) and\\nthere are Θ(n1+\\x0f) = Θ(N (2+2\\x0f)/3) queries that need to be answered. Thus we obtain the following\\nlower bound tradeoff: tp +N\\n(2+2\\x0f)/3 · tq = Ω\\n(\\nN4/3\\ng(N2/3)\\n)\\n. The 3SUM conjecture implies g(x) = xo(1).\\nIf, for example, we only allow linear preprocessing, letting \\x0f tend to 1/2 gives a query lower bound\\nof Ω(N\\n1\\n3\\n−o(1)). If we demand constant time queries, we obtain a lower bound of Ω(N\\n4\\n3\\n−o(1)) on the\\npreprocessing time. We show next how our new framework provides better tradeoffs.\\nTheorem 1.7. Assume the 3SUM conjecture. For any 0 < γ < 1, any data structure for SetDis-\\njointness has\\ntp +N\\n1+γ\\n2−γ · tq = Ω\\n(\\nN\\n2\\n2−γ−o(1)\\n)\\n.\\nProof. Using Theorem 1.4, we have N = Θ(n2−γ log n), and the number of queries to answer is\\nΘ(n1+γ log n) = Θ˜(N\\n1+γ\\n2−γ ). By the 3SUM conjecture, answering these queries takes time Ω(n2−o(1)) =\\nΩ(N\\n2\\n2−γ−o(1)).\\n5\\nTriangle Enumeration Bounds\\nAuthors Time Bound Remarks\\nItai & Rodeh O(m3/2)\\nChiba & Nishizeki O(mα) α = arboricity\\nPaˇtras¸cu Ω(m4/3−o(1)) t ≈ m = n1.5+o(1) triangles\\nO(m\\n2ω\\nω+1 +m\\n3(ω−1)\\nω+1 t\\n3−ω\\nω+1 )\\nBjørklund, Pagh, O(nω + n\\n3(ω−1)\\n5−ω t\\n2(3−ω)\\n5−ω )\\nω = matrix mult. exp.\\nWilliams & Zwick O(m4/3+o(1) + t · ( m\\nt2/3\\n))\\nO(n2+o(1) + t · ( n\\nt1/3\\n))\\nAssuming ω = 2\\nKopelowitz, Pettie & Porat O(mdα log lognlogn e+ t) Randomized, w.h.p.\\nEppstein, Goodrich, Randomized, w.h.p.\\nMitzenmacher & Torres\\nO(mdα logww e+ t) w = word size\\nΩ(mα1−o(1)) every arboricity α\\nnew\\nΩ(min{m3/2−o(1), t · ( m\\nt2/3\\n)1−o(1)})\\n(Assuming 3SUM Conjecture)\\nΩ(min{n3−o(1), t · ( n\\nt1/3\\n)1−o(1)})\\nFigure 2: Bjørklund et al. [9] also proved several lower bounds conditioned on the QES Conjecture.\\nThis conjecture was later refuted by Lokshtanov, Patrui, Tamaki, Williams, and Yu [47].\\nTheorem 1.7 implies, for example, that if we only allow linear preprocessing time, then by\\nmaking γ tend to 0 the query time must be Ω(N\\n1\\n2\\n−o(1)). This CLB is comparable with the data\\nstructure of Cohen and Porat [15] (see also [41]) where tp = O(N\\n√\\nN) and tq = O(\\n√\\nN) . Further-\\nmore, if we only allow constant query time, then by making γ tend to 1 the preprocessing time\\nmust be Ω(N2−o(1)), matching that of the trivial preprocessing algorithm that computes all answers\\nin advance. A clean expression of this tradeoff curve is given in the following corollary; refer to\\nSection 5.2 for the proof.\\nCorollary 1.8. Assume the 3SUM conjecture. Fix constants p ∈ [1, 2) and q ∈ [0, 1/2] and suppose\\nthere is a data structure for SetDisjointness where tp = O(N\\np) and tq = O(N\\nq). Then\\np+ 2q ≥ 2.\\n1.5 Triangle Enumeration\\nGiven a graph with n vertices and m edges, the Triangle Enumeration problem is to list all t\\ntriangles (3-cycles), or alternatively, to list up to a given number t of triangles. The maximum\\nnumber of triangles in a graph is O(m3/2). (For example, consider a clique.) Itai and Rodeh [35]\\nobtained a Triangle Enumeration algorithm running in O(m3/2) time, which is optimal inasmuch\\nas it is linear in the worst case output size. The Itai-Rodeh algorithm can also run in Ω(m3/2)\\ntime even if there are zero triangles, which highlights the need for more nuanced ways to measure\\nthe performance of enumeration algorithms. Chiba and Nishizeki [14] showed that all triangles\\n6\\ncould be enumerated in O(mα) time where α = α(G) is the arboricity2 of the graph. The Chiba-\\nNishizeki bound subsumes the Itai-Rodeh bound because α is always upper bounded by\\n√\\nm; it is\\nalso optimal inasmuch as the output size can be as large as Ω(mα), for any α ≥ 2 and m = O(nα).\\nOne of the big open questions in this area is to understand the relationship between Triangle\\nDetection and Triangle Enumeration, or more generally, to separate the one-time costs of enumer-\\nation (e.g., in terms of m,α) from the per-triangle costs (in terms of t). In particular, is Ω(mα)\\ntime necessary for reasons other than worst case output size considerations? Kopelowitz, Pettie,\\nand Porat [41] proved that enumerating t triangles takes O(mdα/ lognlog logne + t) time. Eppstein et\\nal. [21] designed an algorithm for the w-bit word RAM model running in O(mdα/ wlogwe+ t) time.\\nStrictly speaking, these algorithms disprove the hypothesis that Ω(mα) is necessary, but perhaps\\nit cannot be improved more than polylogarithmic factors.\\nPaˇtras¸cu showed that, conditioned on the 3SUM conjecture, there exists a graph with t = O(m)\\ntriangles, for which listing all triangles must take Ω(m4/3−o(1)) time. A careful examination of\\nPaˇtras¸cu’s proof shows that the arboricity of this graph is indeed roughly m1/3, implying the\\nessential optimality of Chiba and Nishizeki’s algorithm and of [41, 21], at least for one particular\\narboricity. However, Paˇtras¸cu’s CLB does not extend to any α \\x1d m1/3. This left open the\\npossibility of clever algorithms that dramatically improve the Ω(mα) bound on dense graphs. For\\nexample, in many problems, improvements based on fast matrix multiplication only “kick in” when\\nthe graph is sufficiently dense.\\nUsing our new framework we prove an Ω(mα1−o(1)) CLB for Triangle Enumeration, for all\\npossible arboricities 1\\x1c α\\x1c m1/2. We emphasize that the number of triangles in these instances,\\nt, is polynomially smaller than mα, implying that the hardness does not stem solely from the output\\nsize. Thus, the Chiba-Nishizeki algorithm and the algorithms of [41, 21] are essentially optimal for\\nthe entire spectrum of arboricities. The proof of Theorem 1.9 is given in Section 3.\\nTheorem 1.9. Assume the 3SUM conjecture. For any constants x ∈ (0, 1) and y ∈ (0, 1/2)\\nsuch that x ≤ 2y, there exists a constant \\x0f > 0 and a graph with n vertices, m edges, arboricity\\nα = Θ(nx) = Θ(my), and t < mα1−\\x0f triangles, such that listing all triangles requires Ω(mα1−o(1))\\nexpected time.\\nFor a comparison between the known upper and conditional lower bounds, see Figure 2.\\nOutput-sensitive triangle enumeration algorithms. Another approach for enumerating tri-\\nangles takes t as a parameter and enumerates up to t triangles, even if the graph contains more.\\nThe time for such algorithms has a one-time cost (depending on graph parameters such as n and\\nm, but not t), and a cost per triangle. An algorithm of Bjørklund, Pagh, Williams, and Zwick [9]\\nshows that if the matrix multiplication exponent is ω = 2, then listing up to t triangles takes\\nO˜(min{n2 +nt2/3, m4/3 +mt1/3}) time. Notice that this runtime can be expressed as paying either\\na one-time cost of n2 or m4/3 and a per triangle cost of n\\nt1/3\\nor m\\nt2/3\\n.\\nWe prove that, assuming the 3SUM conjecture and assuming ω = 2, this per triangle cost is\\nessentially optimal. This lower bound is obtained by considering the extreme case of listing all\\ntriangles in the graph, which by Theorem 1.9 requires Ω(mα1−o(1)) expected time, combined with\\ncontrolling the number of triangles in the graph so that t = α3. Such a result seems unobtainable in\\n2The arboricity of an undirected graph G = (V,E) is the number of forests needed to cover E; by the Nash-\\nWilliams theorem [49, 50] it is precisely α(G) = maxU⊆V : |U|≥2\\n⌈\\n|E(U)|\\n|U|−1\\n⌉\\n, where E(U) is the set of edges induced by\\nU .\\n7\\nPaˇtras¸cu’s framework since the corresponding graphs have arboricity at most m1/3. The following\\ntheorem is proved in Section 3.\\nTheorem 1.10. Assume the 3SUM conjecture. Then any algorithm for listing t triangles whose\\nruntime is expressed in terms of the number of edges m must take Ω(min{m3/2−o(1), t ·( m\\nt2/3\\n)1−o(1)})\\nexpected time. If the runtime of the algorithm is expressed in terms of the number of vertices n it\\nmust take Ω(min{n3−o(1), t · ( n\\nt1/3\\n)1−o(1)}) expected time.\\nIn particular, Theorem 1.10 implies that if we do not spend Ω(m3/2) time for listing just\\nt triangles (which is enough time to report all triangles), then the time per triangle must be\\nΩ(( m\\nt2/3\\n)1−o(1)).\\n1.6 Conditional Lower Bounds on Graph and Pattern Matching Problems\\nWe also prove polynomial CLBs for data structure versions of Document Retrieval problems, Max-\\nimum Cardinality Matching (improving [1]), d-failure Connectivity Oracles, and Distance Oracles\\nfor Colors. The new CLB for Maximum Cardinality Matching is of particular interest since it\\nintroduces new techniques for obtaining amortized lower bounds; see Section 4.1.\\nMaximum Cardinality Matching. In the Dynamic Maximum Cardinality Matching problem\\nwe are interested in maintaining a dynamic graph G = (V,E), with n = |V | and m = |E|,\\nto support maximum cardinality matching (MCM) queries, which report the size of the current\\nMCM. When both insertions and deletions are supported we say that G is fully dynamic, while\\nif only insertions are supported we say that G is incremental. The trivial algorithm for updating\\nan MCM takes O(m) time by finding an augmenting path [25, 24]. Sankowski [56] gave a fully\\ndynamic algorithm with an amortized time bound of O(n1.495) based on fast matrix multiplication.\\nIn the bipartite vertex-addition model, where vertices on one side of the graph arrive online with\\nall of their edges, Bosek, Leniowski, Sankowski, and Zych [10] recently showed how to maintain a\\nmaximum cardinality matching whose total update time is O(m\\n√\\nn) time; see also [8].\\nAbboud and Vassilevska Williams [1] showed that, based on the 3SUM conjecture, either the\\npreprocesing time of Dynamic MCM is Ω(m4/3−o(1)), the amortized update time is Ω(mα−o(1)), or\\nthe amortized query time is Ω(m2/3−α−o(1)), for any α ∈ [1/6, 1/3]. In our setting we will require\\nthe size of the MCM to be reported after each update, and so the CLB of [1] implies that if the\\npreprocessing time tp is O(m\\n4/3−Ω(1)) then the update time tu is Ω(m1/3−o(1)). Using Theorem 1.4\\nwe are able to prove the following. Refer to Section 4.2 for proof of Theorem 1.11.\\nTheorem 1.11. Assume the 3SUM conjecture, and fix any γ ∈ (0, 1). Suppose a fully dynamic\\nMCM algorithm is given an MCM of the initial graph at preprocessing, and let tp and tu be its\\npreprocessing and update times, respectively. Then\\ntp +m\\n1+γ\\n2−γ · tu = Ω(m\\n2\\n2−γ−o(1)).\\nMoreover, the same bound holds even for the class of approximate MCM algorithms that report the\\nsize of some matching without length-7 augmenting paths.\\nBy having γ approach 0, one implication of Theorem 1.11 is that if tp = O(m\\n1+o(1)) then\\nthe update time must be Ω(m1/2−o(1)). This improves on the results of Abboud and Vassilevska\\n8\\nWilliams [1].3 Guaranteeing the absence of short augmenting paths is one way to achieve a provably\\ngood approximation to the MCM; see [51].\\nIncremental Maximum Cardinality Matching. In this setting we consider an initially empty\\ngraph G and so there is no preprocessing phase. Abboud and Vassilevska Williams [1] show that\\ntheir lower bounds for fully dynamic MCM extend to incremental MCM, but only for worst case time\\nbounds. They highlight the difficulty of obtaining amortized lower bounds using their approach.\\nThe worst-case lower bounds of Abboud and Vassilevska Williams [1] can be phrased in terms\\nof nˆ, the number of vertices when an operation takes place. Either the update or query time is\\nΩ(nˆ1/2−o(1)). It is straightforward to show that if the graph is allowed to grow with each query\\nthen one can obtain an amortized expected Ω(nˆ1/3−o(1)) lower bound.\\nWe focus on improving the amortized lower bound for incremental MCM in terms of nˆ, using\\nTheorem 1.4. Our strategy is to answer SetDisjointness queries using an incremental dynamic MCM\\nalgorithm. The construction has the property that queries can be simulated by two vertex insertions\\nand two edge insertions and then examining the change in the size of the MCM. There are two\\nways to undo these four insertions, (i) rolling back the state of the data structure to its original\\nstate, or (ii) inserting two more vertices and two more edges. By dynamically choosing which of\\n(i) or (ii) to employ we can better control the total number of vertices inserted into the graph, and\\ntherefore get better lower bounds in terms of nˆ.\\nTheorem 1.12. Assume the 3SUM conjecture. Any algorithm for incremental MCM has amortized\\nexpected update time of Ω(nˆ\\n√\\n17−1\\n8\\n−o(1)) = Ω(nˆ0.3903−o(1)) where nˆ is the number of vertices in the\\ngraph following the update.\\nThe proof of Theorem 1.12 appears in Section 4.1. Dahlgaard [16] later proved a conditional\\nlower bound of Ω(n1−o(1)) on incremental MCM, but from the OMv conjecture.\\nOrganization of the Paper. In Section 2 we prove Theorems 1.4 and 1.5 that reduce 3SUM to\\nSetDisjointness and SetIntersection. In Section 3 we apply these results to the triangle enumeration\\nproblem, and prove Theorems 1.9 and 1.10 concerning the optimality of the Chiba-Nishizeki [14]\\nalgorithm and the Bjørklund et al. [9] algorithm, respectively. Section 4 presents proofs of Theo-\\nrems 1.11 and 1.12 on fully dynamic cardinality matching and incremental cardinality matching,\\nrespectively. In Section 5 we provide a number of new conditional lower bounds for online SetInter-\\nsection data structures, d-failure connectivity oracles, and various document retrieval problems.\\nFinally, in Section 6, we prove Theorem 1.6 on the near-equivalence between the randomized com-\\nplexities of 3SUM and Convolution3SUM.\\n2 The Main Reductions — Proofs of Theorems 1.4 and 1.5\\nLet H be a family of hash functions from [u]→ [m]. We call H linear if\\nfor any h ∈ H and any x, x′ ∈ [u], we have h(x) + h(x′) ≡ h(x+ x′) (modm).\\n3They [1] also get CLBs on approximate MCM algorithms that eliminate short augmenting paths, but from\\ndifferent conjectures concerning the complexity of triangle detection and combinatorial BMM.\\n9\\nand call H balanced if\\nfor any h ∈ H and any S ⊂ [u], |{x ∈ S : h(x) = i}| ≤ 3|S|m .\\nNote that no hash family is balanced according to this definition (unless m = O(1)). Nonetheless,\\nin the first part of the proof we will engage in a little magical thinking, and suppose that for any\\nu and m a hash fairy can summon for us a hash family H that is magically linear, balanced, and\\npairwise independent. In Section 2.1 we argue that a modified version of the proof works even if H\\nis almost linear, almost balanced, and (exactly) pairwise independent, and exhibit a specific hash\\nfunction that satisfies these properties.\\nCombined proof of Theorem 1.4 and Theorem 1.5. Since the proofs of both theorems follow a sim-\\nilar path we describe them together. We are looking for three distinct input elements x, y, z such\\nthat x− y = z. Let R = nγ and set Q = (5n/R)2 in Theorem 1.4 and Q = n1+δ/R in Theorem 1.5.\\nWithout loss of generality we assume that\\n√\\nQ is an integer. Finally, we assume that the input is\\ndrawn from the integer universe [2w], where w = Ω(log n) is the machine’s word length.\\nWe pick a random hash function h1 : [2\\nw] → [R] from a family that is linear and balanced.\\nUsing h, we create R buckets B1, . . . ,BR such that Bi = {x : h1(x) = i}. Since h1 is balanced, each\\nbucket contains at most 3n/R elements. This bucketing is similar to Paˇtras¸cu’s reduction [55].\\nNext, we pick a random hash function h2 : [2\\nw] → [Q] where h2 is chosen from a pair-wise\\nindependent and linear family. For each bucket Bi we create 2\\n√\\nQ shifted sets as follows. For each\\nj ∈ [0,√Q) let\\nB↑i,j = {(h2(x) + j ·\\n√\\nQ) modQ | x ∈ Bi}\\nand B↓i,j = {(h2(x)− j) modQ | x ∈ Bi}.\\nNext, for each z ∈ A we want to determine if there exists x and y in A such that x− y = z. To do\\nthis we utilize the linearity of h1 and h2, which implies that\\nh1(x)− h1(y) ≡ h1(z) (modR)\\nand h2(x)− h2(y) ≡ h2(z) (modQ).\\nIf x ∈ Bi then y must be in bucket Bi−h1(z) modR. Thus, for each i ∈ [R] we would like check the\\nintersection Bi ∩ (Bi−h1(z) modR + z) to find candidate pairs x, y for which x − y = z. Denote the\\nhigh-order and low-order halves of h2 by\\nh↑2(z) =\\n⌊\\nh2(z)√\\nQ\\n⌋\\nand h↓2(z) = h2(z) mod\\n√\\nQ.\\nDue to the linearity of h2, every element in the intersection of\\nBi ∩ (Bi−h1(z) modR + z) (1)\\nhas a corresponding element in the intersection of(\\nB↓\\ni,h↓2(z)\\n)\\n∩\\n(\\nB↑\\ni−h1(z) modR,h↑2(z)\\n)\\n. (2)\\n10\\nOf course, the reverse direction is not true since taking the projection of these sets under h2 may\\nintroduce false positives into (2) that were not present in (1). Nonetheless, if (2) is empty then (1)\\nis empty as well, meaning there are no 3SUM witnesses involving z and any x ∈ Bi. The number\\nof set intersection queries is nR since there are n choices for z and R choices for i.\\nFix z and let k = h2(z). Since h2 is pairwise independent and linear then for any pair x, y ∈ U\\nwhere x 6= y we have that if x− y 6= z then\\nPr[h2(x)− h2(y) = k] = Pr[h2(x− y) = h2(z)] = 1\\nQ\\n.\\nThis is where the proofs of the two theorems diverge.\\nDetails for Theorem 1.4. Since each bucket contains at most 3n/R elements, the probability\\nof a false positive stemming from two buckets Bi,Bj and a given offset k = h2(z) is, by a union\\nbound,\\nPr[h2(Bi) ∩ (h2(Bj) + k) 6= ∅] ≤\\n(\\n3n\\nR\\n)2 1\\nQ\\n=\\n9\\n25\\n.\\n(Recall that Q = (5n/R)2.) In order to reduce the probability of false positives, we repeat the\\nprocess with O(log n) different choices of h2, but using the same h1. This blows up the number of\\nsets by a factor of O(log n), but not the universe. If the sets intersect under all O(log n) choices of\\nh2 then we spend O(n/R) time to find x and y within buckets Bi and Bj , which is either part of\\na 3SUM witness (and the algorithm halts), or a false positive, which only occurs with probability\\n1/poly(n).\\nDetails for Theorem 1.5. We bound the expected number of false positives. Each pair of\\nbuckets defines at most (3nR )\\n2 pairs of elements, one from each bucket, so the expected number of\\nfalse positives arising from this intersection is\\nE[|h2(Bi) ∩ (h2(Bj) + k)|] =\\n(\\n3n\\nR\\n)2 1\\nQ\\n= O\\n(\\nn1−δ\\nR\\n)\\n,\\nsince Q = Θ(n1+δ/R). Thus, the expected number of false positives over all O(nR) intersections is\\nO(nR n\\nRnδ\\n) = O(n2−δ) ≤ c · n2−δ from some constant c > 0. It takes constant time to verify that a\\npair x, y is a false positive rather than (part of) a valid 3SUM witness. If the number of verifications\\nexceeds 2c · n2−δ without finding a 3SUM witness, which, by Markov’s inequality, happens with\\nprobability at most 1/2, then we restart the entire algorithm with fresh hash functions.\\nFinal details. To summarize, for SetDisjointness (SetIntersection) we create a total ofO(R\\n√\\nQ log n)\\nsets (O(R\\n√\\nQ) sets). These sets are partitioned into two families A and B where all of the ↑-type\\nsets are in A and all of the ↓-type sets are in B. All of the intersections we are interested in are\\nbetween a set from A and a set from B. The universe U of the elements in the sets is of size Q.\\nThe family F is A∪B. The number of queries is O(nR log n) = O(n1+γ log n) (O(nR) = O(n1+γ)).\\nThis concludes the proof of Theorems 1.4 and 1.5, under the simplifying assumption that h1, h2\\nare magically linear and balanced, which is addressed in Section 2.1.\\n11\\n2.1 Almost Linear and Almost Balanced Hashing\\nWe now describe how to overcome the assumption that there exist pair-wise independent hash\\nfunctions that are magically linear and balanced by relaxing both definitions. A family H of hash\\nfunctions from [u] → [m] is called almost linear if for any h ∈ H there exists an integer ch such\\nthat for any x, x′ ∈ [u],\\nh(x) + h(x′) ≡ h(x+ x′) + ch + {−1, 0, 1} (modm)\\nI.e., it is linear, up to a offset ch that depends on h and a ±1 error. Given a hash function h ∈ H\\nwe say that a value i ∈ m is heavy for set S ⊆ [u] if\\n|{x ∈ S : h(x) = i}| > 3|S|\\nm\\n,\\ni.e., 3 times more than the expected load. H is called almost balanced if for any set S ⊆ [u], the\\nexpected number of elements from S that are hashed to heavy values is O(m).\\nWe emphasize that the notion of almost linearity that we use here is more general than the\\none used by Baran et al. [7], Paˇtras¸cu [55], and Section 6 of this paper. Whereas Section 6 can\\nuse any O(1)-universal almost linear hash family, here we require a pairwise independent (and\\nhence exactly 1-universal) almost linear hash family. The hash family [18] proposed by Baran et\\nal. [7] and Paˇtras¸cu [55] is almost linear (under Section 6’s definition) but it is only known to be\\n2-universal [18, Lem. 2.4] and is definitely not pairwise independent. This issue was also noted\\nin [36].\\nWe will show that there exists a family H of pairwise independent hash functions that is almost\\nlinear and almost balanced, which is suitable for use in the reductions of Theorems 1.4 and 1.5.\\nEach step in the proofs of Theorems 1.4 and 1.5 that used linearity is replaced by three parallel\\nsteps making use of the almost linearity of H. The reduction algorithm must consider all three\\noptions for ζ ∈ {−1, 0, 1} such that h(x) + h(x′) ≡ h(x+ x′) + ch + ζ (modm). This only blows up\\nthe running time by a factor of nine: whereas before we assumed h1 and h2 were perfectly linear,\\nwe now have to entertain three options for h1 hash values and three options for h2 hash values.\\nThe balance assumption is overcome by directly verifying, for each element mapped to a heavy\\nvalue, whether it is part of a 3SUM witness. This takes O(n) time per element. The expected\\nnumber of such elements is O(nγ), and so the expected time spent on such elements is O(n1+γ).\\nThe number of intersection queries q is already O(n1+γ) and so the cost of dealing with elements\\nmapped to heavy values may be ignored. For the rest of the elements (those that are not assigned\\nto heavy values) we proceed as in the proofs of Theorem 1.4 and 1.5.\\nThe Hash Family. Baran et al. [7] showed that any 1-universal family of hash functions is almost\\nbalanced; see Jafargholi and Viola [36] for a somewhat simpler proof.4 Rather than use the family\\nof [18], we use one analyzed by Dietzfelbinger [17].\\nTheorem 2.1. ([17, Theorem 3]) The family Hu,m,r defined below is pairwise independent and\\nhence 1-universal whenever r = km for some k ≥ u/2, and u,m, and r are all powers of 2.\\nHu,m,r = {ha,b : [u]→ [m] | a ∈ [r] is an odd integer and b ∈ [r]}\\nand ha,b(x) = ((ax+ b) div(r/m)) modm\\n4Furthermore, it is straightforward to exhibit a class of (1 + \\x0f)-universal functions that are not close to being\\nalmost balanced; see, e.g., https://simons.berkeley.edu/talks/seth-pettie-2015-11-30, starting at minute 16:00.\\n12\\nIn our application u = 2w is naturally a power of 2, m can be rounded up to the next power of\\n2, and k can be fixed at u/2. Since Hu,m,r is 1-universal it is also almost balanced [7, 36]. We need\\nto prove that it is almost linear.\\nLemma 2.2. The family Hu,m,r is almost linear, with cha,b = bdiv(r/m).\\nProof. Consider any elements x, x′ ∈ [u]. Let g(x) = (ax+b) div(r/m) be the hash function without\\nthe “modm” operation. Then we have\\ng(x) + g(x′) =\\n⌊\\nax+ b\\nr/m\\n⌋\\n+\\n⌊\\nax′ + b\\nr/m\\n⌋\\ng(x+ x′) + bdiv(r/m) =\\n⌊\\na(x+ x′) + b\\nr/m\\n⌋\\n+\\n⌊\\nb\\nr/m\\n⌋\\nIn general, whenever α1+α2 = α3+α4, bα1c+bα2c differs from bα3c+bα4c by at most one. Hence, g\\nis almost linear with offset bdiv(r/m) and error in {−1, 0, 1}. Taking g modulo m preserves almost\\nlinearity (modm), hence h is also almost linear.\\n3 Triangle Enumeration\\nFollowing Paˇtras¸cu [55] we express a SetIntersection instance as a tripartite graph in which triangles\\nare in one-to-one correspondence with the elements output by SetIntersection queries.\\nProof of Theorem 1.9. The SetIntersection instance of Theorem 1.5 is interpreted as a tripartite\\ngraph G on vertex set A ∪ B ∪ U where A and B are two copies of F . Each element e ∈ U has\\nedges to the sets in A and B that contain e, and the edges between A and B correspond to the\\nSetIntersection queries. Thus, |U | = Θ(n1+δ−γ), |A| = |B| = |F| = Θ(\\n√\\nn1+δ+γ), there are Θ(n1+γ)\\nedges between A and B, and at most O(n1−γ) edges between each vertex in A∪B and elements in U .\\nThus the total number of edges between A∪B and U is at most O(n 12 (1+δ+γ) ·n1−γ) = O(n 12 (3+δ−γ)).\\nNotice that there is a bijection between the output elements in the SetIntersection instance and the\\ntriangles in the graph. Thus, after enumerating O(n2−δ) triangles we are done.\\nWe prove that enumerating all triangles essentially requires Ω(Mα) time for any feasible com-\\nbination of N (the number of vertices), M (the number of edges), and arboricity α. By feasible\\nwe mean that α = Θ(Mx) = Θ(Ny), y ≤ 2x. We assume that each vertex has a degree of at least\\n2 (since it is easy to filter out other vertices) and then x ≤ y. Notice that proving a lower bound\\nfor α = Θ(Nx) implies a lower bound for Θ(Nx\\n′\\n) for any constant x′ < x since one can always add\\nsingleton vertices.\\nFor our lower bound proof it suffices to consider the case where n1+γ ≥ n 12 (3+δ−γ), and so\\nγ ≥ 1/3 + δ/3. Furthermore, this implies that |F| > |U |. Thus,\\nN = Θ(n\\n1\\n2\\n(1+δ+γ))\\nM = Θ(n1+γ).\\nOur aim is to show that α is at most O(n1−γ), since for each edge we must spend at least Ω(n1−γ)\\ntime (assuming the 3SUM conjecture), and so if α ≤ O(n1−γ) we conclude that the total runtime\\nis at least Ω(Mα). However, this may not be the case in G, so we devise a triangle-preserving\\nreduction to a new graph G′ with N ′ vertices and M ′ = O(M) edges such that there is an injective\\n13\\nfunction between triangles in the original graph and triangles in G′. To bound the arboricity α′ of\\nG′ we show that there exists an orientation of G′ with max out-degree O(n1−γ). It is well known\\nthat the maximum out-degree in any orientation must be at least α− 1 [40].\\nDenote by E(u,X) the set of edges between a vertex u and a vertex set X. Consider a vertex\\na ∈ A. Since |E(a, U)| = O(n1−γ), if |E(a,B)| = O(n1−γ) then we orient all of the edges of a to\\nleave a. However, it is possible that E(a,B) is too large. To deal with this, we create d |E(a,B)|\\nn1−γ e\\ncopies of a. The neighbors of a in B are arbitrarily partitioned into d |E(a,B)|\\nn1−γ e sets of size at most\\nn1−γ , and the ith copy of a has as its neighbors the ith set in the partition. All of the edges\\ntouching copies of a are oriented outwards from those copies. Furthermore, each copy of a has\\noutgoing edges towards the O(n1−γ) neighbors of a in U . Thus, the out-degree of each copy of a is\\nalso at most O(n1−γ). By orienting all of the edges between B and U to leave B, the out-degree of\\nany vertex in this orientation is at most O(n1−γ), and so the arboricity of this new graph is at most\\nα′ = O(n1−γ). It is straightforward to see that this new graph G′ is a triangle-preserving substitute\\nfor G. The number of edges of this graph is M ′ ≤ 2M since we only increase the number of edges\\nby adding new edges between copies and U , but each such edge can be charged to an edge between\\nA and B in the initial graph. Also, since there are Θ(n1+γ) edges between A and B, the number\\nof copies that are created is at most O(n1+γ/n1−γ) = O(n2γ). Hence, the number of vertices in\\nthe new graph is N ′ = O(N + n2γ) = O(n\\n1\\n2 (1+δ+γ) + n2γ). Finally, since γ ≥ 1/3 + δ/3 we have\\n(1 + δ + γ)/2 ≤ 2γ and so N ′ = O(n2γ).\\nTo summarize, we have obtained a graph with M ′ = Θ(n1+γ) edges, N ′ = O(n2γ) vertices,\\nand α′ = O(n1−γ). Thus, enumerating all of the triangles in O(M ′(α′)1−\\x0f) = O(n2−Ω(\\x0f)) time\\ncontradicts the 3SUM conjecture. We will now show that this lower bound holds for the entire\\nspectrum of possible polynomial dependencies of α′ on N ′ and M ′.\\nRecall that we always have M ′ ≤ N ′α′. Since we can always increase the number of vertices, it\\nis enough to prove that the lower bound holds for all combinations of M ′ = N ′α′. This is exactly\\nthe case here, since\\nM ′ = Θ(n1+γ) = Θ(n2γn1−γ) = Θ(N ′α′).\\nFurthermore, we capture the entire spectrum of values of α′. To see this for M ′ notice that α′ is\\non the order of M\\n′ 1−γ\\n1+γ = M ′x. As γ admits values between 1/3 and 1 (exclusive), x admits values\\nbetween 1/2 and 0 (exclusive). Similarly, α′ is on the order of N ′\\n1−γ\\n2γ = N ′y, so y admits values\\nbetween 0 and 1 (exclusive). In this case the number of triangles, n2−δ, is bounded away from\\nthe lower bound Ω(n2−o(1)) whenever δ > 0, which illustrates that the lower bound is not solely a\\nresult of the size of the output.\\nUsing the same construction we can also prove the Theorem 1.10, which is in terms of m,n,\\nand t rather than m and α.\\nProof of Theorem 1.10. Suppose, for the sake of obtaining a contradiction, that there exists an\\nalgorithm for enumerating t triangles that takes M (3/2)(1−ρ) + t(M/t2/3)1−ρ time, for some constant\\nρ > 0. We use the same graph construction as in Theorem 1.9, setting γ = 1/3 + δ/3 and δ = 4ρ.\\nThus, the graph has N = Θ(n2γ) vertices, M = Θ(n1+γ) edges, and t = Ω(n2−δ) = Ω(n3−3γ)\\n14\\n(A) (B)\\nFigure 3: An illustration of the graph before the set intersection query a ∩ b = ∅?. There is a\\nunique perfect matching before the query: matched edges are drawn thick and unmatched ones thin.\\nDashed edges are inserted in the course of the query. (A) For the worst case bound we insert edges\\n(x, a′′), (y, b′′), check if the size of the MCM has increased (implying a∩b 6= ∅), then delete them. (B)\\nFor the amortized bound we insert new vertices xa,b, x\\n′\\na,b, ya,b, y\\n′\\na,b insert edges (xa,b, a\\n′′), (ya,b, b′′),\\nthen check whether the size of the MCM has increased, then insert edges (x′a,b, xa,b), (y\\n′\\na,b, ya,b).\\nDepending on the actual time of these operations, we either do nothing or roll back all edge and\\nvertex insertions.\\ntriangles. The running time of the algorithm on this instance is therefore\\nM (3/2)(1−ρ) + t(M/t2/3)1−ρ\\n= (n1+γ)(3/2)(1−ρ) + n3−3γ(n1+γ−(2/3)(3−3γ))1−ρ\\n= n(4/3)(1+δ/4)(3/2)(1−ρ) + n2−Θ(ρ) γ = 1/3 + δ/3\\n= n2−Θ(ρ\\n2) + n2−Θ(ρ) ρ = δ/4.\\nThis contradicts the 3SUM conjecture. The calculations that depend on N rather than M are\\nsimilar. Suppose there is an algorithm that enumerates t triangles in N3(1−ρ) + t(N/t1/3)1−ρ time.\\nThen, using the same graph construction, but with δ = ρ, we can solve 3SUM in time\\nN3(1−ρ) + t(N/t1/3)1−ρ\\n= n6γ(1−ρ) + n3−3γ(n2γ−(1/3)(3−3γ))1−ρ\\n= n2(1+δ)(1−ρ) + n2−Θ(ρ) γ = 1/3 + δ/3\\n= n2−Θ(ρ\\n2) + n2−Θ(ρ) δ = ρ,\\nwhich contradicts the 3SUM conjecture.\\n4 Maximum Cardinality Matching\\n4.1 Incremental MCM — Proof of Theorem 1.12\\nIn this section n denotes the size of the 3SUM instance and N and M denote the number of vertices\\nand edges in the graph on which we compute maximum cardinality matchings.\\nIn amortized analysis we want to bound the total cost of a sequence S = (σ1, . . . , σk) of k\\noperations and let cost(σi) be the time cost of operation σi. A function f assigns valid amortized\\n15\\ncosts if\\n∑k\\ni=1 f(σi) ≥\\n∑k\\ni=1 cost(σi). We prove that in any MCM algorithm, if Ni is the number of\\nvertices after σi, then\\n∑k\\ni=1 cost(σi) ≥ Ω\\n(∑k\\ni=1N\\n√\\n17−1\\n8\\n−o(1)\\ni\\n)\\n, that is, any amortization function\\nf that is a function of the current number of vertices Nˆ has f(Nˆ) = Ω\\n(\\nNˆ\\n√\\n17−1\\n8\\n−o(1)\\n)\\n= Ω(Nˆ0.39).\\nConsider the following instance of the incremental MCM problem which is created from an\\ninstance of the offline SetDisjointness problem. First, we create two copies of F which are denoted\\nby A and B. For each c ∈ U we create two vertices cA and cB with an edge between them. We say\\nthat cA and cB are copies of c. For each a ∈ A (b ∈ B) we create two vertices a′ and a′′ (b′ and b′′)\\nwith an edge between them, and for each c ∈ a (c ∈ b) there is an edge between a′ and cA (b′ and\\ncB). We say that a\\n′ and a′′ (b′ and b′′) are copies of a (b). We also add 2 additional vertices, x and\\ny.\\nThe initialization of this graph is implemented by inserting all of the edges one at a time using\\nthe incremental MCM algorithm. This initial graph has Θ(n+ n2−2γ) vertices and Θ(n2−γ) edges.\\nNotice that this initial graph (without the extra 2 vertices x and y) has a unique perfect matching\\nwhich is the set of edges between copies. To implement a SetDisjointness query between a and b we\\nadd edges (x, a′′) and (y, b′′). See Figure 3(A). Now, a and b are disjoint iff there is no augmenting\\npath after adding the two edges. Thus, an increase in the MCM implies that a∩ b 6= ∅. In order to\\nfacilitate additional SetDisjointness queries we undo the effect of adding (x, a′′), (y, b′′), using one of\\nthe following two approaches.\\nRollback. One approach is to delete the two edges that were added. An incremental data struc-\\nture can always support deletions of the last element that was inserted by keeping track of the\\nmemory modifications that took place during the last insertion, and reversing them within the\\nsame time cost as the cost of the insertion itself. Notice that this approach does not blend well\\nwith amortized time bounds since we have no a priori bound on the maximum time per operation.\\nCreating Perfect Matchings. The second approach is to add another two edges per SetDis-\\njointness query for a total of 4 edges, and create four separate dummy vertices xa,b, x\\n′\\na,b, yb,a, and\\ny′b,a associated with each SetDisjointness query on a ∈ A and b ∈ B. For the SetDisjointness query\\nwe add edges (xa,b, a\\n′′) and (yb,a, b′′) to the graph and as before the MCM increases iff the sets a and\\nb intersect. See Figure 3(B). Then, we add edges (xa,b, x\\n′\\na,b) and (yb,a, y\\n′\\nb,a) which guarantee that\\nthe resulting graph has a perfect matching that is comprised of the perfect matching of the graph\\nprior to the insertion of the 4 edges together with edges (xa,b, x\\n′\\na,b) and (yb,a, y\\n′\\nb,a). The MCM has\\nincreased by 2 after the insertion of the 4 edges, regardless of whether the sets intersect or not. The\\ndownside of this approach is that the number of vertices grows with the number of SetDisjointness\\nqueries, leading to weaker lower bounds in terms of the number of vertices.\\nCombining the Two. Assume that the amortized cost of each edge or vertex insertion is bounded\\nby Nˆα for some constant α > 0, where Nˆ is the number of vertices in the graph when the insertion\\ntakes place. To answer a SetDisjointness query we first add the four vertices and four edges, thereby\\ncreating a perfect matching. If the insertion time of these vertices and edges is less than 9Nˆα\\n(within Nˆα of the budget for 8 insertions) then we rollback the insertion. Otherwise, we leave the\\nfour edges and continue to the next SetDisjointness query. Intuitively, our goal with this combined\\nmethod is to guarantee that the graph does not grow by too much, while maintaining the amortized\\ncost in order to obtain a higher lower bound. SetDisjointness queries for which we perform a rollback\\n16\\ncost O(Nˆα) time each. By assumption the subsequence of remaining operations (those inserts used\\nto set up the initial graph and subsequent inserts not rolled back) has amortized cost O(Nˆα).\\nNext we bound the number of vertices at the end of the process, denoted by N . After the graph\\nsetup there is O(n2−γ(n+ n2−2γ)α) credit for performing expensive insertions later. For our proof\\nwe will focus on γ ≤ 1/2 and so the amount of credit becomes O(n2−γ+(2−2γ)α). Each expensive\\nSetDisjointness query uses up at least Nˆα of that credit, and so the total credit used during all of\\nthe expensive insertions is at least Ω(\\n∑N\\ni=0(n + i)\\nα) = Ω(N1+α). Since we can never be in credit\\ndebt, we have that n2−γ+(2−2γ)α ≥ Ω(N1+α) and so N ≤ O(n 2−γ+(2−2γ)α1+α ).\\nThe number of cheaper insertions that we rolled back is O(n1+γ). Each one of these costs at\\nmost Nα. So the total time of the entire sequence of operations which solves 3SUM is\\nO(n1+γNα +N1+α) ≤ O(n1+γ+ (2−γ+(2−2γ)α)α1+α + n2−γ+(2−2γ)α).\\nThe 3SUM conjecture implies this bound must be Ω(n2−o(1)), so up to the o(1) term we must have\\n2 ≤ max\\n{\\n1 + γ +\\n(2− γ + (2− 2γ)α)α\\n1 + α\\n, 2− γ + (2− 2γ)α\\n}\\n.\\nThe two terms are equal when γ = 1+α2+3α and then 2 ≤ 2− γ + (2− 2γ)α, implying that\\nα ≥ γ\\n2(1− γ) =\\n(1 + α)/(2 + 3α)\\n2(1 + 2α)/(2 + 3α)\\n=\\n1 + α\\n2 + 4α\\n.\\nRearranging terms, we have 4α2 + α− 1 ≥ 0, and we can set α to be\\n√\\n17−1\\n8 > 0.3903.\\n4.2 Dynamic MCM with Preprocessing — Proof of Theorem 1.11\\nThe proof follows the same lines as the proof of Theorem 1.12. Let n denote the size of the 3SUM\\ninstance and let m denote the number of edges in the graph on which we compute maximum\\ncardinality matchings.\\nThe initial graph created from an instance of the offline SetDisjointness problem is exactly the\\nsame as the initial graph from Section 4.1. Recall that this graph has m = Θ(n2−γ) edges and a\\nSetDisjointness query is implemented by adding two edges to the graph. Since in the proof here we\\nsupport fully dynamic graphs, this time in order to facilitate additional SetDisjointness queries we\\nundo the effect of adding the two edges by deleting both of these edges. Thus each SetDisjointness\\nquery is processed by executing four edge updates, but not changing the number of vertices.\\nBy Theorem 1.4, assuming the 3SUM conjecture, if the number of SetDisjointness queries is\\nq = Θ(n1+γ log n) = Θ(m\\n1+γ\\n2−γ log n) then either tp = Ω(n\\n2−o(1)) or q · tu = Ω(n2−o(1)). Together this\\nimplies that\\ntp +m\\n1+γ\\n2−γ · tu = tp + q · tu\\n= Ω(n2−o(1))\\n= Ω(m\\n2\\n2−γ−o(1)).\\nFinally, notice that providing a specific initial MCM does not assist in answering the SetDis-\\njointness queries, since the answer to the SetDisjointness queries depends only on whether the MCM\\n17\\nincreases in size. Moreover, by the construction of the graph given in Section 4.1, the longest\\nlength of an augmenting path encountered as edges are added and deleted is 7. Thus, the same\\nlower bound holds for any algorithm for approximating MCM in a fully dynamic graph that reports\\nthe size of some matching without length-7 augmenting paths.\\n5 Applications\\n5.1 Online SetIntersection\\nWe consider online SetIntersection data structures that have a specified preprocessing time, query\\ntime, and reporting time (per element). By the trivial reduction from offline SetIntersection to\\nonline SetIntersection, we discover tradeoffs between these three time bounds.\\nTheorem 5.1. Assume the 3SUM conjecture. Fix any γ ∈ [0, 1), δ ∈ (0, 1], and any online\\nSetIntersection data structure. Let tp be its expected preprocessing time, tq be its amortized expected\\nquery time, and tr be its amortized expected reporting time per element. Then\\ntp +N\\n2(1+γ)\\n3+δ−γ · tq +N\\n2(2−δ)\\n3+δ−γ · tr = Ω\\n(\\nN\\n4\\n3+δ−γ−o(1)\\n)\\n.\\nProof. We use the same reduction as the one in the proof of Theorem 1.7. Using Theorem 1.5, we\\nhave N = Θ(n1−γ\\n√\\nn1+δ−γ) = Θ(n\\n3+δ−γ\\n2 ), the number of queries is Θ(n1+γ) = Θ(N\\n2(1+γ)\\n3+δ−γ ), and\\nthe total size of the output is Θ(n2−δ) = Θ(N\\n2(2−δ)\\n3+δ−γ ). Thus, we obtain the following lower bound\\ntradeoff:\\ntp +N\\n2(1+γ)\\n3+δ−γ · tq +N\\n2(2−δ)\\n3+δ−γ · tr = Ω(n2−o(1)) = Ω\\n(\\nN\\n4\\n3+δ−γ−o(1)\\n)\\n.\\nLet us illustrate some implications of Theorem 5.1.\\nCorollary 5.2. Assume the 3SUM conjecture. Fix constants p ∈ [4/3, 2) and q ∈ [0, 2/3] and\\nsuppose there is a data structure for SetIntersection where tp = O(N\\np), tq = O(N\\nq), and tr = N\\no(1).\\nThen\\np+ q ≥ 2.\\nProof. Assume, for the purpose of obtaining a contradiction, that p + q is strictly smaller than 2,\\nsay q = 2 − p − \\x0f for some constant \\x0f > 0. By Theorem 1.7, for any constants γ ∈ (0, 1) and\\nδ ∈ (0, 1],\\n4\\n3 + δ − γ ≤ max\\n{\\np,\\n2(1 + γ)\\n3 + δ − γ + q,\\n2(2− δ)\\n3 + δ − γ + o(1)\\n}\\n= max\\n{\\np,\\n2(1 + γ)\\n3 + δ − γ + 2− p− \\x0f\\n}\\n. (3)\\nObserve that the maximum of the three expressions in (3) can never be equal to the third, since\\n2(2−δ)\\n3+δ−γ + o(1) <\\n4\\n3+δ−γ , which justifies the last equality of (3). We now want to choose γ, δ such\\nthat the maximum of (3) is achieved in the second expression.\\n18\\nSince p ∈ [4/3, 2), we can always set γ ∈ [0, 1) and δ ∈ (0, 1] such that\\nγ − δ = 3− 4\\np\\n+ \\x0f′, (4)\\nfor some constant \\x0f′ such that 0 < δ \\x1c \\x0f′ \\x1c \\x0f. E.g., when p is close to 2 we set γ to be slightly\\nless than 1, \\x0f′ to be slightly less than 4/p− 2, and δ \\x1c \\x0f′ appropriately. Rewriting (4), we have\\np =\\n4− p\\x0f′\\n3 + δ − γ <\\n4\\n3 + δ − γ .\\nThis implies that the maximum of (3) is attained in the second expression, so\\n4\\n3 + δ − γ ≤\\n2(1 + γ)\\n3 + δ − γ + 2− p− \\x0f.\\nRearranging terms, we have\\n\\x0f ≤ 2(1 + γ)\\n3 + δ − γ + 2− p−\\n4\\n3 + δ − γ\\n=\\n4 + 2δ\\n3 + δ − γ − p =\\n4 + 2δ\\n3− (3− 4p + \\x0f′)\\n− p from (4)\\n=\\n4 + 2δ\\n4\\np − \\x0f′\\n− p = p ·\\n(\\n4 + 2δ\\n4− p\\x0f′ − 1\\n)\\n= Θ(\\x0f′) < \\x0f, since δ < \\x0f′ \\x1c \\x0f and p ∈ [4/3, 2)\\nwhich is a contradiction, hence p+ q cannot be strictly smaller than 2.\\n5.2 Online SetDisjointness — Proof of Corollary 1.8\\nRecall that Corollary 1.8 stated that if the preprocessing and query times of a SetDisjointness data\\nstructure were O(Np) and O(N q), then the 3SUM conjecture implies p+ 2q ≥ 2. The proof follows\\nthe same lines as that of Corollary 5.2, but is somewhat simpler.\\nProof. Assume, for the purpose of obtaining a contradiction, that p+ 2q is strictly smaller than 2,\\nsay q = 2−p2 − \\x0f. By Theorem 1.7, for any γ ∈ (0, 1),\\n2\\n2− γ ≤ max\\n{\\np,\\n1 + γ\\n2− γ + q\\n}\\n= max\\n{\\np,\\n1 + γ\\n2− γ +\\n2− p\\n2\\n− \\x0f\\n}\\n. (5)\\nSince p ∈ [1, 2), we can always set γ ∈ (0, 1) such that γ = 2 − 2p + \\x0f′, for some \\x0f′ such that\\n0 < \\x0f′ \\x1c \\x0f. This implies p = 2−p\\x0f′2−γ < 22−γ , which means we can conclude the maximum of (5) must\\nbe attained in the second expression, so\\n2\\n2− γ ≤\\n1 + γ\\n2− γ +\\n2− p\\n2\\n− \\x0f.\\nRearranging terms, we have\\n\\x0f ≤ 1 + γ\\n2− γ +\\n2− p\\n2\\n− 2\\n2− γ =\\n1\\n2− γ −\\np\\n2\\n=\\n1\\n2− (2− 2p + \\x0f′)\\n− p\\n2\\n= p\\n(\\n1\\n2− p\\x0f′ −\\n1\\n2\\n)\\n= Θ(\\x0f′) < \\x0f,\\nwhich is a contradiction, hence p+ 2q cannot be strictly smaller than 2.\\n19\\n5.3 d-Failure Connectivity\\nIn the d-Failure Connectivity Oracle problem we wish to preprocess an undirected graph G = (V,E)\\nin order to support a single batch of a set F ⊂ V of d vertex failures and subsequent connectivity\\nqueries in the subgraph induced by V \\\\F .\\nDuan and Pettie [19] introduced a d-failure connectivity structure whose preprocessing and\\nbatch deletion times are O(d1−2/cmn1/c poly(log n)) and O(d2c+4 poly(log n)), where c ≥ 1 is an\\ninteger parameter. The connectivity query time is O(d), independent of c,m, and n. The same\\nauthors presented a different d-failure connectivity structure [20] with O(mn log n) preprocessing\\ntime, O(d2 poly(log n)) batch deletion time, and the same O(d) query time. The main open question\\nin this line of work is whether the O(d) query time of [19, 20] could be improved to match the d-edge\\nfailure connectivity oracles [53, 38, 19, 20], whose query time is O˜(1), independent of d. Here we\\nprove that with preprocessing and batch deletion times similar to [19], the query time must depend\\non d, and be Ω(d1/2−o(1)).5 Subsequent to the initial publication of this work [42], Henzinger et\\nal. [32] proved that, conditioned on the OMv conjecture, d-failure connectivity oracles with poly(n)\\npreprocessing time and reasonable batch deletion times require Ω(d1−o(1)) query times. Some recent\\nupper bounds [46, 11] have cast some doubt on the validity of the OMv conjecture.\\nTheorem 5.3. Assume the 3SUM conjecture. For any 1/2 ≤ γ < 1 suppose there is a d-failure con-\\nnectivity structure for d\\n2−γ\\n2−2γ -edge, d\\n1\\n2−2γ -vertex graphs with expected preprocessing time tp, expected\\ndeletion time td, and expected query time tq. Then,\\ntp + d\\n1\\n2−2γ · td + d\\n1+γ\\n2−2γ · tq = Ω(d\\n1\\n1−γ−o(1)).\\nProof. We reduce the SetDisjointness problem to the d-failure connectivity as follows. We make use\\nof Theorem 1.4 and set d = |U | = O(n2−2γ). Construct a tripartite graph G = (V,E) on vertices\\nV = A ∪B ∪ U , where A and B are copies of F , and edges\\nE = {(a, c) | c ∈ a} ∪ {(b, c) | c ∈ b}\\nWe now need to answer n1+γ = O(d\\n1+γ\\n2−2γ ) SetDisjointness queries using a black-box data structure\\nfor d-failure connectivity on G. For each a ∈ A separately we perform up to d deletions and then\\nanswer all SetDisjointness queries involving a using connectivity queries. To do this we delete all\\nvertices in U that correspond to elements not in a and let G[a] be the resulting graph. Notice\\nthat in G[a], a is only connected to sets in A ∪ B that intersect a. We can therefore answer any\\nSetDisjointness query “a ∩ b = ∅?” by asking one connectivity query in G[a].\\nObserve that G is an M -edge, N -vertex graph where N = |F| + |U | = O(n log n) = O˜(d 12−2γ )\\nand M = O(n2−γ) = O(d\\n2−γ\\n2−2γ ). Thus, tp + (n log n) · td + n1+γ · tq = Ω(n2−o(1)). Substituting\\nn = Ω(d\\n1\\n2−2γ ) completes the proof.\\nNote that γ does not affect the final conclusion that connectivity queries require Ω(d1/2−o(1))\\ntime. The role of the γ parameter is to make the total time for all batch deletions negligible. For\\nexample, if td = d\\n100, we would have to set γ very close to 1 so that d\\n1\\n2−2γ · d100 \\x1c d 11−γ .\\n5Our lower bound does not apply to [20], whose preprocessing time is quadratic, and therefore already large enough\\nto solve the underlying 3SUM instance.\\n20\\n5.4 Document Retrieval Problems with Multiple Patterns\\nOne of the services offered by search engines is the retrieval of documents whose text satisfies some\\npredicate, typically the inclusion (or exclusion) of multiple keywords. In this section we prove lower\\nbounds on several problems of this type.\\n5.4.1 Two Pattern Document Retrieval\\nIn the Document Retrieval problem [48] we are interested in preprocessing a corpus of documents\\nX = {D1, · · · , Dk} where N =\\n∑\\nD∈X |D|, so that given a pattern P we can quickly report all of\\nthe documents that contain P . We are usually interested in run times that depend on the number\\nof documents that contain P , not on the total number of occurrences of P in the entire corpus. In\\nthe Two Pattern Document Retrieval problem we are given two patterns P1 and P2 at query time,\\nand wish to report all of the documents that contain both P1 and P2. We consider two versions\\nof the Two Pattern Document Retrieval problem. In the reporting version we are interested in\\nenumerating all documents that contain both patterns. In the decision version we only want to\\ndecide whether the output is non-empty or not.\\nAll known solutions for the Two Pattern Document Retrieval problem with non trivial pre-\\nprocessing use at least Ω˜(\\n√\\nN) time per query [48, 15, 33, 34, 41]. Larsen, Munro, Nielsen, and\\nThankachan [44] prove lower bounds on Two Pattern Document Retrieval, conditioned on the hard-\\nness of combinatorial boolean matrix multiplication. (A data structure with N3/2−\\x0f preprocessing\\nand (\\n√\\nN)1−\\x0f-time queries implies a subcubic combinatorial BMM algorithm.) We provide some\\nadditional evidence of hardness conditioned on the 3SUM conjecture.\\nIt is straightforward to see that the two versions of Two Pattern Document Retrieval solve\\nSetIntersection and SetDisjointness, respectively. In particular, the reduction creates an alphabet Σ\\nwhich corresponds to all of the sets in F . For each e ∈ U we create a document that contains the\\ncharacters corresponding to the sets that contain e. The intersection between S, S′ ∈ F directly\\ncorresponds to all the documents that contain both symbols S and S′. Thus, all of the lower\\nbound tradeoffs for intersection problems are lower bound tradeoffs for the Two Pattern Document\\nRetrieval problem.\\nTheorem 5.4. Assume the 3SUM conjecture. Fix any γ ∈ [0, 1), and consider any data structure\\nfor Two Pattern Document Retrieval for a corpus X with expected preprocessing time tp and query\\ntime tq. These time bounds are a function of N =\\n∑\\nD∈X |D|, the size of the corpus. Then\\ntp +N\\n1+γ\\n2−γ · tq = Ω\\n(\\nN\\n2\\n2−γ−o(1)\\n)\\n.\\nTheorem 5.5. Assume the 3SUM conjecture. Fix any γ ∈ [0, 1) and δ ∈ (0, 1), and consider any\\ndata structure for Two Pattern Document Retrieval for a corpus X with expected preprocessing\\ntime tp, query time tq, and reporting time (per document) tr. These time bounds are a function of\\nN =\\n∑\\nD∈X |D|, the size of the corpus. Then\\ntp +N\\n2(1+γ)\\n3+δ−γ · tq +N\\n2(2−δ)\\n3+δ−γ · tr = Ω\\n(\\nN\\n4\\n3+δ−γ−o(1)\\n)\\n.\\n5.4.2 Forbidden Pattern Document Retrieval\\nIn the Forbidden Pattern Document Retrieval problem [22] we are still interested in preprocessing\\na fixed document corpus. A query now consists of two patterns P+, P− and must report all of the\\n21\\ndocuments that contain P+ and do not contain P− (reporting version), or decide whether there\\nexists no such document (decision version).\\nAll known solutions for the Forbidden Pattern Document Retrieval problem with non trivial pre-\\nprocessing use at least Ω(\\n√\\nN) time per query [22, 34]. Larsen, Munro, Nielsen, and Thankachan [44]\\nalso give lower bounds on this problem, conditioned on the combinatorial BMM conjecture. Here\\nwe provide some additional evidence of hardness conditioned on the 3SUM conjecture.\\nTheorem 5.6. Assume the 3SUM conjecture. For any γ ∈ [0, 1) and any data structure for\\nForbidden Pattern Document Retrieval for a corpus X with expected preprocessing time tp and\\nexpected query time tq, which depend on N =\\n∑\\nD∈X |D|. Then\\ntp +N\\n1+γ\\n3−2γ · tq = Ω(N\\n2\\n3−2γ−o(1)).\\nProof. We create two copies of mathcalF , denoted by A and B, and similar to the proof of Theo-\\nrem 5.4 we set Σ = A∪B. For each e ∈ U we create a document that contains all of the characters\\ncorresponding to sets from A that contain c and sets from B that do not contain c.\\nUsing Theorem 1.4, we have N = Θ(n3−2γ), and the number of queries to answer is Θ(n1+γ) =\\nΘ(N\\n1+γ\\n3−2γ ). Thus we obtain the following lower bound tradeoff:\\ntp +N\\n1+γ\\n3−2γ · tq = Ω(n2−o(1)) = Ω(N\\n2\\n3−2γ−o(1)).\\nNotice that if we only allow linear preprocessing time then by making γ arbitrarily small we\\nobtain a query time lower bound of Ω(N\\n1\\n3\\n−o(1)).\\nTheorem 5.7. Assume the 3SUM conjecture. Fix any γ ∈ [0, 1), δ ∈ (0, 1), and any data structure\\nfor the reporting version of Forbidden Pattern Document Retrieval for a corpus X, with expected\\npreprocessing time tp, expected query time tq, and amortized expected reporting time tr (per docu-\\nment), which depend on N =\\n∑\\nD∈X |D|. Then\\ntp +N\\n1+γ\\n3\\n2 (1+δ−\\nγ\\n3 ) · tq +N\\n2−δ\\n3\\n2 (1+δ−\\nγ\\n3 ) · tr = Ω(N\\n2\\n3\\n2 (1+δ−\\nγ\\n3 )\\n−o(1)\\n).\\nProof. Our proof is similar to the proof of Theorem 5.6, only this time we use Theorem 1.5. So we\\nhave N = Θ(n1+δ−γ\\n√\\nn1+δ+γ) = Θ(n\\n3\\n2\\n(1+δ− γ\\n3\\n)), the number of queries is Θ(n1+γ) = Θ(N\\n1+γ\\n3\\n2 (1+δ−\\nγ\\n3 ) ),\\nand the total size of the output is Θ(n2−δ) = Θ(N\\n2−δ\\n3\\n2 (1+δ−\\nγ\\n3 ) ). Thus, we obtain the following lower\\nbound tradeoff:\\ntp +N\\n1+γ\\n3\\n2 (1+δ−\\nγ\\n3 ) · tq +N\\n2−δ\\n3\\n2 (1+δ−\\nγ\\n3 ) · tr = Ω(n2−o(1)) = Ω(N\\n2\\n3\\n2 (1+δ−\\nγ\\n3 )\\n−o(1)\\n).\\nNotice that allowing only linear preprocessing time and a constant reporting time, then by\\nmaking γ and δ arbitrarily small we obtain a query time lower bound of Ω(N\\n2\\n3\\n−o(1)).\\n22\\n6 Conv3SUM vs. 3SUM — Proof of Theorem 1.6\\nIn this section we can tolerate approximately universal hash functions, and a more natural definition\\nof almost linearity.\\nDefinition 6.1. (Universality and Linearity) Let H be a family of hash functions from [u]→\\n[m].\\n1. H is called c-universal if for any distinct x, x′ ∈ [u],\\nPr\\nh∈H\\n(h(x) = h(x′)) ≤ c\\nm\\n.\\n2. H is called almost linear if for any h ∈ H and any x, x′ ∈ [u],\\nh(x) + h(x′) ≡ h(x+ x′) + {−1, 0} (modm).\\nBy tolerating approximate universality, we can use the simple hash functions analyzed by Di-\\netzfelbinger et al. [18].\\nTheorem 6.1. (Dietzfelbinger, Hagerup, Katajainen, and Penttonen [18]) Let u and m be powers\\nof two, with m < u. The family Hu,m is 2-universal and almost linear, where\\nHu,m = {ha : [u]→ [m] | a ∈ [u] is an odd integer}\\nand ha(x) = (axmodu) div(u/m).\\nBecause the modular arithmetic and division are by powers of two, the hash functions of The-\\norem 6.1 are very easy to implement using standard multiplication and shifts. If u = 2w, where\\nw is the number of bits per word, and m = 2s, the function is written in C as (a*x) >> (w-s).\\nDietzfelbinger et al. [18] proved that it is 2-universal. It is clearly almost linear.\\n6.1 Hashing and Coding Preliminaries\\nThe reduction in the next section makes use of any constant rate, constant relative distance binary\\ncode. The expander codes of Sipser and Spielman [57] are sufficient for our application.\\nTheorem 6.2. (See Sipser and Spielman [57]) There is a constant \\x0f > 0 such that for any suffi-\\nciently large δ > δ(\\x0f), there is a binary code C : {0, 1}N → {0, 1}δN such that for any x, y ∈ {0, 1}N ,\\nthe Hamming distance between C(x) and C(y) is at least \\x0f · δN . Moreover, C(x) can be computed\\nin O(δN) time.\\n6.2 The Reduction\\nLet [u]\\\\{0} = [2w]\\\\{0} be the universe. It is convenient to assume that 0 is excluded from A, but\\nthis is without loss of generality since all witnesses involving 0 can be enumerated in O(n log n)\\ntime by sorting A. Choose L hash functions (hi)i∈[L] independently from Hu,m, where m = 2dlogne\\nis the least power of two larger than n. Ideally a hash function will map A injectively into the\\nbuckets [m], or at least put a constant load on each bucket, but this cannot be guaranteed. Some\\nbuckets will be overloaded and the items in them discarded.\\n23\\nDefinition 6.2. (Overloaded Buckets, Discarded Elements) For each i ∈ [L] and j ∈ [m]\\ndefine\\nbucketi(j) = {x ∈ A | hi(x) = j}\\nto be the set of elements hashed by hi to the jth bucket. The truncation of this bucket is defined as\\nbucket?i (j) =\\n{\\nbucketi(j) if |bucketi(j)| ≤ T\\n∅ otherwise\\nwhere T = O(1) is a constant threshold to be determined. If bucket?i (j) = ∅ we say that the\\nelements of bucketi(j) were discarded by hi. An element is called bad if it is discarded by a\\n4/T -fraction of the hash functions.\\nLemma 6.3. The probability that an element is bad is at most exp\\n(− 2L3T ).\\nProof. Since each hi is 2-universal, the expected number of other elements in x’s bucket is, by\\nlinearity of expectation, at most 2(n − 1)/m < 2. By Markov’s inequality the probability that x\\nis discarded by hi is less than 2/T . Let X be the number of hash functions that discard x, so\\nE(X) < 2L/T . By definition x is bad if X > 4L/T > 2 · E(X). Since the hash functions were\\nchosen independently, by a Chernoff bound, Pr(x is bad) < exp\\n(− 2L3T ).\\nWe will set T = O(1) and L = Θ(log n) to be sufficiently large so that the probability that\\nno elements are bad is 1 − 1/poly(n). We proceed under the assumption that there are no bad\\nelements.\\nLemma 6.4. Suppose there are no bad elements with respect to (hi)i∈[L]. For any three a, b, c ∈ A,\\nthere are more than\\n(\\n1− 12T\\n)\\nL indices i ∈ [L] such that hi discards none of {a, b, c}.\\nProof. Each of a, b, c is discarded by less than 4L/T hash functions, so none are discarded by at\\nleast L− 12L/T hash functions.\\nLet δ > 1, \\x0f > 0 be the parameters of Theorem 6.2, where N = dlog ne and L = δN . We assign\\neach x ∈ A an L-bit codeword Cx such that any two Cx, Cy disagree in at least \\x0fL positions.\\nWe make 8TL calls to a Convolution3SUM algorithm on vectors {A`}`∈[L]×{−1,0}×{0,1}×[2T ],\\neach of length 14m = O(n). For reasons that will become clear we index the calls by tuples\\n` = (i, α, β, γ) ∈ [L]×{−1, 0}×{0, 1}× [2T ]. The first coordinate i of ` identifies the hash function.\\nThe second coordinate α indicates that we are looking for witnesses a, b, a + b ∈ A for which\\nhi(a)+hi(b) = hi(a+b)+α (modm). A natural way to define A` creates multiple copies of elements\\nbut can lead to a situation where there are false positives: we may have A`(p) +A`(q) = A`(p+ q)\\nand yet this is not a witness for the original 3SUM instance because A`(p) = A`(q).\\n6 In each call to\\nConvolution3SUM we look for witnesses where each element can play the role of either “p” or “q”\\nin the example above, but not both; all elements will be eligible to play the role of “p + q.” The\\nparity of Cx(i) xor β tells us which roles x is allowed to play, where β is the third coordinate of `.\\nThe fourth coordinate γ of ` effects a cyclic shift of the order of elements within a bucket.\\nEach vector A` is partitioned into 2m contiguous blocks, each of length 7T . Many of the locations\\nof A` are filled with a dummy value∞, which is some sufficiently large number that cannot be part\\n6This minor bug appears in Paˇtras¸cu’s reduction from 3SUM to Convolution3SUM.\\n24\\nFigure 4: Block j in A` occupies positions j(7T ) through (j + 1)(7T ) − 1. In the first half of\\nA`, a block is partitioned into five intervals. The first interval covers positions 0 through T − 1\\nand is always filled with a dummy value ∞. The second and third intervals run, respectively,\\nfrom positions T through 2T − 1 and positions 2T through 3T − 1. They contain those elements\\nx ∈ bucket?i (j − α) for which Cx(i) xor β is, respectively, 0 and 1. The fourth interval runs\\nfrom positions 3T through 5T − 1 and contains all members of bucket?i (j − α), cyclically shifted\\nby γ. The last interval, from positions 5T through 7T − 1, is always filled with dummies. The\\ncomposition of a block j in the second half of A` is similar, except that the second and third\\nintervals (positions T through 3T − 1) contain only dummies, and the fourth interval contains all\\nmembers of bucket?i ((j − α) modm).\\nof any witness, say 2 max(A) + 1. The elements of the jth bucket each appear three times in A`,\\ntwice in the first half and once in the second.\\nOrder the elements of bucket?i (j) arbitrarily as (x(i, j, k))k∈[T ], where x(i, j, k) does not exist\\nif k ≥ |bucket?i (j)|. Define the vector A(i,α,β,γ) as follows.\\nA(i,α,β,γ)(j(7T ) + t) =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\nx(i, j, k) if t = T + k, k ∈ [T ], and Cx(i,j,k)(i) xor β = 0\\nx(i, j, k) if t = 2T + k, k ∈ [T ], and Cx(i,j,k)(i) xor β = 1\\nx(i, (j − α) modm, k) if t = 3T + ((k + γ) mod 2T ) and k ∈ [T ]\\n∞ otherwise.\\nThe last case applies when j, k, or t is out of range or if the given element, say x(i, j, k), does not\\nexist because |bucket?i (j)| ≤ k. See Figure 4.\\nLemma 6.5. (No False Negatives) Suppose a, b, a + b ∈ A is a witness to the 3SUM instance\\nA. For some ` = (i, α, β, γ), this is also a witness in the Convolution3SUM instance A`.\\nProof. Set the threshold T = 12/\\x0f = O(1). By Lemma 6.4 there are more than L(1 − 12/T ) =\\nL(1−\\x0f) indices i ∈ [L] such that none of {a, b, a+b} are discarded by hi. Moreover, by the properties\\nof the error correcting code (Theorem 6.2) there are at least \\x0fL indices i for which Ca(i) 6= Cb(i),\\nwhich implies that both criteria are satisfied for at least one i. Fix any such i.\\nLet ja = hi(a), jb = hi(b), and ja+b = hi(a + b) be the bucket indices of a, b, and a + b. Let\\nka, kb, ka+b be their positions in those buckets, that is, a = x(i, ja, ka) and b = x(i, jb, kb), and\\na+ b = x(i, ja+b, ka+b). Without loss of generality ja ≤ jb. Let β = Ca(i), so Ca(i) xor β = 0 and\\nCb(i) xor β = 1. Let α ∈ {−1, 0} be such that hi(a) + hi(b) ≡ hi(a+ b) + α (modm).\\nIn the vector A(i,α,β,γ),\\n• a is at position ja(7T ) + T + ka, because Ca(i) xor β = 0,\\n• b is at position jb(7T ) + 2T + kb, because Cb(i) xor β = 1,\\n• and since ja+b ≡ ja+jb−α (modm), a+b is at position (ja+jb)(7T )+3T+((ka+b+γ) mod 2T ).\\n25\\nThus, for γ = (ka+kb−ka+b) mod 2T , the triple (a, b, a+b) forms a witness for the Convolution3SUM\\nvector A`.\\nLemma 6.6. (No False Positives) If (a, b, a+b) is a witness in some Convolution3SUM instance\\nA`, it is also a witness in the original 3SUM instance A.\\nProof. None of {a, b, a + b} can be the dummy ∞ in A`, so they must all be members of A. The\\nonly way it cannot be an witness for 3SUM is if b = a, that is, (a, a, 2a) is not a triple of distinct\\nnumbers. If a is not discarded, it appears at exactly three positions in A`. Regardless of the bit\\nCa(i), a appears at both A`((ja+α)(7T )+3T +((ka+γ) mod 2T )) and A`((m+ja+α)(7T )+3T +\\n((ka+γ) mod 2T )) for some ka ∈ [T ] and γ ∈ [2T ]. Depending on the parity of Cx(i) xor β, a also\\nappears at either A`(ja(7T )+T+ka) or A`(ja(7T )+2T+ka). For (a, a, 2a) to be a Convolution3SUM\\nwitness we would need 2a to appear either at A`\\n(\\n(2ja + α)(7T ) + 4T + ka + ((ka + γ) mod 2T )\\n)\\nor\\nA`\\n(\\n(2ja + α)(7T ) + 5T + ka + ((ka + γ) mod 2T )\\n)\\n. However, in both of those positions A` is ∞ by\\ndefinition. See Figure 4.\\nWe have shown that the randomized (Las Vegas) complexities of 3SUM and Convolution3SUM\\nare equivalent up to a logarithmic factor. Since hashing plays such an essential role in the reduction,\\nit would be surprising if our construction could be efficiently derandomized, or if it could be\\ngeneralized to show that 3SUM and Convolution3SUM over the reals are essentially equivalent.\\nThe O(log n)-factor gap in Theorem 1.6 stems from our solution to two technical difficulties, (i)\\nensuring that all triples appear in lightly loaded buckets with respect to a large fraction of the hash\\nfunctions, and (ii) ensuring that no non-3SUM witnesses (a, a, 2a) occur as witnesses in any Con-\\nvolution3SUM instance. We leave it as an open problem to show that 3SUM and Convolution3SUM\\nare asymptotically equivalent, without the O(log n)-factor gap.\\nReferences\\n[1] Amir Abboud and Virginia Vassilevska Williams. Popular conjectures imply strong lower\\nbounds for dynamic problems. In Proceedings of the 55th Annual IEEE Symposium on Foun-\\ndations of Computer Science (FOCS), pages 434–443, 2014.\\n[2] Amir Abboud, Virginia Vassilevska Williams, and Oren Weimann. Consequences of faster\\nalignment of sequences. In Proceedings of the 41st International Colloquium on Automata,\\nLanguages, and Programming (ICALP), pages 39–51, 2014.\\n[3] Amir Abboud, Virginia Vassilevska Williams, and Huacheng Yu. Matching triangles and\\nbasing hardness on an extremely popular conjecture. In Proceedings of the 47th Annual ACM\\nSymposium on Theory of Computing (STOC), pages 41–50, 2015.\\n[4] Josh Alman, Joshua R. Wang, and Huacheng Yu. Cell-probe lower bounds for dynamic prob-\\nlems via a new communication model. In Proceedings 50th Annual ACM Symposium on Theory\\nof Computing (STOC), pages 1003–1012, 2018.\\n[5] Amihood Amir, Timothy M. Chan, Moshe Lewenstein, and Noa Lewenstein. On hardness\\nof jumbled indexing. In Proceedings of the 41st International Colloquium on Automata, Lan-\\nguages, and Programming (ICALP), pages 114–125, 2014.\\n26\\n[6] Amihood Amir, Tsvi Kopelowitz4, Avivit Levy, Seth Pettie, Ely Porat, and B. Riva Shalom.\\nMind the gap: Essentially optimal algorithms for online dictionary matching with one gap. In\\nProceedings of the 27th International Symposium on Algorithms and Computation (ISAAC),\\npages 12:1–12:12, 2016.\\n[7] Ilya Baran, Erik D. Demaine, and Mihai Paˇtras¸cu. Subquadratic algorithms for 3SUM. Algo-\\nrithmica, 50(4):584–596, 2008.\\n[8] Aaron Bernstein, Jacob Holm, and Eva Rotenberg. Online bipartite matching with amor-\\ntized replacements. In Proceedings of the 29th Annual ACM-SIAM Symposium on Discrete\\nAlgorithms (SODA), pages 947–959, 2018.\\n[9] Andreas Bjo¨rklund, Rasmus Pagh, Virginia Vassilevska Williams, and Uri Zwick. Listing\\ntriangles. In Proceedings of the 41st International Colloquium on Automata, Languages, and\\nProgramming (ICALP), pages 223–234, 2014.\\n[10] Bartlomiej Bosek, Dariusz Leniowski, Piotr Sankowski, and Anna Zych. Online bipartite\\nmatching in offline time. In Proceedings of the 55th Annual IEEE Symposium on Foundations\\nof Computer Science (FOCS), pages 384–393, 2014.\\n[11] Diptarka Chakraborty, Lior Kamma, and Kasper Green Larsen. Tight cell probe bounds for\\nsuccinct Boolean matrix-vector multiplication. In Proceedings 50th Annual ACM Symposium\\non Theory of Computing (STOC), pages ??–??, 2018.\\n[12] Timothy M. Chan. More logarithmic-factor speedups for 3SUM, (median,+)-convolution,\\nand some geometric 3SUM-hard problems. In Proceedings of the 29th Annual ACM-SIAM\\nSymposium on Discrete Algorithms (SODA), pages 881–897, 2018.\\n[13] Timothy M. Chan and Moshe Lewenstein. Clustered integer 3SUM via additive combinatorics.\\nIn Proceedings of the 47th Annual ACM Symposium on Theory of Computing (STOC), pages\\n31–40, 2015.\\n[14] Norishige Chiba and Takao Nishizeki. Arboricity and subgraph listing algorithms. SIAM J.\\nComput., 14(1):210–223, 1985.\\n[15] Hagai Cohen and Ely Porat. Fast set intersection and two-patterns matching. Theor. Comput.\\nSci., 411(40-42):3795–3800, 2010.\\n[16] Søren Dahlgaard. On the hardness of partially dynamic graph problems and connections\\nto diameter. In Proceedings 43rd International Colloquium on Automata, Languages, and\\nProgramming, (ICALP), pages 48:1–48:14, 2016.\\n[17] Martin Dietzfelbinger. Universal hashing and k-wise independent random variables via integer\\narithmetic without primes. In Proceedings of the 13th Annual Symposium on Theoretical\\nAspects of Computer Science (STACS), pages 569–580, 1996.\\n[18] Martin Dietzfelbinger, Torben Hagerup, Jyrki Katajainen, and Martti Penttonen. A reliable\\nrandomized algorithm for the closest-pair problem. J. Algor., 25(1):19–51, 1997.\\n[19] Ran Duan and Seth Pettie. Connectivity oracles for failure prone graphs. In Proceedings of\\nthe 42nd ACM Symposium on Theory of Computing (STOC), pages 465–474, 2010.\\n27\\n[20] Ran Duan and Seth Pettie. Connectivity oracles for graphs subject to vertex failures. In\\nProceedings of the 28th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages\\n490–509, 2017.\\n[21] David Eppstein, Michael T. Goodrich, Michael Mitzenmacher, and Manuel R. Torres. 2-3\\nCuckoo filters for faster triangle listing and set intersection. In Proceedings of the 36th ACM\\nSIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems (PODS), pages 247–\\n260, 2017.\\n[22] Johannes Fischer, Travis Gagie, Tsvi Kopelowitz, Moshe Lewenstein, Veli Ma¨kinen, Leena\\nSalmela, and Niko Va¨lima¨ki. Forbidden patterns. In Proceedings of the 10th Latin American\\nSymposium on Theoretical Informatics (LATIN), pages 327–337, 2012.\\n[23] Ari Freund. Improved subquadratic 3SUM. Algorithmica, 77(2):440–458, 2017.\\n[24] Harold N. Gabow and Robert E. Tarjan. A linear-time algorithm for a special case of disjoint\\nset union. J. Comput. Syst. Sci., 30(2):209–221, 1985.\\n[25] Harold N. Gabow and Robert E. Tarjan. Faster scaling algorithms for general graph-matching\\nproblems. J. ACM, 38(4):815–853, 1991.\\n[26] Anka Gajentaan and Mark H. Overmars. On a class of O(n2) problems in computational\\ngeometry. Comput. Geom., 5:165–185, 1995.\\n[27] Omer Gold and Micha Sharir. Improved bounds for 3SUM, k-SUM, and linear degeneracy. In\\nProceedings of the 25th Annual European Symposium on Algorithms (ESA), pages 42:1–42:13,\\n2017.\\n[28] Isaac Goldstein, Tsvi Kopelowitz, Moshe Lewenstein, and Ely Porat. How hard is it to find\\n(honest) witnesses? In Proceedings of the 24th Annual European Symposium on Algorithms\\n(ESA), pages 45:1–45:16, 2016.\\n[29] Isaac Goldstein, Tsvi Kopelowitz, Moshe Lewenstein, and Ely Porat. Conditional lower bounds\\nfor space/time tradeoffs. In Proceedings of the 15th International Symposium on Algorithms\\nand Data Structures (WADS), pages 421–436, 2017.\\n[30] Allan Grønlund and Seth Pettie. Threesomes, degenerates, and love triangles. In Proceedings\\nof the 55th Annual IEEE Symposium on Foundations of Computer Science, FOCS, pages\\n621–630, 2014.\\n[31] Allan Grønlund and Seth Pettie. Threesomes, degenerates, and love triangles. J. ACM, 2018.\\nto appear.\\n[32] Monika Henzinger, Sebastian Krinninger, Danupon Nanongkai, and Thatchaphol Saranurak.\\nUnifying and strengthening hardness for dynamic problems via the online matrix-vector mul-\\ntiplication conjecture. In Proceedings of the 47th Annual ACM Symposium on Theory of\\nComputing (STOC), pages 21–30, 2015.\\n[33] Wing-Kai Hon, Rahul Shah, Sharma V. Thankachan, and Jeffrey Scott Vitter. String retrieval\\nfor multi-pattern queries. In Proceedings of the 17th International Symposium on String Pro-\\ncessing and Information Retrieval (SPIRE), pages 55–66, 2010.\\n28\\n[34] Wing-Kai Hon, Rahul Shah, Sharma V. Thankachan, and Jeffrey Scott Vitter. Document\\nlisting for queries with excluded pattern. In Proceedings of the 23rd Annual Symposium on\\nCombinatorial Pattern Matching (CPM), pages 185–195, 2012.\\n[35] Alon Itai and Michael Rodeh. Finding a minimum circuit in a graph. SIAM J. Comput.,\\n7(4):413–423, 1978.\\n[36] Zahra Jafargholi and Emanuele Viola. 3SUM, 3XOR, triangles. Algorithmica, pages 1–18,\\n2014.\\n[37] Daniel M. Kane, Shachar Lovett, and Shay Moran. Near-optimal linear decision trees for\\nk-SUM and related problems. J. ACM. to appear.\\n[38] Bruce M. Kapron, Valerie King, and Ben Mountjoy. Dynamic graph connectivity in poly-\\nlogarithmic worst case time. In Proceedings of the 24th Annual ACM-SIAM Symposium on\\nDiscrete Algorithms (SODA), pages 1131–1142, 2013.\\n[39] T. Kopelowitz and R. Krauthgamer. Color-distance oracles and snippets. In 27th Annual\\nSymposium on Combinatorial Pattern Matching, CPM, pages 24:1–24:10, 2016.\\n[40] Tsvi Kopelowitz, Robert Krauthgamer, Ely Porat, and Shay Solomon. Orienting fully dynamic\\ngraphs with worst-case time bounds. In Proceedings of the 41st International Colloquium on\\nAutomata, Languages, and Programming (ICALP), pages 532–543, 2014.\\n[41] Tsvi Kopelowitz, Seth Pettie, and Ely Porat. Dynamic set intersection. In Proceedings 14th\\nInternational Symposium on Algorithms and Data Structures (WADS), pages 470–481, 2015.\\n[42] Tsvi Kopelowitz, Seth Pettie, and Ely Porat. Higher lower bounds from the 3SUM conjecture.\\nIn Proceedings 27th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages\\n1272–1287, 2016.\\n[43] Kasper Green Larsen. The cell probe complexity of dynamic range counting. In Proceedings\\nof the 44th Annual ACM Symposium on Theory of Computing (STOC), pages 85–94, 2012.\\n[44] Kasper Green Larsen, J. Ian Munro, Jesper Sindahl Nielsen, and Sharma V. Thankachan. On\\nhardness of several string indexing problems. In Proceedings of the 25th Annual Symposium\\non Combinatorial Pattern Matching (CPM), pages 242–251, 2014.\\n[45] Kasper Green Larsen, Omri Weinstein, and Huacheng Yu. Crossing the logarithmic barrier for\\ndynamic boolean data structure lower bounds. In Proceedings 50th Annual ACM Symposium\\non Theory of Computing (STOC), pages 978–989, 2018.\\n[46] Kasper Green Larsen and R. Ryan Williams. Faster online matrix-vector multiplication. In\\nProceedings of the 28th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages\\n2182–2189, 2017.\\n[47] Daniel Lokshtanov, Ramamohan Paturi, Suguru Tamaki, R. Ryan Williams, and Huacheng\\nYu. Beating brute force for systems of polynomial equations over finite fields. In Proceedings of\\nthe 28th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 2190–2202,\\n2017.\\n29\\n[48] S. Muthukrishnan. Efficient algorithms for document retrieval problems. In Proceedings of the\\n13th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 657–666, 2002.\\n[49] C. St.J. A. Nash-Williams. Edge-disjoint spanning trees in finite graphs. Journal of the London\\nMathematical Society, 36(1):445–450, 1961.\\n[50] C. St.J. A. Nash-Williams. Decomposition of finite graphs into forests. Journal of the London\\nMathematical Society, 39(1):12, 1964.\\n[51] Ofer Neiman and Shay Solomon. Simple deterministic algorithms for fully dynamic maxi-\\nmal matching. In Proceedings of the 45th Annual ACM Symposium on Theory of Computing\\n(STOC), pages 745–754, 2013.\\n[52] Mihai Paˇtras¸cu and Erik D. Demaine. Logarithmic lower bounds in the cell-probe model.\\nSIAM J. Comput., 35(4):932–963, 2006.\\n[53] Mihai Paˇtras¸cu and Mikkel Thorup. Planning for fast connectivity updates. In Proceedings\\nof the 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages\\n263–271, 2007.\\n[54] Mihai Paˇtras¸cu and Mikkel Thorup. Don’t rush into a union: take time to find your roots.\\nIn Proceedings of the 43rd Annual ACM Symposium on Theory of Computing (STOC), pages\\n559–568, 2011.\\n[55] Mihai Paˇtras¸cu. Towards polynomial lower bounds for dynamic problems. In Proceedings of\\nthe 42nd Annual ACM Symposium on Theory of Computing (STOC), pages 603–610, 2010.\\n[56] Piotr Sankowski. Faster dynamic matchings and vertex connectivity. In Proceedings of the\\n28th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 118–126, 2007.\\n[57] Michael Sipser and Daniel A. Spielman. Expander codes. IEEE Transactions on Information\\nTheory, 42(6):1710–1722, 1996.\\n[58] Ryan Williams. Faster all-pairs shortest paths via circuit complexity. In Proceedings of the\\n46th Annual ACM Symposium on Theory of Computing (STOC), pages 664–673, 2014.\\n[59] Virginia Vassilevska Williams and Ryan Williams. Finding, minimizing, and counting weighted\\nsubgraphs. SIAM J. Comput., 42(3):831–854, 2013.\\n[60] Huacheng Yu. Cell-probe lower bounds for dynamic problems via a new communication model.\\nIn Proceedings 48th Annual ACM Symposium on Theory of Computing (STOC), pages 362–\\n374, 2016.\\n30\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd19'), 'authors': 'Abram J., Demetrescu C., Lynch N. A., Santoro N.', 'year': '2014', 'title': 'Distributed Approximation Algorithms for Weighted Shortest Paths', 'full_text': 'Distributed Approximation Algorithms for Weighted Shortest Paths\\nDanupon Nanongkai∗\\nAbstract\\nA distributed network is modeled by a graph having n nodes (processors) and diameter\\nD. We study the time complexity of approximating weighted (undirected) shortest paths on\\ndistributed networks with a O(log n) bandwidth restriction on edges (the standard synchronous\\nCONGEST model). The question whether approximation algorithms help speed up the shortest\\npaths (more precisely distance computation) was raised since at least 2004 by Elkin (SIGACT\\nNews 2004). The unweighted case of this problem is well-understood while its weighted counter-\\npart is fundamental problem in the area of distributed approximation algorithms and remains\\nwidely open. We present new algorithms for computing both single-source shortest paths (SSSP)\\nand all-pairs shortest paths (APSP) in the weighted case.\\nOur main result is an algorithm for SSSP. Previous results are the classic O(n)-time Bellman-\\nFord algorithm and an O˜(n1/2+1/2k + D)-time (8kdlog(k + 1)e − 1)-approximation algorithm,\\nfor any integer k ≥ 1, which follows from the result of Lenzen and Patt-Shamir (STOC 2013).\\n(Note that Lenzen and Patt-Shamir in fact solve a harder problem, and we use O˜(·) to hide the\\nO(poly log n) term.) We present an O˜(n1/2D1/4 +D)-time (1 + o(1))-approximation algorithm\\nfor SSSP. This algorithm is sublinear-time as long as D is sublinear, thus yielding a sublinear-\\ntime algorithm with almost optimal solution. When D is small, our running time matches the\\nlower bound of Ω˜(n1/2 + D) by Das Sarma et al. (SICOMP 2012), which holds even when\\nD = Θ(log n), up to a poly log n factor.\\nAs a by-product of our technique, we obtain a simple O˜(n)-time (1 + o(1))-approximation\\nalgorithm for APSP, improving the previous O˜(n)-time O(1)-approximation algorithm following\\nfrom the results of Lenzen and Patt-Shamir. We also prove a matching lower bound. Our\\ntechniques also yield an O˜(n1/2) time algorithm on fully-connected networks, which guarantees\\nan exact solution for SSSP and a (2 + o(1))-approximate solution for APSP. All our algorithms\\nrely on two new simple tools: light-weight algorithm for bounded-hop SSSP and shortest-path\\ndiameter reduction via shortcuts. These tools might be of an independent interest and useful in\\ndesigning other distributed algorithms.\\n∗ICERM, Brown University, USA. Work partially done while at University of Vienna, Austria and Nanyang Tech-\\nnological University, Singapore, and while supported in part by the following research grants: Nanyang Technological\\nUniversity grant M58110000, Singapore Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 2 grant\\nMOE2010-T2-2-082, and Singapore MOE AcRF Tier 1 grant MOE2012-T1-001-094.\\ni\\nar\\nX\\niv\\n:1\\n40\\n3.\\n51\\n71\\nv2\\n  [\\ncs\\n.D\\nS]\\n  2\\n2 M\\nay\\n 20\\n14\\nContents\\n1 Introduction 1\\n1.1 The Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\\n1.2 Problems & Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\\n1.3 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.4 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2 Overview 7\\n2.1 Tool 1: Light-Weight Bounded-Hop SSSP (Details in Section 3.1) . . . . . . . . . . . 7\\n2.2 Tool 2: Shortest-Path Diameter Reduction Using Shortcuts (Details in Section 3.2) . 8\\n2.3 Sketches of Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3 Main Tools 11\\n3.1 Light-Weight Bounded-Hop Single-Source and Multi-Source Shortest Paths . . . . . 11\\n3.1.1 Reducing Bounded-Hop Distance by Approximating Weights . . . . . . . . . 12\\n3.1.2 Algorithm for Bounded-Hop SSSP (Proof of Theorem 3.2) . . . . . . . . . . . 14\\n3.1.3 Bounded-Hop Multi-Source Shortest Paths . . . . . . . . . . . . . . . . . . . 15\\n3.2 Shortest-Path Diameter Reduction Using Shortcuts . . . . . . . . . . . . . . . . . . . 17\\n4 Algorithms on General Networks 19\\n4.1 Reduction to Single-Source Shortest Path on Overlay Networks . . . . . . . . . . . . 19\\n4.2 Reducing the Shortest Path Diameter of Overlay Network (G′, w′) . . . . . . . . . . 21\\n4.3 Computing SSSP on Overlay Network (G′′, w′′) . . . . . . . . . . . . . . . . . . . . . 22\\n4.4 Putting Everything Together (Proof of Theorem 1.2) . . . . . . . . . . . . . . . . . . 23\\n5 Algorithms on Fully-Connected Networks 23\\n5.1 O˜(\\n√\\nn)-time Exact Algorithm for SSSP . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n5.2 O˜(\\n√\\nn)-time (2 + o(1))-Approximation Algorithm for APSP . . . . . . . . . . . . . . 25\\n6 Lower Bound for Approximating APSP (Proof of Observation 1.4) 28\\n7 Open Problems 29\\n8 Acknowledgement 30\\nii\\n1 Introduction\\nIt is a fundamental issue to understand the possibilities and limitations of distributed/decentralized\\ncomputation, i.e., to what degree local information is sufficient to solve global tasks. Many tasks\\ncan be solved entirely via local communication, for instance, how many friends of friends one has.\\nResearch in the last 30 years has shown that some classic combinatorial optimization problems\\nsuch as matching, coloring, dominating set, or approximations thereof can be solved using small\\n(i.e., polylogarithmic) local communication. However, many important optimization problems are\\n“global” problems from the distributed computation point of view. To count the total number\\nof nodes, to determining the diameter of the system, or to compute a spanning tree, information\\nnecessarily must travel to the farthest nodes in a system. If exchanging a message over a single\\nedge costs one time unit, one needs Ω(D) time units to compute the result, where D is the network’s\\ndiameter. In a model where message size could be unbounded (often known as the LOCAL model),\\none can simply collect all the information in O(D) time (ignoring time for the local computation),\\nand then compute the result. A more realistic model, however, has to take into account the\\ncongestion issue and limits the size of a message allowed to be sent in a single communication\\nround to some B bits, where B is typically set to log n. This model is often called synchronous\\nCONGEST (or CONGEST(B) if B 6= log n). Time complexity in this model is one of the major\\nstudies in distributed computing [Pel00].\\nMany previous works in this model, including several previous FOCS/STOC papers (e.g. [GKP98,\\nPR00, Elk06, DHK+12, LPS13]), concern graph problems. Here, we want to learn some topo-\\nlogical properties of a network, such as minimum spanning tree (MST), minimum cut (mincut),\\nand distances. These problems can be trivially solved in O(m) rounds, where m is the number\\nof edges, by aggregating the whole network into one node. Of course, this is neither interest-\\ning nor satisfactory. The holy grail in the area of distributed graph algorithms is to beat this\\nbound and, in many case, obtain a sublinear-time algorithm whose running time is in the form\\nO˜(n1−\\x0f + D) for some constant \\x0f > 0, where n is the number of nodes and D is the network’s\\ndiameter. For example, through decades of extensive research, we now have an algorithm that can\\nfind an MST in O˜(n1/2 + D) time [GKP98, KP98], and we know that this running time is tight\\n[PR00]. This algorithm serves as a building block for several other sublinear-time algorithms (e.g.\\n[Thu97, PT11, GK13, Nan14, Su14]).\\nIt is also natural to ask whether we can further improve the running time of existing graph\\nalgorithms by mean of approximation, e.g., if we allow an algorithm to output a spanning tree\\nthat is almost, but not, minimum. This question has generated a research in the direction of\\ndistributed approximation algorithms which has become fruitful in the recent years. On the negative\\nside, Das Sarma et al. [DHK+12] (building on [PR00, Elk06, KKP13]) show that MST and a\\ndozen other problems, including mincut and computing the distance between two nodes, cannot\\nbe computed faster than O˜(n1/2 + D) in the synchronous CONGEST model even when we allow\\na large approximation ratio. On the positive side, we start to be able to solve some problems in\\nsublinear time by sacrifying a small approximation factor; e.g., we can (1+\\x0f)-approximate mincut in\\nO˜(n1/2+D) time [Nan14, Su14] and (3/2)-approximate the network’s diameter in O(n3/4+D) time\\n[HW12, PRT12]. The question whether distributed approximation algorithms can help improving\\nthe time complexity of computing shortest paths was raised a decade ago by Elkin [Elk04]. It\\nis surprising that, despite so much progress on other problems in the last decade, the problem\\nof computing shortest paths is still widely open, especially when we want a small approximation\\nguarantee. Prior to our work, sublinear-time algorithms for computing single-source shortest path\\n1\\n(SSSP) and linear-time algorithms for computing all-pairs shortest paths (APSP) have to pay a high\\napproximation factor [LPS13]. This paper fills this gap with algorithms having small approximation\\nguarantees.\\n1.1 The Model\\nConsider a network of processors modeled by an undirected unweighted n-node m-edge graph\\nG, where nodes model the processors and edges model the bounded-bandwidth links between the\\nprocessors. Let V (G) and E(G) denote the set of nodes and edges of G, respectively. The processors\\n(henceforth, nodes) are assumed to have unique IDs in the range of {1, . . . ,poly(n)} and infinite\\ncomputational power. Each node has limited topological knowledge; in particular, it only knows\\nthe IDs of its neighbors and knows no other topological information (e.g., whether its neighbors\\nare linked by an edge or not). Nodes may also accept some additional inputs as specified by the\\nproblem at hand.\\nFor the case of graph problems, the additional input is edge weights. Let w : E(G) →\\n{1, 2, . . . ,poly(n)} be the edge weight assignment. We refer to network G with weight assign-\\nment w as the weighted network, denoted by (G,w). The weight w(uv) of each edge uv is known\\nonly to u and v. As commonly done in the literature (e.g., [KP08, LPSR09, KP98, GKP98, GK13]),\\nwe will assume that the maximum weight is poly(n); so, each edge weight can be sent through an\\nedge (link) in one round.1\\nThere are several measures to analyze the performance of such algorithms, a fundamental one\\nbeing the running time, defined as the worst-case number of rounds of distributed communication.\\nAt the beginning of each round, all nodes wake up simultaneously. Each node u then sends an\\narbitrary message of B = log n bits through each edge uv, and the message will arrive at node v at\\nthe end of the round.\\nWe assume that nodes always know the number of the current round. To simplify notations,\\nwe will name nodes using their IDs, i.e. we let V (G) ⊆ {1, . . . ,poly(n)}. Thus, we use u ∈ V (G)\\nto represent a node, as well as its ID. The running time is analyzed in terms of number of nodes\\nn, number of edge m, and D, the diameter of the network G. Since we can compute n and 2-\\napproximate D in O(D) time, we will assume that every node knows n and the 2-approximate value\\nof D. We say that an event holds with high probability (w.h.p.) if it holds with probability at least\\n1− 1/nc, where c is an arbitrarily large constant.\\n1.2 Problems & Definitions\\nFor any nodes u and v, a u-v path P is a path 〈u = x0, x1, . . . , x` = v〉 where xixi+1 ∈ E(G) for all\\ni. For any weight assignment w, we define the weight or distance of P as w(P ) =\\n∑`−1\\ni=0 w(xixi+1).\\nLet PG(u, v) denote the set of all u-v paths in G. We use distG,w(u, v) to denote the distance from\\nu to v in (G,w); i.e., distG,w(u, v) = minP∈PG(u,v)w(P ). We say that a path P is a shortest u-v\\npath in (G,w) if w(P ) = distG,w(u, v). The diameter of (G,w) is D(G,w) = maxu,v distG,w(u, v).\\nWhen we want to talk about the properties of the underlying undirected unweighted network G,\\nwe will drop w from the notations. Thus, distG(u, v) is the distance between u and v in G and,\\nD(G) is the diameter of G. We refer to D(G) by “hop diameter”, or sometimes simply “diameter”,\\n1We note that, besides needing this assumption to ensure that weights can be encoded by O(logn) bits, we also\\nneed it in the analysis of the running time of our algorithms: most running times of our algorithms are logarithmic\\nof the largest edge weight. This is in the same spirit as, e.g., [LPSR09, GK13, KP08].\\n2\\nProblems Topology References Time Approximation\\nSSSP\\nGeneral\\nBellman&Ford [Bel58, For56] O˜(n) exact\\nLenzen&Patt-Shamir [LPS13]2 O˜(n1/2+1/2k + D) 8kdlog(k + 1)e − 1\\nthis paper O˜(n1/2D1/4 + D) 1 + o(1)\\n(= O˜(n3/4 + D))\\nFully-Connected\\nBaswana&Sen [BS07] O˜(n1/k) 2k − 1\\nthis paper O˜(n1/2) exact\\nAPSP\\nGeneral\\nTrivial O(m) exact\\nLenzen&Patt-Shamir [LPS13] O˜(n) O(1)\\nthis paper O˜(n) 1 + o(1)\\nFully-Connected\\nBaswana&Sen [BS07] O˜(n1/k) 2k − 1\\nthis paper O˜(n1/2) 2 + o(1)\\nTable 1: Summary of previous and our results (presented in Sections 1.3 and 1.4). Parameter k ≥ 1\\nis an integer (note that the time complexities above are sublinear only when k ≥ 2). Note that this\\ntable omits previous running times based on other parameters (such as shortest-path diameter).\\nand D(G,w) by “weighted diameter”. When it is clear from the context, we use D to denote D(G).\\nWe emphasize that, like other papers in the literature, the term D which appears in the running\\ntime of our algorithms is the diameter of the underlying unweighted network G.\\nDefinition 1.1 (Single-Source and All-Pairs Shortest Paths (SSSP, APSP)). In the single-source\\nshortest paths problem (SSSP), we are given a weighted network (G,w) as above and a source node\\ns (the ID of s is known to every node). We want to find the distance between s and every node v\\nin (G,w), denoted by distG,w(s, v). In particular, we want v to know the value of distG,w(s, v). In\\nthe all-pairs shortest paths problem (APSP), we want to find distG,w(u, v) for every pair (u, v) of\\nnodes. In particular, we want both u and v to know the value of distG,w(u, v).\\nFor any α, we say an algorithm A is an α-approximation algorithm for SSSP if it outputs d˜istG,w\\nsuch that distG,w(s, v) ≤ d˜istG,w(s, v) ≤ α distG,w(s, v) for all v. Similarly, we say that A is an α-\\napproximation algorithm for APSP if it outputs d˜istG,w such that distG,w(u, v) ≤ d˜istG,w(u, v) ≤\\nα distG,w(u, v) for all u and v.\\nRemark. We emphasize that we do not require every node to know all distances. Also note that,\\nwhile our paper focuses on computing distances between nodes, it can be used to find a routing\\npath or compute a routing table as well. For example, after solving APSP, nodes can exchange\\nall distance information with their neighbors in O(n) time. Then, when a node u wants to send a\\nmessage to node v, it simply sends such message to the neighbor x with smallest distG,w(v, x). The\\nname shortest paths is inherited from [DHK+12] (see the definition of shortest s-t path problem in\\n[DHK+12, Section 2.5]) and in particular the lower bound in [DHK+12] holds for our problem).\\n1.3 Our Results\\nOur and previous results are summarized in Table 1 (see Section 1.4 for the details of previous\\nresults). As shown in the table, previous algorithms either have large approximation guarantee or\\nlarge running time. In this paper, we aim at algorithms with both small approximation guarantees\\n3\\nand small running time. We consider both SSSP and APSP and study algorithms on both general\\nnetworks and fully-connected networks. Our main result is a sublinear-time (1+o(1))-approximation\\nalgorithm for SSSP on general graphs:\\nTheorem 1.2 (SSSP on general graph). There is a distributed (1 + o(1))-approximation algorithm\\nthat solves SSSP on any weighted n-node network (G,w). It finishes in O˜(n1/2D1/4+D) time w.h.p.\\nFor typical real-world networks (e.g., ad hoc networks and peer-to-peer networks) D is small\\n(usually O˜(1)). (In some networks, an even stronger property also holds; e.g., a peer-to-peer\\nnetwork is usually assumed to be an expander [APRU12].) It is thus of a special interest to develop\\nan algorithm in this setting. For example, [LPSP06] studied MST on constant-diameter networks.\\nDas Sarma et al. [DNPT13] developed a O˜((`D)1/2)-time algorithm for computing a random walk\\nof length `, which is faster than the trivial O(`)-time algorithm when D is small. In the same spirit,\\nour algorithm is faster than previous algorithms. Moreover, in this case our running time matches\\nthe lower bound of Ω˜(n1/2 + D) [DHK+12, EKNP12], which holds even for any algorithm with\\npoly(n) approximation ratio; thus, our result settles the status of SSSP for this case. Additionally,\\nsince the same lower bound also holds in the quantum setting [EKNP12], our result makes SSSP\\namong a few problems (others are MST and mincut) that quantum communication cannot help\\nspeeding up distributed algorithms significantly.\\nObserve that our running time is sublinear as long as D is sublinear in n (since O˜(n1/2D1/4 +D)\\ncan be written as O˜(n3/4 + D)). As shown in Table 1, previously we can either solve SSSP exactly\\nin O˜(n) time using Bellman-Ford algorithm [Bel58, For56] or (8kdlog(k + 1)e − 1)-approximately,\\nfor any k > 1, in O˜(n1/2+1/2k + D) time by applying the algorithm of Lenzen and Patt-Shamir\\n[LPS13]2. Our algorithm is the first that gives an output very close to the optimal solution in\\nsublinear time. Our result also points to an interesting direction in proving a stronger lower bound\\nfor SSSP: in contrast to previous lower bound techniques which usually work on low-diameter\\nnetworks, proving a stronger lower bound for SSSP needs a new technique that must exploit the\\nfact that the network’s diameter is fairly large. As a by-product of our techniques, we also obtain\\na linear-time algorithm for APSP.\\nTheorem 1.3 (APSP on general graphs). There is a distributed (1+o(1))-approximation algorithm\\nthat solves APSP on any weighted n-node network (G,w) which finishes in O˜(n) time w.h.p.\\nWe also observe that this algorithm is essentially tight:\\nObservation 1.4 (Lower bound for APSP). Any poly(n)-approximation algorithm for APSP on an\\nn-node weighted network G requires Ω( nlogn) time. This lower bound holds even when the underlying\\nnetwork G has diameter D(G) = 2. Moreover, for any α(n) = O(n), any α(n)-approximation\\nalgorithm on an unweighted network requires Ω( nα(n) logn) time.\\nObservation 1.4 implies that the running time of our algorithm in Theorem 1.3 is tight up to\\na poly log n factor, unless we allow a prohibitively large approximation factor of poly n. Moreover,\\neven when we restrict ourselves to unweighted networks, we still cannot significantly improve the\\nrunning time, unless the approximation ratio is fairly large; e.g., any n1−δ-time algorithm must\\nallow an approximation ratio of Ω(nδ/ log n). We note that a similar result to Observation 1.4 has\\n2Note that by applying the technique of Lenzen and Patt-Shamir with carefully selected parameters, the approx-\\nimation ratio can be reduced to 4k − 1. We thank Christoph Lenzen (personal communication) for this information.\\n4\\nbeen independently proved by Lenzen and Patt-Shamir [LPS13] in the context of name-independent\\nrouting scheme.\\nOther by-products of our techniques are efficient algorithms on fully-connected distributed\\nnetworks, i.e., when G is a complete graph. As mentioned earlier, it is of an interest to study\\nalgorithms on low-diameter networks. The case of fully-connected networks is an extreme case\\nwhere D = 1. This special setting captures, e.g., overlay and peer-to-peer networks, and has\\nreceived a considerable attention recently (e.g. [LPSPP05, LW11, PST11, Len13, DLP12, BHP12]).\\nObviously, this model gives more power to algorithms since every node can directly communicate\\nwith all other nodes; for example, MST can be constructed in O(log log n) time [LPSPP05], as\\nopposed to the Ω˜(n1/2 + D) lower bound on general networks. No sublinear-time algorithm for\\nSSSP and APSP is known even on this model if we want an optimal or near-optimal solution. In\\nthis paper, we show such an algorithm. First, note that our O˜(n1/2D1/4 + D)-time algorithm in\\nTheorem 1.2 already implies that SSSP can be (1 + o(1))-approximated in O˜(n1/2) time. We show\\nthat, as an application of our techniques for proving Theorem 1.2, we can get an exact algorithm\\nwithin the same running time. More importantly, we show that these techniques, combined with\\nsome new ideas, lead to a (2 + o(1))-approximation O˜(\\n√\\nn)-time algorithm for APSP. The latter\\nresult is in contrast with the general setting where we show that a sublinear running time is\\nimpossible even when we allow large approximation ratios (Observation 1.4).\\nTheorem 1.5 (Sublinear time algorithm on fully-connected networks). On any fully-connected\\nweighted network, in O˜(n1/2) time, SSSP can be solved exactly and APSP can be (2 + o(1))-\\napproximated w.h.p.\\n1.4 Related Work\\nUnweighted Case. SSSP and APSP are essentially well-understood in the unweighted case. SSSP\\ncan be trivially solved in O(D) time using a breadth-first search tree [Pel00, Lyn96]. Frischknecht,\\nHolzer, and Wattenhofer [FHW12, HW12] show a (surprising) lower bound of Ω(n/ log n) for com-\\nputing the diameter of unweighted networks, which implies a lower bound for solving unweighted\\nAPSP. This lower bound holds even for (3/2 − \\x0f)-approximation algorithms. This lower bound is\\nmatched (up to a poly log n factor) by O(n)-time deterministic exact algorithms for unweighted\\nAPSP found independently by [HW12] and [PRT12]. Another case that has been considered is\\nwhen nodes can talk to any other node in one time unit. This can be thought of as a special case\\nof APSP on fully-connected networks where edge weights are either 1 or ∞. In this case, Holzer\\n[Hol13] shows that SSSP can be solved in O˜(n1/2) time3.\\nName-Dependent Routing Scheme. For the weighted SSSP and APSP on general networks,\\nthe best known results follow from the recent algorithm for computing tables for name-dependent\\nrouting and distance approximation by Lenzen and Patt-Shamir [LPS13]. In particular, consider\\nany integer k > 1. Lenzen and Patt-Shamir [LPS13, Theorem 4.12] showed that in time τ =\\nO˜(n1/2+1/2k + D) every node u can compute a label λ(u) of size σ = O(log(k + 1) log n) and a\\nfunction distu that maps label λ(v) of any node v to a distance approximation distu(v) such that\\ndistG,w(u, v) ≤ distu(v) ≤ ρ distG,w(u, v) where ρ = 8kdlog(k + 1)e − 1.4 We can solve SSSP\\n3We thank Stephan Holzer for pointing this out.\\n4We note that Lenzen and Patt-Shamir [LPS13, Theorem 4.12] actually allow k = 1. However, their algorithm\\nrelies on the result of Baswana and Sen (see Theorem 4.7 in their paper) which does not allow k = 1.\\n5\\nby running the above algorithm of Lenzen and Patt-Shamir and broadcasting the label λ(s) of the\\nsource s to all nodes. This takes time τ+σ = O˜(n1/2+1/2k+D) and has an approximation guarantee\\nof ρ. We can solve APSP by broadcasting λ(v) for all v, taking time τ + nσ = O˜(n).\\nSparsification. The shortest path problem is one of the main motivations to study distributed\\nalgorithms for graph sparsification. These algorithms5 have either super-linear time or large approx-\\nimation guarantees. For example, Elkin and Zhang [EZ06] present an algorithm for the unweighted\\ncase based on a sparse spanner that takes (very roughly) O(nξ) time and gives (1 + \\x0f)-approximate\\nsolution, for small constants ξ and \\x0f. The algorithm is also extended to the weighted case but both\\nrunning time and approximation guarantee are large (linear in terms of the largest edge weight).\\nThe running time could be traded-off with the approximation guarantee using, e.g., a (2k − 1)-\\nspanner of size O(kn1+1/k) [BS07] where k can vary; e.g., by setting k = log n/ log log n, we have an\\nO(n log n)-time O(log n/ log log n)-approximation algorithm (we need O(k2) to construct a spanner\\nand O(kn1+1/k) = O(n log n) to aggregate it). The spanner of [Pet10] can also be used to get a\\nlinear-time (2O(log\\n∗ n) log n)-approximation algorithm in the unweighted case.\\nIn general, it is not clear how to use graph sparsification for computing shortest paths since we\\nstill need at least linear time to collect the sparse graph. However, it plays a crucial role in some\\nprevious algorithms, including the algorithm of Lenzen and Patt-Shamir [LPS13]. Moreover, by\\nrunning the graph sparsification algorithm of Baswana and Sen [BS07] and collecting the network to\\none node, we can (2k−1)-approximate APSP in O(kn1+1/k) time on general networks and O(kn1/k)\\ntime on fully-connected networks, for any integer k ≥ 2. This gives the fastest algorithm (with\\nhigh approximation guarantees) on fully-connected networks.\\nOther Parameters. There are also some approximation algorithms whose running time is based\\non other parameters. These algorithms do not give any improvement for the worst values of their\\nparameters. We do not consider these parameters in this paper since they are less standard. One\\nimportant parameter is the shortest-path diameter, denoted by SPDiam(G,w). This parameter\\ncaptures the number of edges in a shortest path between any pair of nodes (see Definition 3.8\\nfor details). It naturally arises in the analysis of several algorithms. For example, Bellman-Ford\\nalgorithm [Bel58, For56] can be analyzed to have O(SPDiam(G,w)) time for SSSP. Khan et al.\\n[KKM+12] gives a O˜(n · SPDiam(G,w))-time O(log n)-approximation algorithm via metric tree\\nembeddings [FRT04]. We can also construct Thorup-Zwick distance sketches [TZ05] of size O(kn1/k)\\nand stretch 2k−1 in O˜(kn1/k ·SPDiam(G,w)) time [DDP12]. Since SPDiam(G,w) can be as large as\\nn, these algorithms do not give any improvement to previous algorithms when analyzed in terms of\\nn and D. One crucial component of our algorithms involves reducing the shortest-path diameter to\\nbe much less than n (more in Section 2). Another shortest path algorithm with running time based\\non the network’s local path diameter is developed as a subroutine of the approximation algorithm\\nfor MST [KP08]. This algorithm solves a slightly different problem (in particular, nodes only have\\nto know the distance to some nearby nodes) and cannot be used to solve SSSP and APSP.\\nLower Bounds. The lower bound of Das Sarma et al. [DHK+12] (building on [Elk06, PR00,\\nKKP13]) shows that solving SSSP requires Ω˜(\\n√\\nn+ D) time, even when we allow poly(n) approxi-\\nmation ratio and the network has O(log n) diameter. This implies the same lower bound for APSP.\\nRecently, [EKNP12] shows that the same Ω˜(\\n√\\nn+ D) lower bound holds even in the quantum set-\\nting. These lower bounds are subsumed by Observation 1.4 for the case of APSP. Das Sarma et al.\\n5We note that some of these algorithms (e.g., [Elk05, KKM+12]) can actually solve a more general problem called\\nthe S-shortest path problem. To avoid confusions, we will focus only on SSSP and APSP.\\n6\\n(building on [LPSP06]) also shows a polynomial lower bound on networks of diameter 3 and 4. It is\\nstill open whether there is a non-trivial lower bound on networks of diameter one and two [Elk04].\\nOther Works. While computing shortest paths is among the earliest studied problems in dis-\\ntributed computing, many classic works on this problem concern other objectives, such as the\\nmessage complexity and convergence. When faced with the bandwidth constraint, the time com-\\nplexities of these algorithms become higher than the trivial O(m)-time algorithm; e.g., Bellman-\\nFord algorithm and algorithms in [AR82, Hal97, AR93] require Ω(n2) time.\\nTo the best of our knowledge, there is still no exact distributed algorithm for APSP that is\\nfaster than the trivial O(m)-time algorithm6, except for the special case of BHC network, whose\\ntopology is structured as a balanced hierarchy of clusters. In this special case, the problem can\\nbe solved in O(n log n)-time [AHT92]. For the related problem of computing network’s diameter\\nand girth, many results are known in the unweighted case but none is previously known for the\\nweighted case. Peleg, Roditty, and Tal [PRT12] shows that we can 32 -approximate the network’s\\ndiameter in O(n1/2D) time, in the unweighted case, and Holzer and Wattenhofer [HW12] presents\\nan O( nD + D)-time (1 + \\x0f)-approximation algorithm. By combining both algorithms, we get a\\n3\\n2 -approximation O(n\\n3/4 + D)-time algorithm. In contrast, any (32 − \\x0f)-approximation and (2 −\\n\\x0f)-approximation algorithm for computing the network’s diameter and girth requires Ω(n/ log n)\\ntime [HW12] and Ω(\\n√\\nn/ log n) time [FHW12], respectively. These bounds imply the same lower\\nbound for approximation algorithms for APSP on unweighted networks. In particular, they imply\\nthat our approximation algorithms are tight, even on unweighted networks.\\n2 Overview\\n2.1 Tool 1: Light-Weight Bounded-Hop SSSP (Details in Section 3.1)\\nAt the core of our algorithms is the light-weight (1 + o(1))-approximation algorithm for computing\\nbounded-hop distances. Informally, an h-hop path is a path containing at most h edges. The h-hop\\ndistance between two nodes u and v, denoted by disthG,w(u, v), is the minimum weight among all\\nh-hop paths between u and v. The h-hop SSSP problem is to find the h-hop distance between a\\ngiven source node s and all other nodes. This problem can be solved exactly in O(h) time using\\nthe distributed version of Bellman-Ford algorithm. This algorithm is, however, not suitable for\\nparallelization, i.e. when we want to solve h-hop SSSP from k different sources. The reason is\\nthat Bellman-Ford algorithm is heavy-weight in the sense that they require so much communication\\nbetween each neighboring nodes; in particular, this algorithm may require as many asO(h) messages\\non each edge. Thus, running k copies of this algorithm in parallel may require as many as O(hk)\\nmessages on each edge, which will require O(hk) time.\\nWe show a simple algorithm that is not as accurate as Bellman-Ford algorithm but more suitable\\nfor parallelization: it can (1+o(1))-approximate h-hop SSSP in O˜(h) time and is light-weight in the\\nsense that every node sends a message (of size O(log n)) to its neighbors only O˜(1) times. Thus,\\nwhen we run k copies of this algorithm in parallel, we will require to send only O˜(k) messages\\nthrough each edge, which gives us a hope that we will require only additional O˜(k) time. By a\\n6The problem can also be solved by running the distributed version of Bellman-Ford algorithm [Pel00, Lyn96,\\nSan06] from every node, but this takes O(n2) time in the worst case. So this is always worse than the trivial algorithm.\\n7\\ncareful paralellization (based on the random delay technique of [LMR94]7), we can solve h-hop\\nSSSP from k sources in O˜(h+ k) time. This is the first tool that we will use later.\\nClaim 2.1 (See Theorem 3.6 for a formal statement). We can (1 + o(1))-approximate h-hop SSSP\\nfrom any k nodes in O˜(h+ k) time.\\nThe idea behind Claim 2.1 is actually very simple. Consider any path P having at most h\\nhops. Let \\x0f = 1/ log n and W ′ = (1 + \\x0f)i where i is such that W ′ ≤ w(P ) ≤ (1 + \\x0f)W ′ (recall\\nthat w(P ) is the sum of weights of edges in P ). Consider changing weight w slightly to w′ where\\nw′(uv) = dhw(uv)\\x0fW ′ e. Because w′(uv)− hw(uv)\\x0fW ′ ≤ 1, we have that\\nw(uv) ≤ w′(uv)× \\x0fW\\n′\\nh\\n≤ w(uv) +O(\\x0f)W\\n′\\nh\\n.\\nIt follows that\\nw(P ) ≤ w′(P )× \\x0fW\\n′\\nh\\n≤ w(P ) +O(\\x0f)W ′ = (1 + o(1))w(P ).\\nIn other words, it is sufficient for us to find w′(P ). To this end, we observe that w′(P ) = O(h/\\x0f).\\nThus, we can simply use the breadth-first search (BFS) algorithm [Pel00, Lyn96] on (G,w′) for\\nO(h/\\x0f) rounds. The BFS algorithm is light-weight: it sends at most one message through each\\nedge. Now to use this algorithm to solve h-hop SSSP, we have to try different values of W ′ in the\\nform (1 + \\x0f)i. This makes our algorithm send O˜(1) messages through each edge.\\nTo the best of our knowledge, this simple technique has not been used before in the literature\\nof distributed algorithms. In the dynamic data structure context, Bernstein has independently\\nused a similar weight rounding technique to construct a bounded-hop data structure, which plays\\nan important role in his recent breakthrough [Ber13]. Also, it was recently pointed out to us by\\na STOC 2014 reviewer that this technique is similar to the one used in the PRAM algorithm of\\nKlein-Sairam [KS92] which was originally proposed for VLSI routing by Raghavan and Thomson\\n[RT85]. The main difference between this and our weight approximation technique is that we\\nalways round edge weights up while the previous technique has to round the weights up and down\\nrandomly (with some appropriate probability). So, if we adopt the previous technique, then the\\napproximation guarantee of our light-weight SSSP algorithm will hold only with high probability\\n(in contrast, it always holds in this paper). More importantly, randomly rounding the weight could\\ncause some edge to have a zero weight after rounding. This problem can be handled in the PRAM\\nsetting by contracting edges of weight zero. However, this will be a serious problem for us since we\\ndo not know how to handle zero edge weight.\\n2.2 Tool 2: Shortest-Path Diameter Reduction Using Shortcuts (Details in\\nSection 3.2)\\nThe other crucial idea that we need is the shortest-path diameter reduction technique. Recall that\\nthe shortest-path diameter of a weighted graph (G,w), denoted by SPDiam(G,w), is the minimum\\nnumber h such that for any nodes u and v, there is a shortest u-v path in (G,w) having at most\\nh edges; in other words, disthG,w(u, v) = distG,w(u, v) for all u and v. As discussed in Section 1.4\\n7Note that the random delay technique makes the algorithm randomized. Techniques in [HW12, PRT12] might\\nenable us to get a deterministic algorithm. We do not discuss these techniques here since other parts of our algorithms\\nwill also heavily rely on randomness.\\n8\\nthere are algorithms that need O˜(SPDiam(G,w)) time to solve SSSP and APSP, e.g. Bellman-Ford\\nalgorithm. Thus, it is intuitively important to try to make the shortest-path diameter small. The\\nsecond crucial tool of our algorithm is the following claim.\\nClaim 2.2 (See Theorem 3.10 for a formal statement). If we add k edges called shortcuts from\\nevery node u to its k nearest nodes (breaking tie arbitrarily), where for each such node v the shortcut\\nedge uv has weight distG,w(u, v), then we can bound the shortest-path diameter to O(n/k).\\nWe note that the above claim would be trivially true if we add a shortcut from every node to all\\nnodes within k hops from it. The non-trivial part is showing that it is sufficient to add shortcuts\\nto only k nearest nodes. Note that this claim holds only for undirected graphs and the proof has\\nto carefully exploit the fact that the network is undirected.\\nTo the best of our knowledge, there is no previous work that proves and uses this fact in the\\ndistributed setting. Previous work that is somewhat related is the BSP algorithm of Lenzen and\\nPatt-Shamir [LPS13] which finds h-hop distances to k nearest nodes in O(hk) time. In this work,\\nthe algorithm is not used to create shortcuts, but rather to collect information about a sufficient\\nnumber of nodes so that one of them is also in some set of uniformly sampled nodes. Another\\nrelated work is the notion of (d, \\x0f)-hop set introduced by Cohen [Coh00] in the PRAM setting: our\\nshortest path diameter reduction technique can be considered as a simple construction of (d, 0)-hop\\nset of size O(n2/d). It might be possible to improve our algorithm by applying a more advanced\\nconstruction of such hop set to the distributed setting.\\n2.3 Sketches of Algorithms\\nAPSP on General Networks (details in Section 4). Algorithm for APSP follows almost\\nimmediately from the the first tool above. By applying Claim 2.1 with h = k = n, we can\\n(1 + o(1))-approximate SSSP with every node as a source in O˜(n) time; in other words, we can\\n(1 + o(1))-approximate APSP in O˜(n) time on general networks.\\nSSSP on Fully-Connected Networks (details in Section 5.1). This result follows easily from\\nthe the second tool above. To compute SSSP exactly on fully-connected networks, we will compute\\nk shortcuts from every node, where k = n1/2. To do this, we show that it is enough for every\\nnode to send k lightest-weight edges incident to it to all other nodes (since running k rounds of\\nDijkstra’s algorithm will only need these edges). This takes O(n1/2) time. Using this information\\nto modify the weight assignment from w to w′, we can reduce the shortest-path diameter of the\\nnetwork to SPDiam(G,w′) ≤ √n without changing the distance between nodes; this fact is due\\nto Claim 2.2. We then run Bellman-Ford algorithm on this (G,w′) to solve SSSP; this takes\\nSPDiam(G,w′) = O(n1/2) time.\\nAPSP on Fully-Connected Networks (details in Section 5.2). We will need both tools\\nfor this result. Step 1: Like the previous algorithm for SSSP on fully-connected network, we\\ncompute n1/2 shortcuts from every node in O(n1/2) time. Again, by Claim 2.2, this gives us a\\ngraph (G,w′) such that SPDiam(G,w′) = O(n1/2). Additionally, every node sends these shortcuts\\nto all other nodes (taking O(n1/2) time). Step 2: We then randomly pick n1/2 poly log n nodes and\\nrun the light-weight h-hop SSSP algorithm from these nodes, where h = SPDiam(G,w′) = O(n1/2).\\nBy Claim 2.1, this takes O˜(n1/2) time w.h.p. and gives us (1 + o(1))-approximate values of the\\ndistances distG,w(x, v) between each random node x and all other nodes v (known by v). Each node\\n9\\nv broadcasts distances to these n1/2 poly log n random nodes to all other nodes, taking O˜(n1/2)\\ntime.\\nAfter this, we show that every node can use the information they have received so far to compute\\n(2+o(1))-approximate values of its distances to all other nodes. (In particular, every node uses the\\ndistances it receives to build a graph and uses the distances in such graph as approximate distances\\nbetween itself and other nodes.) To explain the main idea, we show how to prove a (3 + o(1))\\napproximation factor instead of 2 + o(1): Consider any two nodes u and v, and let P be a shortest\\npath between them. If v is one of the n1/2 nodes nearest to u, then u already knows distG,w(u, v)\\nfrom the first step (when we compute shortcuts). Otherwise, by a standard hitting set argument,\\none of these n1/2 nearest nodes must be picked as one of n1/2 poly log n random nodes; let x be\\nsuch a node. Observe that distG,w(u, x) ≤ distG,w(u, v). By triangle inequality\\ndistG,w(x, v) ≤ distG,w(x, u) + distG,w(u, v) ≤ 2 distG,w(u, v).\\nAgain, by triangle inequality,\\ndistG,w(u, v) ≤ distG,w(u, x) + distG,w(x, v) ≤ 3 distG,w(u, v);\\nin other words, distG,w(u, x) + distG,w(x, v) is a 3-approximate value of distG,w(u, v). Note that u\\nknows the exact value of distG,w(u, x) (from the first step) and the (1 + o(1))-approximate value\\nof distG,w(x, v) (from the second step). So, it can compute a (1 + o(1))-approximate value of\\ndistG,w(u, x) +distG,w(x, v) which is a (3 +o(1))-approximate value of distG,w(u, v). Using the same\\nargument, v can also compute a (3 + o(1))-approximate value of distG,w(u, v). To extend this idea\\nto a (2 + o(1))-approximation algorithm, we use exactly the same algorithm but has to consider a\\nfew more cases.\\nSSSP on General Networks (details in Section 4). Approximating SSSP in sublinear time\\nneeds both tools above and a few other ideas. First, we let S be a set of n\\n1/2\\nD1/4\\npoly log n random\\nnodes and the source s. We need the following.\\nClaim 2.3 (details in Section 4.1). Let h = n1/2D1/4. Every node v can compute an approximate\\ndistance to s if it knows (i) approximate h-hop distances between itself and all nodes in S, and (ii)\\ndistances between the source s and all nodes in S in the following weighted graph (G′, w′): nodes in\\nG′ are those in S, and every edge uv in G′ has weight equal to the h-hop distance between u and v\\nin G.\\nWe call graph (G′, w′) an overlay network since it can be viewed as a network sitting on the\\noriginal network (G,w). The idea of using the overlay network to compute distances is not new.\\nIt is a crucial tool in the context of dynamic data structures and distance oracle (e.g. [DFI05]). In\\ndistributed computing literature, it has appeared (in a slightly different form) in, e.g., [LPS13].\\nOur main task is now to achieve (i) and (ii) in Claim 2.3. Achieving (i) is in fact very easy:\\nWe simply run our light-weight h-hop SSSP from all nodes in S. By Claim 2.1, this takes time\\nO˜(|S| + h) = O˜(n1/2D1/4).8 In fact, by doing this we already partly achieve (ii): every node in S\\nalready know the h-hop distance to all other nodes in S, thus it already has a “local” perspective\\nin the overlay network (G′, w′). To finish (ii), it is left to solve SSSP on (G′, w′).\\n8Note that nodes actually only know (1 + o(1)) distances. To keep our discussion simple, we will pretend that\\nthey know the real distance.\\n10\\nTo do this, we will first reduce the shortest-path diameter of the overlay network (G′, w′) by\\ncreating k shortcuts, where k = D1/2. As noted in the SSSP algorithm on fully-connected network,\\nit is enough for every node in (G′, w′) to send k lightest-weight edges incident to it to all other\\nnodes (since running k rounds of Dijkstra’s algorithm will only need these edges). Broadcasting\\neach such edge can be done in O(D) time via the breadth-first search tree, and broadcasting all\\nk|S| = O˜(n1/2D1/4) edges takes O˜(n1/2D1/4 + D) time by pipelining. (See details in Section 4.2.)\\nLet (G′′, w′′) be an overlay network obtained from adding k shortcuts to (G′, w′). (As usual, nodes u\\nin (G′′, w′′) only know weights w′′(uv) of edges uv incident to it.) By Claim 2.2, SPDiam(G′′, w′′) =\\nO(|S|/k) = O˜(n1/2/D3/4). Finally, we simulate our light-weight h′-hop SSSP algorithm to solve\\nSSSP from source s on overlay (G′′, w′′), where h′ = SPDiam(G′′, w′′) = O˜(n1/2/D3/4). To do this\\nefficiently, we need a slightly stronger property of our light-weight h′-hop SSSP algorithm: recall\\nthat we have claimed that in our light-weight SSSP algorithm, each node sends a message through\\neach edge only O˜(1) times. In fact, we can show the following stronger claim.\\nClaim 2.4 (details in Theorem 3.2). In the light-weight SSSP algorithm, each node communicates\\nin each round by broadcasting the same message to its neighbors. Moreover, each node broadcasts\\nmessages only for O˜(1) times.\\nThe intuition behind the above claim is simple: at the heart of our light-weight SSSP algo-\\nrithm, we solve O˜(1) breadth-first search algorithms where, for each of these algorithms, each node\\nbroadcasts only once; it broadcasts its distance to the root, say d, at time d. Now we simulate\\nour light-weight SSSP algorithm on (G′′, w′′) as follows. When each node v wants to broadcast a\\nmessage to all its neighbors in G′′, we broadcast this message to all nodes in G, using the breadth-\\nfirst search tree of G (see details in Section 4.3). This takes O(D) time. If we want to broadcast\\nMi messages in a round i of our light-weight SSSP algorithm, we can do so in O(D +Mi) time by\\npipelining. It can then be shown that the time we need to simulate all r = O˜(h′) = O˜(n1/2/D3/4)\\nrounds of our light-weight h′-hop SSSP algorithm takes O˜(Dh′+\\n∑r\\ni=1Mi) = O˜(n\\n1/2D1/4+D) (note\\nthat\\n∑r\\ni=1Mi = O˜(|S|) by Claim 2.4). (See details in Section 4.4.) This completes (ii) in Claim 2.3,\\nand thus we can solve SSSP on (G,w) in O˜(n1/2D1/4 + D) time.\\n3 Main Tools\\n3.1 Light-Weight Bounded-Hop Single-Source and Multi-Source Shortest Paths\\nA key tool for our algorithm is a simple idea for computing a bounded-hop single-source shortest\\npath and its extensions. Informally, an h-hop path is a path containing at most h edges. The\\nh-hop distance between two nodes u and v is the minimum weight among all u-v h-hop paths. The\\nproblem of h-hop SSSP is to find the h-hop distance between a given source node s and all other\\nnodes. Formally:\\nDefinition 3.1 (h-hop SSSP). Consider any network G with edge weight w and integer h. For any\\nnodes u and v, let Ph(u, v) be a set of u-v paths containing at most h edges. We call Ph(u, v) a\\nset of h-hop u-v paths. Define the h-hop distance between u and v as\\ndisthG,w(u, v) =\\n{\\nminP∈Ph(u,v)w(P ) if Ph(u, v) 6= ∅\\n∞ otherwise.\\n11\\nLet h-hop SSSP be the problem where, for a given weighted network (G,w), source node s (node s\\nknows that it is the source), and integer h (known to every node), we want every node u to know\\ndisthG,w(s, u).\\nThis problem can be solved in O(h + D) time using, e.g., the distributed version of Bellman-\\nFord’s algorithm. However, previous algorithms are “heavy-weight” in the sense that they require\\nso much communication (i.e., there could be as large as Ω(h) messages sent through an edge)\\nand thus are not suitable for parallelization. In this paper, we show a simple algorithm that can\\n(1 + o(1))-approximate h-hop SSSP in O˜(h + D) time. Our algorithm is light-weight in the sense\\nthat every node broadcasts a message (of size O(poly log n)) to their neighbors only O(log n) times:\\nTheorem 3.2 (Light-weight h-hop SSSP algorithm; proof in Section 3.1.2). There is an algorithm\\nthat solves h-hop SSSP on network G with weight w in O˜(h + D)-time and, during the whole\\ncomputation, every node u broadcasts O(log n) messages, each of size O(poly log n), to its neighbors\\nv.\\nTheorem 3.2, in its own form, cannot be directly used. We will extend it to an algorithm for\\ncomputing h-hop multi-source shortest paths (MSSP). (Later, in Section 4.1 we will also extend this\\nresult to overlay networks.) The rest of this subsection is devoted to proving Theorem 3.2.\\n3.1.1 Reducing Bounded-Hop Distance by Approximating Weights\\nTheorem 3.3 (Reducing Bounded-Hop Distance by Approximating Weights). Consider any n-\\nnode weighted graph (G,w) and an integer h. Let \\x0f = 1/ log n. For any i and edge xy, let D′i = 2\\ni\\nand w′i(xy) = d2hw(xy)\\x0fD′i e. For any nodes u and v, if we let\\nd˜ist\\nh\\nG,w(u, v) = min\\n{\\n\\x0fD′i\\n2h\\n× distG,w′i(u, v) | i : distG,w′i(u, v) ≤ (1 + 2/\\x0f)h\\n}\\n,\\nthen disthG,w(u, v) ≤ d˜ist\\nh\\nG,w(u, v) ≤ (1 + \\x0f) disthG,w(u, v).\\nNote that the min term in Theorem 3.3 is over all i such that distG,w′i(u, v) ≤ (1 + 2/\\x0f)h. The\\nproof of Theorem 3.3 heavily relies on Lemma 3.4 below.\\nLemma 3.4 (Key Lemma for Reducing Bounded-Hop Distance by Approximating Weights). Con-\\nsider any nodes u and v. For any i, let wi and D\\n′\\ni be as in Theorem 3.3. Then,\\n\\x0fD′i\\n2h\\n× distG,w′i(u, v) ≥ disthG,w(u, v). (1)\\nMoreover, for i∗ such that D′i∗−1 ≤ disthG,w(u, v) ≤ D′i∗, we have that\\ndistG,w′\\ni∗\\n(u, v) ≤ (1 + 2/\\x0f)h and (2)\\n\\x0fDi∗\\n2h\\n× distG,w′\\ni∗\\n(u, v) ≤ (1 + \\x0f) disthG,w(u, v). (3)\\n12\\nProof. Let P = 〈u = x0, x1, . . . , x` = v〉 be any shortest h-hop path between u and v (thus\\n2i\\n∗−1 ≤ w(P ) ≤ 2i∗ and ` ≤ h). Then,\\ndistG,w′\\ni∗\\n(u, v) =\\n`−1∑\\nj=0\\n⌈\\n2hw(xjxj+1)\\n\\x0fD′i∗\\n⌉\\n≤ 2h\\n\\x0fD′i∗\\n`−1∑\\nj=0\\nw(xjxj+1) + `\\n≤ 2h\\n\\x0fD′i∗\\ndisthG,w(u, v) + h (4)\\n≤ (1 + 2/\\x0f)h\\nwhere the last inequality is because D′i∗ ≥ disthG,w(u, v). This proves Equation (2). Using Equa-\\ntion (4), we also have that\\n\\x0fD′i∗\\n2h\\n× distG,w′\\ni∗\\n(u, v) ≤ \\x0fD\\n′\\ni∗\\n2h\\n×\\n(\\n2h\\n\\x0fD′i∗\\ndisthG,w(u, v) + h\\n)\\n≤ disthG,w(u, v) +\\n\\x0f\\n2\\nD′i∗\\n≤ (1 + \\x0f) disthG,w(u, v)\\nwhere the last inequality is because D′i∗ = 2D\\n′\\ni∗−1 ≤ 2 disthG,w(u, v). This proves Equation (3).\\nFinally, observe that for any i and the path P defined as before, we have\\ndistG,w′i(u, v) ≥\\n`−1∑\\nj=0\\n2hw(xjxj+1)\\n\\x0fD′i\\n=\\n2h\\n\\x0fD′i\\ndisthG,w(u, v) .\\nIt follows that\\n\\x0fD′i\\n2h\\n× distG,w′i(u, v) ≥\\n\\x0fD′i\\n2h\\n×\\n(\\n2h\\n\\x0fD′i\\ndisthG,w(u, v)\\n)\\n= disthG,w(u, v) .\\nThis proves Equation (1) and completes the proof of Lemma 3.4.\\nNow we are ready to prove Theorem 3.3.\\nProof of Theorem 3.3. Note that\\nd˜hG,w(u, v) = min\\n{\\n\\x0fD′i\\n2h\\n× distG,w′i(u, v) | i : distG,w′i(u, v) ≤ (1 + 2/\\x0f)h\\n}\\n≤ \\x0fD\\n′\\ni∗\\n2h\\n× distG,w′\\ni∗\\n(u, v)\\n≤ (1 + \\x0f) disthG,w(u, v)\\n13\\nwhere i∗ is as in Lemma 3.4, the second inequality is due to the fact that distG,w′\\ni∗\\n(u, v) ≤ (1+2/\\x0f)h\\nas in Equation (2), and the third inequality follows from Equation (3). This proves the second\\ninequality in Theorem 3.3. The first inequality of Theorem 3.3 simply follows from the fact that\\n\\x0fD′i\\n2h × distG,w′i(u, v) ≥ disthG,w(u, v) for all i, by Equation (1).\\n3.1.2 Algorithm for Bounded-Hop SSSP (Proof of Theorem 3.2)\\nWe now show that we can solve h-hop SSSP in O˜(h+D) time while each node broadcasts O(log n)\\nmessages of size O(log n), as claimed in Theorem 3.2. Our algorithm is outlined in Algorithm 3.1.\\nGiven a parameter h (known to all nodes) and weighted network (G,w), it computes wi, for all\\ni, as defined in Theorem 3.3; i.e., every node u internally computes wi(u, v) for all neighbors v.\\nNote that this step needs no communication. Next, in Line 4 of Algorithm 3.1, for each value of\\ni, the algorithm executes an algorithm for the bounded-distance SSSP problem with parameter\\n(G,w, s,K), where (1 + 2/\\x0f)h, as outlined in Algorithm 3.2 (we will explain this algorithm next).\\nAt the end of the execution of Algorithm 3.2, every node u knows dist′i(s, u) such that dist\\n′\\ni(s, u) =\\ndistG,wi(s, u) if distG,wi(s, u) ≤ K and dist′i(s, u) = ∞ otherwise. Finally, we set d˜ist\\nh\\nG,wi(s, u) =\\nmini dist\\n′\\ni(s, u). By Theorem 3.3, we have that dist\\nh\\nG,w(u, v) ≤ d˜ist\\nh\\nG,w(u, v) ≤ (1 + \\x0f) disthG,w(u, v) =\\n(1 + o(1)) disthG,w(u, v) as desired.\\nWe now explain Algorithm 3.2 for solving the bounded-distance SSSP problem. It is a simple\\nmodification of a standard bread-first tree algorithm. It runs for K rounds. In the initial round\\n(Round 0), the source node s broadcasts a message (s, 0) to all its neighbors to start the algorithm.\\nThis message is to inform all its neighbors that its distance from the source (itself) is 0. In general,\\nwe will make sure that every node v whose distance to s is distG,w(s, u) = ` will broadcast a\\nmessage (s, `) to its neighbor at Round `. Every time a node u receives a message of the form (s, `)\\nfrom its neighbor v, it knows that distG,w(s, v) = `; so, u updates its distance to the minimum\\nbetween the current distance and ` + w(uv). It is easy to check that every node u such that\\ndistG,w(s, u) < K broadcasts its message to all neighbors once at Round ` = distG,w(s, u). The\\ncorrectness of Algorithm 3.2 immediately follows. Moreover, since we execute Algorithm 3.2 for\\nO(log n) different values of i (since the maximum weight is poly(n)), it follows that every node\\nbroadcasts a message to their neighbors O(log n) times. Theorem 3.2 follows.\\nAlgorithm 3.1 Bounded-Hop SSSP (G,w, s, h)\\nInput: Weighted undirected graph (G,w), source node s, and integer h.\\nOutput: Every node u knows the value of d˜ist\\nh\\nG,wi(s, u) such that dist\\nh\\nG,w(s, u) ≤ d˜ist\\nh\\nG,wi(s, u) ≤\\n(1 + o(1)) disthG,wi(s, u).\\n1: Let \\x0f = 1/ log n. For any i and edge xy, let D′i = 2\\ni and w′i(xy) = d2hw(xy)\\x0fD′i e. Let K = (1+2/\\x0f)h.\\n2: Let t be the time this algorithm starts. We can assume that all nodes know t.\\n3: for all i do\\n4: Solve bounded-distance SSSP with parameters (G,wi, s,K) using Algorithm 3.2. (This\\ntakes O˜(K) = O˜(h) time.) Let dist′i(s, u) be the distance returned to node u.\\n5: end for\\n6: Each node u computes d˜ist\\nh\\nG,wi(s, u) = mini dist\\n′\\ni(s, u).\\n14\\nAlgorithm 3.2 Bounded-Distance SSSP (G,w, s,K)\\nInput: Weighted undirected graph (G,w), source node s, and integer K.\\nOutput: Every node u knows dist′G,w(s, u) where dist\\n′\\nG,w = distG,w(s, u) if distG,w(s, u) ≤ K and\\ndist′G,w =∞ otherwise.\\n1: Let t be the time this algorithm starts. We can assume that all nodes know t.\\n2: Initially, every node u sets dist′G,w(s, u) =∞.\\n3: In the beginning of this algorithm (i.e., at time t) source node s sends a message (s, 0) to itself.\\n4: if a node u receives a message (s, `) for some ` from node v, then\\n5: if (`+ w(u, v) ≤ K) and (`+ w(u, v) < dist′G(s, u)) then\\n6: u sets dist′G(s, u) = `+ w(u, v).\\n7: end if\\n8: end if\\n9: For any x ≤ K, at time t+ x, every node u such that dist′G(s, u) = x broadcasts message (s, x)\\nto all its neighbors to announce that dist′G(s, u) = x.\\n3.1.3 Bounded-Hop Multi-Source Shortest Paths\\nThe fact that our algorithm for the bounded-hop single-source shortest path problem in Theorem 3.2\\nis light-weight allows us to solve its multi-source version, where there are many sources in parallel.\\nThe problem of bounded-hop multi-source shortest path is as follows.\\nDefinition 3.5 (h-hop k-source shortest paths). Given a weighted network (G,w), integer h (known\\nto every node), and sources s1, . . . , sk (each node si knows that it is a source), the goal of the h-hop\\nk-source shortest paths problem is to make every node u knows disthG,w(si, u) for all 1 ≤ i ≤ k.\\nThe main result of this section is an algorithm for solving this problem in O˜(k + h+D) time,\\nas follows.\\nTheorem 3.6 (k-source h-hop shortest path algorithm). There is an algorithm that (1 + o(1))-\\napproximates the h-hop k-source shortest paths problem on weighted network (G,w) in O˜(k+h+D)\\ntime; i.e., at its termination every node u knows dist′G,w(si, u) such that\\ndisthG,w(si, u) ≤ dist′G,w(si, u) ≤ (1 + o(1)) disthG,w(si, u)\\nfor all sources si.\\nThe algorithm is conceptually easy: we simply run the algorithm for bounded-hop single-source\\nshortest path in Theorem 3.2 (i.e. Algorithm 3.1) from k sources in parallel. Obviously, this\\nalgorithm needs at least Ω˜(h + D) time since this is the guarantee we can get for the case of\\nsingle source. Moreover, it is possible that one need has to broadcast O(log n) messages for each\\nexecution of Algorithm 3.1, making a total of O(k log n) messages; this will require O˜(k) time. So,\\nthe best running time we can hope for is O˜(k + h+D). It is, however, not obvious to achieve this\\nrunning time since one execution could delay other executions; i.e., it is possible that all executions\\nof Algorithm 3.1 might want a node to send a message at the same time making some of them\\nunable to proceed to the next round. We show that by simply adding a small delay to the starting\\ntime of each execution, it is unlikely that many executions will delay each other.\\n15\\nAlgorithm 3.3 Multi-Source Bounded-Hop Shortest Path (G,w, {s1, . . . , sk}, h)\\nInput: Weighted undirected graph (G,w), k source nodes s1, . . . , sk, and integer h.\\nOutput: Every node u knows disthG,w(si, u) for all i.\\n1: Let r1, . . . , rk be a number selected uniformly at random from [0, k log n]. We can assume that\\nall nodes know ri, for all ri. This can be done by, e.g., broadcasting all ri to all nodes in\\nO(k +D) time.\\n2: Let t be the time this algorithm starts. We can assume that all nodes know t.\\n3: At time t+ ri, execute the bounded-hop single-source shortest path algorithm (Algorithm 3.1)\\non (G,w, si, h).\\n4: If at any time, more than log n messages is sent through an edge, we say that the algorithm\\nfails. (We show that the algorithm fails with probability O(1/n2) in Lemma 3.7.)\\nThe algorithm is very simple: Instead of starting the execution of Algorithm 3.1 from different\\nsource nodes at the same time, each execution starts with a random delay randomly selected from\\nintegers from 0 to k log n. The algorithm is outlined in Algorithm 3.3. The crucial thing is to\\nshow that many executions of Algorithm 3.1 launched by Algorithm 3.3 do not delay each other.\\nIn particular, that we show that at most log n messages will be sent through each edge in every\\nround, with high probability (if this does not happen, we say that the algorithm fails). We prove\\nthis in Lemma 3.7 below. Our proof is simply an adaptation of the random delay technique for\\npackage scheduling [LMR94]. Lemma 3.7 immediately implies Theorem 3.6, since each execution,\\nwhich start at time O˜(k) will finish in O˜(h+D) rounds without being delayed.\\nLemma 3.7 (Congestion guaranteed by the random delay technique). For any source si and node\\nu, let Mi,u be the set of messages broadcasted by u during the execution of Algorithm 3.1 with\\nparameter (G,w, si, h). Note that |Mi,u| ≤ c log n for some constant c, by Theorem 3.2. Then, the\\nprobability that, in Algorithm 3.3, there exists time t, node u, and a set M ⊆ ⋃iMi,u such that\\n|M| ≥ log n, and all messages in M are broadcasted by u at time t = O(k + h+D), is O(1/n2).\\nProof. Fix any node u, time t, and setM as above. Observe that, for any i, the time that a message\\nM ∈ Mi,u, is broadcasted by u is determined by the random delay ri – there is only one value of\\nri that makes u broadcasts M at time t. In other words, for fixed u, t, and message M ∈\\n⋃\\niMi,u,\\nPr[M is sent by u at time t] ≤ 1\\nk log n\\n.\\nIt follows that for fixed u, t, and set of messages M,\\nPr[all messages in M is sent by u at time t] ≤\\n(\\n1\\nk log n\\n)|M|\\n.\\nNote that we can assume that |M∩Mi,u| ≤ 1 since, for an execution of Algorithm 3.1 on a source\\nsi, every node u broadcasts at most one message per round. This implies that |M| ≤ k, and, for\\nany m ≤ k, the number of such set M of size exactly m is at most ( km)(c log n)m since each set\\nM can be constructed by picking m different sets Mi,u, and picking one message out of c log n\\nmessages from each Mi,u. Thus, for fixed u and t, the probability that there exists M such that\\n16\\n|M| ≥ log n and all messages in M is sent by u at time t is at most\\nk∑\\nm=logn\\n(\\nk\\nm\\n)\\n(c log n)m\\n(\\n1\\nk log n\\n)m\\n.\\nUsing the fact that for any 0 < b < a,\\n(\\na\\nb\\n) ≤ (ae/b)b, the previous quantity is at most\\nk∑\\nm=logn\\n(\\nke\\nm\\n)m\\n(c log n)m\\n(\\n1\\nk log n\\n)m\\n≤\\nk∑\\nm=logn\\n(ec\\nm\\n)m\\n≤ k\\n(\\nec\\nlog n\\n)logn\\n.\\nFor large enough n, the above quantity is at most 1/n4. We conclude that for fixed u and t, the\\nprobability that there exists M such that |M| ≥ log n and all messages in M is sent by u at time\\nt is at most 1/n4. By summing this probability over all nodes u and t = O(k + h + D) = O(n),\\nLemma 3.7 follows.\\n3.2 Shortest-Path Diameter Reduction Using Shortcuts\\nIn this section, we show a simple way to augment a graph with some edges (called “shortcuts”) to\\nreduce the shortest-path diameter. The notion of shortest path diameter is defined as follows.\\nDefinition 3.8 (Shortest-path distance and diameter). For any weighted graph (G,w), the shortest-\\npath distance between any two nodes u and v, denoted by spdistG,w(u, v), is the minimum integer\\nh such that disthG,w(u, v) = distG,w(u, v). That is, it is the minimum number of edges among the\\nshortest u-v paths. The shortest-path diameter of (G,w), denoted by SPDiam(G,w), is defined to be\\nmaxu,v∈V (G) spdistG,w(u, v). In other words, it is the minimum integer h such that dist\\nh\\nG,w(u, v) =\\ndistG,w(u, v) for all nodes u and v.\\nDefinition 3.9 (k-shortcut graph). Consider any n-node weighted graph (G,w) and an integer\\nk 6= n − 1. For any node u, let SkG,w(u) ⊆ V (G) be the set of exactly k nodes nearest to u\\n(excluding u); i.e. u /∈ SkG,w(u), |SkG,w(u)| = k, and for all v ∈ SkG,w(u) and v′ /∈ SkG,w(u),\\ndistG,w(u, v) ≤ distG,w(u, v′). The k-shortcut graph of (G,w), denoted by (G,w)k, is a weighted\\ngraph resulting from adding an edge uv of weight distG,w(u, v) for every u ∈ V (G) and v ∈ SkG,w(u).\\nWhen it is clear from the context, we will write Sk(u) instead of SkG,w(u).\\nTheorem 3.10 (Main result of Section 3.2: Reducing the shortest-path diameter by shortcuts).\\nFor any n-node weighted undirected graph (G,w) and integer k, if (G′, w′) is the k-shortcut graph\\nof (G,w), then SPDiam(G′, w′) < 4n/k.\\nProof. Consider any nodes u and v, and let\\nP = 〈u = x0, x1, . . . , x` = v〉\\nbe the shortest u-v path in (G′, w′) with smallest number of edges; i.e. there is no path Q in (G′, w′)\\nsuch that |E(Q)| < |E(P )| and w′(Q) ≤ w′(P ). For any node x, let Sk(x) be the set of k nodes\\n17\\nnearest to x in (G,w), as in Definition 3.9. We claim that for any integer 0 ≤ i ≤ (`/4) − 1, we\\nhave\\nSk(x4i) ∩ Sk(x4(i+1)) = ∅.\\nThis claim immediately implies that ` < 4n/k, thus Theorem 3.10; otherwise, |⋃0≤i≤`/4 Sk(x4i)| ≥\\n(k+1)(n/k) > n, which is impossible. It is thus left to prove the claim that Sk(x4i)∩Sk(x4(i+1)) = ∅.\\nNow, consider any 1 ≤ i ≤ `/4. Observe that\\nx4i+2 /∈ Sk(x4i). (5)\\nOtherwise, (G′, w′) will contain edge x4ix4i+2 of weight distG,w(x4i, x4i+2). This implies that P ′ =\\n〈x0, . . . , x4i−1, x4i, x4i+2, x4i+3, . . . , x` = v〉 is a shortest u-v path in (G′, w′) containing `− 1 edges,\\ncontradicting the fact that P has the smallest number of edges among shortest u-v paths in (G′, w′).\\nBy the same argument, we have\\nx4i+2 /∈ Sk(x4(i+1)). (6)\\nBy the definition of Sk, Equations (5) and (6) imply that\\n∀y ∈ Sk(x4i) distG,w(x4i, y) ≤ distG,w(x4i, x4i+2), and (7)\\n∀y ∈ Sk(x4(i+1)) distG,w(x4(i+1), y) ≤ distG,w(x4(i+1), x4i+2) (8)\\nrespectively. Now, assume for a contradiction that there is a node y ∈ Sk(x4i) ∩ Sk(x4(i+1)).\\nConsider a path\\nP ′′ = 〈x0, . . . , x4i−1, x4i, y, x4(i+1), x4(i+1)+1, . . . , x` = v〉.\\nObserve that P ′′ contains `− 3 edges. Moreover, Equations (7) and (8) imply that\\nw′(x4iy) = distG,w(x4i, y) ≤ distG,w(x4i, x4i+2) and\\nw′(x4(i+1)y) = distG,w(x4(i+1), y) ≤ distG,w(x4(i+1), x4i+2)\\nwhich further imply that w′(P ′′) ≤ w′(P ). This means that P ′′ is a shortest u-v paths in (G′, w′)\\nand contradicts the fact that P has the smallest number of edges among shortest u-v paths in\\n(G′, w′). Thus, Sk(x4i) ∩ Sk(x4(i+1)) = ∅ as desired.\\nWe note a simple fact that will be used throughout this paper: we can compute SkG,w(u) and\\ndistG,w(u, v) for all v ∈ SkG,w(u) if we know k smallest edges incident to every nodes. The precise\\nstatement is as follows.\\nDefinition 3.11 (Ek(u) and (Gk, w)). For any node u, let Ek(u) be the set of k edges incident to\\nu with minimum weight (breaking tie arbitrarily); i.e., for every edge uv ∈ Ek(u) and uv′ /∈ Ek(u),\\nwe have w(uv) ≤ w(uv′). Let (Gk, w) be the subgraph of (G,w) whose edge set is ⋃v∈V (G)Ek(v).\\nWe note that for some graph (G,w), the sets Ek(u) and SkG,w(u) might not be uniquely defined.\\nTo simplify our statement and proofs, we will assume that (G,w) has the following property, which\\nmakes both SkG,w(u) and S\\nk\\nGk,w\\n(u) unique: every edge uv ∈ E(G) has a unique value of w(uv), and\\nevery pair of nodes u and v has a unique value of distG,w(u, v) and distGk,w(u, v). Removing these\\nassumptions can be easily done by breaking ties arbitrarily.\\n18\\nObservation 3.12 (Computing shortcut edges using (Gk, w)). For any node u,\\n• SkG,w(u) = SkGk,w(u), and\\n• distG,w(u, v) = distGk,w(u, v), for any v ∈ SkG,w(u).\\nIn other words, using only edges in\\n⋃\\nv∈V (G)E\\nk(v) and their weights, we can compute all k-shortcut\\nedges.\\nProof. The intuition behind Observation 3.12 is that Dijkstra’s algorithm can be used to compute\\nSkG,w(u) and {distG,w(u, v)}v∈SkG,w(u) by executing it for k iterations, and this process will never\\nneed any edge besides those in (Gk, w). Below we provide a formal proof that does not require the\\nknowledge of Dijkstra’s algorithm.\\nLet Tu be a shortest path tree rooted at u in (G,w). Assume for a contradiction that there is\\na node v ∈ SkG,w(u) \\\\ SkGk,w(u). Let z be the parent of v in Tu. The fact that v /∈ SkGk,w(u) implies\\nthat zv /∈ Ek(z); thus, there exists v′ in Ek(z) \\\\ Sk\\nGk,w\\n(u) (since |Ek(z)| = |Sk\\nGk,w\\n(u)| = k and\\nSk\\nGk,w\\n(u) \\\\ Ek(z) 6= ∅). Note that since v /∈ Ek(z), we have w(zv′) < w(zv) (recall that we assume\\nthat edge weights are distinct). This, however, implies that\\ndistG,w(u, v\\n′) ≤ distG,w(u, z) + w(zv′) < distG,w(u, v).\\nThis contradicts the fact that v ∈ SkG,w(u) and v /∈ SkG,w(u).\\n4 Algorithms on General Networks\\nIn this section, we present algorithms for SSSP and APSP on general distributed networks, as\\nstated in Theorem 1.2 and Theorem 1.3. First, observe that APSP is simply a special case of the\\nh-hop k-source shortest paths problem defined in Section 3.1.3 where we use h = k = n. Thus, by\\nTheorem 3.6, there is an (1+o(1))-approximation algorithm that solves APSP in O˜(k+h+D) = O˜(n)\\ntime with high probability. This immediately proves Theorem 1.3. The rest of this section is then\\ndevoted to showing a O˜(n1/2D1/4 + D)-time algorithm for SSSP as in Theorem 1.2, which require\\nseveral non-trivial steps.\\n4.1 Reduction to Single-Source Shortest Path on Overlay Networks\\nIn this section, we show that solving the single source shortest path problem on a network (G,w)\\ncan be reduced to the same problem on a certain type of an overlay network, usually known as a\\nlandmark or skeleton (e.g. [Som12, LPS13]). In general, an overlay network G′ is a virtual network of\\nnodes and logical links that is built on top of an underlying real network G; i.e., V (G′) ⊂ V (G) and\\nan edge inG′ (a “virtual edge”) corresponds to a path inG (see, e.g., [EFK+12]). Its implementation\\nis usually abstracted as a routing scheme that maps virtual edges to underlying routes. However,\\nfor the purpose of this paper, we do not need a routing scheme but will need the notion of hop-\\nstretch which captures the number of hops in G between two neighboring virtual nodes in V (G′),\\nas follows.\\nDefinition 4.1 (Overlay network of hop-stretch λ). Consider any network G. For any λ, a weighted\\nnetwork (G′, w′) is said to be an overlay network of hop-stretch λ embedded in G if\\n19\\n1. V (G′) ⊆ V (G),\\n2. distG(u, v) ≤ λ for every virtual edge uv ∈ E(G′), and\\n3. for every virtual edge uv ∈ E(G′), both u and v (as a node in G) knows the value of w′(uv).\\nWe emphasize that λ captures the number of hops (distG(u, v)) between two neighboring nodes\\nu and v, not the weighted distance (distG,w(u, v)). The main result of this section is an algorithm\\nto construct an overlay network such that, if we can solve the single-source shortest path problem\\non such network, we can solve the single-source shortest path on the whole graph:\\nTheorem 4.2 (Main result of Section 4.1: Reduction to an overlay network). For any weighted\\ngraph (G,w), source node s, and integer α, there is an O˜(α+ n/α+D)-time distributed algorithm\\nthat embeds an overlay network (G′, w′) in G such that, with high probability,\\n1. s ∈ V (G′),\\n2. |V (G′)| = O˜(α), and\\n3. if every node u ∈ V (G) knows a (1 + o(1))-approximate value of distG′,w′(s, v) for every node\\nv ∈ V (G′), then u knows the (1 + o(1))-approximate value of distG,w(s, u).\\nProof. Our algorithm is as follows. First, every node u selects itself to be in V (G′) with probability\\nα/n. Additionally, we always keep source s in V (G′); this guarantees the first condition. Observe\\nthat E[|V (G′)|] = 1 + (α/n) · (n − 1) ≤ 2α; so, by Chernoff’s bound (e.g. [MU05, Theorem 4.4]),\\nPr[|(G′)| ≥ 12α log n] ≤ 1/n. This proves the second condition. To guarantee the last condition, we\\nhave to define edges and their weights in G′. To do this, we invoke an algorithm for the bounded-\\nhop multi-source shortest path problem in Theorem 3.6 (page 15), with nodes in V (G′) as sources\\nand h = n log n/α hops. By Theorem 3.6, the algorithm takes O˜(|V (G′)|+h+D) = O˜(α+n/α+D)\\ntime, and every node u ∈ V (G) will know dist′G,w(u, v) such that\\ndisthG,w(u, v) ≤ dist′G,w(u, v) ≤ (1 + o(1)) disthG,w(u, v)\\nfor all v ∈ V (G′). For any u, v ∈ V (G′) such that dist′G,w(u, v) < ∞, we add edge uv with weight\\nw′(u, v) = dist′G,w(u, v) to G′. Note that both u and v knows the existence and weight of this edge.\\nThis completes the description of an overlay network (G′, w′) embedded in G.\\nWe are now ready to show the third condition, i.e., if a node u ∈ V (G) knows a (1 + o(1))-\\napproximate value of distG′,w′(s, v) for all v ∈ V (G′), then it knows a (1 + o(1))-approximate value\\nof distG,w(s, u). Consider any node u ∈ V (G), and let\\nP = 〈u = v0, v1, . . . , vk = s〉,\\nfor some k, be a shortest path between s and u in (G,w). Observe that if k ≤ n log n/α, then\\nu knows dist′G,w(u, v) which is a (1 + o(1))-approximate value of distG,w(s, u), and thus the third\\ncondition holds even when u does not know a (1 + o(1))-approximate value of distG′,w′(s, v) for any\\nv ∈ V (G′). It is thus left to consider the case where k ≥ n log n/α. Let i1 < i2 < . . . < it be\\nsuch that vi1 , . . . , vit are nodes in P ∩ V (G′). Let i0 = 0 (i.e., vi0 = u). We note the following\\nsimple fact, which is very easy and well-known (e.g. [UY91]). We provide its proof here only for\\ncompleteness.\\n20\\nLemma 4.3 (Bound on the number of hops between two landmarks in a path). For any j, ij −\\nij−1 ≤ n log n/α, with probability at least 1− 2−βn, for some constant β > 0 and sufficiently large\\nn.\\nProof. We note a well-known fact that a set of random selected nodes |V (G′)| of size α will “hit”\\na simple path of length at least cn log n/|V (G′), for some constant c, with high probability. To\\nthe best of our knowledge, this fact was first shown in [GK81] and has been used many times in\\ndynamic graph algorithms (e.g. [DFI05] and references there in). The following fact appears as\\nTheorem 36.5 in [DFI05] (attributed to [UY91]).\\nFact 4.4 (Ullman and Yannakakis [UY91]). Let S ⊆ V (G) be a set of vertices chosen uniformly at\\nrandom. Then the probability that a given simple path has a sequence of more than (cn log n)/|S|\\nvertices, none of which are from S, for any c > 0, is, for sufficiently large n, bounded by 2−βn for\\nsome positive β.\\nUsing S = V (G′), which has size Θ˜(α), we have that every subpath of P of length at least\\nn log n/α contains a node in V (G′), with high probability. The lemma follows by union bouding\\nover the subpaths of P . This completes the proof of Lemma 4.3.\\nIt follows from the above lemma that u knows dist′G,w(u, vi1) and, for any j ≥ 1, vijvij+1 is an\\nedge in the overlay network G′ of weight w′(vijvij+1) = dist\\n′\\nG,w(vij , vij+1), with probability at least\\n1− 2−βn. Thus, with high probability,\\ndistG′,w′(vi1 , s) ≤\\nk−1∑\\nj=1\\ndist′G,w(vij , vij+1).\\nSince u already knows dist′G,w(u, vi1), it can now compute\\ndist′G,w(u, vi1) +\\nk−1∑\\nj=1\\ndist′G,w(vij , vij+1)\\nwhich is at least distG,w(u, s) and at most (1 + o(1)) distG,w(u, s). We note one detail that, in fact,\\nu does not known which node is v1, so it has to use the value of\\nmin\\nv∈V (G′)\\ndist′G,w(u, v) + distG′,w′(v, s)\\nas an estimate. By union bounding over all nodes u, Theorem 4.2 follows.\\n4.2 Reducing the Shortest Path Diameter of Overlay Network (G′, w′)\\nIn this section, we assume that we are given an overlay network (G′, w′) embedded in the original\\nnetwork (G,w), as show in Theorem 4.2. Our goal is to solve the single-source shortest path\\nproblem on (G′, w′). Recall that (G′, w′) has |V (G′)| = O˜(α) nodes, for some parameter α, which\\nwill be fixed later. Note that the shortest path diameter (SPDiam(G,w)) of (G′, w′) might be as\\nlarge as |V (G′)| = O˜(α). Since the running time of our algorithm for single-source shortest path\\nwill depend on the shortest path diameter, we wish to reduce the shortest path diameter. We will\\napply the technique from Section 3.2 to do this task, as follows.\\n21\\nTheorem 4.5 (SPDiam reduction of an overlay network). For any parameter α and β, consider\\nan overlay network (G′, w′) of O˜(α) nodes, embedded in network (G,w). There is a distributed\\nalgorithm that terminates in O˜(αβ + D(G)) time and gives an overlay network (G′′, w′′) such\\nthat V (G′) = V (G′′), SPDiam(G′′, w′′) = O˜(α/β), and for any nodes u and v, distG′′,w′′(u, v) =\\ndistG′,w′(u, v).\\nProof. Consider the following algorithm. First, every node in the overlay network (G′, w′) broad-\\ncasts to all other nodes the values of β edges incident to it with smallest weights (breaking ties\\narbitrarily). This step takes O˜(αβ) time since there are αβ edges broadcasted. Using these broad-\\ncasted edges, every node v can compute β nodes nearest to it (since any shortest path algorithm\\n– Disjkstra’s algorithm for example – will only need to know β smallest-weight edges to com-\\npute β nearest nodes). Thus, v can add β shortcuts to the network (G′, w′) to construct network\\n(G′′, w′′). In fact, the added shortcuts could be broadcasted to all nodes in O˜(αβ + D(G)) time\\nsince each node will broadcast only β shortcuts. This implies that we can build an overlay net-\\nwork (G′′, w′′) in O˜(αβ) time and, by Theorem 3.10, the shortest-path diameter of (G′′, w′′) is\\nSPDiam(G′′, w′′) = O˜(α/β).\\n4.3 Computing SSSP on Overlay Network (G′′, w′′)\\nIn the final step of our sublinear-time SSSP algorithm, we solve SSSP on overlay network (G′′, w′′)\\nembedded in (G,w) obtained in the previous section. Recall that for parameters α and β which\\nwill be fixed later, |V (G′′)| = Θ˜(α) and SPDiam(G′′, w′′) = O˜(α/β).\\nLemma 4.6 ((1 + o(1))-approximate SSSP on (G′′, w′′)). We can (1 + o(1))-approximate SSSP on\\n(G′′, w′′) in O˜(D(G)α/β + α) time.\\nProof. We will simulate the light-weight h-hop SSSP algorithm in Theorem 3.2 on the overlay\\nnetwork (G′′, w′′) by using h = SPDiam(G′′, w′′) = O˜(α/β). To simulate this algorithm, we will\\nview (G′′, w′′) as a fully-connected overlay network where every node can communicate with other\\nnodes by broadcasting, i.e. sending a message to every node in the original network G, which takes\\nO(D(G)) time. In particular, every node in (G′′, w′′) will simulate each round of this algorithm and\\nwait until the messages that are sent in such round by all nodes are received by all nodes before\\nstarting the next round (see Algorithm 4.1).\\nAlgorithm 4.1 Similating a broadcasting algorithm on an overlay network (G′′, w′′)\\nInput: An overlay network (G′′, w′′) embedded on network G and an algorithm A such that nodes\\ncommunicate only by broadcasting a message to all its neighbors.\\nGoal: Simulate A on (G′′, w′′) when we view G′′ as a fully-connected overlay network.\\n1: for each round i of algorithm A do\\n2: Count the number of nodes in G′′ that want to broadcast a message in this round of A. Let\\nMi be such number. Make every node in G knows Mi. (This step takes O(D(G)) time.)\\n3: Every node in G′′ that wants to send a message broadcasts such message to every node in\\nG. Wait for D(G) + Mi rounds to make sure that every node receives all Mi messages before\\nproceeding to round i+ 1.\\n4: end for\\nSimulating each round i of this algorithm will take O˜(D(G) + Mi), where Mi is the total\\nnumber of messages broadcasted by all nodes in round i. This is because broadcasting Mi messages\\n22\\nto all nodes in the network (not just all neighbors) takes O˜(D(G) + Mi) time. Note that, by\\nTheorem 3.2, this algorithm finishes in O˜(h) rounds; thus, the total time needed to simulate this\\nalgorithm is O˜(hD(G) + M) where M is the total number of messages broadcasted by all nodes\\nthroughout the algorithm. Since this algorithm is light-weight, every node in G′′ broadcasts only\\nO(log n) messages, and thus we can bound M by O˜(|V (G′′)|) = O˜(α). So, the total running time\\nis O˜(hD(G) + α) = O˜(D(G)α/β + α) as claimed.\\n4.4 Putting Everything Together (Proof of Theorem 1.2)\\nBy Lemma 4.6, we can (1 + o(1))-approximate SSSP on (G′′, w′′) which, in turn, (1 + o(1))-\\napproximates SSSP on (G′, w′), by Theorem 4.5. Then, by Theorem 4.2, we know that we can\\n(1 + o(1))-approximate SSSP on the original network (G,w) as desired. We now analyze the run-\\nning time. Constructing (G′, w′) takes O˜(α + n/α + D(G)), as in Theorem 4.2. Adding shortcuts\\nto (G′, w′) to construct (G′′, w′′) takes O˜(αβ + D(G)), by Theorem 4.5. Finally, solving SSSP on\\n(G′′, w′′) takes O˜(D(G)α/β + α) by Lemma 4.6. So, the total running time of our algorithm is\\nO˜(n/α+ D(G) + αβ + D(G)α/β).\\nBy setting α = n1/2/(D(G))1/4 and β = (D(G))1/2, we get the running time of O˜(n1/2(D(G))1/4 +\\nD(G)) as desired. Note that it is possible that β ≥ α. In this case, we will simply set β = α to get\\nthe claimed running time; in fact, this happens only when D(G) ≥ n2/3, and the running time will\\nbe O˜(D(G)) in this case.\\n5 Algorithms on Fully-Connected Networks\\n5.1 O˜(\\n√\\nn)-time Exact Algorithm for SSSP\\nIn this section, we present an algorithm that solves SSSP exactly in O˜(\\n√\\nn) time on fully-connected\\nnetworks. The algorithm has two simple phases, as shown in Algorithm 5.1. In the first phase, it\\nreduces the shortest-path diameter using the techniques developed in Section 3.2. In particular,\\nevery node u broadcasts k =\\n√\\nn edges of smallest weight. Then, every node uses the information\\nit receives to compute a k-shortcut graph (G,w′), which can be done due to Observation 3.12. By\\nTheorem 3.10, we have\\nSPDiam(G,w′) < 4\\n√\\nn and ∀u, v : distG,w(u, v) = distG,w′(u, v).\\nIn the second phase, the algorithm simulates Bellman-Ford’s algorithm on (G,w′). In particular,\\nevery node iteratively uses the distance from s to other nodes to update its distance; i.e., every\\nnode v sets d(s, v) to minu(d(s, u) +w\\n′(uv)). It can be easily shown that by repeating this process\\nfor SPDiam(G,w′) iteration, d(s, u) = distG,w′(s, u) for every node u. We provide the sketch of this\\nclaim for completeness, as follows.\\nClaim 5.1 (Correctness of Phase 2 of Algorithm 5.1). Phase 2 of Algorithm 5.1 returns a function\\nd such that, for every node u, d(s, u) = distG,w′(s, u).\\nProof. We will show by induction that after the ith iteration, the value of d(s, u) will be at most the\\nvalue of the i-hop distance between s and u, i.e. d(s, u) ≤ distiG,w′(s, u) (recall that distiG,w′(s, u) is\\ndefined in Definition 3.1). This trivially holds before we start the first iteration since dist0G,w′(s, s) =\\n23\\nAlgorithm 5.1 O˜(\\n√\\nn)-time Exact Algorithm for SSSP\\nInput: A fully connected network (G,w) and source node s. Weight w(uv) of each edge uv is\\nknown to u and v.\\nOutput: Every node u knows d(s, u) which is the equal to distG,w(s, u).\\nPhase 1: Shortest path diameter reduction. This phase gives a new weight w′ such that\\nSPDiam(G,w′) < 4\\n√\\nn. The weight w′(uv) of an edge uv is known to its end-nodes u and v.\\n1: Let k =\\n√\\nn.\\n2: Each node u sends k edges of smallest weight, i.e. edges in Ek(u) as in Definition 3.11, to all\\nother nodes.\\n3: Every node v uses\\n⋃\\nv∈V (G)E\\nk(v) construct (Gk, w) and compute k-shortcut edges, i.e. compute\\nSkG,w(u) and {distG,w(u, v)}v∈SkG,w(u). // This step can be done internally (without communication) due\\nto Observation 3.12.\\n4: Augment (G,w) with k-shortcut edges: for any edge uv, let w′(uv) = distG,w(u, v) if u ∈ SkG,w(v)\\nor v ∈ SkG,w(u); otherwise, w′(uv) = w(uv). // This step can be done internally since both u and v know\\nall information needed, i.e. SkG,w(u), S\\nk\\nG,w(v), {distG,w(u, v)}v∈Sk\\nG,w\\n(u), and {distG,w(u, v)}u∈Sk\\nG,w\\n(v).\\nPhrase 2: Simulate Bellman-Ford algorithm on (G,w′). This phase makes every node u knows\\nd(s, u) where we claim that d(s, u) = distG,w(s, u).\\n5: Let d(s, s) = 0 and d(s, u) =∞ for every node u.\\n6: for i = 1 . . . 4\\n√\\nn do\\n7: Every node u sends d(s, u) to all other nodes.\\n8: Every node v updates d(s, v) to minu(d(s, u) + w\\n′(uv)).\\n9: end for\\n24\\n0 and dist0G,w′(s, u) = ∞. Assume for an induction that it holds for some i ≥ 0. For any node\\nu, let P be a shortest (i + 1)-hop s-u path, and v be the node preceding u in such path. By the\\ninduction hypothesis, after the ith iteration, d(s, v) ≤ distiG,w′(s, v). So, after the (i+ 1)th iteration,\\nd(s, u) ≤ d(s, v) + w′(vu) ≤ distG,w′(s, u). The claim thus holds for the (i+ 1)th iteration.\\nLet h = SPDiam(G,w′). Since disthG,w′(s, u) = distG,w′(s, u) for every node u, we have that\\nd(s, u) ≤ distG,w′(s, u) after h iterations. Since it is clear that d(s, u) ≥ distG,w′ , Claim 5.1 follows.\\n5.2 O˜(\\n√\\nn)-time (2 + o(1))-Approximation Algorithm for APSP\\nWe now present a (2 + o(1))-approximation algorithm for APSP, which also has O˜(\\n√\\nn) time. Our\\nalgorithm is outlined in Algorithm 5.2. In the first phase of this algorithm is almost the same as\\nthe first phase of Algorithm 5.1 presented in the previous section: by having every node u sending\\nout E\\n√\\nn(u) to all other nodes, we get a network (G,w′) such that\\nSPDiam(G,w′) < 4\\n√\\nn and ∀u, v : distG,w(u, v) = distG,w′(u, v).\\nThe only difference is that, in addition to performing Phase 1 of Algorithm 5.1 to get the properties\\nabove, we also make sure that\\nw′(uv) ≤ min\\nz∈Sk(u)\\ndistG,w(u, z) + w(zv) . (9)\\nThis is done by having every node u broadcasts Sk(u) and {distG,w(u, z)}z∈SkG,w(u) (which are also\\ncomputed in Phase 1 of Algorithm 5.1) to all other nodes, where k =\\n√\\nn. Then every node v\\ncan internally update w′(uv), for every node u, to w′(uv) = min{w′(uv),minz∈Sk(u) distG,w(u, z) +\\nw(zv)}. Phase 1 takes O˜(√n) time since performing Phase 1 of Algorithm 5.1 takes O˜(√n) time\\nand broadcasting Sk(u) and {distG,w(u, z)}z∈SkG,w(u), which are sets of size O˜(\\n√\\nn), also takes O˜(\\n√\\nn)\\ntime.\\nIn the second phase, we pick Θ(\\n√\\nn log n) nodes uniformly at random. Let R be the set of these\\nrandom nodes. We run the light-weight h-hop t-source SSSP algorithm (Algorithm 3.3) from these\\nrandom nodes using h =\\n√\\nn and t = |R|. By Theorem 3.6, we will finish in O˜(|R| + h) = O˜(√n)\\nrounds with high probability. Moreover, since the shortest-path distance is reduced to\\n√\\nn, every\\nnode will know an (1+o(1))-approximate distance to all random nodes in R. Every node broadcasts\\nthese distances to nodes in R to all other nodes. This takes O(|R|) time since the network is fully\\nconnected. In the final phase, every node u uses these broadcasted distances and the distances it\\ncomputes in the previous step (by simulating Dijkstra’s algorithm) to compute the approximate\\ndistance between itself and other nodes. In particular, for any node u, consider the following graph\\n(Gu, wu).\\nDefinition 5.2 (Graph (Gu, wu)). Graph (Gu, wu) consists of the following edges.\\n1. edges from u to all other nodes v of weight w′(u, v),\\n2. edges from x 6= u to nodes y ∈ Sk(x) of weight w′(x, y) = distG,w(x, y), and\\n3. edges from every random node r ∈ R to all other nodes v of weight d′′(r, v). (Recall that\\ndistG,w(r, v) ≤ d′′(r, v) ≤ (1 + o(1)) distG,w(r, v).)\\n25\\nAlgorithm 5.2 O˜(\\n√\\nn)-time (2 + o(1))-Approximation Algorithm for APSP\\nInput: A fully connected network (G,w). Weight w(uv) of each edge uv is known to u and v.\\nOutput: Every node u knows (2 + o(1))-approximate value of distG,w(u, v) for every node v.\\nPhase 1: Let k =\\n√\\nn. Compute Sk(u), for every node u, and weight assignment w′ such that\\nSPDiam(G,w′) < 4\\n√\\nn and w′(uv) ≤ minz∈Sk(u) distG,w(u, z) + w(zv).\\n1: Perform Phase 1 of Algorithm 5.1. This step makes every node u knows Sk(u) and\\n{distG,w(u, z)}z∈SkG,w(u).\\n2: Every node u sends Sk(u) and {distG,w(u, z)}z∈SkG,w(u) to all other nodes.\\n3: Every node v updates w′(uv), for every node u, to w′(uv) =\\nmin{w′(uv),minz∈Sk(u) distG,w(u, z) + w(zv)}. // This step can be done without communication\\nsince every node v knows Sk(u) and {distG,w(u, z)}z∈Sk\\nG,w\\n(u) for all nodes u\\nPhase 2: Compute (4\\n√\\nn)-hop (\\n√\\nn log n)-source shortest paths for\\n√\\nn log n random sources.\\n4: Let R be a set of randomly selected\\n√\\nn log n nodes.\\n5: Run the multi-souce bounded-hop shortest paths algorithm from Theorem 3.6 (Algorithm 3.3)\\non (G,w′) for h = 4\\n√\\nn hops using nodes in R as sources. // At the end of this process, every node\\nv knows a (1 + o(1))-approximate value of disthG,w′(r, v), which equals to distG,w′(r, v) since SPDiam(G,w\\n′) <\\n4\\n√\\nn, for all r ∈ R. We denote this approximate value by d′(u, v); thus, distG,w′(r, v) ≤ d′(u, v) ≤ (1 +\\no(1)) distG,w′(r, v).\\n6: Every node v sends d′(r, v), for all r ∈ R, to all nodes.\\nFinal Phase: Every node v uses the information it knows so far (see Definition 5.2) to compute\\nd′′(u, v) for all nodes u, which is claimed to be a (2 + o(1)) approximation of distG,w(u, v) (see\\nLemma 5.3).\\n26\\nfully\\t\\r \\xa0connected\\t\\r \\xa0APSP\\t\\r \\xa0\\nx1\\t\\r \\xa0u\\t\\r \\xa0 xi\\t\\r \\xa0…\\t\\r \\xa0 xi+1\\t\\r \\xa0xj\\t\\r \\xa0 …\\t\\r \\xa0 …\\t\\r \\xa0 v\\t\\r \\xa0\\nSk(u)\\t\\r \\xa0 Sk(v)\\t\\r \\xa0\\n≤\\t\\r \\xa0distG,w(u,\\t\\r \\xa0xi+1)\\t\\r \\xa0 ≤\\t\\r \\xa0distG,w(xi+1,v)\\t\\r \\xa0\\nu\\t\\r \\xa0 xi\\t\\r \\xa0…\\t\\r \\xa0 xi+1\\t\\r \\xa0 xj\\t\\r \\xa0 …\\t\\r \\xa0 v\\t\\r \\xa0\\nSk(u)\\t\\r \\xa0\\nSk(v)\\t\\r \\xa0\\n…\\t\\r \\xa0 xj-\\xad‐1\\t\\r \\xa0\\n≤\\t\\r \\xa0distG,w(u,\\t\\r \\xa0v)/2\\t\\r \\xa0\\nr\\t\\r \\xa0\\n≤\\t\\r \\xa0distG,w(u,\\t\\r \\xa0\\nv)+distG,w(u\\n,\\t\\r \\xa0r)\\t\\r \\xa0\\n(a) Case 1: j ≤ i+ 1\\nfully\\t\\r \\xa0connected\\t\\r \\xa0APSP\\t\\r \\xa0\\nx1\\t\\r \\xa0u\\t\\r \\xa0 xi\\t\\r \\xa0…\\t\\r \\xa0 xi+1\\t\\r \\xa0xj\\t\\r \\xa0 …\\t\\r \\xa0 …\\t\\r \\xa0 v\\t\\r \\xa0\\nSk(u)\\t\\r \\xa0 Sk(v)\\t\\r \\xa0\\n≤\\t\\r \\xa0distG,w(u,\\t\\r \\xa0xi+1)\\t\\r \\xa0 ≤\\t\\r \\xa0distG,w(xi+1,v)\\t\\r \\xa0\\nu\\t\\r \\xa0 xi\\t\\r \\xa0…\\t\\r \\xa0 xi+1\\t\\r \\xa0 xj\\t\\r \\xa0 …\\t\\r \\xa0 v\\t\\r \\xa0\\nSk(u)\\t\\r \\xa0\\nSk(v)\\t\\r \\xa0\\n…\\t\\r \\xa0 xj-\\xad‐1\\t\\r \\xa0\\n≤\\t\\r \\xa0distG,w(u,\\t\\r \\xa0v)/2\\t\\r \\xa0\\nr\\t\\r \\xa0\\n≤\\t\\r \\xa0distG,w(u,\\t\\r \\xa0\\nv)+distG,w(u\\n,\\t\\r \\xa0r)\\t\\r \\xa0\\n(b) Case 2: j > i+ 1.\\nFigure 1: Outline of the proof of Lemma 5.3\\nNode u will use distGu,wu(u, v) as an approximate distance of distGu,w(u, v). We now show that\\nthis gives a (2 + o(1))-approximate distance.\\nLemma 5.3 (Approximation guarantee of Algorithm 5.2). For every pair of nodes u and v,\\ndistG,w(u, v) ≤ distGu,wu(u, v) ≤ (2 + o(1)) distG,w(u, v).\\nProof. It is clear that distG,w(u, v) ≤ distGu,wu(u, v) since wu(xy) ≥ distG,w(x, y) for every pair of\\nnodes x and y. It is left to prove that distGu,wu(u, v) ≤ (2 + o(1)) distG,w(u, v). Let\\nP = 〈u = x1, x2, . . . , xk = v〉\\nbe a shortest u-v path. Note that we can assume that k ≤ √n since the shortest-path diameter is√\\nn. Let xi be the furthest node from u that is in S\\nk(u) ∩ P and, similarly, let xj be the furthest\\nnode from v that is in Sk(v) ∩ P ; i.e.\\ni = arg max\\ni′\\n(xi′ ∈ Sk(u) ∩ P ) and j = arg min\\nj′\\n(xj′ ∈ Sk(v) ∩ P ) .\\nNote that x1, . . . , xi are all in S\\nk(u) since all nodes x1, . . . , xi−1 are nearer to u than xi. Similarly,\\nxj , . . . , xk are all in S\\nk(v). Note further that w′(u, xi+1) = distG,w(u, xi+1) since Phase 1 guarantees\\nEquation (9) which implies that\\nw′(u, xi+1) ≤ min\\nz∈Sk(u)\\ndistG,w(u, z) + w(zxi+1)\\n≤ distG,w(u, xi) + w(xixi+1) (since xi ∈ Sk(u))\\n= distG,w(u, xi+1) .\\nWe now consider two cases (see Figure 1 for an outline). First, if j ≤ i + 1, then we have\\nxi+1 ∈ Sk(u) ∩ Sk(v). This means that wu(xi+1, v) = w′(xi+1, v) = distG,w(xi+1, v).\\ndistGu,wu(u, v) ≤ wu(u, xi+1) + wu(xi+1, v)\\n= w′(u, xi+1) + w′(xi+1, v)\\n= distG,w(u, xi+1) + distG,w(xi+1, v)\\n= distG,w(u, v).\\n27\\nNow we consider the second case where j > i+ 1. In this case, we have that either\\ndistG,w(u, xi+1) ≤ distG,w(u, v)/2 or distG,w(v, xj−1) ≤ distG,w(u, v)/2 .\\nSince the analyses for both cases are essentially the same, we will only show the analysis when\\ndistG,w(u, xi+1) ≤ distG,w(u, v)/2. Observe that, with high probability, there is a random node\\nr ∈ R that is in Sk(u) since |Sk(u)| ≥ √n and R consists of Θ˜(√n) random nodes (see Fact 4.4).\\nRecall that, for every node z,\\nwu(r, z) ≤ d′′(r, z) ≤ (1 + o(1)) distG,w(r, z).\\nIt follows that\\ndistGu,wu(u, v) ≤ wu(u, r) + wu(r, v) (10)\\n≤ (1 + o(1))(distG,w(u, r) + distG,w(r, v)) (11)\\n≤ (1 + o(1))(distG,w(u, r) + (distG,w(u, r) + distG,w(u, v))) (12)\\n≤ (1 + o(1))(2 distG,w(u, xi+1) + distG,w(u, v)) (13)\\n≤ (2 + o(1)) distG,w(u, v). (14)\\nEquation (12) is by triangle inequality. Equation (13) is because r ∈ Su and xi+1 /∈ Su. Equa-\\ntion (14) is because of the assumption that distG,w(u, xi+1) ≤ distG,w(u, v)/2.\\n6 Lower Bound for Approximating APSP (Proof of Observation 1.4)\\nObservation 1.4 (Lower bound for APSP). Any poly(n)-approximation algorithm for APSP on an\\nn-node weighted network G requires Ω( nlogn) time. This lower bound holds even when the underlying\\nnetwork G has diameter D(G) = 2. Moreover, for any α(n) = O(n), any α(n)-approximation\\nalgorithm on an unweighted network requires Ω( nα(n) logn) time.\\nProof. Our proof simply formalizes the fact that a node needs to receive at least n bits of information\\nin order to know its distance to all other nodes. We start from the following messag sending problem:\\nAlice receive a β-bit binary vector, denoted by 〈x1, . . . , xβ〉, where we set β = n− 2. She wants to\\nsend this vector to Bob. Intuitively, to be sure that Bob gets the value of the vector correctly with\\na good probability, i.e. with probability at least 1− \\x0f for some small \\x0f > 0, Alice has to send Ω(β)\\nbits to Bob, regardless of what Bob sends to her. This fact can be formally proved in many ways\\n(e.g., by using communication complexity lower bounds) and is true even in the quantum setting\\n(see, e.g., Holevo’s theorem [Hol73]).\\nNow, let A be an α(n)-approximation T -time algorithm for weighted APSP. We show that\\nAlice can use A to send her message to Bob using O(T log n) bits, as follows. Construct a graph G\\nconsisting of n = β + 2 nodes, denoted by a1, . . . , aβ, a\\n∗ and b. There are edges between all nodes\\nto a∗. The weight of edge a∗b is always w(a∗b) = 1. Weight of every edge aia∗ is set by Alice: if\\nxi = 1 then she sets weight of aia\\n∗ to w(aia∗) = 1; otherwise she sets it to w(aia∗) = 2α(n). Then,\\nAlice simulates A on a1, . . . , aβ and a∗, and Bob simulates A on b. If A wants to send any message\\nfrom a∗ to b, Alice will send this message to Bob so that Bob can continue simulating b. Similarly,\\nif A wants to send any message from b to a∗, Bob will send this message to Alice so that Alice can\\ncontinue simulating a∗. If A finishes in T rounds, then Alice will send at most O(T log n) bits to\\nBob in total.\\n28\\nObserve that, for any i, if xi = 1 then distG,w(ai, b) = 2; otherwise, distG,w(ai, b) = 2α(n) + 1.\\nSince A is α(n) approximation, A must answer ˜distG,w(ai, b) ≤ 2α(n) if xi = 1; otherwise, A must\\nanswer ˜distG,w(ai, b) ≥ 2α(n) + 1. Since ˜distG,w(ai, b) is known to b, Bob can get the value of\\n˜distG,w(ai, b) by reading it from b (which he is simulating). Then he can reconstructs xi. Thus,\\nBob can reconstruct all bits x1, . . . , xβ after getting O(T log n) bits from Alice. The lower bound\\nof the message sending problem thus implies that T = Ω(β/ log n) = Ω(n/ log n).\\nNote that since the highest weight we can put on an edge is poly(n), we require that α(n) ≤\\npoly(n). We use the same argument for the unweighted case, but this time we use β = n/α(n) and\\nreplace an edge of weight 2α(n) by a path of length 2α(n).\\nNote that in the proof above we show a lower bound for computing distances between all pairs\\nof nodes. Since the lower bound graph is a star, the routing problem is trivial (since there is always\\none option to send a message). We can easily modify the above graph to give the same lower bound\\nfor the routing problem on weighted graphs: First, instead of using weight 2α(n) in the graph\\nabove, use weight 2α2(n) + α(n) instead. Second, add a new node c and edges of weight 2α(n)\\nbetween c and all nodes a1 and an edge of weight 1 between c and b. Observe that if xi = 1, then\\nwe have to route a message through a∗, giving a distance of 2 (while routing through c gives a\\ndistance of 2α(n) + 1. If xi = 0, we should route through c which gives a distance of 2α(n) + 1\\nsince routing through a∗ will cost 2α2(n) + α(n) + 1.\\n7 Open Problems\\nThe main question left by our SSSP algorithm is the following.\\nProblem 7.1. Close the gap between the upper bound of O˜(n1/2D1/4) presented in this paper and the\\nlower bound of Ω˜(n1/2) presented in [DHK+12] for (1 + \\x0f)-approximating the single-source shortest\\npaths problem on general networks.\\nImproving the current upper bound is important since there are many problems that can be\\npotentially solved by using the same technique. Moreover, giving a lower bound in the form\\nΩ˜(n1/2Dδ) for some δ > 0 will be quite surprising since such lower bound has not been observed\\nbefore. It should also be fairly interesting to refine our upper bound to achieve a O˜(n1/2D\\x0f)-time\\nO(1/\\x0f)-approximation algorithm for any δ > 0. Another question that should be very interesting\\nis understanding the exact case:\\nProblem 7.2. Can we solve SSSPexactly in sublinear-time?\\nIt is also interesting to solve APSPexactly in linear-time (recall that sublinear-time is not pos-\\nsible). In some settings, an exact algorithm for computing shortest paths is crucial; e.g. some\\nInternet protocols such as OSPF and IS-IS use edge weights to control the traffic and using an ap-\\nproximate shortest paths with this protocol is unacceptable9. The next question is a generalization\\nof our SSSP:\\nProblem 7.3 (Asymmetric SSSP). How fast can we solve SSSP on networks whose edge weights\\ncould be asymmetric, i.e. if we think of each edge uv as two directed edges −→uv and −→vu, it is possible\\nthat w(−→uv) 6= w(−→vu).\\n9We thank Mikkel Thorup for pointing out this fact\\n29\\nNote that we are particularly interested in the case where weights do not affect communication;\\nin other words, if u can send a message to v, then v can also send a message to u. Also note\\nthat our light-weight SSSP algorithm can be used to solve this problem (but not the shortest-path\\ndiameter reduction technique). By adjusting parameters appropriately, we can (1 + \\x0f)-approximate\\nthis problem in O˜(min(n2/3, n1/2D1/2)) time. In fact, improving this running time for the following\\nvery special case seems challenging already:\\nProblem 7.4 (s-t Reachability Problem). Given a directed graph G and two special nodes s and\\nt, we want to know whether there is a directed path from s to t. The communication network is\\nthe underlying undirected graph; i.e. the communication can be done independent of edge directions\\nand the diameter D is defined to be the diameter of the underlying undirected graph. Can we answer\\nthis question in O˜(\\n√\\nn+ D) time?\\nThis problem shows limitations of the techniques presented in this paper, and we believe that\\nsolving it will give a new insight into solving all above open problems. Our last set of questions:\\nProblem 7.5. Can we improve the O˜(n1/2)-time upper bound for SSSP on fully-connected networks\\nwhile keeping the approximation ratio small (say, at most two)? Is it possible to prove a nontrivial\\nω(1) lower bound?\\nNote that the last question was asked earlier by Elkin [Elk04].\\n8 Acknowledgement\\nThe sublinear-time SSSP algorithm was inspired by the discussions with Jittat Fakcharoenphol\\nand Jakarin Chawachat, who refused to co-author this paper. Several techniques borrowed from\\ndynamic graph algorithms benefit from many intensive discussions with Sebastian Krinninger and\\nMonika Henzinger. I also would like to thank Radhika Arava and Peter Robinson for explaining\\nthe algorithm of Baswana and Sen [BS07] to him and Gopal Pandurangan, Chunming Li, Anisur\\nRahaman, David Peleg, Atish Das Sarma, Parinya Chalermsook, Bundit Laekhanukit, Boaz Patt-\\nShamir, Shay Kutten, Christoph Lenzen, Stephan Holzer, Mohsen Ghaffari, Nancy Lynch, and\\nMikkel Thorup, for discussions, comments, and pointers to related results. I also thank all reviewers\\nof STOC 2014 for many thoughtful comments.\\nReferences\\n[AHT92] John K. Antonio, Garng M. Huang, and Wei Kang Tsai. A fast distributed shortest path\\nalgorithm for a class of hierarchically clustered data networks. IEEE Trans. Computers,\\n41(6):710–724, 1992. 7\\n[APRU12] John Augustine, Gopal Pandurangan, Peter Robinson, and Eli Upfal. Towards robust\\nand efficient computation in dynamic peer-to-peer networks. In SODA, pages 551–569,\\n2012. 4\\n[AR82] J. Abram and I. Rhodes. Some shortest path algorithms with decentralized infor-\\nmation and communication requirements. Automatic Control, IEEE Transactions on,\\n27(3):570–582, 1982. 7\\n30\\n[AR93] Yehuda Afek and Moty Ricklin. Sparser: A paradigm for running distributed algo-\\nrithms. J. Algorithms, 14(2):316–328, 1993. 7\\n[Bel58] R. Bellman. On a routing problem. Quarterly of Applied Mathematics, 16:87–90, 1958.\\n3, 4, 6\\n[Ber13] Aaron Bernstein. Maintaining shortest paths under deletions in weighted directed\\ngraphs: [extended abstract]. In STOC, pages 725–734, 2013. 8\\n[BHP12] Andrew Berns, James Hegeman, and Sriram V. Pemmaraju. Super-fast distributed\\nalgorithms for metric facility location. In ICALP (2), pages 428–439, 2012. 5\\n[BS07] Surender Baswana and Sandeep Sen. A simple and linear time randomized algorithm for\\ncomputing sparse spanners in weighted graphs. Random Struct. Algorithms, 30(4):532–\\n563, 2007. Also in ICALP’03. 3, 6, 30\\n[Coh00] Edith Cohen. Polylog-time and near-linear work approximation scheme for undirected\\nshortest paths. J. ACM, 47(1):132–166, 2000. Announced at STOC 1994. 9\\n[DDP12] Atish Das Sarma, Michael Dinitz, and Gopal Pandurangan. Efficient computation of\\ndistance sketches in distributed networks. In SPAA, pages 318–326, 2012. 6\\n[DFI05] Camil Demetrescu, Irene Finocchi, and Giuseppe F. Italiano. Handbook on Data Struc-\\ntures and Applications, chapter 36: Dynamic Graphs. Dinesh Mehta and Sartaj Sahni\\n(eds.), CRC Press Series, in Computer and Information Science, 2005. 10, 21\\n[DHK+12] Atish Das Sarma, Stephan Holzer, Liah Kor, Amos Korman, Danupon Nanongkai,\\nGopal Pandurangan, David Peleg, and Roger Wattenhofer. Distributed verification\\nand hardness of distributed approximation. SIAM J. Comput., 41(5):1235–1265, 2012.\\nAnnounced at STOC 2011. 1, 3, 4, 6, 29\\n[DLP12] Danny Dolev, Christoph Lenzen, and Shir Peled. ”tri, tri again”: Finding triangles and\\nsmall subgraphs in a distributed setting - (extended abstract). In DISC, pages 195–209,\\n2012. 5\\n[DNPT13] Atish Das Sarma, Danupon Nanongkai, Gopal Pandurangan, and Prasad Tetali. Dis-\\ntributed random walks. J. ACM, 60(1):2, 2013. Also in PODC’09 and PODC’10.\\n4\\n[EFK+12] Yuval Emek, Pierre Fraigniaud, Amos Korman, Shay Kutten, and David Peleg. Notions\\nof connectivity in overlay networks. In SIROCCO, pages 25–35, 2012. 19\\n[EKNP12] Michael Elkin, Hartmut Klauck, Danupon Nanongkai, and Gopal Pandurangan.\\nQuantum distributed network computing: Lower bounds and techniques. CoRR,\\nabs/1207.5211, 2012. 4, 6\\n[Elk04] Michael Elkin. Distributed approximation: a survey. SIGACT News, 35(4):40–57, 2004.\\n1, 7, 30\\n[Elk05] Michael Elkin. Computing almost shortest paths. ACM Transactions on Algorithms,\\n1(2):283–323, 2005. Also in PODC’01. 6\\n31\\n[Elk06] Michael Elkin. An unconditional lower bound on the time-approximation trade-off for\\nthe distributed minimum spanning tree problem. SIAM J. Comput., 36(2):433–456,\\n2006. Also in STOC’04. 1, 6\\n[EZ06] Michael Elkin and Jian Zhang. Efficient algorithms for constructing (1+epsilon, beta)-\\nspanners in the distributed and streaming models. Distributed Computing, 18(5):375–\\n385, 2006. Also in PODC’04. 6\\n[FHW12] Silvio Frischknecht, Stephan Holzer, and Roger Wattenhofer. Networks cannot compute\\ntheir diameter in sublinear time. In SODA, pages 1150–1162, 2012. 5, 7\\n[For56] Lester R. Ford. Network Flow Theory. Report P-923, The Rand Corporation, 1956. 3,\\n4, 6\\n[FRT04] Jittat Fakcharoenphol, Satish Rao, and Kunal Talwar. A tight bound on approximating\\narbitrary metrics by tree metrics. J. Comput. Syst. Sci., 69(3):485–497, 2004. Also in\\nSTOC’03. 6\\n[GK81] D.H. Greene and D.E. Knuth. Mathematics for the analysis of algorithms. Progress in\\ncomputer science. Birkha¨user, 1981. 21\\n[GK13] Mohsen Ghaffari and Fabian Kuhn. Distributed minimum cut approximation. In DISC,\\npages 1–15, 2013. 1, 2\\n[GKP98] Juan A. Garay, Shay Kutten, and David Peleg. A sublinear time distributed algorithm\\nfor minimum-weight spanning trees. SIAM J. Comput., 27(1):302–316, 1998. Also in\\nFOCS’93. 1, 2\\n[Hal97] S. Haldar. An ‘all pairs shortest paths’ distributed algorithm using 2n2 messages. J.\\nAlgorithms, 24(1):20–36, 1997. 7\\n[Hol73] A. S. Holevo. Bounds for the quantity of information transmitted by a quantum commu-\\nnication channel. Problemy Peredachi Informatsii, 9(3):3–11, 1973. English translation\\nin Problems of Information Transmission, 9:177–183, 1973. 28\\n[Hol13] Stephan Holzer. Distance Computation, Information Dissemination, and Wireless Ca-\\npacity in Networks, Diss. ETH No. 21444. Phd thesis, ETH Zurich, Zurich, Switzerland,\\n2013. 5\\n[HW12] Stephan Holzer and Roger Wattenhofer. Optimal distributed all pairs shortest paths\\nand applications. In PODC, pages 355–364, 2012. 1, 5, 7, 8\\n[KKM+12] Maleq Khan, Fabian Kuhn, Dahlia Malkhi, Gopal Pandurangan, and Kunal Talwar.\\nEfficient distributed approximation algorithms via probabilistic tree embeddings. Dis-\\ntributed Computing, 25(3):189–205, 2012. Announced at PODC 2008. 6\\n[KKP13] Liah Kor, Amos Korman, and David Peleg. Tight bounds for distributed minimum-\\nweight spanning tree verification. Theory Comput. Syst., 53(2):318–340, 2013. 1, 6\\n[KP98] Shay Kutten and David Peleg. Fast distributed construction of small k-dominating sets\\nand applications. J. Algorithms, 28(1):40–66, 1998. 1, 2\\n32\\n[KP08] Maleq Khan and Gopal Pandurangan. A fast distributed approximation algorithm\\nfor minimum spanning trees. Distributed Computing, 20(6):391–402, 2008. Also in\\nDISC’06. 2, 6\\n[KS92] Philip N. Klein and Sairam Sairam. A parallel randomized approximation scheme for\\nshortest paths. In STOC, pages 750–758, 1992. 8\\n[Len13] Christoph Lenzen. Optimal deterministic routing and sorting on the congested clique.\\nIn PODC, pages 42–50, 2013. 5\\n[LMR94] Frank Thomson Leighton, Bruce M. Maggs, and Satish Rao. Packet Routing and Job-\\nShop Scheduling in O(Congestion + Dilation) Steps. Combinatorica, 14(2):167–186,\\n1994. Also in FOCS’88. 8, 16\\n[LPS13] Christoph Lenzen and Boaz Patt-Shamir. Fast routing table construction using small\\nmessages: extended abstract. In STOC, pages 381–390, 2013. 1, 2, 3, 4, 5, 6, 9, 10, 19\\n[LPSP06] Zvi Lotker, Boaz Patt-Shamir, and David Peleg. Distributed mst for constant diameter\\ngraphs. Distributed Computing, 18(6):453–460, 2006. 4, 7\\n[LPSPP05] Zvi Lotker, Boaz Patt-Shamir, Elan Pavlov, and David Peleg. Minimum-Weight Span-\\nning Tree Construction in O(log log n) Communication Rounds. SIAM J. Comput.,\\n35(1):120–131, July 2005. Also in SPAA’03. 5\\n[LPSR09] Zvi Lotker, Boaz Patt-Shamir, and Adi Rose´n. Distributed approximate matching.\\nSIAM J. Comput., 39(2):445–460, 2009. 2\\n[LW11] Christoph Lenzen and Roger Wattenhofer. Tight bounds for parallel randomized load\\nbalancing: extended abstract. In Proceedings of the 43rd annual ACM symposium on\\nTheory of computing, STOC ’11, pages 11–20, New York, NY, USA, 2011. ACM. 5\\n[Lyn96] Nancy A. Lynch. Distributed Algorithms. Morgan Kaufmann Publishers Inc., San\\nFrancisco, CA, USA, 1996. 5, 7, 8\\n[MU05] Michael Mitzenmacher and Eli Upfal. Probability and computing - randomized algo-\\nrithms and probabilistic analysis. Cambridge University Press, 2005. 20\\n[Nan14] Danupon Nanongkai. Brief announcement: Almost-tight approximation distributed\\nalgorithm for minimum cut. In PODC, 2014. 1\\n[Pel00] D. Peleg. Distributed computing: a locality-sensitive approach. Society for Industrial\\nand Applied Mathematics, Philadelphia, PA, USA, 2000. 1, 5, 7, 8\\n[Pet10] Seth Pettie. Distributed algorithms for ultrasparse spanners and linear size skeletons.\\nDistributed Computing, 22(3):147–166, 2010. Also in PODC’08. 6\\n[PR00] David Peleg and Vitaly Rubinovich. A near-tight lower bound on the time complexity of\\ndistributed minimum-weight spanning tree construction. SIAM J. Comput., 30(5):1427–\\n1442, 2000. Also in FOCS’99. 1, 6\\n33\\n[PRT12] David Peleg, Liam Roditty, and Elad Tal. Distributed algorithms for network diameter\\nand girth. In ICALP (2), pages 660–672, 2012. 1, 5, 7, 8\\n[PST11] Boaz Patt-Shamir and Marat Teplitsky. The round complexity of distributed sorting:\\nextended abstract. In Proceedings of the 30th annual ACM SIGACT-SIGOPS sympo-\\nsium on Principles of distributed computing, PODC ’11, pages 249–256, New York, NY,\\nUSA, 2011. ACM. 5\\n[PT11] David Pritchard and Ramakrishna Thurimella. Fast computation of small cuts via cycle\\nspace sampling. ACM Transactions on Algorithms, 7(4):46, 2011. Also in ICALP’08. 1\\n[RT85] Prabhakar Raghavan and Clark D. Thompson. Provably good routing in graphs: Reg-\\nular arrays. In STOC, pages 79–87, 1985. 8\\n[San06] Nicola Santoro. Design and Analysis of Distributed Algorithms (Wiley Series on Parallel\\nand Distributed Computing). Wiley-Interscience, 2006. 7\\n[Som12] Christian Sommer. Shortest-path queries in static networks, 2012. submitted. 19\\n[Su14] Hsin-Hao Su. Brief announcement: A distributed minimum cut approximation scheme.\\nIn SPAA, 2014. 1\\n[Thu97] Ramakrishna Thurimella. Sub-linear distributed algorithms for sparse certificates and\\nbiconnected components. J. Algorithms, 23(1):160–179, 1997. Also in PODC’95. 1\\n[TZ05] Mikkel Thorup and Uri Zwick. Approximate distance oracles. J. ACM, 52(1):1–24,\\n2005. Also in STOC’11. 6\\n[UY91] Jeffrey D. Ullman and Mihalis Yannakakis. High-probability parallel transitive-closure\\nalgorithms. SIAM J. Comput., 20(1):100–125, 1991. 20, 21\\n34\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd1a'), 'authors': 'Panizzi, Emanuele, Pastorelli, Bernardo', 'year': '2012', 'title': 'Multimethods and separate static typechecking in a language with\\n  C++-like object model', 'full_text': '  \\n \\n \\n \\n \\n \\n \\n \\n \\nUniversità dell’Aquila \\nDipartimento di Ingegneria Elettrica \\n \\n \\n \\n \\n \\nMultimethods and separate static  \\ntypechecking in a language with  \\nC++-like object model \\n \\n \\nEmanuele Panizzi, Bernardo Pastorelli \\n \\n \\nResearch report R.99-33\\n 1 \\nMultimethods and separate static typechecking in a language with \\nC++-like object model \\nEmanuele Panizzi, Bernardo Pastorelli \\n{panizzi, pastorelli}@ing.univaq.it \\n \\n Università degli Studi dell’Aquila, Dipartimento di Ingegneria Elettrica \\n67040 Monteluco di Roio, L’Aquila, Italy \\n \\n \\nAbstract \\nThe goal of this paper is the description and analysis \\nof multimethod implementation in a new object-\\noriented, class-based programming language called \\nOOLANG. The implementation of the multimethod \\ntypecheck and selection, deeply analyzed in the \\npaper, is performed in two phases in order to allow \\nstatic typechecking and separate compilation of \\nmodules. The first phase is performed at compile \\ntime, while the second is executed at link time and \\ndoes not require the modules’ source code. \\nOOLANG has syntax similar to C++; the main \\ndifferences are the absence of pointers and the \\nrealization of polymorphism through subsumption. It \\nadopts the C++ object model and supports multiple \\ninheritance as well as virtual base classes. For this \\nreason, it has been necessary to define techniques for \\nrealigning argument and return value addresses when \\nperforming multimethod invocations. \\n \\n \\nKeywords: multimethods, object-oriented language, \\nobject model, subsumption, static typecheck, separate \\ntypechecking, pointer realignment, OOLANG \\n1. Introduction \\nWhen writing object-oriented mathematical software \\nit would be useful to define a polymorphic binary \\noperator in a base class and in a class derived from it \\nin such a way that: \\n• each definition accept two arguments (e.g. the \\ninvocation object and a parameter) of the same \\ntype of the class to which it belongs; \\n• the proper definition be called according to the \\nruntime type of both the operands.  \\nThis is generally not possible in traditional object-\\noriented languages because they use single \\ndispatching techniques, so only the type of one \\noperand (the invocation object) is used to select the \\ndefinition to call. \\nOne of the solutions to this problem (also known as \\nthe problem of binary methods [7]) is the use of \\nmultimethods [7][19]. They are polimorphic \\nfunctions selected considering not only the type of \\nthe invocation object but also the type of all the other \\narguments.  \\nThis solution has been implemented in OOLANG, a \\nnew class-based object-oriented language targeted at \\nmathematical software1. \\nFor performance reasons, one of the goals of \\nOOLANG is the ability to perform typechecking at \\ncompile-time (i.e. to support static typechecking). \\nMoreover, in order to support precompiled libraries, \\nOOLANG allows separate compilation. But it\\'s \\nsurely challenging the separate static typechecking of \\nmultimethods: in fact, as multimethods can be \\ndeclared and defined outside class declarations (and \\neventually in different modules) two source files that \\ntypecheck successfully when compiled (separately) \\nmay interfere and generate type errors when linked \\ntogether. In other languages that support \\nmultimethods this problem is solved either by \\nrequiring that the typechecking of the entire program \\nbe done in a whole or by imposing restrictions on the \\nsymmetry of multimethods. OOLANG, instead, \\nallows the separate static typechecking of arbitrary \\nmultimethods, requiring at link-time only a simple \\ncheck that is efficiently performed during the \\nconstruction of the compressed dispatch tables [18]. \\nMoreover, OOLANG implements multimethods over \\nthe C++ object model. This opens two problems due \\nto the representation of objects that are instances of \\nclasses with multiple parents. The first is the loss (to \\na certain extent) of transitivity in the subtype relation. \\nThe second is the necessity to realign objects when \\n                                                        \\n1\\n In fact, OOLANG was developed for the APEmille  \\nSPMD supercomputer [2][4]. In this paper only the object-\\noriented features are analyzed, avoiding the description of \\nthe parallel constructs, not relevant to the argument of the \\npaper. \\n 2 \\npassed as arguments to a multimethod or when they \\nare returned by multimethods to static functions. \\nThe OOLANG language does not support pointers. It \\nhowever supports subsumption, i.e. the possibility to \\nuse an instance of a derived class where an instance \\nof a parent class is expected. Parameters (of a \\nfunction or multimethod) can be passed by value (the \\nwhole object passed as argument is copied on the \\nstack) or by reference (&). It is also possible to \\ndeclare a parameter as constant and, to allow this, it \\nhas been necessary to solve a problem of interference \\nof constant parameters with multimethod selection. \\nFinally, because of the impossibility to know which \\nspecialization of a multimethod will be invoked at \\nrun-time and in order to effectively take advantage of \\nthe different parameter passing possibilities, it has \\nbeen necessary to develop a parameter passing \\nscheme that requires the cooperation of the caller and \\nthe callee. \\nThe OOLANG compiler produces APEmille code as \\nwell as portable C code. \\nThe paper is organized as follows: section 2 \\nintroduces the main features of the OOLANG \\nlanguage. Section 3 discusses the OOLANG object \\nmodel, pointing out the loss of transitivity in the \\nsubtype relation and the necessity of realigning \\nobjects. Section 4 presents the details of OOLANG \\ntypechecking of multimethods. Section 5 describes \\nthe mechanism of parameter passing and address \\nrealignment. Section 6 analyzes the related work \\nwhile section 7 drains the conclusions and presents \\nsome future developments. \\n2. Introduction to the OOLANG language \\nOOLANG is a class-based object-oriented language \\nwith syntax similar to the C++ one [25]. The most \\nsignificant OOLANG characteristics are shown in \\nthis section. \\nA first OOLANG feature is the support for virtual \\nfunctions, i.e. functions that are selected at runtime \\nconsidering the type of the invocation object (similar \\nto those of C++). The main difference is that, unlike \\nC++, OOLANG is pointer-less and allows \\npolymorphism through subsumption, i.e. through the \\npossibility of using an instance of a derived class \\nwhere an instance of a parent class is expected2. \\nFigure 1.a presents a fragment of OOLANG code3: \\nthe class ColorPoint inherits the x and y fields \\nfrom the class Point (which is a public parent of \\nColorPoint). The static function print(), that \\naccepts a Point argument, is invoked once using an \\nargument of type Point and then using an argument \\nof type ColorPoint. The print() function in \\nturn invokes the virtual function dump(). Although \\nthe static type of the parameter p is Point and there \\n                                                        \\n2\\n C++ on the other hand allows polymorphism only \\nthrough pointers, e.g. when the invocation object of a \\nvirtual function is a pointer. \\n3\\n The code showed in the following figures is written in \\nOOLANG language unless differently specified. \\nclass Point { \\n int x,y; \\n    virtual void dump(){ \\n  printf(“Point\\\\n”); \\n    } \\n}; \\n \\nclass ColorPoint: public Point { \\n int color; \\n    void dump(){ \\n  printf(“ColorPoint\\\\n”); \\n    } \\n} \\n \\nvoid print(Point p) { \\n    p.dump(); \\n} \\n \\nint main() { \\n Point a; \\n ColorPoint b; \\n \\n    print(a); \\n    print(b); \\n}; \\n \\n(a) \\n \\n \\nPoint \\nColorPoint \\n \\n(b) \\nFigure 1. Example of OOLANG program (a) and its \\noutput (b) \\nclass A{ \\n int a; \\n}; \\nclass B: virtual A{ \\n int b; \\n}; \\n \\nclass C: virtual A{ \\n int c; \\n}; \\nclass D: public B, public C{ \\n int d; \\n}; \\n \\nFigure 2. Virtual inheritance \\n 3 \\nare no pointers (as the language is pointer-less), \\nthanks to subsumption, the invoked dump() \\nfunction is the one defined in the class of the actual \\nparameter (A or B respectively) as shown by the \\nprogram output (Figure 1.b). \\nAnother OOLANG characteristic is the support of \\nmultiple inheritance and virtual parents: if a class D \\ninherits from two classes B and C that have a \\ncommon parent A declared virtual (Figure 2), its \\nobjects contain only one copy of the fields of the A \\nparent. This topic will be further analyzed in section \\n3. \\nThe main OOLANG feature that will be extensively \\nanalyzed in this paper is the implementation of \\nmultimethods. Multimethods allow avoiding a \\nfundamental limitation of virtual functions.  \\nFigure 3 presents a new version of the Point and \\nColorPoint classes containing a virtual function \\nequal() that tests if two points are equal. Both the \\nversions of the equal() function have a parameter \\nof type Point. This example (adapted from [7]) \\nshows the cited limitation of virtual functions. It is \\nnot possible to define, in the derived class, a version \\nof the equal() function that accepts an instance of \\nthe derived class as parameter: the parameters of a \\nvirtual function can’t be specialized when the \\nfunction is overridden in a new class. \\nIf this constraint is relaxed, run-time type errors can \\narise. In fact, suppose that the equal() function \\ndefined in ColorPoint class accept an instance of \\nthis class as parameter (as in Figure 4, whose code is \\nnot good OOLANG). The invocation of function \\nfunc() is correct because the ColorPoint object \\na is subsumed to an instance of Point. The \\ninvocation of the equal() method in function \\nfunc() typechecks at compile-time because it is \\nchecked against the equal() method declared in \\nclass Point, having p1 static type Point. At run-\\ntime, the dynamic type of p1 is ColorPoint \\nbecause the first argument of func() is the a \\nobject; so the equal() method declared in class \\nColorPoint is invoked. This method expects a \\nsecond argument of type ColorPoint while p2 \\nhas static and dynamic type Point (it is a copy of \\nthe b object): this results in a run-time error because \\nan instance of Point hasn\\'t the color field that is \\naccessed in the code of the method (this error, in the \\nworst case, does not raise an exception). \\nTo solve this problem OOLANG uses multimethods. \\nThese functions are selected considering the types of \\nall the arguments of a method invocation and not \\nonly the type of the first one (the invocation object). \\nOOLANG treats all the operators (for example \\noperator== or operator+) like multimethods. \\nIn fact mathematical operators are the classical \\nexamples of binary methods and they are the primary \\nreason of the inclusion of multimethods in \\nOOLANG. The programmer can define other \\nmultimethods in the same way he defines ordinary \\nmethods; the only difference is that a multimethod \\nname begins with @4. \\n                                                        \\n4\\n In the present version of OOLANG if a function is \\ndeclared to be a multimethod prefixing its name with @, \\nall of its parameters of user-defined type will be used \\nduring the selection. In fact the programmer can\\'t decide \\nwhich parameters of a multimethod have to be used.  \\nclass Point { \\n int x,y; \\n virtual bool equal(Point p){ \\n  return (x==p.x)&&(y==p.y); \\n } \\n}; \\n \\nclass ColorPoint: public Point { \\n int color; \\n virtual bool equal(Point p){ \\n  return (x==p.x)&&(y==p.y); \\n } \\n}; \\nFigure 3. Limitations of virtual functions \\nclass Point { \\n int x,y; \\n virtual bool equal(Point p){ \\n  return (x==p.x)&&(y==p.y); \\n } \\n}; \\n \\nclass ColorPoint: public Point { \\n int color; \\n virtual bool equal(ColorPoint p){ \\n  return (x==p.x)&&(y==p.y)&& \\n   (color==p.color); \\n } \\n}; \\n \\nbool func(Point p1, Point p2){ \\n return p1.equal(p2); \\n} \\n \\nint main(){ \\n ColorPoint a; \\n Point b; \\n \\n return func(a,b); \\n} \\nFigure 4. Problems with the specialization of virtual \\nfunctions parameters. The code presented is not \\ngood OOLANG. \\n \\n 4 \\nIn the same manner as virtual functions can be \\ndefined once for every class accepted as invocation \\nobject, multimethods can have a different definition \\nfor each combination of types of their arguments. In \\nthe following every single definition is referred to as \\nspecialization of the multimethod, while the word \\nmultimethod is used to refer to the collection of \\nspecializations without emphasis on a particular one5. \\nAn example of multimethod is reported in Figure 5. It \\nis the @equal() multimethod and has two \\nspecializations: the first accepts two Point \\narguments while the second accepts two \\nColorPoint arguments (including the invocation \\nobject). As the two arguments p1 and p2 in \\nfunc() have run-time types ColorPoint and \\nPoint respectively, the only applicable \\nspecialization of the multimethod is the first one (i.e. \\nPoint::@equal(Point &)). Thus that \\nspecialization will be invoked although the type of \\n                                                        \\n5\\n The built-in types (integer, float, …) are not used during \\nrun-time selection but they are used to select the \\nmultimethod at compile-time. In fact, for example, \\n@m(int,Point) and @m(float,ColorPoint) are not two \\nspecialization of the same multimethod but are two \\ndifferent multimethods with the same name and number of \\nparameters but a different built-in type as first parameter. \\nAt compile-time, during an invocation @m(a,b) the first or \\nthe second multimethod is chosen based on the fact that a \\nhas type int or float. \\nthe invocation object is ColorPoint (it will be \\nconverted to Point). \\nA specialization of a multimethod can be declared \\nand defined inside or outside a class (Figure 6). For \\nexample, the @equal() specialization defined \\ninside the Point class could be equivalently defined \\noutside using @equal(Point &this, Point \\n&p). In fact a multimethod defined inside a class is \\ntreated internally as a multimethod that has the same \\nparameters plus, as first parameter, a reference (&) to \\nthe class type (Point in the example). \\nThe invocation of a multimethod can be done using \\nthe syntax for a function invocation (as in the first \\nclass Point { \\n int x,y; \\n bool @equal(Point &p){ \\n  return (x==px)&&(y==p.y); \\n } \\n}; \\n \\nclass ColorPoint: public Point { \\n int color; \\n}; \\n \\nbool @equal(ColorPoint &p1, \\n  ColorPoint &p2) { \\n return (p1.x==p2.x)&&(p1.y==p2.y)&& \\n  (p1.color==p2.color); \\n} \\n \\nint main() { \\n Point p; \\n ColorPoint cp; \\n \\n @equal(p,cp); \\n return cp.@equal(p); \\n} \\nFigure 6. Examples of multimethod \\nclass A{ \\n int a; \\n}; \\nclass B: public A{ \\n int b; \\n} \\nclass C: public B{ \\n int c; \\n}; \\n \\n \\nFigure 7. Sample class hierarchy and object layout \\nclass Point { \\n int x,y; \\n bool @equal(Point p){ \\n  return (x==p.x)&&(y==p.y); \\n } \\n}; \\n \\nclass ColorPoint: public Point { \\n int color; \\n bool @equal(ColorPoint p){ \\n  return (x==p.x)&&(y==p.y)&& \\n   (color==p.color); \\n } \\n}; \\n \\nbool func(Point p1, Point p2){ \\n return p1.@equal(p2); \\n} \\n \\nint main(){ \\n ColorPoint a; \\n Point b; \\n \\n return func(a,b); \\n} \\nFigure 5. Use of multimethods in OOLANG to solve \\nthe binary method problem. \\n 5 \\ncase in the function main()) or the method \\ninvocation syntax (as in the latter statement of \\nmain()). This second invocation syntax, together \\nwith the possibility of defining specializations of \\nmultimethods outside classes, allows to extend a \\nclass without modifying its source code and to invoke \\nthe new specializations as if they were methods of \\nthe class. For example, in Figure 6, @equal() is \\nadded to the class ColorPoint without modifying \\nits source code and it is then invoked as if it were a \\nmember of the class. \\nFinally, a multimethod specialization is said to be \\nmore specific than another if the type of each \\nparameter of the former is either the same type, or a \\nderived type, of the corresponding parameter of the \\nlatter. \\n3. The OOLANG object model \\nThe object layout in the OOLANG language has been \\nchosen taking into account the time and space \\nefficiency required by the applications for which the \\nlanguage has been developed, as well as the multiple \\ninheritance support. Among the different possibilities \\nfound in literature [13][17][24], the C++ one has \\nbeen chosen because it allows direct access to object \\nfields (i.e. with a single memory access), prevents the \\ncreation of unused space in objects and supports \\nmultiple inheritance. \\nAccording to this object model, fields inherited from \\nparents are located at the beginning of the object \\nlayout while fields relative to the object’s class are at \\nthe end. Thus an object of a derived class is built \\n“recursively” appending the fields of its class to the \\nfields inherited from its base classes which have been \\nset up in the same way. So each group of fields \\nrelated to a base class is organized with the same \\nlayout as in its original class, setting up, in this way, \\na subobject (Figure 7).  \\nIn this model, when there is no multiple inheritance, \\neach subobject starts at the beginning of the host \\nclass A {…}; \\nclass B {…}; \\nclass C {…}; \\nclass D: public A, public B {…}; \\nclass E: public C, public D {…}; \\n \\nFigure 8. Class hierarchy and object layout in \\npresence of multiple inheritance \\nclass A{ \\n int a; \\n}; \\nclass B: virtual A{ \\n int b; \\n}; \\nclass C: virtual A{ \\n int c; \\n}; \\nclass D: public B, \\npublic C{ \\n int d; \\n}; \\n \\n \\n \\nFigure 10. Class hierarchy and object layout in \\npresence of multiple inheritance and virtual parent \\nclass A{ \\n int a; \\n}; \\nclass B: public A{ \\n int b; \\n}; \\nclass C: public A{ \\n int c; \\n}; \\n \\nclass D: public B, public C{ \\n int d; \\n}; \\n \\n \\nFigure 9. Class hierarchy and object layout in \\npresence of multiple inheritance \\n \\n 6 \\nobject. In case of multiple inheritance, on the other \\nhand, there are some subobjects that will not start at \\nthe beginning of the host object (e.g. subobject D in \\nobject E in Figure 8). We show in the following that \\nthis leads to the necessity of realigning the pointers \\ngenerated by the compiler behind the scene in case of \\nsubsumption or multimethod invocation. \\nDue to the object model adopted, some ambiguities \\nmay arise in case of subsumption. In fact, when a \\nclass D (like in Figure 9) inherits from two classes B \\nand C and both these classes inherit from the same \\nclass A (without virtual derivation), an instance of D \\ncontains two subobjects related to the A parents. In \\nfact, a subobject A is contained both in the subobject \\nB and in the subobject C. This leads to an ambiguity \\nwhen trying to subsume an instance of D into an \\ninstance of A. It is important to stress that in the \\nexample above the transitivity of the subtype relation \\nhas been violated. In fact: \\nACCD\\nABBD\\n≤≤\\n≤≤\\n  \\n  \\nand\\nand\\n but ( )AD ≤¬  \\n(where ≤ is used to indicate subtype relation). D is an \\nambiguous subtype of A and an instance of D cannot \\nbe converted to an instance of A. \\nTo avoid the ambiguity in subtype relation it is \\npossible to declare a parent class as virtual (Figure \\n10) as explained in section 2. When a class D inherits \\nfrom other classes B and C that have the same virtual \\nparent A, only one subobject relative to A is included \\nin every instance of D. So an instance of D can be \\nconverted to an instance of A without ambiguity. \\nMoreover, the subobject relative to a virtual parent is \\nlocated at the end of the whole object (Figure 10). \\nOOLANG uses tables to dispatch virtual functions, to \\nmaintain pointers to base virtual classes and to \\nmaintain information needed during the realignment \\nof return types. The first two tables are similar to \\nthose used by C++, so the reader can refer to [20] for \\nfurther information. The third table (RTTABLE) is \\ndescribed below. \\n The RTTABLE for an object o contains: \\n• the id of the type associated to o (needed for \\nmultimethod selection, see section 4) \\n• the size of o (used for the copy of the object on \\nthe secondary stack, see section 5) \\n• the offset of the subobject from the beginning of \\nthe host object (the offset is 0 if o is a complete \\nobject) \\n• the number of parents of the class associated to o  \\n• for each parent, the couple (p_id,p_off) \\nwhere p_id is the parent id and p_off is the \\noffset of the parent subobject from the beginning \\nof the host object (o) \\nThe last two pieces of information are necessary to \\nrealign an object to one of its parents (see section \\n5.3) while the first three are necessary for \\nmultimethod selection and parameter passing. \\n4. Multimethod typechecking and \\nselection \\nDue to the fact that the OOLANG language allows to \\ndeclare multimethod specializations outside class \\ndeclarations (and eventually in different modules), it \\nhas been necessary to conceive a typechecking and \\nselection mechanism divided into two phases. \\nThe first phase takes place during compilation and \\ntakes care of the typecheck of multimethod \\ninvocations. For each invocation, at least an \\napplicable specialization must be granted and a static \\nreturn type is recognized. The impossibility of \\nfinding an applicable specialization is reported as \\nerror and stops the compilation, as described in the \\nnext section. On the other hand, any inconsistency or \\nconflict among specialization declarations detected \\nduring this phase is only reported as a warning, \\nbecause it could be solved by declarations made in \\nother modules. \\nThe second phase takes place at link time, is \\nintegrated in the generation of the compressed \\ndispatch table needed for the runtime multimethod \\nselection and doesn\\'t require access to the source \\ncode of functions or multimethods. This is called pre-\\nlink phase and is needed to ensure that no conflicts or \\ninconsistencies among multimethod specializations \\narise from the integration of different modules. \\nIt is interesting to note that all the source code is \\ntypechecked during the first phase. The only problem \\nthat is checked at link time is the consistency of \\nmultimethods hierarchies, i.e. the absence of \\nanomalies in the selection of the proper specialization \\nand the satisfaction of a constraint on the return types \\nof specializations defined in different modules.  \\nThese checks warrant that no error can occur at run-\\ntime due to multimethod management. \\n \\n4.1 The first phase of multimethod type \\nchecking \\nWhen compiling code containing multimethods, it is \\nnecessary to check each multimethod invocation in \\n 7 \\norder to establish if there exist at least an applicable \\nspecialization and to calculate the static return type of \\nthe invocation.  \\nThree kinds of anomalies can occur during \\ncompilation: \\na) no multimethod specialization is applicable to a \\nparticular multimethod invocation. Obviously it \\nis possible that such a specialization be available \\nin another source file, but if it is not available in \\nthe module under compilation then it is not \\npossible for the compiler to type the multimethod \\ninvocation. It is thus necessary to stop the \\ncompilation process. Of course, a mere \\ndeclaration of the proper multimethod \\nspecialization (without its definition) would be \\nsufficient in order to avoid such compilation \\nerror. \\nb) no most-specific specialization exists. These \\nsituations are not necessarily errors. In fact an \\nerror is reported only if there exists an invocation \\nthat requires to be checked using the conflicting \\nspecializations. In the case of Figure 11.a, for \\nexample, it is not possible to establish the return \\ntype of the multimethod invocation because both \\nthe specializations are applicable and it is not \\npossible to type the invocation. In this case it is \\nnecessary to report an error and stop the \\ncompilation because it is not possible to establish \\nif the call to function f1() is correct. In the case \\nof Figure 11.b, the f2() function can be \\ntypechecked without problems and thus the \\nambiguity problem is only reported as a warning \\nand the compilation is not aborted. In fact the \\nlinking of different modules can create \\nambiguities but can also solve them: supposing \\nthat a module contains a specialization \\n@m(C,C), the ambiguity in Figure 11.b will be \\nremoved at link time. So if ambiguity doesn\\'t \\ninterfere with the compilation process no error is \\nreported and every decision is deferred at link \\ntime. \\nc) the last kind of anomaly can arise because of the \\nambiguity in subtype relation. For example \\nconsidering the class hierarchy of Figure 9 and a \\nmultimethod specialization @m(A,A), an \\ninvocation @m(D,D) will rise the anomaly \\nbecause D is an ambiguous subtype of A. But in \\nthis situation only a warning is reported at \\ncompile-time because the return type of the \\ninvocation @m(D,D) is calculable. This kind of \\nanomaly cannot interfere with the compilation \\nprocess and could eventually be resolved at link-\\ntime due to other declarations present in different \\nmodules.  \\nTherefore, during the first phase of compilation, an \\nerror is reported only if the anomaly interferes with \\nthe compilation process, preventing the calculation of \\nthe static type of a multimethod invocation (i.e. the \\nreturn type of the multimethod that is statically \\nforeseeable). In all the other cases, the anomaly \\ngenerates a warning because it is possible that the \\nunion of more modules removes it. \\nTo warrant soundness of static typing, OOLANG \\nimposes on the return type of multimethods the same \\nconstraint presented in [3]: if a specialization of a \\nmultimethod is more specific than another, the return \\ntype of the former must be a subtype of the return \\ntype of the latter (in order to be accepted in \\nexpressions where the return type of the less specific \\nspecialization was statically expected). Moreover, \\nbecause of ambiguity in the subtype relation, it is \\nnecessary to verify that the return type is not an \\nambiguous subtype of the return type of the less \\nspecific specialization. This check, that is made \\ndifficult by the loss of transitivity in the subtype \\nrelation, will be better analyzed in section 4.3. \\nclass A { … }; \\nclass B { … }; \\nclass C: public A, public B { … }; \\n \\nA @m(A,A); \\nB @m(B,B); \\nint f1(A); \\n \\nint f2(C o1, C o2){ \\n    return f1(@m(o1,o2)); \\n} \\n \\n(a) \\nclass A { … }; \\nclass B { … }; \\nclass C: public A, public B { … }; \\n \\nA @m(A,A); \\nB @m(B,B); \\nint f1(A); \\n \\nint f2(A o1, A o2){ \\n    return f1(@m(o1,o2)); \\n} \\n \\n(b) \\nFigure 11. Influence of anomalies on the determination of the return type of a multimethod \\n 8 \\n \\n4.2. Problems of separate compilation \\nStatic typechecking of arbitrary multimethods is \\nchallenging due to possible interference arising in the \\npresence of multiple modules (i.e. source files). \\nSource files that typecheck when compiled \\nseparately, can create ambiguity when linked \\ntogether. In particular three ambiguous situations \\n(three anomalies) can arise: \\n1) in Figure 12.a each of the two modules \\ntypechecks separately, but when they are linked, \\nthe invocation of @m becomes ambiguous \\nbecause neither of the specialization is more \\nspecific than the other; \\n2) in Figure 12.b the ambiguity arises because of \\nmultiple inheritance. C inherits from A and from \\nB so both the specializations declared in the first \\nmodule are applicable to an invocation with two \\ninstances of C as arguments, but neither is more \\nspecific; \\n3) the last kind of ambiguity arises because of the \\nloss of transitivity of the subtype relation. In \\nFigure 12.c the definition of class D in the second \\nmodule creates an ambiguity because an \\ninvocation of @m with run-time types (D,D) \\nmust select the specialization @m(A,A); but D is \\nan ambiguous subtype of A (an instance of D \\ncontains two subobjects relative to A), so it is not \\nclear which subobject of d1 and d2 to chose as \\nactual parameter for a1 and a2. \\nThe check against the occurrence of any of these \\nanomalies is described in the following. \\n4.3 The link-time checks and the calculation \\nof compressed dispatch tables \\nTo achieve a fast dispatch of multimethods, \\nOOLANG uses compressed dispatch tables. Their \\ncreation and management is discussed in the \\nfollowing after a short introduction to non \\ncompressed tables and to their drawbacks.  \\nA non compressed dispatch table associated with a \\nmultimethod @m is an n-dimensional matrix, where n \\nis the number of parameters used for the selection of \\n@m. Each dimension of the matrix is indexed by the \\nid-s associated to each type (class) in the program. \\nSo, when a multimethod is invoked, the n id-s \\nassociated to the dynamic types of its arguments are \\nobtained and are used to index the dispatch table. The \\ncorresponding entry in the table contains a reference \\nmodule classes.h: \\nclass A {...}; \\nclass B: public A {...}; \\n \\n \\nmodule classes.h: \\nclass A {...}; \\nclass B: public A {...}; \\n \\n \\nmodule classes.h: \\nclass A {...}; \\nclass B: public A {...}; \\nclass C: public A {...}; \\n \\nfirst source file: \\n#include \"classes.h\" \\nint @m(A a,B b){...} \\nint f() { \\n    B b1,b2; \\n    return @m(b1,b2); \\n} \\n \\n \\nfirst source file: \\n#include \"classes.h\" \\nint @m(A a1,A a2){...} \\nint @m(B b1,B b2){...} \\n \\n \\nfirst source file: \\n#include \"classes.h\" \\nint @m(A a1,A a2){...} \\n \\nsecond source file: \\n#include \"classes.h\" \\nint @m(B b,A a){...} \\nint main() { \\n    B b1,b2; \\n    return @m(b1,b2); \\n} \\n \\n \\n \\n(a) \\nsecond source file: \\n#include \"classes.h\" \\nclass C: public A, public B \\n{...} \\n \\nint main() { \\n    C c1,c2; \\n    ... \\n    return @m(c1,c2); \\n} \\n \\n(b) \\nsecond source file: \\n#include \"classes.h\" \\nclass D: public B, public C \\n{...} \\n \\nint main() { \\n    D d1,d2; \\n    ... \\n    return @m(d1,d2); \\n} \\n \\n(c) \\nFigure 12. The three static typechecking difficulties arising in presence of separate compilation \\n 9 \\nto the most specific specialization of the multimethod \\napplicable to the n-ple of types used for the \\ninvocation; this reference is used to invoke the \\nspecialization. \\nThis method of dispatching multimethods is very fast \\nbecause it requires only to access a n-dimensional \\narray, but it has the drawback of being very space \\nconsuming. In practical cases, fortunately, the tables \\nare sparse and this allows calculating compressed \\ndispatch tables. Compressed tables are obtained by \\nOOLANG using a slight variation of the algorithm in \\n[18]. \\nFor each parameter of the multimethod, all the types \\nare analyzed and divided into groups. The reader can \\nrefer to [18] for the criteria used in grouping. Every \\ngroup contains a type that is a supertype of all the \\nother group members; this type is chosen as the \\nrepresentative of the group and is called the pole of \\nthe group. For example the grouping for the \\nmultimethod @m in Figure 13 is the following: \\n• for the first parameter the groups are {B} and \\n{D,E} and the first pole (P1) is B while the \\nsecond (P2) is D; \\n• for the second parameter the groups are {B} and \\n{D,E} and the first pole (P1) is B while the \\nsecond (P2) is D. \\nBecause of loss of transitivity of subtype relation, \\nOOLANG also verifies that no member of the group \\nis an ambiguous subtype of the associated pole. \\nAfter the construction of the groups and the election \\nof the poles it is possible to build the tables. To each \\nmultimethod @m with n arguments are associated n \\nvectors and an n-dimensional matrix (Figure 13). \\nThe i-th vector is related to the i-th formal parameter. \\nThe vectors are indexed with type numbers, i.e. the \\nunique id-s associated to each type in the program \\n(for clarity, however, the type names instead of the \\ntype numbers are reported in Figure 13). Thus each \\nvector has size equal to the number of types in the \\nprogram. The elements in the vectors are the poles. \\nThus each vector associates each of the types usable \\nfor one parameter to the corresponding pole. \\nThe i-th dimension of the matrix has size equal to the \\nnumber of poles associated to the i-th parameter. \\nThus the matrix associates tuples of n poles to the \\nmost specific applicable methods of @m. \\nAt runtime, in an invocation of a multimethod @m, \\nfor each actual parameter i, the i-th vector is accessed \\nin order to find the corresponding pole ip . Then the \\nmatrix is accessed at co-ordinates (p1,…,pn) in \\norder to find the multimethod specialization to call. \\nThis algorithm for table building requires traversing \\nall the types in a program and all the multimethod \\nspecializations. Moreover, during the compressed \\ntable fill-up, the algorithm verifies that no conflicting \\nspecializations are present. So part of the \\nverifications needed at link-time are done for free \\nduring the construction of compressed tables. \\nclass A {…}; \\nclass B {…}; \\nclass C {…}; \\nclass D: public A, \\n    public B {…}; \\nclass E: public C, \\n    public D {…}; \\n \\nint @m(B,B); \\nint @m(D,D); \\n \\ninheritance hierarchy and objects layout: \\n \\nCompressed dispatch structures: \\nfirst vector: \\n \\nsecond vector: \\n \\ncompressed matrix: \\n \\nFigure 13. A sample class hierarchy and multimethods and the related compressed dispatch table \\n 10 \\nThe only thing still to be checked is the constraint on \\nthe return type: if a specialization is more specific \\nthan another the return type of the former must be a \\nsubtype of the return type of the latter. During the \\ncalculation of compressed tables the specializations \\nare compared to establish which is more specific: the \\nreturn type check is performed in this phase. So the \\nadditional cost introduced by this test is very limited \\n(constant) because the order of the specializations \\n(from the more specific to the less specific) is already \\nobtained for other purposes. \\nA problem is introduced by the loss of transitivity of \\nthe subtype relation. It is possible to notice that it is \\nnot sufficient to compare the return type of a \\nspecialization with only that of the nearest less \\nspecific specialization. Consider the hierarchy in \\nFigure 14: @m(D,D) returns D that is a subtype of \\nthe type returned by @m(C,C). The same way, \\n@m(C,C) returns a subtype of the type returned by \\n@m(A,A); so, comparing only couples of neighbour \\nspecializations, it seems that the hierarchy is \\nconsistent. But @m(D,D) can be dynamically \\ninvoked when @m(A,A) is statically expected and \\nthe return type of @m(D,D) is not a subtype of the \\ntype returned by @m(A,A): it is, in fact, an \\nambiguous subtype. So, to establish the consistency \\nof a multimethod hierarchy, it is necessary to \\ncompare the return type of a specialization with the \\nreturn type of all the other applicable ones. The \\nalgorithm for calculating compressed tables allows \\nthis kind of check because it collects all the \\napplicable specializations for every entry of the table \\nand compares them to calculate the most specific. \\nDuring this phase it is also possible to check if the \\nreturn type of the most specific one is a non \\nambiguous subtype of the return types of all the other \\nspecializations. \\nAll the reasons above show how the checks needed at \\nlink-time to warrant the consistency of multimethod \\nhierarchies are partially done by the algorithm for \\ncompressed multimethod dispatch table generation \\nand, for the remaining part, can be integrated into this \\nalgorithm without affecting its efficiency. This means \\nthat the greatest part of the check can be done for free \\nbecause is needed by the compressed dispatch table \\ncomputation algorithm. The complete algorithm for \\ncompressed multimethod table generation and \\nverification is presented in [23]. \\nSo, at link time, only information about the class \\nhierarchy in the program and the multimethods \\nspecializations declared is needed. This information \\nis obtained from the object files. \\n4.4 The interaction of constant arguments \\nwith multimethod dispatch \\nOOLANG supports declaring parameters of functions \\nand multimethods as constant. The presence of \\nconstant parameters interferes with the selection of \\nmultimethods. Consider the example in Figure 15: \\nthe two specializations of the multimethod @m are \\ndifferent in that one accepts only constant instances \\nof A. \\nDuring the selection it is necessary to consider \\nwhether an argument is constant or not. So for every \\ntype, the selection tables contain two entries, one for \\nthe constant version of the type and the other for the \\nnon-constant one. This doubles the number of types \\nused for the dispatch; but due to the use of \\ncompressed tables, only the size of the vectors \\ndoubles while the variation of the matrix size \\ndepends on the structure and the number of \\nspecializations with constant parameters. \\nThe algorithms presented in [18] are usable even if \\nconstant versions of types are considered. In fact, in \\nthe example of Figure 15, four types are considered \\nduring dispatch: the constant and non-constant \\nversions of A and the constant and non-constant \\nversions of B. The constant version is considered as a \\nparent of the non-constant version: in fact a \\nclass A {…}; \\nclass B: public A {…}; \\nclass C: public A {…}; \\nclass D: public B, public C {…}; \\n \\nA @m(A,A); \\nB @m(B,B); \\nC @m(C,C); \\nD @m(D,D); \\nFigure 14. Subtype relation and check of the return \\ntype of multimethods \\nclass A {…}; \\nclass B: public A {…}; \\n \\nint @m(A,B); \\nint @m(const A,B); \\n \\n \\nFigure 15. Example of multimethod with constant \\nparameter and dispatch hierarchy \\n 11 \\nmultimethod that expects a constant instance can \\naccept a non-constant instance. Instead, if a non-\\nconstant instance is expected, it is not possible to \\naccept a constant instance because the body of \\nmultimethod will probably need to modify it. At the \\nsame time the constant version of A is considered as a \\nparent of the constant version of B, because the latter \\nclass is a subtype of the former (Figure 15). \\n5. Invocation of multimethods: parameter \\npassing and realignment  \\n5.1 Parameter passing \\nParameters can be passed to an OOLANG function or \\nmultimethod in two different modes: by value or by \\nreference. Moreover, a function or multimethod can \\ndefine a parameter as constant, meaning that the \\nparameter will not be modified by the code of the \\nfunction. \\nThe passing by reference is internally treated passing \\nonly the pointer to the object, as it happens in many \\nother languages.  \\nWhen a parameter is declared as constant, the \\nOOLANG compiler treats it like a parameter passed \\nby reference for efficiency reasons and according to \\nthe fact that a constant object cannot be modified so \\ncopying would be useless. \\nIn the passing by value it is necessary to perform a \\ncopy of the object on the stack. Because of \\nsubsumption it is not possible to know statically the \\ntype and the size of the objects passed as parameters. \\nIn fact a function accepting a parameter of type A can \\nbe called passing an instance of A or an instance of a \\nclass derived from A (Figure 16). If only the \\nsubobject relative to the expected parameter type \\nwere copied, problems could arise when a virtual \\nfunction or a multimethod is called from inside the \\nstatic function (as explained in [1]). Moreover it is \\nnot possible to know statically the size of the \\nactivation record, because the sizes of the function \\nparameters are not known at compile time. For these \\nreasons OOLANG uses a secondary stack to store the \\ncopy of the parameters and puts in the primary \\n(normal) stack only the pointers to the objects located \\non the secondary stack. As calls are nested, in fact, a \\nstack is the proper data structure to keep copies of the \\nparameters. The inefficiency of the double \\nindirection needed to access objects on secondary \\nstack through the pointers that resides on the primary \\nstack is easily optimized by common subexpression \\nelimination [21]. \\nThe parameter passing is performed as a cooperative \\ntask. The caller pushes the pointers to the objects \\npassed as arguments on the primary stack. The callee, \\nin case of passing by value of non constant \\narguments, performs the copy of the objects on the \\nsecondary stack updating the pointers on the primary \\nstack to point to the copies. This is necessary \\nbecause, in case of multimethod call, the caller \\ndoesn’t know which specialization will be called \\nneither if the specialization will actually need to \\nperform a copy of the parameters (this is not \\nnecessary for example when the selected \\nspecialization of the multimethod requires a constant \\nobject). \\nThe copy of parameters on the secondary stack is \\ndone through copy constructors that are automatically \\ngenerated by the OOLANG compiler. The proper \\ncopy constructor must be selected at runtime because, \\nas explained, it is not possible to know statically the \\ntype of the parameters. In OOLANG the copy \\nconstructors are multimethods but traditional virtual \\nfunctions could be used as well. The same \\nconsiderations are applicable to the destruction of \\ncopies of parameters on exiting the functions. \\n5.2 Realignment of arguments during \\nmultimethod invocation \\nDuring the analysis of the OOLANG object model \\n(Section 3) was noted that in presence of multiple \\ninheritance not all the subobjects relative to the \\nparents start where the whole object starts (Figure 9). \\nFor this reason, when a multimethod is invoked, it is \\nnecessary to realign its arguments, i.e. to modify the \\npointers passed by the caller to align them to the \\nsubobjects statically expected by the multimethod \\nspecialization selected. \\nclass A {...}; \\nclass B: public A {...}; \\n \\nint @m(A a) {...} \\nint @m(B b) {...} \\n \\nint f(A a) { \\n return @m(a); \\n} \\n \\nint main() { \\n B b; \\n \\n return f(b); \\n} \\nFigure 16. Interaction between static functions and \\nmultimethods in presence of passing by value \\n 12 \\nThe realignment process takes place during \\nmultimethod selection using some structures \\ngenerated by the pre-linker. These structures are \\nsimilar to those used for the selection (Figure 17). \\nFor every parameter i of a multimethod @m, besides \\nthe vector used for multimethod selection, there is a \\nsecond vector that contains, for every type, the offset \\nof the subobject relative to the pole associated with \\nthe type. So during the selection one vector is used to \\nobtain the id of the pole associated to the type while \\nthe other is used to calculate the offset of the pole \\nfrom the beginning of the object. \\nThen a second structure is used. It is a matrix with \\nthe same number of components and the same size of \\nthe dispatch matrix. Every entry is an n-tuple of \\noffsets; these offsets have to be applied too in order \\nto obtain a pointer to the subobject expected by the \\nspecialization selected (Figure 17). For example, for \\na multimethod invocation @m(b,e), where b is of B \\ntype and e is of E type, the two pointers pb and pe \\ninternally passed by the caller are modified as \\nfollows: \\npb’ = pb + 0 + 0 \\npe’ = pe + Sc + Sa \\nUsing vectors and tables during multimethod \\nselection it is possible to realign the arguments of the \\ninvocation to the types expected by the specialization \\nselected. Moreover these structures allow realizing \\nthe realignment in a very fast way. \\n5.3 Interaction between multimethods and \\nstatic functions \\nSome problems arise when the object returned by a \\nmultimethod is passed to a static function. In fact, \\noften, the object returned by a multimethod is of a \\nsubtype of the statically expected type, i.e. the type of \\nthe object returned by the specialization that is \\nstatically expected to be invoked. But the static \\nfunction expects the static type of its argument. For \\nexample in Figure 18 the invocation @m(b1,b2) is \\nstatically expected to return an object of B class and \\nan object of this class is expected as parameter of \\nfunction f1(). But, if b1 and b2 have dynamic type \\nC, the specialization invoked returns an object of the \\nC class; it is necessary to realign this object so that \\nthe pointer passed to the f1() function points to the \\nB subobject contained in the C object, a subobject \\nthat does not start at the beginning of the whole \\nobject. \\nThis example shows how it is necessary to realign the \\nobjects dynamically returned by multimethods to the \\nstatically expected type. The RTTABLE described in \\nSection 3 is used to obtain the realignment: first, the \\nnumber of parents is read from the RTTABLE. Then \\nthe list of couples parents-offsets is accessed to \\nsearch the target parent (i.e. the type statically \\nexpected) and the offset (p_off) is read. Finally the \\noffset is added to the host object address to compute \\nthe address of the subobject relative to the expected \\nparent. \\nThe realignment is necessary wherever the object \\nreturned by a dynamically selected function \\nFirst realignment vector: \\n \\nSecond realignment vector: \\n \\nSc = Size of the C subobject \\nRealignment matrix: \\n \\n \\nSa = Size of the A subobject \\nFigure 17. Realignment structures relative to the classes and multimethod in figure 12 \\nclass A {…}; \\nclass B {…}; \\nclass C: public A, public B {…}; \\n \\nB @m(B,B); \\nC @m(C,C); \\n \\nint f1(B); \\n \\nint f2(B b1, B b2) { \\n    return f1(@m(b1,b2)); \\n} \\nFigure 18. Realignment of the return type \\n 13 \\n(multimethod or virtual function as in [22]) is passed \\nto a static function; the object has to be realigned to \\nthe statically expected return type. \\n6. Related work \\nMost of the existing languages that support \\nmultimethods are object-based languages. For this \\nreason, they don’t face the problems found by \\nOOLANG due to the ambiguity of the subtype \\nrelation and to the necessity of object realignment. \\nAmong class-based languages there are CLOS, \\nPolyglot and a variation of Java. \\nCLOS [16] has been the first language supporting \\nmultimethods. It allows the selection of generic \\nfunctions based on the type of multiple parameters. \\nThe main feature of OOLANG with respect to CLOS \\nis the ability to perform static type checking. \\nPolyglot compiles in two phases and checks the \\nreturn type constraint satisfaction [3] to obtain static \\ntypechecking. However it does not support separate \\ncompilation and differs mainly from OOLANG under \\nthe aspects of the object model and the absence of a \\npre-linking phase. \\nParasitic multimethods [6] are a variation of \\nencapsulated multimethods [8] designed for the Java \\nprogramming language. They allow separate \\ncompilation without link-time checking but loss the \\nsymmetry of dispatching because they privilege a \\nreceiver argument. Moreover a textual ordering is \\nused to avoid conflict between multimethod \\nspecializations. \\nThere are however some object based languages that \\nare interesting for their capabilities related to static \\ntypechecking and separate compilation. \\nCecil [11] is dynamic, and a static typing can be \\nobtained using particular constructs. In [10] some \\nideas are described about modular typechecking of \\nmultimethods. Cecil does not perform separate \\ncompilation, but implements a development \\nenvironment that keeps the relations among the \\ndifferent parts of the program allowing selective \\nrecompilation [9]. Cecil performs heavy \\noptimizations that allow to reduce the costs of \\nmultimethods selection thanks to its object oriented \\noptimizer [15].  \\nDubious [12] allows separate static typing of \\nmultimethods. Three type systems are presented \\nallowing different balance between expressiveness \\nand separate compilation. The first type system \\nimposes some limitations on inheritance and \\nmultimethod parameters allowing totally modular \\ntypechecking; the other type systems relax some \\nconstraints but require simple link-time regional \\nchecking. \\n7. Conclusions and future work \\nOOLANG presents the integration of multimethods \\nin an object model similar to the C++ one and allows \\nseparate compilation requiring only a link-time check \\nthat can be integrated in the algorithm for \\ncompressed dispatch table computation, obtaining it \\nefficiently. Moreover, OOLANG addresses the \\nproblems of ambiguity and realignment arisen \\nbecause of the object model adopted. \\nNow OOLANG is in active development and new \\nfeatures are planned: \\n• the separation of the subtype relation from the \\nsubclass one [5][14] \\n• the realization of a visual development \\nenvironment \\n• the development of an object oriented optimizer \\nReferences \\n[1] Abadi, M. and Cardelli, L. 1996. A theory of \\nobjects. Springer-Verlag. \\n[2] Aglietti F. et al., 1998. The Teraflop \\nSupercomputer APEmille: Architecture, \\nSoftware and Project Status Report. Computer \\nPhysics Communications, 110, 216-219. \\n[3] Agrawal, R., DeMichiel, L.G. and Lindsay, \\nB.G., 1991. Static Type Checking of Multi-\\nMethods. In Proceedings of OOPSLA ’91, 113-\\n128. \\n[4] Bartoloni, A. et al., 1998. Progress and status of \\nAPEmille. Nuclear Physics B, Proc. Suppl. 63A-\\nC, 991-993. \\n[5] Baumgarten, G. and Russo V.F., 1996. \\nImplementing signatures for C++. ACM \\nTransactions on Programming Languages and \\nSystems 19 (1), 153-187. \\n[6] Boyland, J. And Castagna, G., 1997. Parasitic \\nMethods: An Implementation of Multi-Methods \\nfor Java. In Proceedings of OOPSLA \\'97. \\n[7] Bruce, K., Cardelli, L., Castagna, G., The \\nHopkins Objects Group, Leavens, G. T., Pierce \\nB., 1995. On Binary Methods. Technical Report \\n#95-08, Department of Computer Science, Iowa \\nState University. \\n[8] Castagna, G., 1995. Covariance and \\nContravariance: Conflict without a Cause. \\nACM Transactions on Programming Languages \\nand Systems 17(3); 431-447. \\n 14 \\n[9] Chambers, C., Dean, J. And Grove, D., 1995. A \\nFramework for Selective Recompilation in the \\nPresence of Complex Intermodule \\nDependencies. In Proceeding of 17th \\nInternational Conference on Software \\nEngineering (ICSE’17). \\n[10] Chambers, C. and Leavens G.T., 1995. \\nTypechecking and Modules for Multimethods. \\nACM Transaction on Programming Languages \\nand Systems 17(6), 805-843. \\n[11] Chambers, C. 1997. The Cecil Language \\nSpecification and Rationale. Department of \\nComputer Science and Engineering, University \\nof Washington. \\n[12] Chambers, C. and Millstein, T., 1999. Modular \\nStatically Typed Multimethods. In Proceedings \\nof Sixth International Workshop on Foundations \\nof Object-Oriented Languages. \\n[13] Connor, R.C., Dearle, A., Morrison, R. and \\nBrown, A.L., 1989. An object addressing \\nmechanism for statically typed languages with \\nmultiple inheritance. SIGPLAN Notices 24(10), \\n279-285. \\n[14] Cook, W.R., Hill, W.L. and Canning, P.S., \\n1990. Inheritance is not subtyping. In \\nProceedings of 17th ACM Symposium on \\nPrinciples of Programming Languages, 125-\\n135. \\n[15] Dean, J., DeFouw G., Grove D., Litvinov V. \\nand Chambers C., 1996. Vortex: An Optimizing \\nCompiler for Object-Oriented Languages. In \\nProceeding of OOPSLA ’96. \\n[16] DeMichiel L.G., Gabriel R.P., 1987. Common \\nLisp Object System overview. In Proceedings of \\nthe European Conference on Object-Oriented \\nProgramming. Lecture Notes in Computer \\nScience 276, 151-170. Springer-Verlag. \\n[17] Dixon, R., McKee, T., Schweizer, P. and \\nVaughan, M., 1989. A fast method dispatcher \\nfor compiled languages with multiple \\ninheritance. In Proceedings of OOPSLA ’89, \\n211-214. \\n[18] Dujardin, E., Amiel, E. and Simon, E., 1998. \\nFast Algorithms for Compressed Multimethod \\nDispatch Table Generation. ACM Transactions \\non Programming Languages and Systems 20(1), \\n116-165. \\n[19] Ingalls, D.H.H., 1986. A simple technique for \\nhandling multiple polymorphism. In proceeding \\nof OOPSLA ’86, 347-349. \\n[20] Lippman, S.B., 1996. Inside the C++ Object \\nModel. Addison Wesley. \\n[21] Morel, E. and Renvoise, C., 1981. \\nInterprocedural Elimination of Partial \\nRedundancies. In Program Flow Analysis: \\nTheory and Applications, Muchnick, S.S. and \\nJones N.D., editors, 160-188. \\n[22] Panizzi, E. and Pastorelli, B., On the return \\ntypes of virtual functions. Submitted to ACM \\nSIGPLAN Notices. \\n[23] Pastorelli, B., 1998. OOLANG: un linguaggio \\nad oggetti con supporto dei multi-metodi, typing \\nstatico e compilazione separata. Tesi di laurea, \\nUniversità degli Studi dell’Aquila. \\n[24] Rose, J.R., 1988. Fast dispatch mechanisms for \\nstock hardware. In Proceeding of OOPSLA ’88, \\n27-35. \\n[25] Stroustrup, B. 1997. The C++ Programming \\nLanguage. 3rd ed. Addison-Wesley. \\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd1b'), 'authors': 'C. Demetrescu, D. Peleg, D. Peleg, D. Peleg, J. Hershberger, L. Roditty, M. Thorup, S. Baswana, S. Chechik, T. Lukovszki', 'year': '2014', 'title': 'Sparse Fault-Tolerant BFS Trees', 'full_text': \"Sparse Fault-Tolerant BFS Trees\\nMerav Parter ∗† David Peleg ∗\\nOctober 18, 2018\\nAbstract\\nA fault-tolerant structure for a network is required to continue functioning fol-\\nlowing the failure of some of the network’s edges or vertices. This paper considers\\nbreadth-first search (BFS) spanning trees, and addresses the problem of designing\\na sparse fault-tolerant BFS tree, or FT-BFS tree for short, namely, a sparse sub-\\ngraph T of the given network G such that subsequent to the failure of a single edge\\nor vertex, the surviving part T ′ of T still contains a BFS spanning tree for (the\\nsurviving part of) G. For a source node s, a target node t and an edge e ∈ G,\\nthe shortest s− t path Ps,t,e that does not go through e is known as a replacement\\npath. Thus, our FT-BFS tree contains the collection of all replacement paths Ps,t,e\\nfor every t ∈ V (G) and every failed edge e ∈ E(G).\\nOur main results are as follows. We present an algorithm that for every n-vertex\\ngraph G and source node s constructs a (single edge failure) FT-BFS tree rooted at\\ns with O(n · min{Depth(s),√n}) edges, where Depth(s) is the depth of the BFS\\ntree rooted at s. This result is complemented by a matching lower bound, showing\\nthat there exist n-vertex graphs with a source node s for which any edge (or vertex)\\nFT-BFS tree rooted at s has Ω(n3/2) edges.\\nWe then consider fault-tolerant multi-source BFS trees, or FT-MBFS trees for\\nshort, aiming to provide (following a failure) a BFS tree rooted at each source s ∈ S\\nfor some subset of sources S ⊆ V . Again, tight bounds are provided, showing that\\nthere exists a poly-time algorithm that for every n-vertex graph and source set\\nS ⊆ V of size σ constructs a (single failure) FT-MBFS tree T ∗(S) from each source\\nsi ∈ S, with O(\\n√\\nσ · n3/2) edges, and on the other hand there exist n-vertex graphs\\nwith source sets S ⊆ V of cardinality σ, on which any FT-MBFS tree from S has\\nΩ(\\n√\\nσ · n3/2) edges.\\n∗The Weizmann Institute of Science, Rehovot, Israel. Email: {merav.parter,david.peleg}@\\nweizmann.ac.il. Supported in part by the Israel Science Foundation (grant 894/09), the United States-\\nIsrael Binational Science Foundation (grant 2008348), the Israel Ministry of Science and Technology\\n(infrastructures grant), and the Citi Foundation.\\n†Recipient of the Google Europe Fellowship in distributed computing; research supported in part by\\nthis Google Fellowship.\\n1\\nar\\nX\\niv\\n:1\\n30\\n2.\\n54\\n01\\nv1\\n  [\\ncs\\n.D\\nS]\\n  2\\n1 F\\neb\\n 20\\n13\\nFinally, we propose anO(log n) approximation algorithm for constructing FT-BFS\\nand FT-MBFS structures. The latter is complemented by a hardness result stating\\nthat there exists no Ω(log n) approximation algorithm for these problems under\\nstandard complexity assumptions. In comparison with the randomized FT-BFS con-\\nstruction implicit in [14], our algorithm is deterministic and may improve the num-\\nber of edges by a factor of up to\\n√\\nn for some instances. All our algorithms can be\\nextended to deal with one vertex failure as well, with the same performance.\\n1 Introduction\\nBackground and motivation Modern day communication networks support a variety\\nof logical structures and services, and depend on their undisrupted operation. As the\\nvertices and edges of the network may occasionally fail or malfunction, it is desirable\\nto make those structures robust against failures. Indeed, the problem of designing fault-\\ntolerant constructions for various network structures and services has received considerable\\nattention over the years.\\nFault-resilience can be introduced into the network in several different ways. This\\npaper focuses on a notion of fault-tolerance whereby the structure at hand is augmented\\nor “reinforced” (by adding to it various components) so that subsequent to the failure\\nof some of the network’s vertices or edges, the surviving part of the structure is still\\noperational. As this reinforcement carries certain costs, it is desirable to minimize the\\nnumber of added components.\\nTo illustrate this type of fault tolerance, let us consider the structure of graph k-\\nspanners (cf. [17, 19, 20]). A graph spanner H can be thought of as a skeleton structure\\nthat generalizes the concept of spanning trees and allows us to faithfully represent the\\nunderlying network using few edges, in the sense that for any two vertices of the network,\\nthe distance in the spanner is stretched by only a small factor. More formally, consider\\na weighted graph G and let k ≥ 1 be an integer. Let dist(u, v,G) denote the (weighted)\\ndistance between u and v in G. Then a k-spanner H satisfies that dist(u, v,H) ≤ k ·\\ndist(u, v,G) for every u, v ∈ V .\\nTowards introducing fault tolerance, we say that a subgraph H is an f -edge fault-\\ntolerant k-spanner of G if dist(u, v,H \\\\ F ) ≤ k · dist(u, v,G \\\\ F ) for any set F ⊆ E of\\nsize at most f , and any pair of vertices u, v ∈ V . A similar definition applies to f -vertex\\nfault-tolerant k-spanners. Sparse fault-tolerant spanner constructions were presented in\\n[6, 11].\\nThis paper considers breadth-first search (BFS) spanning trees, and addresses the\\nproblem of designing fault-tolerant BFS trees, or FT-BFS trees for short. By this we mean\\na subgraph T of the given network G, such that subsequent to the failure of some of the\\n2\\nvertices or edges, the surviving part T ′ of T still contains a BFS spanning tree for the\\nsurviving part of G. We also consider a generalized structure referred to as a fault-tolerant\\nmulti-source BFS tree, or FT-MBFS tree for short, aiming to provide a BFS tree rooted at\\neach source s ∈ S for some subset of sources S ⊆ V .\\nThe notion of FT-BFS trees is closely related to the problem of constructing replacement\\npaths and in particular to its single source variant, the single-source replacement paths\\nproblem, studied in [14]. That problem requires to compute the collection Ps of all s− t\\nreplacement paths Ps,t,e for every t ∈ V and every failed edge e that appears on the s− t\\nshortest-path in G. The vast literature on replacement paths (cf. [4, 14, 23, 25, 28]) focuses\\non time-efficient computation of the these paths as well as their efficient maintenance in\\ndata structures (a.k.a distance oracles). In contrast, the main concern in the current\\npaper is with optimizing the size of the resulting fault tolerant structure that contains\\nthe collection Ps of all replacement paths given a source node s. A typical motivation\\nfor such a setting is where the graph edges represent the channels of a communication\\nnetwork, and the system designer would like to purchase or lease a minimal collection of\\nchannels (i.e., a subgraph G′ ⊆ G) that maintains its functionality as a “BFS tree” with\\nrespect to the source s upon any single edge or vertex failure in G. In such a context, the\\ncost of computation at the preprocessing stage may often be negligible compared to the\\npurchasing/leasing cost of the resulting structure. Hence, our key cost measure in this\\npaper is the size of the fault tolerant structure, and our main goal is to achieve sparse (or\\ncompact) structures.\\nMost previous work on sparse / compact fault-tolerant structures and services con-\\ncerned structures that are distance-preserving (i.e., dealing with distances, shortest paths\\nor shortest routes), global (i.e., centered on “all-pairs” variants), and approximate (i.e.,\\nsettling for near optimal distances), such as spanners, distance oracles and compact rout-\\ning schemes. The problem considered here, namely, the construction of FT-BFS trees,\\nstill concerns a distance preserving structure. However, it deviates from tradition with\\nrespect to the two other features, namely, it concerns a “single source” variant, and it\\ninsists on exact shortest paths. Hence our problem is on the one hand easier, yet on the\\nother hand harder, than previously studied ones. Noting that in previous studies, the\\n“cost” of adding fault-tolerance (in the relevant complexity measure) was often low (e.g.,\\nmerely polylogarithmic in the graph size n), one might be tempted to conjecture that\\na similar phenomenon may reveal itself in our problem as well. Perhaps surprisingly, it\\nturns out that our insistence on exact distances plays a dominant role and makes the\\nproblem significantly harder, outweighing our willingness to settle for a “single source”\\nsolution.\\nContributions We obtain the following results. In Sec. 2, we define the Minimum\\nFT-BFS and Minimum FT-MBFS problems, aiming at finding the minimum such structures\\n3\\ntolerant against a single edge or vertex fault. Section 3 presents lower bound constructions\\nfor these problems. For the single source case, in Subsec. 3.1, we present a lower bound\\nstating that for every n there exists an n-vertex graph and a source node s ⊆ V for which\\nany FT-MBFS tree from s requires Ω(n3/2) edges. In Subsec. 3.2, we then show that there\\nexist n-vertex graphs with source sets S ⊆ V of size σ, on which any FT-MBFS tree from\\nthe source set S has Ω(\\n√\\nσ · n3/2) edges.\\nThese results are complemented by matching upper bounds. In Subsec. 4.1, we\\npresent a simple algorithm that for every n-vertex graph G and source node s, constructs\\na (single edge failure) FT-BFS tree rooted at s with O(n · min{Depth(s),√n}) edges. A\\nsimilar algorithm yields an FT-BFS tree tolerant to one vertex failure, with the same size\\nbound. In addition, for the multi source case, in Subsec. 4.2, we show that there exists\\na polynomial time algorithm that for every n-vertex graph and source set S ⊆ V of size\\n|S| = σ constructs a (single failure) FT-MBFS tree T ∗(S) from each source si ∈ S, with\\nO(\\n√\\nσ · n3/2) edges.\\nIn Sec. 5, we show that the minimum FT-BFS problem is NP-hard and moreover,\\ncannot be approximated (under standard complexity assumptions) to within a factor of\\nΩ(log n), where n is the number of vertices of the input graph G. Note that while the\\nalgorithms of Sec. 4 match the worst-case lower bounds, they might still be far from\\noptimal for certain instances, as illustrated in Sec. 6. Consequently, in Sec. 6, we\\ncomplete the upper bound analysis by presenting an O(log n) approximation algorithm\\nfor the Minimum FT-MBFS problem. This approximation algorithm is superior in instances\\nwhere the graph enjoys a sparse FT-MBFS tree, hence paying O(n3/2) edges (as does the\\nalgorithm of Sec. 4) is wasteful. In light of the hardness result for these problems (of Sec.\\n5), the approximability result is tight (up to constants).\\nRelated work To the best of our knowledge, this paper is the first to study the spar-\\nsity of fault-tolerant BFS structures for graphs. The question of whether it is possible\\nto construct a sparse fault tolerant spanner for an arbitrary undirected weighted graph,\\nraised in [9], was answered in the affirmative in [6], presenting algorithms for construct-\\ning an f -vertex fault tolerant (2k − 1)-spanner of size O(f 2kf+1 · n1+1/k log1−1/k n) and\\nan f -edge fault tolerant 2k − 1 spanner of size O(f · n1+1/k) for a graph of size n. A\\nrandomized construction attaining an improved tradeoff for vertex fault-tolerant span-\\nners was shortly afterwards presented in [11], yielding (with high probability) for every\\ngraph G = (V,E), odd integer s and integer f , an f -vertex fault-tolerant s-spanner\\nwith O\\n(\\nf 2−\\n2\\ns+1n1+\\n2\\ns+1 log n\\n)\\nedges. This should be contrasted with the best stretch-size\\ntradeoff currently known for non-fault-tolerant spanners [24], namely, 2k− 1 stretch with\\nO˜(n1+1/k) edges.\\nAn efficient algorithm that given a set V of n points in d-dimensional Euclidean space\\nconstructs an f -vertex fault tolerant geometric (1 + \\x0f)-spanner for V , namely, a sparse\\n4\\ngraph H satisfying that dist(u, v,H \\\\ F ) ≤ (1 + \\x0f)dist(u, v,G) for any set F ⊆ V of\\nsize f and any pair of points u, v ∈ V \\\\ F , was presented in [15]. A fault tolerant\\ngeometric spanner of improved size was later presented in [16]; finally, a fault tolerant\\ngeometric spanner of optimal maximum degree and total weight was presented in [9]. The\\ndistinction between the stronger type of fault-tolerance obtained for geometric graphs\\n(termed rigid fault-tolerance) and the more flexible type required for handling general\\ngraphs (termed competitive fault-tolerance) is elaborated upon in [18].\\nA related network service is the distance oracle [3, 22, 25], which is a succinct data\\nstructure capable of supporting efficient responses to distance queries on a weighted graph\\nG. A distance query (s, t) requires finding, for a given pair of vertices s and t in V , the\\ndistance (namely, the length of the shortest path) between u and v in G. The query\\nprotocol of an oracle S correctly answers distance queries on G. In a fault tolerant distance\\noracle, the query may include also a set F of failed edges or vertices (or both), and the\\noracle S must return, in response to a query (s, t, F ), the distance between s and t in\\nG′ = G \\\\ F . Such a structure is sometimes called an F -sensitivity distance oracle. The\\nfocus is on both fast preprocessing time, fast query time and low space. It has been\\nshown in [10] that given a directed weighted graph G of size n, it is possible to construct\\nin time O˜(mn2) a 1-sensitivity fault tolerant distance oracle of size O(n2 log n) capable of\\nanswering distance queries in O(1) time in the presence of a single failed edge or vertex.\\nThe preprocessing time was recently improved to O˜(mn), with unchanged size and query\\ntime [4]. A 2-sensitivity fault tolerant distance oracle of size O(n2 log3 n), capable of\\nanswering 2-sensitivity queries in O(log n) time, was presented in [12].\\nRecently, distance sensitivity oracles have been considered for weighted and directed\\ngraphs in the single source setting [14]. Specifically, Grandoni and Williams considered\\nthe problem of single-source replacement paths where one aims to compute the collection\\nof all replacement paths for a given source node s, and proposed an efficient random-\\nized algorithm that does so in O˜(APSP (n,M)) where APSP (n,M) is the time required\\nto compute all-pairs-shortest-paths in a weighted graph with integer weights [−M,M ].\\nInterestingly, although their algorithm does not aim explicitly at minimizing the total\\nnumber of edges used by the resulting collection of replacement paths, one can show that\\nthe resulting construction yields a rather sparse path collection, with at most O(n3/2 log n)\\nedges (although it may also be far from optimal in some instances).\\nLabel-based fault-tolerant distance oracles for graphs of bounded clique-width are\\npresented in [8]. The structure is composed of a label L(v) assigned to each vertex v, and\\nhandles queries of the form (L(s), L(t), F ) for a set of failures F . For an n-vertex graph\\nof tree-width or clique-width k, the constructed labels are of size O(k2 log2 n).\\nA relaxed variant of distance oracles, in which distance queries are answered by ap-\\nproximate distance estimates instead of exact ones, was introduced in [25], where it was\\nshown how to construct, for a given weighted undirected n-vertex graph G, an approxi-\\n5\\nmate distance oracle of size O(n1+1/k) capable of answering distance queries in O(k) time,\\nwhere the stretch (multiplicative approximation factor) of the returned distances is at\\nmost 2k − 1.\\nAn f -sensitivity approximate distance oracle S was presented in [5]. For an integer\\nparameter k ≥ 1, the size of S is O(kn1+ 8(f+1)k+2(f+1) log (nW )), where W is the weight of the\\nheaviest edge in G, the stretch of the returned distance is 2k − 1, and the query time is\\nO(|F | · log2 n · log log n · log log d), where d is the distance between s and t in G \\\\ F .\\nA fault-tolerant label-based (1+\\x0f)-approximate distance oracle for the family of graphs\\nwith doubling dimension bounded by α is presented in [2]. For an n-vertex graph G(V,E)\\nin this family, and for desired precision parameter \\x0f > 0, the distance oracle constructs and\\nstores an O(log n/\\x0f2α)-bit label at each vertex. Given the labels of two end-vertices s and\\nt and of collections FV and FE of failed (or “forbidden”) vertices and edges, the oracle\\ncomputes, in time polynomial in the length of the labels, an estimate for the distance\\nbetween s and t in the surviving graph G(V \\\\ FV , E \\\\ FE), which approximates the true\\ndistance by a factor of 1 + \\x0f.\\nOur final example concerns fault tolerant routing schemes. A fault-tolerant routing\\nprotocol is a distributed algorithm that, for any set of failed edges F , enables any source\\nvertex sˆ to route a message to any destination vertex dˆ along a shortest or near-shortest\\npath in the surviving network G \\\\ F in an efficient manner (and without knowing F in\\nadvance).\\nIn addition to route efficiency, it is often desirable to optimize also the amount of\\nmemory stored in the routing tables of the vertices, possibly at the cost of lower route\\nefficiency, giving rise to the problem of designing compact routing schemes (cf. [1, 7, 17,\\n21, 24]).\\nLabel-based fault-tolerant routing schemes for graphs of bounded clique-width are\\npresented in [8]. To route from s to t, the source needs to specify the labels L(s) and\\nL(t) and the set of failures F , and the scheme efficiently calculates the shortest path\\nbetween s and t that avoids F . For an n-vertex graph of tree-width or clique-width k,\\nthe constructed labels are of size O(k2 log2 n).\\nFault-tolerant compact routing schemes are considered in [5], for up to two edge fail-\\nures. Given a message M destined to t at a source vertex s, in the presence of a failed edge\\nset F of size |F | ≤ 2 (unknown to s), the scheme presented therein routes M from s to t in\\na distributed manner, over a path of length at most O(k) times the length of the optimal\\npath (avoiding F ). The total amount of information stored in vertices of G on average is\\nbounded by O(kn1+1/k). This should be compared with the best memory-stretch tradeoff\\ncurrently known for non-fault-tolerant compact routing [24], namely, 2k − 1 stretch with\\nO˜(n1+1/k) memory per vertex.\\nA compact routing scheme capable of handling multiple edge failures is presented\\n6\\nin [7]. The scheme routes messages (provided their source s and destination t are still\\nconnected in the surviving graph G \\\\ F ) over a path whose length is proportional to the\\ndistance between s and t in G \\\\F , to |F |3 and to some poly-log factor. The routing table\\nrequired at a node v is of size proportional to v’s degree and some poly-log factor.\\nA routing scheme with stretch 1 + \\x0f for graphs of bounded doubling dimension is also\\npresented in [2]. The scheme can be generalized also to the family of weighted graphs of\\nbounded doubling dimension and bounded degree. In this case, the label size will also\\ndepend linearly on the maximum vertex degree ∆, and this is shown to be necessary.\\n2 Preliminaries\\nNotation Given a graph G = (V,E) and a source node s, let T0(s) ⊆ G be a shortest\\npaths (or BFS) tree rooted at s. For a source node set S ⊆ V , let T0(S) =\\n⋃\\ns∈S T0(s) be\\na union of the single source BFS trees. Let pi(s, v, T ) be the s − v shortest-path in tree\\nT , when the tree T = T0(s), we may omit it and simply write pi(s, v). Let Γ(v,G) be\\nthe set of v neighbors in G. Let E(v,G) = {(u, v) ∈ E(G)} be the set of edges incident\\nto v in the graph G and let deg(v,G) = |E(v,G)| denote the degree of node v in G.\\nWhen the graph G is clear from the context, we may omit it and simply write deg(v).\\nLet depth(s, v) = dist(s, v,G) denote the depth of v in the BFS tree T0(s). When the\\nsource node s is clear from the context, we may omit it and simply write depth(v). Let\\nDepth(s) = maxu∈V {depth(s, u)} be the depth of T0(s). For a subgraph G′ = (V ′, E ′) ⊆ G\\n(where V ′ ⊆ V and E ′ ⊆ E) and a pair of nodes u, v ∈ V , let dist(u, v,G′) denote the\\nshortest-path distance in edges between u and v in G′. For a path P = [v1, . . . , vk], let\\nLastE(P ) be the last edge of path P . Let |P | denote the length of the path and P [vi, vj]\\nbe the subpath of P from vi to vj. For paths P1 and P2, P1 ◦P2 denote the path obtained\\nby concatenating P2 to P1. Assuming an edge weight function W : E(G) → R+, let\\nSP (s, vi, G,W ) be the set of s− vi shortest-paths in G according to the edge weights of\\nW . Throughout, the edges of these paths are considered to be directed away from the\\nsource node s. Given an s− v path P and an edge e = (x, y) ∈ P , let dist(s, e, P ) be the\\ndistance (in edges) between s and e on P . In addition, for an edge e = (x, y) ∈ T0(s),\\ndefine dist(s, e) = i if depth(x) = i− 1 and depth(y) = i.\\nDefinition 2.1 A graph T ∗ is an edge (resp., vertex) FT-BFS tree for G with respect to\\na source node s ∈ V , iff for every edge f ∈ E(G) (resp., vertex f ∈ V ) and for every\\nv ∈ V , dist(s, v, T ∗ \\\\ {f}) = dist(s, v,G \\\\ {f}).\\nA graph T ∗ is an edge (resp., vertex) FT-MBFS tree for G with respect to source set\\nS ⊆ V , iff for every edge f ∈ E(G) (resp., vertex f ∈ V ) and for every s ∈ S and v ∈ V ,\\ndist(s, v, T ∗ \\\\ {f}) = dist(s, v,G \\\\ {f}).\\n7\\nTo avoid cumbersome notation, we refer to edge FT-BFS (resp., edge FT-MBFS) trees\\nsimply by FT-BFS (resp., FT-MBFS) trees. Throughout, we focus on edge fault, yet the\\nentire analysis extends trivially to the case of vertex fault as well.\\nThe Minimum FT-BFS problem Denote the set of solutions for the instance (G, s) by\\nT (s,G) = {T̂ ⊆ G | T̂ is an FT-BFS tree w.r.t. s}. Let Cost∗(s,G) = min{|E(T̂ )| | T̂ ∈\\nT (s,G)} be the minimum number of edges in any FT-BFS subgraph of G. These definitions\\nnaturally extend to the multi-source case where we are given a source set S ⊆ V of size\\nσ. Then\\nT (S,G) = {T̂ ⊆ G | T̂ is a FT-MBFS with respect to S}\\nand\\nCost∗(S,G) = min{|E(T̂ )| | T̂ ∈ T (S,G)}.\\nIn the Minimum FT-BFS problem we are given a graph G and a source node s and\\nthe goal is to compute an FT-BFS T̂ ∈ T (s,G) of minimum size, i.e., such that |E(T̂ )| =\\nCost∗(s,G). Similarly, in the Minimum FT-MBFS problem we are given a graph G and a\\nsource node set S and the goal is to compute an FT-MBFS T̂ ∈ T (S,G) of minimum size\\ni.e., such that |E(T̂ )| = Cost∗(S,G).\\n3 Lower Bounds\\nIn this section we establish lower bounds on the size of the FT-BFS and FT-MBFS structures.\\nIn Subsec. 3.1 we consider the single source case and in Subsec. 3.2 we consider the case\\nof multiple sources.\\n3.1 Single Source\\nWe begin with a lower bound for the case of a single source.\\nTheorem 3.1 There exists an n-vertex graph G(V,E) and a source node s ∈ V such that\\nany FT-BFS tree rooted at s has Ω(n3/2) edges, i.e., Cost∗(s,G) = Ω(n3/2).\\nProof: Let us first describe the structure of the graph G = (V,E). Set d = b√n/2c.\\nThe graph consists of four main components. The first is a path pi = [s = v1, . . . , vd+1 =\\nv∗] of length d. The second component consists of a node set Z = {z1, . . . , zd} and a collec-\\ntion of d disjoint paths of deceasing length, P1, . . . , Pd, where Pj = [vj = p\\nj\\n1, . . . , zj = p\\nj\\ntj ]\\nconnects vj with zj and its length is tj = |Pj| = 6 + 2(d − j), for every j ∈ 1, · · · , d.\\nAltogether, the set of nodes in these paths, Q =\\n⋃d\\nj=1 V (Pj), is of size |Q| = d2 + 7d. The\\n8\\nthird component is a set of nodes X of size n − (d2 + 7d), all connected to the terminal\\nnode v∗. The last component is a complete bipartite graph B = (X,Z, Eˆ) connecting X\\nto Z. Overall, V = X ∪Q and E = Eˆ ∪ E(pi) ∪⋃dj=1E(Pj). Note that n/4 ≤ |Q| ≤ n/2\\nfor sufficiently large n. Consequently, |X| = n− |Q| ≥ n/2, and |Eˆ| = |Q| · |X| ≥ n3/2/4.\\nX \\nZ \\nxi \\nzj \\n𝑑 = 𝑂( 𝑛) \\nej \\nvj \\nPj \\nv* \\nB \\nS \\nz1 \\nvd \\nzd \\nFigure 1: Lower bound construction for FT-BFST˙he original BFS tree consists of the non-\\ndashed edges. The dashed edges are the ones necessary to make it an FT-BFS tree. For\\nexample, the bold dashed edge (xj, zi) is required upon failure of the edge ei.\\nA BFS tree T0 rooted at s for this G (illustrated by the solid edges in Fig. 1) is given\\nby\\nE(T0) = {(xi, zi) | i ∈ {1, . . . , d}} ∪\\nd⋃\\nj=1\\nE(Pj) \\\\ {(pj`j , pj`j−1)},\\nwhere `j = tj − (d − j) for every j ∈ {1, . . . , d}. We now show that every FT-BFS tree\\nT ′ ∈ T (s,G) must contain all the edges of B, namely, the edges ei,j = (xi, zj) for every\\ni ∈ {1, . . . , |X|} and j ∈ {1, . . . , d} (the dashed edges in Figure 1). Assume, towards\\ncontradiction, that there exists a T ′ ∈ T (s,G) that does not contain ei,j (the bold dashed\\nedge (xi, zj) in the figure). (the bold dashed edge (xi, zj) in Figure 1). Note that upon the\\nfailure of the edge ej = (vj, vj+1) ∈ pi, the unique s − xi shortest-path connecting s and\\nxi in G \\\\ {ej} is P ′j = pi[v1, vj] ◦ Pj ◦ [zj, xi], and all other alternatives are strictly longer.\\nSince ei,j /∈ T ′, also P ′j * T ′, and therefore dist(s, xi, G \\\\ {ej}) < dist(s, xi, T ′ \\\\ {ej}), in\\ncontradiction to the fact that T ′ is an FT-BFS tree. It follows that every FT-BFS tree T ′\\nmust contain at least |Eˆ| = Ω(n3/2) edges. The theorem follows.\\n9\\n3.2 Multiple Sources\\nWe next consider an intermediate setting where it is necessary to construct a fault-tolerant\\nsubgraph FT-MBFS containing several FT-BFS trees in parallel, one for each source s ∈ S,\\nfor some S ⊆ V . We establish the following.\\nTheorem 3.2 There exists an n-vertex graph G(V,E) and a source set S ⊆ V of cardi-\\nnality σ, such that any FT-MBFS tree from the source set S has Ω(\\n√\\nσ · n3/2) edges, i.e.,\\nCost∗(S,G) = Ω(\\n√\\nσ · n3/2).\\nProof: Our construction is based on the graph G(d) = (V1, E1), which consists of three\\ncomponents: (1) a set of vertices U = {u1, . . . , ud} connected by a path P1 = [u1, . . . , ud],\\n(2) a set of terminal vertices Z = {z1, . . . , zd} (viewed by convention as ordered from left\\nto right), and (3) a collection of d vertex disjoint paths Qi of length |Qi| = 6 + 2(d − i)\\nconnecting ui and zi for every i ∈ {1, . . . , d}. Thus |Q11| > . . . > |Q1d|. The vertex\\nr(G(d)) = ud is fixed as the root of G(d), hence the edges of the paths Qi are viewed\\nas directed away from ui, and the terminal vertices of Z are viewed as the leaves of the\\ngraph, denoted Leaf(G(d)) = Z. See Fig. 2 for illustration.\\nu11 \\nu12 \\nu13 \\nu14 \\nz1 z2 z3 z4 \\nFigure 2: The graph G1(d).\\nOverall, the vertex and edge sets of G(d) are V1 = U ∪ Z ∪\\n⋃d\\ni=1 V (Qi) and E1 =\\nE(P1) ∪\\n⋃d\\ni=1E(Qi).\\nObservation 3.3 (a) The number of leaves in G(d) is |Leaf(G(d))| = d.\\n(b) |V1| = c · d2 for some constant c.\\n10\\nTake σ copies, G′1, . . . , G\\n′\\nσ, of G(d), where d = O((n/σ)\\n1/2). Note that Obs. 3.3,\\neach copy G′i consists of O(n/σ) nodes. Let yi be the node ud and si = r(G\\n′\\ni) in the\\nith copy G′i. Add a node v\\n∗ connected to a set X of Ω(n) nodes and connect v∗ to\\neach of the nodes yi, for i ∈ {1, . . . , d}. Finally, connect the set X to the σ leaf sets\\nLeaf(G′1), . . . , Leaf(G\\n′\\nσ) by a complete bipartite graph, adjusting the size of the set X\\nin the construction so that |V (G)| = n. Since nLeaf(G′i) = Ω((n/σ)1/2) (see Obs. 3.3),\\noverall |E(G)| = Ω(n ·σ ·nLeaf(G1(d))) = Ω(n · (σn)1/2). Since the path from each source\\nsi to X cannot aid the nodes of G\\n′\\nj for j 6= i, the analysis of the single-source case can be\\napplied to show that each of the bipartite graph edges in necessary upon a certain edge\\nfault. See Fig. 3 for an illustration.\\ns \\nX \\n𝑂(𝑛) \\ns \\n𝑑 \\ns \\ns𝜎 \\nZ𝜎 \\nv* \\nG’𝜎 \\n𝑑 \\ns2 s1 \\nG’2 G’1 \\n𝑑 = 𝑂( 𝑛/𝜎) \\n \\nZ2 Z1 \\nFigure 3: Illustration of the lower bound for the multi-source case.\\n4 Upper Bounds\\nIn this section we provide tight matching upper bounds to the lower bounds presented in\\nSec. 3.\\n4.1 Single Source\\nFor the case of FT-BFS trees, we establish the following.\\n11\\nTheorem 4.1 There exists an O(nm) time algorithm that for every n-vertex graph G\\nand source node s constructs an FT-BFS tree rooted at s with O(n · min{Depth(s),√n})\\nedges.\\nTo prove the theorem, we first describe a simple algorithm for the problem and then prove\\nits correctness and analyze the size of the resulting FT-BFS tree. We note that using the\\nsparsity lemma of [23] and the tools of [14], one can provide a randomized construction for\\nan FT-BFS tree with O(n3/2 log n) edges with high probability. In contrast, the algorithm\\npresented in this paper is deterministic and achieve an FT-BFS tree with O(n3/2) edges,\\nmatching exactly the lower bound established in Sec. 3.\\nThe Algorithm To avoid complications due to shortest-paths of the same length, we\\nassume all shortest-path are computed with a weight assignment W that guarantees the\\nuniqueness of the shortest-paths. This can be achieved by considering a weight function\\nW defined so as to ensure that the shortest paths are also of minimal number of edges but\\nat the same time guarantees the uniqueness of the u−v shortest-path, for every u, v ∈ V .\\nLet e1, . . . , em be some arbitrary ordering of E(G). Then set W (ek) = 2\\nm+1 + 2k. Let\\nT0 = BFS(s,G) be the BFS tree rooted at s in G, computed according to the weight\\nassignment W . For every ej ∈ T0, let T0(ej) be the BFS tree rooted at s in G \\\\ {ej}.\\nThen the final FT-BFS tree is given by\\nT ∗(s) = T0 ∪\\n⋃\\nej∈T0\\nT0(ej).\\nThe correctness is immediate by construction.\\nObservation 4.2 T ∗(s) is an FT-BFS tree.\\nProof: Consider a vertex v and an edge e. If e /∈ pi(s, v), then pi(s, v) ⊆ T ∗(s)\\\\{e}, hence\\ndist(s, v, T ∗(s) \\\\ {e}) = |pi(s, v)| = dist(s, v,G \\\\ {e}). Otherwise, e ∈ pi(s, v) ⊆ T0. Then\\nby construction, T0(e) ⊆ T ∗(s). By definition, dist(s, v, T ∗(s) \\\\ {e}) = dist(s, v, T0(e)) =\\ndist(s, v,G \\\\ {e}). The observation follows.\\nDue to [26] each of the n−1 BFS trees T0(ej) can be constructed in O(m) time, hence\\nO(nm) rounds are required in total. It therefore remains to bound the size of T ∗(s).\\nSize Analysis We first provide some notation. For a path P , let Cost(P ) =\\n∑\\ne∈P W (e)\\nbe the weighted cost of P , i.e., the sum of its edge weights. An edge e ∈ G is defined as\\nnew if e /∈ E(T0). For every vi ∈ V and ej ∈ T0, let P ∗i,j = pi(s, vi, T0(ej)) ∈ SP (s, vi, G \\\\\\n{ej},W ) be the optimal replacement path of s and vi upon the failure of ej ∈ T0. Let\\nNew(P ) = E(P ) \\\\ E(T0) and\\nNew(vi) = {LastE(P ∗i,j) | ej ∈ T0} \\\\ E(T0)\\n12\\nbe the set of vi new edges appearing as the last edge in the replacement paths P\\n∗\\ni,j of vi\\nand ej ∈ T0. It is convenient to view the edges of T0(ej) as directed away from s. We\\nthen have that\\nT ∗ = T0 ∪\\n⋃\\nvi∈V \\\\{s}\\nNew(vi).\\nI.e., the set of new edges that participate in the final FT-BFS tree T ∗ are those that appear\\nas a last edge in some replacement path.\\nWe now upper bound the size of the FT-BFS tree T ∗. Our goal is to prove that New(vi)\\ncontains at most O(\\n√\\nn) edges for every vi ∈ V . The following observation is crucial in\\nthis context.\\nObservation 4.3 If LastE(P ∗i,j) /∈ E(T0), then ej ∈ pi(s, vi).\\nProof: Assume, towards contradiction, that ej /∈ pi(s, vi) and let P ∗i,j ⊆ T0(ej) be the s−vi\\nreplacement path in G \\\\ {ej} according to the weight assignment W . Since LastE(P ∗i,j) /∈\\nE(T0), we have two different s − vi shortest paths in G \\\\ {ej}, namely, pi(s, vi) and\\nP ∗i,j. By the optimality of pi(s, vi) in G, i.e., pi(s, vi) ∈ SP (s, vi, G,W ), it holds that\\nCost(pi(s, u)) < Cost(P ∗i,j). On the other hand, by the optimality of P\\n∗\\ni,j in G \\\\ {ej}, i.e.,\\nP ∗i,j ∈ SP (s, vi, G\\\\{ej},W ), we have that Cost(pi(s, u)) > Cost(P ∗i,j). Contradiction.\\nObs. 4.3 also yields the following.\\nCorollary 4.4 (1) New(vi) = {LastE(P ∗i,j) | ej ∈ pi(s, vi)} \\\\ E(T0) and\\n(2) |New(vi)| ≤ min{depth(vi), deg(vi)}.\\nThis holds since the edges of New(vi) are coming from at most depth(vi) replacement\\npaths P ∗i,j (one for every ej ∈ pi(s, vi)), and each such path contributes at most one edge\\nincident to vi.\\nFor the reminder of the analysis, let us focus on one specific node u = vi and let\\npi = pi(s, u), N = |New(u)|. For every edge ek ∈ New(u), we define the following parameters.\\nLet f(ek) ∈ pi be the failed edge such that ek ∈ T0(f(ek)) appears in the replacement path\\nPk = pi(s, u, T\\n′) for T ′ = T0(f(ek)). (Note that ek might appear as the last edge on the\\npath pi(s, u, T0(e\\n′)) for several edges e′ ∈ pi; in this case, one such e′ is chosen arbitrarily).\\nLet bk be the last divergence point of Pk and pi, i.e., the last vertex on the replacement\\npath Pk that belongs to V (pi) \\\\ {u}. Since LastE(Pk) /∈ E(T0), it holds that bk is not the\\nneighbor of u in Pk.\\nLet New(u) = {e1, . . . , eN} be sorted in non-decreasing order of the distance between\\nbk and u, dist(bk, u, pi) = |pi(bk, u)|. I.e.,\\ndist(b1, u, pi) ≤ dist(b2, u, pi) . . . ≤ dist(bN , u, pi). (1)\\nWe consider the set of truncated paths P ′k = Pk[bk, u] and show that these paths are\\n13\\nvertex-disjoint except for the last common endpoint u. We then use this fact to bound\\nthe number of these paths, hence bound the number N of new edges. The following\\nobservation follows immediately by the definition of bk.\\nObservation 4.5 (V (P ′k) ∩ V (pi)) \\\\ {bk, u} = ∅.\\nLemma 4.6\\n(\\nV (P ′i ) ∩ V (P ′j)\\n) \\\\ {u} = ∅ for every i, j ∈ {1, . . . , N}, i 6= j.\\nProof: Assume towards contradiction that there exist i 6= j, and a node\\nu′ ∈ (V (P ′i ) ∩ V (P ′j)) \\\\ {u}\\nin the intersection. Since LastE(P ′i ) 6= LastE(P ′j), by Obs. 4.5 we have that P ′i , P ′j ⊆\\nG \\\\ E(pi). The faulty edges f(ei), f(ej) belong to E(pi). Hence there are two distinct\\nu′ − u shortest paths in G \\\\ {f(ei), f(ej)}. By the optimality of P ′i in T0(f(ei)), (i.e.,\\nPi ∈ SP (s, u,G\\\\{f(ei)},W )), we have that Cost(P ′i [u′, u]) < Cost(P ′j [u′, u]). In addition,\\nby the optimality of P ′j in T0(f(ej)), (i.e., Pj ∈ SP (s, u,G \\\\ {f(ej)},W )), we have that\\nCost(P ′j [u\\n′, u]) < Cost(P ′i [u\\n′, u]). Contradiction.\\nWe are now ready to prove our key lemma.\\nLemma 4.7 |New(u)| = O(n1/2) for every u ∈ V .\\nProof: Assume towards contradiction that N = |New(u)| > √2n. By Lemma 4.6, we\\nhave that b1, . . . , bN are distinct and by definition they all appear on the path pi. There-\\nfore, by the ordering of the P ′k, we have that the inequalities of Eq. (1) are strict, i.e.,\\ndist(b1, u, pi) < dist(b2, u, pi) < . . . < dist(bN , u, pi). Since b1 6= u (by definition), we also\\nhave that dist(b1, u, pi) ≥ 1. We Conclude that\\ndist(bk, u, pi) = |pi(bk, u)| ≥ k . (2)\\nNext, note that each P ′k is a replacement bk−u path and hence it cannot be shorter than\\npi(bk, u), implying that |P ′k| ≥ |pi(bk, u)|. Combining, with Eq. (2), we have that\\n|P ′k| ≥ k for every k ∈ {1, . . . , N} . (3)\\nSince by Lemma 4.6, the paths P ′k are vertex disjoint (except for the common vertex u),\\nwe have that∣∣∣∣∣\\nN⋃\\nk=1\\n(V (P ′k) \\\\ {u})\\n∣∣∣∣∣ =\\nN∑\\nk=1\\n|V (P ′k) \\\\ {u}| ≥\\nN∑\\nk=1\\n(k − 1) > n,\\nwhere the first inequality follows by Eq. (3) and the last inequality by the assumption\\nthat N >\\n√\\n2n. Since there are a total of n nodes in G, we end with contradiction.\\nTurning to the case of a single vertex failure, the entire proof goes through almost\\nwithout change, yielding the following.\\n14\\nTheorem 4.8 There exists a polynomial time algorithm that for every n-vertex graph\\nand source node s constructs an FT-BFS tree from s tolerant to one vertex failure, with\\nO(n ·min{Depth(s),√n}) edges.\\n4.2 Multiple Sources\\nFor the case of multiple sources, we establish the following upper bound.\\nTheorem 4.9 There exists a polynomial time algorithm that for every n-vertex graph\\nG = (V,E) and source set S ⊆ V of size |S| = σ constructs an FT-MBFS tree T ∗(S)\\nfrom each source si ∈ S, tolerant to one edge or vertex failure, with a total number of\\nn ·min{∑si∈S depth(si), O(√σn)} edges.\\nThe algorithm As in the single source case, to avoid complications due to shortest-\\npaths of the same length, all shortest path distances in G are computed using a weight\\nfunction W defined so as to ensure the uniqueness of a single u − v shortest-path. For\\nevery si ∈ S and every ej ∈ T0(si), let T (si, ej) be the BFS tree rooted at si in G \\\\ {ej}.\\nLet\\nT0(S) =\\n⋃\\nsi∈S\\nT0(si)\\nbe the joint structure containing all the BFS trees of S. Then by the previous section,\\nthe FT-BFS tree for si is T\\n∗(si) = T0 ∪\\n⋃\\nej∈T0(si) T (si, ej). Define the FT-MBFS for S as\\nT ∗(S) =\\n⋃\\nsi∈S\\nT ∗(si) =\\n⋃\\nsi∈S,ej∈T0(si)\\nT (si, ej).\\nAnalysis The correctness follows immediately by the single source case. It remains to\\nbound the number of edges of T ∗(S). An edge e is new if e /∈ T0(S). For every vi ∈ V ,\\ndefine its new edge set in the graph T ∗(S) by\\nNew(S, vi) = {LastE(pi(s, vi, T (si, ej))) | si ∈ S, ej ∈ T0(si)} \\\\ E(T0(S)).\\nTo bound the size of T ∗(S), we focus on node u = vi, and bound its new edges New(S, u) =\\n{e1, . . . , eN}. Obs. 4.3 yields the following.\\nCorollary 4.10 New(S, u) ≤∑si∈S depth(si).\\nTowards the end of this section, we prove that New(S, u) contains at most O(\\n√\\nσn) new\\nedges. For ease of notation, let pi(si) = pi(si, u) for every i ∈ {1, . . . , σ}. For every edge\\nek ∈ New(S, u), we define the following parameters. Let s(ek) ∈ S and f(ek) ∈ T0(s(ek))\\nbe such that ek ∈ T (s(ek), f(ek)). I.e., the edge ek appears in the replacement s(ek) − u\\n15\\npath Pk = pi(s, u, T\\n′), where T ′ = T (s(ek), f(ek)) is the BFS tree rooted at s(ek) in\\nG \\\\ {f(ek)}. By Obs. 4.3, f(ek) ∈ pi(s(ek)). (Note that for a given new edge ek there\\nmight be several s′ and e′ such that ek = LastE(pi(s′, u, T (s′, e′))); in this case one such\\npair s′, e′ is chosen arbitrarily.) For every replacement path Pk (whose last edge is ek),\\ndenote by bk the last divergence point of Pk and the collection of shortest si − u paths\\nP = ⋃si∈S pi(si, u) \\\\ {u}. I.e., bk is the last point on Pk that belongs to V (P) \\\\ {u}. Let\\nP ′k = Pk[bk, u] be the truncated path from the divergence point bk to u. Note that since\\ne = (x, u) = LastE(Pk) /∈ E(T0(S)) is a new edge, it holds that x /∈ V (P) \\\\ {u} and bk is\\nin V \\\\ {u}. The following observation is useful.\\nObservation 4.11 P ′k ⊆ G \\\\ E(P) for every k ∈ {1, . . . , N}.\\nWe now show that the paths P ′k are vertex disjoint except for their endpoint u (this is\\nregardless of their respective source s(ek)).\\nLemma 4.12\\n(\\nV (P ′i ) ∩ V (P ′j)\\n) \\\\ {u} = ∅ for every i 6= j ∈ {1, . . . , N}.\\nProof: Assume towards contradiction that there exists i 6= j, and a node\\nu′ ∈ (V (P ′i ) ∩ V (P ′j)) \\\\ {u}\\nin the intersection. Since LastE(P ′i ) 6= LastE(P ′j) and by Obs. 4.11, P ′i , P ′j ⊆ G \\\\ E(P),\\nand the faulty edges f(ei), f(ej) ∈ P , we have two distinct u′ − u replacement paths in\\nG \\\\ {f(ei), f(ej)}. By the optimality of P ′i in T (s(ei), f(ei)), (i.e., Pi ∈ SP (s(ei), u,G \\\\\\n{f(ei)},W )), we have that Cost(P ′i ) < Cost(P ′j). Similarly, by the optimality of P ′j\\nin T (s(ej), f(ej)), (i.e., Pj ∈ SP (s(ej), u,G \\\\ {f(ej)},W )), we have that Cost(P ′j) <\\nCost(P ′i ), contradiction. The lemma follows.\\nWe are now ready to state and prove our main lemma.\\nLemma 4.13 N = |New(S, u)| = O(√σn).\\nWe begin by classifying the set of new edges ei ∈ New(S, u) into σ classes according to the\\nposition of the divergence point bi. For every ei ∈ New(S, u), let ŝ(ei) ∈ S be some source\\nnode such that the divergence point bi ∈ pi(ŝ(ei), u) appears on its ŝ(ei)− u shortest path\\nT0(S). If there are several such sources for the edge ei, one is chosen arbitrarily.\\nFor every sj ∈ S, let\\nNew(sj) = {ei ∈ New(S, u) | ŝ(ei) = sj}\\nbe the set of new edges in New(S, u) that are mapped to sj ∈ S. Then, New(S, u) =⋃\\nsj∈S New(sj). Let xj = |New(sj)|.\\nWe now focus on sj. For every ejk ∈ New(sj), k = {1, . . . , xj}, let Pjk = pi(s(ejk), u, T ′)\\nfor T ′ = T (s(ejk), f(ejk)) be the replacement path such that LastE(Pjk) = ejk and bjk\\n16\\nbe its corresponding (last) divergence point with pi(sj, u) (sj = ŝ(ejk)). In addition, the\\ntruncated path is given by P ′jk = Pjk [bjk , u]. Note that LastE(Pjk) = ejk .\\nConsider the set of divergence points bj1 , . . . , bjxj sorted in non-decreasing order of the\\ndistance between bjk and sj on the shortest sj−u path pi(sj) i.e., |pi(bjk , u, T0(sj))|, where\\n|pi(bj1 , u, T0(sj))| ≤ |pi(bj2 , u, T0(sj))| . . . ≤ |pi(bjxj , u, T0(sj))| . (4)\\nNote that by Lemma 4.12, bj` 6= bj`′ for every `, `′ ∈ {1, . . . , xj}. In addition, since each\\nbj` 6= u, |pi(bj1 , u, T0(sj))| ≥ 1. Hence, since bj1 , . . . , bjxj ∈ pi(sj), combining with Eq. (4)\\nwe get that\\n1 ≤ |pi(bj1 , u, T0(sj))| < |pi(bj2 , u, T0(sj))| . . . < |pi(bjxj , u, T0(sj))| . (5)\\nSince P ′j` is an alternative bj` − u replacement path, we have that\\n|P ′j`| ≥ |pi(bj` , u, T0(sj))| ≥ `. (6)\\nwhere the last inequality follows by Eq. (4). Hence, since all P ′j` are vertex disjoint, except\\nfor the last node u, we get the total number of nodes V (sj) =\\n⋃\\nV (P ′j`) \\\\ {u} occupied by\\nP ′j` paths is\\nxj∑\\n`=1\\n|V (P ′j`)| = |V (sj)| = O(x2j).\\nSince the nodes of V (sj1) and V (sj2) are disjoint for every sj1 , sj2 ∈ S, by Lemma\\n4.12, it follows that |New(S, u)| = ∑σj=1 xj but ∑σj=1 |V (sj)| = O(x2j) ≤ n. Therefore,\\n|New(S, u)| = ∑σj=1 xj ≤ O(√σn).\\nAs there are n nodes, combining with Cor. 4.10, we get that the total number of edges\\nin T ∗(S) is given by\\nE(T ∗(S)) ≤ |E(T0(S))|+\\n∑\\nu∈V\\n|New(S, u)| ≤ σn+ n ·min{\\n∑\\nsi∈S\\ndepth(si), O(\\n√\\nσn)},\\nas required. Thm. 4.9 is established. The analysis for the case of vertex faults follows\\nwith almost no changes.\\n5 Hardness of Approximation of the Minimum FT-BFS\\nProblem\\nIn this section we establish the following.\\nTheorem 5.1 The Minimum FT-BFS problem is NP-complete and cannot be approxi-\\nmated to within a factor c log n for some constant c > 0 unless NP ⊆ T IME(npoly log(n)).\\n17\\nWe prove Theorem 5.1 by showing a gap preserving reduction from the Set-Cover problem\\nto the Minimum FT-BFS problem. An instance 〈U,F〉 of the Set-Cover problem consists\\nof a set of N elements U = {u1, . . . , uN} and a collection of M sets F = {S1, . . . , SM}\\nsuch that Si ⊆ U and\\n⋃\\nSi = U . The task is to choose the minimal number of sets in\\nF whose union covers all of U . Fiege [13] showed that the Set Cover problem cannot be\\napproximated within a ratio of (1− o(1)) lnn unless NP ⊆ T IME(Mpoly log(M)).\\nThe Transformation. Given a Set-Cover instance 〈U,F〉, we construct a Minimum\\nFT-BFS instance I(U,F) = (G, s) as follows. LetX = {x1, . . . , xM} (resp., Z = {z1, . . . , zN})\\nbe the vertex set corresponding to the collection of sets F (resp., elements of U). Let\\nBXZ = (X,Z,EXZ) be the bipartite graph corresponding to the input 〈U,F〉, where\\nEXZ = {(xj, zi) | ui ∈ Sj, j ∈ {1, . . . ,M} and i ∈ {1, . . . , N}}. Embed the bi-\\npartite graph BXZ in G in the following manner. Construct a length-(N + 1) path\\nP = [s = p0, p1 . . . , pN , pN+1], connect a vertex v\\n′ to pN and connect a set of ver-\\ntices Y = {y1, . . . , yR} for R = O((MN)3) to the vertex pN+1 by the edges of EpY =\\n{(pN+1, yi) | i ∈ {1, . . . , R}}. Connect these vertices to the bipartite graph BXZ as\\nfollows. For every i ∈ {1, . . . , N}, connect the node pi−1 of P to the node zi of Z\\nby a path Qi = [pi−1 = qi0, . . . , q\\ni\\nti\\n= zi] where ti = |Qi| = 6 + 2(N − i). Thus the\\npaths Qi are monotonely decreasing and vertex disjoint. In addition, connect the vertices\\nv′ and pN+1 to every vertex of X, adding the edge sets EvX = {(v′, xi) | xi ∈ X}\\nand EpX = {(pN+1, xj) | xj ∈ X}. Finally, construct a complete bipartite graph\\nBXY = (X, Y,EXY ) where EXY = {(y`, xj) | xj ∈ X, y` ∈ Y }. This completes the\\ndescription of G. For illustration, see Fig. 4. Overall,\\nV (G) = X ∪ Z ∪ V (P ) ∪\\nN⋃\\ni=1\\nV (Qi) ∪ {v′} ∪ Y,\\nand\\nE(G) = EXZ ∪ E(P ) ∪\\nN⋃\\ni=1\\nE(Qi) ∪ {(pN , v′)} ∪ EpY ∪ EvX ∪ EpX ∪ EXY .\\nNote that |V (G)| = O(R) and that |E(G)| = O(|EXZ |+N2 +MR) = O(MR).\\nFirst, note the following.\\nObservation 5.2 Upon the failure of the edge ei = (pi−1, pi), i ∈ {1, . . . , N}, the follow-\\ning happen:\\n(a) the unique s− zi shortest path in G \\\\ {ei} is given by P˜i = P [s, pi−1] ◦Qi.\\n(b) the shortest-paths connecting s and the vertices of {pN , pN+1, v′} ∪X ∪ Y disconnect\\nand hence the replacement paths in G \\\\ {ei} must go through the Z nodes.\\n18\\ne3 \\np5 \\nx1 x2 x3 x4 x5 \\nz4 z3 z2 z1 \\ns \\nv’ \\nP Qi \\nY \\np1 \\np2 \\np3 \\np4 \\nyl \\np0 \\nBXY \\nBXZ \\nZ \\nX \\nEpY \\nEpX EvX \\nFigure 4: Schematic illustration of the reduction from Set-Cover to Minimum FT-BFS. In\\nthis example F = {S1, S2, . . . , S5} where S1 = {u1, u3, u4}, S2 = {u1, u3}, S3 = {a2, a4},\\nS4 = {a3} and S5 = {a1, a4}. Thus, N = 4 and M = 5. The minimal vertex cover is\\ngiven by S2 and S3. The vertex set Y is fully connected to X. In the optimal FT-BFS\\nT ∗, Y is required to be connected to the xj nodes that corresponds to the sets appearing\\nin the optimal cover. For example, y` is connected to x2 and x3 which “covers” the Z\\nnodes. The red edges are necessary upon the fault of e3. All edges described except for\\nthe (xj, y`) edges are required in any FT-BFS tree.\\nWe begin by observing that all edges except those of BXY are necessary in every\\nFT-BFS tree T̂ ∈ T (s,G). Let E˜ = E(G) \\\\ EXY .\\nObservation 5.3 E˜ ⊆ T̂ for every T̂ ∈ T (s,G).\\nProof: The edges of the paths P and the edges of EpY ∪ {(pN , v′)} are trivially part of\\nevery FT-BFS tree. The edges of the path Qi are necessary, by Obs. 5.2(a), upon the\\nfailure of ei for every i ∈ {1, . . . , N}. To see that the edges of EvX are necessary, note\\nthat upon the failure of the edge (pN , pN+1) or the edge (pN+1, xj), the unique s − xj\\nreplacement path goes through v′ for every j ∈ {1, . . . ,M}. Similarly, the edges EpX are\\nnecessary upon the failure of (pN , v\\n′) or (v′, xj).\\nIt remains to consider the edges of EXZ . Assume, towards contradiction, that there\\nexists some T ′ ∈ T (s,G) that does not contain ej,i = (xj, zi) ∈ EXZ . Note that by Obs.\\n19\\n5.2(a), upon the failure of the edge ei = (pi−1, pi) ∈ P , the unique s − xj shortest-path\\nin G \\\\ {ei} is P ′i = pi[p0, pi−1] ◦ Qi ◦ [zi, xj], and all other alternatives are strictly longer.\\nSince ej,i /∈ T ′, also P ′i * T ′, and therefore dist(s, xj, G \\\\ {ei}) < dist(s, xj, T ′ \\\\ {ei}), in\\ncontradiction to the fact that T ′ ∈ T (s,G). The observation follows.\\nWe now prove the correctness of the reduction and then consider gap-preservation.\\nLet T̂ ∈ T (s,G) and define by Γ(y`, T̂ ) = {xj | (xj, y`) ∈ T̂} the X nodes that are\\nconnected to y` in T̂ , for every y` ∈ Y . Let κ(T̂ ) = min{|Γ(y`, T̂ )| | y` ∈ Y }. Note that\\nsince the edges of E˜ are necessary in every T̂ ∈ T (s,G) it follows that\\n|E(T̂ )| ≥ |E˜|+ κ(T̂ ) ·R . (7)\\nLemma 5.4 If T̂ ∈ T (s,G) then there exists a Set-Cover for 〈U,F〉 of size at most κ(T̂ ).\\nProof: Consider T̂ ∈ T (s,G) and let y` ∈ Y be such that |Γ(y`, T̂ )| = κ(T̂ ). A cover F′\\nfor U for size κ(T̂ ) is constructed as follows. Let F′ = {Sj | xj ∈ Γ(y`, T̂ )}. By definition,\\n|F′| = κ(T̂ ). We now claim that it is a cover for U . Assume, towards contradiction, that\\nthere exists some ui ∈ U not covered by F′. Consider the graph G′ = G \\\\ {ei} where\\nei = (pi−1, pi). Recall that by Obs. 5.2(a), P˜k = P [s, pk−1] ◦ Qk is the s − zk path in\\nG \\\\ {ek}. Note that P˜k * G′ for every k > i and |P˜k| > |P˜i| for every k < i. Hence\\ndenoting the set of neighbors of zi in X by Γ(zi) = {xj | (zi, xj) ∈ EXZ}, by Obs.\\n5.2(b), the unique s − xj shortest-path, for every xj ∈ Γ(zi) such that (zi, xj) ∈ EXY ,\\nis given by P ′j = P˜i ◦ (zi, xj). Therefore the s − y` shortest-paths in G′ are all given by\\nP ′j ◦ (xj, y`), for every xj ∈ Γ(zi). But since (xj, y`) /∈ T̂ for every xj ∈ Γ(zi), we have that\\ndist(s, y`, G\\n′) < dist(s, y`, T̂ \\\\ {ei}), in contradiction to the fact that T̂ ∈ T (s,G).\\nLemma 5.5 If there exists a Set-Cover of size κ then Cost∗(s,G) ≤ |E˜|+ κ ·R.\\nProof: Given a cover F′ ⊆ F, |F′| = κ, construct a FT-BFS tree T̂ ∈ T (s,G) with |E˜|+κ·R\\nedges as follows. Add E˜ to T̂ . In addition, for every Sj ∈ F′, add the edge (y`, xj) to T̂\\nfor every y` ∈ Y . Clearly, |E(T̂ )| = |E˜| + κ · R. It remains to show that T̂ ∈ T (s,G).\\nNote that there is no s − u replacement path that uses any y` ∈ Y as a relay, for any\\nu ∈ V (G) and y` ∈ Y ; this holds as X is connected by two alternative shortest-paths to\\nboth pN+1 and to v\\n′ and the path through y` is strictly longer. In addition, if the edge\\ne ∈ {(pN , pN+1), (pN+1, y`)} fails, then the s − y` shortest path in G \\\\ {e} goes through\\nany neighbor xj of y`. Since each y` has at least one X node neighbor in T̂ , it holds that\\ndist(s, y`, T̂ \\\\ {e}) = dist(s, y`, G \\\\ {e}).\\nSince the only missing edges of T̂ , namely, E(G)\\\\E(T̂ ), are the edges of EXY , it follows\\nthat it remains to check the edges ei = (vi−1, vi) for every i ∈ {1, . . . , N}. Let Sj ∈ F′ such\\nthat ui ∈ Sj. Since F′ is a cover, such Sj exists. Hence, the optimal s − y` replacement\\npath in G \\\\ {ei}, which is by Obs. 5.2(b), P ′ = P˜i ◦ (zi, xj) ◦ (xj, y`), exists in T̂ \\\\ {ei} for\\n20\\nevery y` ∈ Y . It follows that T̂ ∈ T (s,G), hence Cost∗(s,G) ≤ |E(T̂ )| = |E˜|+ κ ·R. The\\nlemma follows.\\nLet κ∗ be the cost of the optimal Set-Cover for the instance 〈U,F〉. We have the\\nfollowing.\\nCorollary 5.6 Cost∗(s,G) = |E˜|+ κ∗ ·R.\\nProof: Let T ∗ ∈ T (s,G) be such that |E(T ∗)| = Cost∗(s,G). It then holds that\\n|E˜|+ κ(T ∗) ·R ≤ |E(T ∗)| = Cost∗(s,G) ≤ |E˜|+ κ∗ ·R,\\nwhere the first inequality holds by Eq. (7) and the second inequality follows by Lemma\\n5.5. Hence, κ(T ∗) ≤ κ∗. Since by Lemma 5.4, there exists a cover of size κ(T ∗), we have\\nthat κ∗ ≤ κ(T ∗). It follows that κ∗ = κ(T ∗) and Cost∗(s,G) = |E˜| + κ∗ · R as desired.\\nWe now show that the reduction is gap-preserving. Assume that there exists an\\nα approximation algorithm A for the Minimum FT-BFS problem. Then applying our\\ntransformation to an instance I(U,F) = (G, s) would result in an FT-BFS tree T̂ ∈ T (s,G)\\nsuch that\\n|E˜|+ κ(T̂ ) ·R < |E(T̂ )| ≤ α(|E˜|+ κ∗ ·R) ≤ 3α · κ∗ ·R ,\\nwhere the first inequality follows by Eq. (7), the second by the approximation guarantee\\nof A and by Cor. 5.6, and the third inequality follows by the fact that |E˜| ≤ 2R. By\\nLemma 5.4, a cover of size κ(T̂ ) ≤ 3ακ∗ can be constructed given T̂ , which results in a 3α\\napproximation to the Set-Cover instance. As the Set-Cover problem is inapproximable\\nwithin a factor of (1 − o(1)) lnn, under an appropriate complexity assumption [13], we\\nget that the Minimum FT-BFS problem is inapproximable within a factor of c · logN for\\nsome constant c > 0. This complete the proof of Thm. 5.1.\\n6 O(log n)-Approximation for FT-MBFS Trees\\nIn Sec. 4.1, we presented an algorithm that for every graph G and source s constructs\\nan FT-BFS tree T̂ ∈ T (s,G) with O(n3/2) edges. In Sec. 3.1, we showed that there exist\\ngraphs G and s ∈ V (G) for which Cost∗(s,G) = Ω(n3/2), establishing tightness of our\\nalgorithm in the worst-case. Yet, there are also inputs (G′, s′) for which the algorithm of\\nSec. 4, as well as algorithms based on the analysis of [14] and [23], might still produce an\\nFT-BFS T̂ ∈ T (s′, G′) which is denser by a factor of Ω(√n) than the size of the optimal\\nFT-BFS tree, i.e., such that |E(T̂ )| ≥ Ω(√n) · Cost∗(s′, G′). For an illustration of such a\\ncase consider the graph G′ = (V,E) which is a modification of the graph G described in\\n21\\nSec. 3.1. The modifications are as follows. First, add a node z0 to Z and connect it to\\nevery xi ∈ X. Replace the last edge e′i = LastE(Pi) of the vi − zi path Pi by a vertex ri\\nthat is connected to the endpoints of the edge e′i for every i ∈ {1, . . . , d}. Let P ′i be the\\ns− zi modified path where LastE(P ′i ) = (ri, zi). Finally, connect the node z0 to all nodes\\nri for every i ∈ {1, . . . , d}. See Fig. 5 for illustration.\\nei \\nv2 \\nvi \\nvi+1 \\nr1 \\ns \\nvd \\nP’i \\n𝑑 = 𝑂( 𝑛) \\nz0 z1 zi \\nri rd \\nzd \\nv* \\nX xj \\nZ \\nR \\nB \\nFigure 5: Bad example for the algorithm of Sec. 4. The weights of the z0 edges are larger\\nthan those of the other edges. Thus, the entire complete bipartite graph B(X,Z \\\\ {z0})\\nof size Ω(n3/2) is included in the resulting FT-BFS tree T̂ ∈ T (s,G) returned by the\\nalgorithm. However, an FT-BFS tree T ∗ of O(n) edges can be given by including the edges\\nof (z0, xi) for every xi ∈ X. The red edges are two optional edges necessary upon the\\nfailure of ei. Adding the edge (xj, z0) is better, yet the algorithm of Sec. 4 adds (xj, zi)\\nto T̂ for every xj ∈ X.\\nObserve that whereas Cost∗(s,G) = Ω(n3/2), the modified G′ has Cost∗(s,G′) = O(n),\\nas the edges of the complete bipartite graph B that are required in every T̂ ∈ T (s,G) are\\nno longer required in every T ′ ∈ T (s,G′); it is sufficient to connect the nodes of X to z0\\nonly, and by that “save” the Ω(n3/2) edges of B in T ′. Nevertheless, as we show next, for\\ncertain weight assignments the algorithm of Sec. 4 constructs an FT-BFS tree T̂ of size\\nO(n3/2). Specifically, let W be such that each of the edges of\\nE ′ = {(z0, ri) | i ∈ {1, . . . , d}} ∪ {(z0, xi) | xi ∈ X}\\nis assigned a weight which is strictly larger than the weights of the other edges. That\\nis, W (ek) > W (e`) for every ek ∈ E ′ and e` ∈ E(G′) \\\\ E ′. Note that for every edge\\nei = (vi, vi+1) ∈ pi, i ∈ {1, . . . , d}, there are two alternative s − xj replacement paths\\nof the same length, namely, Qi,j = pi[s, vi] ◦ P ′i ◦ (zi, xj) that goes through zi and Q̂i,j =\\npi[s, vi]◦P ′i [s, ri]◦(ri, z0)◦(z0, xi) that goes through z0. Although |Qi,j| = |Q̂i,j|, the weight\\n22\\nassignment implies that Cost(Qi,j) < Cost(Q̂i,j) and hence Q̂i,j /∈ SP (s, xj, G \\\\ {ei},W )\\nfor every i ∈ {1, . . . , d} and every xj ∈ X. Therefore, E(B) ⊆ T̂ , for every FT-BFS\\ntree T̂ computed by the algorithm of Sec. 4 with the weight assignment W . Hence\\n|E(T̂ )| = Θ(n3/2) while Cost∗(s,G′) = O(n).\\nClearly, a universally optimal algorithm is unlikely given the hardness of approximation\\nresult of Thm. 5.1. Yet the gap can be narrowed down. The goal of this section is to\\npresent an O(log n) approximation algorithm for the Minimum FT-BFS Problem (hence\\nalso to its special case, the Minimum FT-BFS Problem, where |S| = 1).\\nTo establish this result, we first describe the algorithm and then bound the number of\\nedges. Let ApproxSetCover(F, U) be an O(log n) approximation algorithm for the Set-\\nCover problem, which given a collection of sets F = {S1, . . . , SM} that covers a universe\\nU = {u1, . . . , uN} of size N , returns a cover F′ ⊆ F that is larger by at most O(logN)\\nthan any other F′′ ⊆ F that covers U (cf. [27]).\\nThe Algorithm Starting with T̂ = ∅, the algorithm adds edges to T̂ until it becomes\\nan FT-MBFS tree.\\nSet an arbitrary order on the vertices V (G) = {v1, . . . , vn} and on the edges E+ =\\nE(G)∪ {e0} = {e0, . . . , em} where e0 is a new fictitious edge whose role will be explained\\nlater on. For every node vi ∈ V , define\\nUi = {〈sk, ej〉 | sk ∈ S \\\\ {vi}, ej ∈ E+}.\\nThe algorithm consists of n rounds, where in round i it considers vi. Let Γ(vi, G) =\\n{u1, . . . , udi} be the set of neighbors of vi in some arbitrary order, where di = deg(vi, G).\\nFor every neighbor uj, define a set Si,j ⊆ Ui containing certain source-edge pairs 〈sk, e`〉 ∈\\nUi. Informally, a set Si,j contains the pair 〈sk, e`〉 iff there exists an sk−vi shortest path in\\nG \\\\ {e`} that goes through the neighbor uj of vi. Note that Si,j contains the pair 〈sk, e0〉\\niff there exists an sk − vi shortest-path in G \\\\ {e0} = G that goes through uj. I.e., the\\nfictitious edge e0 is meant to capture the case where no fault occurs, and thus we take\\ncare of true shortest-paths in G. Formally, every pair 〈sk, e`〉 ∈ Ui is included in every set\\nSi,j satisfying that\\ndist(sk, uj, G \\\\ {e`}) = dist(sk, vi, G \\\\ {e`})− 1. (8)\\nLet Fi = {Si,1, . . . , Si,di}. The edges of vi that are added to T̂ in round i are now selected\\nby using algorithm ApproxSetCover to generate an approximate solution for the set cover\\nproblem on the collection F = {Si,j | uj ∈ Γ(vi, G)}. Let F′i = ApproxSetCover(Fi, Ui).\\nFor every Si,j ∈ F′i, add the edge (uj, vi) to T̂ . We now turn to prove the correctness of\\nthis algorithm and establish Thm. 6.5.\\n23\\nAnalysis We first show that algorithm constructs an FT-MBFS T̂ ∈ T (S,G) and then\\nbound its size.\\nLemma 6.1 T̂ ∈ T (S,G).\\nProof: Assume, towards contradiction, that T̂ /∈ T (S,G). Let s ∈ S be some source\\nnode such that T̂ /∈ T (s,G) is not an FT-BFS tree with respect to s. By the assumption,\\nsuch s exists. Let\\nBP = {(i, k) | vi ∈ V, ek ∈ E+ and dist(s, vi, T̂ \\\\ {ek}) > dist(s, vi, G \\\\ {ek})}\\nbe the set of “bad pairs,” namely, vertex-edge pairs (i, k) for which the s−vi shortest path\\ndistance in T̂ \\\\{ek} is greater than that in G\\\\{ek}. (By the assumption that T̂ /∈ T (s,G),\\nit holds that BP 6= ∅.) For every vertex-edge pair (i, k), where vi ∈ V \\\\ {s} and ek ∈ E+,\\ndefine an s− vi shortest-path P ∗i,k in G \\\\ {ek} in the following manner. Let uj ∈ Γ(vi, G)\\nbe such that the pair 〈s, ek〉 ∈ Si,j is covered by the set Si,j of uj and Si,j ∈ F′i is included\\nin the cover returned by the algorithm ApproxSetCover in round i. Thus, (uj, vi) ∈ T̂\\nand dist(s, uj, G \\\\ {ek}) = dist(s, vi, G \\\\ {ek})− 1. Let P ′ ∈ SP (s, uj, G \\\\ {ek}) and define\\nP ∗i,k = P\\n′ ◦ (uj, vi).\\nBy definition, |P ∗i,k| = dist(s, vi, G \\\\ {ek}) and by construction, LastE(P ∗i,k) ∈ T̂ . Define\\nBE(i, k) = P ∗i,k \\\\ E(T̂ ) to be the set of “bad edges,” namely, the set of P ∗i,k edges that\\nare missing in T̂ . By definition, BE(i, k) 6= ∅ for every bad pair (i, k) ∈ BP . Let\\nd(i, k) = maxe∈BE(i,k){dist(s, e, P ∗i,k)} be the maximal depth of a missing edge in BE(i, k),\\nand let DM(i, k) denote that “deepest missing edge” for (i, k), i.e., the edge e on P ∗i,k\\nsatisfying d(i, k) = dist(s, e, P ∗i,k). Finally, let (i\\n′, k′) ∈ BP be the pair that minimizes\\nd(i, k), and let e1 = (v`1 , vi1) ∈ BE(i′, k′) be the deepest missing edge on P ∗i′,k′ , namely,\\ne1 = DM(i\\n′, k′). Note that e1 is the shallowest “deepest missing edge” over all bad\\npairs (i, k) ∈ BP . Let P1 = P ∗i1,k′ , P2 = P ∗i′,k′ [s, vi1 ] and P3 = P ∗i′,k′ [vi1 , vi′ ]; see Fig. 6 for\\nillustration. Note that since (i′, k′) ∈ BP , it follows that also (i1, k′) ∈ BP . (Otherwise, if\\n(i1, k\\n′) /∈ BP , then any s−vi1 shortest-path P ′ ∈ SP (s, vi1 , T̂ \\\\{ek′}) , where |P ′| = |P ∗i1,k′ |,\\ncan be appended to P3 resulting in P\\n′′ = P ′ ◦ P3 such that (1) P ′′ ⊆ T̂ \\\\ {ek′} and (2)\\n|P ′′| = |P ′|+ |P3| = |P2|+ |P3| = |P ∗i′,k′|, contradicting the fact that (i′, k′) ∈ BP .) Thus\\nwe conclude that (i1, k\\n′) ∈ BP . Finally, note that LastE(P1) ∈ T̂ by definition, and\\ntherefore the deepest missing edge of (i, k) must be shallower, i.e., d(i1, k\\n′) < d(i′, k′).\\nHowever, this is in contradiction to our choice of the pair (i′, k′). The lemma follows.\\nLet W : E(G) → R>0 be the weight assignment that guarantees the uniqueness\\nof shortest-paths. Note that the algorithm did not use W in the computation of the\\nshortest-paths. For every node vi, let Γ(vi, G) = {u1, . . . , udi} be its ordered neighbor set\\nas considered by the algorithm. For every FT-MBFS tree T˜ ∈ T (S,G), vi ∈ V, e` ∈ E+ and\\n24\\ns \\nvi' \\nvi1 \\n \\ne1 \\nP2 \\nP3 \\n',\\n*\\n1 1 kiPP \\uf03d\\nFigure 6: Red solid lines correspond to new edges. The “deepest missing edge” for (i′, k′),\\nedge e1, is the shallowest such edge over all bad pairs in BP . Yet the pair (i1, k\\n′) is bad\\ntoo. As the last (green) edge of P1 is included in the FT-MBFS tree, and since P1 and P2\\nare of the same length, it follows that P1 has a shallower “deepest missing edge”.\\nsk ∈ S, let P˜i(sk, e`) ∈ SP (sk, vi, T˜ \\\\ {e`},W ) be an sk − vi shortest-path in T˜ \\\\ {e`}. Let\\nAi(T˜ ) = {LastE(P˜i(sk, e`)) | e` ∈ E+, sk ∈ S \\\\ {vi}}\\nbe the edges of vi that appear as last edges in the shortest-paths and replacement paths\\nfrom S to vi in T˜ . Define\\nFi(T˜ ) = {Si,j | (uj, vi) ∈ Ai(T˜ )}.\\nWe then have that\\n|Fi(T˜ )| = |Ai(T˜ )| . (9)\\n25\\nThe correctness of the algorithm (see Lemma 6.1) established that if a subgraph T˜ ⊆ G\\nsatisfies that Fi(T˜ ) is a cover of Ui for every vi ∈ V , then T˜ ∈ T (S,G). We now turn to\\nshow the reverse direction.\\nLemma 6.2 For every T˜ ∈ T (S,G), the collection Fi(T˜ ) is a cover of Ui, namely,⋃\\nSi,j∈Fi(T˜ ) Si,j = Ui, for every vi ∈ V .\\nProof: Assume, towards contradiction, that there exists an FT-MBFS tree T˜ ∈ T (S,G)\\nand a vertex vi ∈ V whose corresponding collection of sets Fi(T˜ ) does not cover Ui. Hence\\nthere exists at least one uncovered pair 〈sk, e`〉 ∈ Ui, i.e.,\\n〈sk, e`〉 ∈ Ui \\\\\\n⋃\\nSi,j∈Fi(T˜ )\\nSi,j . (10)\\nBy definition sk 6= vi. We next claim that T˜ does not contain an optimal sk − vi path\\nwhen the edge e` fails, contradicting the fact that T˜ ∈ T (S,G). That is, we show that\\ndist(sk, vi, T˜ \\\\ {e`}) > dist(sk, vi, G \\\\ {e`}).\\nTowards contradiction, assume otherwise, and let (uj, vi) = LastE(P\\n∗\\ni,`) where P\\n∗\\ni,` ∈\\nSP (sk, vi, T˜ \\\\ {e`},W ), hence (uj, vi) ∈ Ai(T˜ ) and Si,j ∈ Fi(T˜ ). By the contradictory\\nassumption, |P ∗i,`| = dist(sk, vi, G \\\\ {e`}) and hence dist(sk, uj, G \\\\ {e`}) = dist(sk, vi, G \\\\\\n{e`}) − 1. This implies that 〈sk, e`〉 ∈ Si,j ∈ Fi(T˜ ), in contradiction to Eq. (10), stating\\nthat 〈sk, e`〉 is not covered by Fi(T˜ ). The lemma follows.\\nWe now turn to bound that number of edges in T̂ .\\nLemma 6.3 |E(T̂ )| ≤ O(log n) · Cost∗(S,G).\\nProof: Let δ = c log n be the approximation ratio guarantee of ApproxSetCover. For ease\\nof notation, let Oi = Ai(T\\n∗) for every vi ∈ V . Let Fi = {Si,1, . . . , Si,di} be the collection\\nof vi sets considered at round i where Si,j ⊆ Ui is the set of the neighbor uj ∈ Γ(vi, G)\\ncomputed according to Eq. (8).\\nLet F′i = ApproxSetCover(Si, Ui) be the cover returned by the algorithm and define\\nAi = {(uj, vi) | Si,j ∈ F′i} as the collection of edges whose corresponding sets are included\\nin S ′i. Thus, by Eq. (9), |Oi| = |Fi(T ∗)| and |Ai| = |F′i| for every vi ∈ V .\\nObservation 6.4 |Ai| ≤ δ|Oi| for every vi ∈ V \\\\ {s}.\\nProof: Assume, towards contradiction, that there exists some i such that |Ai| > δ|Oi|.\\nThen by Eq. (9) and by the approximation guarantee of ApproxSetCover where in\\nparticular |Fi(T˜ )| ≤ δ|F′′i | for every F′′i ⊆ Fi that covers Ui, it follows that Fi(T ∗) is not\\n26\\na cover of Ui. Consequently, it follows by Lemma 6.2 that T\\n∗ /∈ T (S,G), contradiction.\\nThe observation follows.\\nSince\\n⋃\\nAi contains precisely the edges that are added by the algorithm to the con-\\nstructed FT-MBFS tree T̂ , we have that\\n|E(T̂ )| ≤\\n∑\\ni\\n|Ai| ≤ δ\\n∑\\ni\\n|Oi| ≤ 2δ · Cost∗(S,G) ,\\nwhere the second inequality follows by Obs. 6.4 and the third by the fact that |E(T ∗)| ≥∑\\ni |Oi|/2 (as every edge in\\n⋃\\nvi∈V Oi can be counted at most twice, by both its endpoints).\\nThe lemma follows.\\nThe following theorem is established.\\nTheorem 6.5 There exists a polynomial time algorithm that for every n-vertex graph G\\nand source node set S ⊆ V constructs an FT-MBFS tree T̂ ∈ T (S,G) such that |E(T̂ )| ≤\\nO(log n) · Cost∗(S,G).\\nAcknowledgment We are grateful to Gilad Braunschvig, Alon Brutzkus, Adam Sealfon\\nand Oren Weimann for helpful discussions.\\n27\\nReferences\\n[1] B. Awerbuch, A. Bar-Noy, N. Linial, and D. Peleg. Compact distributed data struc-\\ntures for adaptive network routing. In Proc. 21st ACM Symp. on Theory of Com-\\nputing, 230–240, 1989.\\n[2] I. Abraham, S. Chechik, C. Gavoille and D. Peleg. Forbidden-Set Distance Labels for\\nGraphs of Bounded Doubling Dimension. In Proc. 29th ACM Symp. on Principles\\nof Distributed Computing, 2010, 192–200.\\n[3] S. Baswana and S. Sen. Approximate distance oracles for unweighted graphs in\\nexpected O(n2) time. ACM Trans. Algorithms, 2(4):557–577, 2006.\\n[4] A. Bernstein and D. Karger. A nearly optimal oracle for avoiding failed vertices and\\nedges. In Proc. 41st ACM Symp. on Theory of Computing, 101–110, 2009.\\n[5] S. Chechik, M. Langberg, D. Peleg, and L. Roditty. f -sensitivity distance oracles\\nand routing schemes. Algorithmica, 861–882, 2012.\\n[6] S. Chechik, M. Langberg, D. Peleg, and L. Roditty. Fault-tolerant spanners for\\ngeneral graphs. In Proc. 41st ACM Symp. on Theory of computing, 435–444, 2009.\\n[7] S. Chechik. Fault-Tolerant Compact Routing Schemes for General Graphs. In Proc.\\n38th Int. Colloq. on Automata, Languages & Prog., 101–112, 2011.\\n[8] B. Courcelle and A. Twigg. Compact forbidden-set routing. In Proc. 24th Symp. on\\nTheoretical Aspects of Computer Science, 37–48, 2007.\\n[9] A. Czumaj and H. Zhao. Fault-tolerant geometric spanners. Discrete & Computa-\\ntional Geometry, 32, 2003.\\n[10] C. Demetrescu, M. Thorup, R. Chowdhury, and V. Ramachandran. Oracles for\\ndistances avoiding a failed node or link. SIAM J. Computing, 37:1299–1318, 2008.\\n[11] M. Dinitz and R. Krauthgamer. Fault-tolerant spanners: better and simpler. In\\nProc. ACM Symp. on Principles of Distributed Computing, 2011, 169-178.\\n[12] R. Duan and S. Pettie. Dual-failure distance and connectivity oracles. In Proc. 20th\\nACM-SIAM Symp. on Discrete Algorithms, 2009.\\n[13] U. Feige. A Threshold of ln n for Approximating Set Cover. J. ACM, 634–652, 1998.\\n[14] F. Grandoni and V.V Williams. Improved Distance Sensitivity Oracles via Fast\\nSingle-Source Replacement Paths. In Proc. 53rd IEEE Symp. on Foundations of\\nComputer Science, 2012.\\n28\\n[15] C. Levcopoulos, G. Narasimhan, and M. Smid. Efficient algorithms for constructing\\nfault-tolerant geometric spanners. In Proc. 30th ACM Symp. on Theory of computing,\\n186–195, 1998.\\n[16] T. Lukovszki. New results of fault tolerant geometric spanners. In Proc. 6th Workshop\\non Algorithms and Data Structures, London 193–204, 1999.\\n[17] D. Peleg. Distributed Computing: A Locality-Sensitive Approach. SIAM, 2000.\\n[18] D. Peleg. As good as it gets: Competitive fault tolerance in network structures.\\nIn Proc. 11th Symp. on Stabilization, Safety, and Security of Distributed Systems,\\nLNCS 5873, 2009, 35–46.\\n[19] D. Peleg and A.A. Scha¨ffer. Graph spanners. J. Graph Theory, 13:99–116, 1989.\\n[20] D. Peleg and J.D. Ullman. An optimal synchronizer for the hypercube. SIAM J.\\nComputing, 18(2):740–747, 1989.\\n[21] D. Peleg and E. Upfal. A trade-off between space and efficiency for routing tables.\\nJ. ACM, 36:510–530, 1989.\\n[22] L. Roditty, M. Thorup, and U. Zwick. Deterministic constructions of approximate\\ndistance oracles and spanners. In Proc. 32nd Int. Colloq. on Automata, Languages\\n& Prog., 261–272, 2005.\\n[23] L. Roditty and U. Zwick. Replacement paths and k simple shortest paths in un-\\nweighted directed graphs. ACM Trans. Algorithms ,2012.\\n[24] M. Thorup and U. Zwick. Compact routing schemes. In Proc. 14th ACM Symp. on\\nParallel Algorithms and Architecture, Hersonissos, Crete, 1–10, 2001.\\n[25] M. Thorup and U. Zwick. Approximate distance oracles. J. ACM, 52:1–24, 2005.\\n[26] M. Thorup. Undirected single-source shortest paths with positive integer weights in\\nlinear time. J. ACM, 362–394, 1999.\\n[27] V. Vazirani. Approximation Algorithms. College of Computing, Georgia Institute of\\nTechnology, 1997.\\n[28] O. Weimann and R. Yuster. Replacement paths via fast matrix multiplication. In\\nProc. 51th IEEE Symp. on Foundations of Computer Science, 2010.\\n29\\n\"}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd1c'), 'authors': 'A Abboud, A Amir, A Gajentaan, H Cohen, KG Larsen, M Patrascu, M Patrascu, R Agarwal, T Kopelowitz', 'year': '2017', 'title': 'Conditional Lower Bounds for Space/Time Tradeoffs', 'full_text': 'ar\\nX\\niv\\n:1\\n70\\n6.\\n05\\n84\\n7v\\n2 \\n [c\\ns.D\\nS]\\n  2\\n5 J\\nul \\n20\\n17\\nConditional Lower Bounds for Space/Time Tradeoffs\\nIsaac Goldstein⋆1, Tsvi Kopelowitz⋆⋆2, Moshe Lewenstein⋆ ⋆ ⋆1, and Ely Porat ⋆ ⋆ ⋆1\\n1Bar-Ilan University , {goldshi,moshe,porately}@cs.biu.ac.il\\n2University of Waterloo , kopelot@gmail.com\\nAbstract. In recent years much effort has been concentrated towards achieving polynomial time lower\\nbounds on algorithms for solving various well-known problems. A useful technique for showing such\\nlower bounds is to prove them conditionally based on well-studied hardness assumptions such as 3SUM,\\nAPSP, SETH, etc. This line of research helps to obtain a better understanding of the complexity inside\\nP.\\nA related question asks to prove conditional space lower bounds on data structures that are constructed\\nto solve certain algorithmic tasks after an initial preprocessing stage. This question received little\\nattention in previous research even though it has potential strong impact.\\nIn this paper we address this question and show that surprisingly many of the well-studied hard problems\\nthat are known to have conditional polynomial time lower bounds are also hard when concerning space.\\nThis hardness is shown as a tradeoff between the space consumed by the data structure and the time\\nneeded to answer queries. The tradeoff may be either smooth or admit one or more singularity points.\\nWe reveal interesting connections between different space hardness conjectures and present matching\\nupper bounds. We also apply these hardness conjectures to both static and dynamic problems and\\nprove their conditional space hardness.\\nWe believe that this novel framework of polynomial space conjectures can play an important role in\\nexpressing polynomial space lower bounds of many important algorithmic problems. Moreover, it seems\\nthat it can also help in achieving a better understanding of the hardness of their corresponding problems\\nin terms of time.\\n1 Introduction\\n1.1 Background\\nLately there has been a concentrated effort to understand the time complexity within P, the class of\\ndecision problems solvable by polynomial time algorithms. The main goal is to explain why certain\\nproblems have time complexity that seems to be non-optimal. For example, all known efficient\\nalgorithmic solutions for the 3SUM problem, where we seek to determine whether there are three\\nelements x, y, z in input set S of size n such that x + y + z = 0, take O˜(n2) time1. However,\\n⋆This research is supported by the Adams Foundation of the Israel Academy of Sciences and Humanities\\n⋆⋆Part of this work took place while the second author was at University of Michigan. This work is supported in\\npart by the Canada Research Chair for Algorithm Design, NSF grants CCF-1217338, CNS-1318294, and CCF-1514383\\n⋆ ⋆ ⋆This work was partially supported by an ISF grant #1278/16\\n1 The O˜ and Ω˜ notations suppress polylogarithmic factors\\nthe only real lower bound that we know is the trivial Ω(n). Likewise, we know how to solve the\\nall pairs shortest path, APSP, problem in O˜(n3) time but we cannot even determine whether it is\\nimpossible to obtain an O˜(n2) time algorithm. One may note that it follows from the time-hierarchy\\ntheorem that there exist problems in P with complexity Ω(nk) for every fixed k. Nevertheless, such\\na separation for natural practical problems seems to be hard to achieve.\\nThe collaborated effort to understand the internals of P has been concentrated on identifying\\nsome basic problems that are conjectured to be hard to solve more efficiently (by polynomial\\nfactors) than their current known complexity. These problems serve as a basis to prove conditional\\nhardness of other problems by using reductions. The reductions are reminiscent of NP-complete\\nreductions but differ in that they are restricted to be of time complexity strictly smaller (by a\\npolynomial factor) than the problem that we are reducing to. Examples of such hard problems\\ninclude the well-known 3SUM problem, the fundamental APSP problem, (combinatorial) Boolean\\nmatrix multiplication, etc. Recently, conditional time lower bounds have been proven based on\\nthe conjectured hardness of these problems for graph algorithms [4,42], edit distance [13], longest\\ncommon subsequence (LCS) [3,15], dynamic algorithms [5,36], jumbled indexing [11], and many\\nother problems [1,2,6,7,14,25,31,34,40].\\n1.2 Motivation\\nIn stark contrast to polynomial time lower bounds, little effort has been devoted to finding poly-\\nnomial space conditional lower bounds. An example of a space lower bound appears in the work\\nof Cohen and Porat [19] and Paˇtras¸cu and Roditty [38] where lower bounds are shown on the size\\nof a distance oracle for sparse graphs based on a conjecture about the best possible data structure\\nfor a set intersection problem (which we call set disjointness in order to differ it from its reporting\\nvariant).\\nA more general question is, for algorithmic problems, what conditional lower bounds of a\\nspace/time tradeoff can be shown based on the set disjointness (intersection) conjecture? Even\\nmore general is to discover what space/time tradeoffs can be achieved based on the other algorith-\\nmic problems that we assumed are hard (in the time sense)? Also, what are the relations between\\nthese identified ”hard” problems in the space/time tradeoff sense? These are the questions which\\nform the basis and framework of this paper.\\nThroughout this paper we show connections between different hardness assumptions, show some\\nmatching upper bounds and propose several conjectures based on this accumulated knowledge.\\nMoreover, we conjecture that there is a strong correlation between polynomial hardness in time\\nand space. We note that in order to discuss space it is often more natural to consider data structure\\nvariants of problems and this is the approach we follow in this paper.\\n1.3 Our Results\\nSet Disjointness. In the SetDisjointness problem mentioned before, it is required to preprocess a\\ncollection of m sets S1, · · · , Sm ⊂ U , where U is the universe of elements and the total number of\\nelements in all sets is N . For a query, a pair of integers (i, j) (1 ≤ i, j ≤ m) is given and we are asked\\nwhether Si ∩ Sj is empty or not. A folklore conjecture, which appears in [18,38], suggests that to\\nachieve a constant query time the space of the data structure constructed in the preprocessing stage\\nneeds to be Ω˜(N2). We call this conjecture the SetDisjointness conjecture. This conjecture does not\\nsay anything about the case where we allow higher query time. Therefore, we suggest a stronger\\n2\\nconjecture which admits a full tradeoff between the space consumed by the data structure (denoted\\nby S) and the query time (denoted by T ). This is what we call the Strong SetDisjointness conjecture.\\nThis conjecture states that for solving SetDisjointness with a query time T our data structure needs\\nΩ˜(N2/T 2) space. A matching upper bound exists for this problem by generalizing ideas from [18]\\n(see also [32]). Our new SetDisjointness conjecture can be used to admit more expressive space lower\\nbounds for a full tradeoff between space and query time.\\n3SUM Indexing. One of the basic and frequently used hardness conjectures is the celebrated 3SUM\\nconjecture. This conjecture was used for about 20 years to show many conditional time lower\\nbounds on various problems. However, we focus on what can be said about its space behavior. To\\ndo this, it is natural to consider a data structure version of 3SUM which allows one to preprocess\\nthe input set S. Then, the query is an external number z for which we need to answer whether\\nthere are x, y ∈ S such that x + y = z. It was pointed out by Chan and Lewenstein [16] that all\\nknown algorithms for 3SUM actually work within this model as well. We call this problem 3SUM\\nIndexing. On one hand, this problem can easily be solved using O(n2) space by sorting x+ y for all\\nx, y ∈ S and then searching for z in O˜(1) time. On the other hand, by just sorting S we can answer\\nqueries by a well-known linear time algorithm. The big question is whether we can obtain better\\nthan Ω˜(n2) space while using just O˜(1) time query? Can it be done even if we allow O˜(n1−Ω(1))\\nquery time? This leads us to our two new hardness conjectures. The 3SUM-Indexing conjecture\\nstates that when using O˜(1) query time we need Ω˜(n2) space to solve 3SUM-Indexing. In the Strong\\n3SUM-Indexing conjecture we say that even when using O˜(n1−Ω(1)) query time we need Ω˜(n2) space\\nto solve 3SUM-Indexing.\\n3SUM Indexing and Set Disjointness. We prove connections between the SetDisjointness conjectures\\nand the 3SUM-Indexing conjectures. Specifically, we show that the Strong 3SUM-Indexing conjecture\\nimplies the Strong SetDisjointness conjecture, while the SetDisjointness conjecture implies the 3SUM-\\nIndexing conjecture. This gives some evidence towards establishing the difficulty within the 3SUM-\\nIndexing conjectures. The usefulness of these conjectures should not be underestimated. As many\\nproblems are known to be 3SUM-hard these new conjectures can play an important role in achieving\\nspace lower bounds on their corresponding data structure variants. Moreover, it is interesting to\\npoint on the difference between SetDisjointness which admits smooth tradeoff between space and\\nquery time and 3SUM-Indexing which admits a big gap between the two trivial extremes. This may\\nexplain why we are unable to show full equivalence between the hardness conjectures of the two\\nproblems. Moreover, it can suggest a separation between problems with smooth space-time behavior\\nand others which have no such tradeoff but rather two ”far” extremes.\\nGeneralizations. Following the discussion on the SetDisjointness and the 3SUM-Indexing conjectures\\nwe investigate their generalizations.\\nI. k-Set Disjointness and (k+1)-SUM Indexing. The first generalization is a natural parametrization\\nof both problems. In the SetDisjointness problem we query about the emptiness of the intersection\\nbetween two sets, while in the 3SUM-Indexing problem we ask, given a query number z, whether two\\nnumbers of the input S sum up to z. In the parameterized versions of these problems we are inter-\\nested in the emptiness of the intersection between k sets and ask if k numbers sum up to a number\\ngiven as a query. These generalized variants are called k-SetDisjointness and (k+1)-SUM-Indexing\\nrespectively. For each problem we give corresponding space lower bounds conjectures which gener-\\nalize those of SetDisjointness and 3SUM-Indexing. These conjectures also have corresponding strong\\n3\\nvariants which are accompanied by matching upper bounds. We prove that the k-SetDisjointness\\nconjecture implies (k+1)-SUM-Indexing conjecture via a novel method using linear equations.\\nII. k-Reachability. A second generalization is the problem we call k-Reachability. In this problem\\nwe are given as an input a directed sparse graph G = (V,E) for preprocessing. Afterwards, for a\\nquery, given as a pair of vertices u, v, we wish to return if there is a path from u to v consisting\\nof at most k edges. We provide an upper bound on this problem for every fixed k ≥ 1. The upper\\nbound admits a tradeoff between the space of the data structure (denoted by S) and the query\\ntime (denoted by T ), which is ST 2/(k−1) = O(n2). We argue that this upper bound is tight. That\\nis, we conjecture that if query takes T time, the space must be Ω˜( n\\n2\\nT 2/(k−1)\\n). We call this conjecture\\nthe k-Reachability conjecture.\\nWe give three indications towards the correctness of this conjecture. First, we prove that the\\nbase case, where k = 2, is equivalent to the SetDisjointness problem. This is why this problem can\\nbe thought of as a generalization of SetDisjointness.\\nSecond, if we consider non-constant k then the smooth tradeoff surprisingly disappears and\\nwe get ”extreme behavior” as Ω˜( n\\n2\\nT 2/(k−1)\\n) eventually becomes Ω˜(n2). This means that to answer\\nreachability queries for non-constant path length, we can either store all answers in advance using\\nn2 space or simply answer queries from scratch using a standard graph traversal algorithm. The\\ngeneral problem where the length of the path from u to v is unlimited in length is sometimes referred\\nto as the problem of constructing efficient reachability oracles. Paˇtras¸cu in [37] leaves it as an open\\nquestion if a data structure with less than Ω˜(n2) space can answer reachability queries efficiently.\\nMoreover, Paˇtras¸cu proved that for constant time query, truly superlinear space is needed. Our k-\\nReachability conjecture points to this direction, while admitting full space-time tradeoff for constant\\nk.\\nThe third indication for the correctness of the k-Reachability conjecture comes from a connection\\nto distance oracles. A distance oracle is a data structure that can be used to quickly answer queries\\nabout the shortest path between two given nodes in a preprocessed undirected graph. As mentioned\\nabove, the SetDisjointness conjecture was used to exclude some possible tradeoffs for sparse graphs.\\nSpecifically, Cohen and Porat [19] showed that obtaining an approximation ratio smaller than 2\\nwith constant query time requires Ω˜(n2) space. Using a somewhat stronger conjecture Paˇtras¸cu\\nand Roditty [38] showed that a (2,1)-distance oracle for unweighted graphs with m = O(n) edges\\nrequires Ω˜(n1.5) space. Later, this result was strengthened by Paˇtras¸cu et al. [39]. However, these\\nresults do not exclude the possibility of compact distance oracles if we allow higher query time.\\nFor stretch-2 and stretch-3 in sparse graphs, Agarwal et. al. [9,10] achieved a space-time tradeoff\\nof S × T = O(n2) and S × T 2 = O(n2), respectively. Agarwal [8] also showed many other results\\nfor stretch-2 and below. We use our k-Reachability conjecture to prove that for stretch-less-than-\\n(1+2/k) distance oracles S × T 2/(k−1) is bounded by Ω˜(n2). This result is interesting in light of\\nAgarwal [8] where a stretch-(5/3) oracle was presented which achieves a space-time tradeoff of\\nS × T = O(n2). This matches our lower bound, where k = 3, if our lower bound would hold not\\nonly for stretch-less-than-(5/3) but also for stretch-(5/3) oracles. Consequently, we see that there\\nis strong evidence for the correctness of the k-Reachability conjecture.\\nMoreover, these observations show that on one hand k-Reachability is a generalization of Set-\\nDisjointness which is closely related to 3SUM-Indexing. On the other hand, k-Reachability is related\\nto distance oracles which solve the famous APSP problem using smaller space by sacrificing the\\naccuracy of the distance between the vertices. Therefore, the k-Reachability conjecture seems as a\\n4\\nconjecture corresponding to the APSP hardness conjecture, while also admitting some connection\\nwith the celebrated 3SUM hardness conjecture.\\nSETH and Orthogonal Vectors. After considering space variants of the 3SUM and APSP conjectures\\nit is natural to consider space variants for the Strong Exponential Time Hypothesis (SETH) and the\\nclosely related conjecture of orthogonal vectors. SETH asserts that for any ǫ > 0 there is an integer\\nk > 3 such that k-SAT cannot be solved in 2(1−ǫ)n time. The orthogonal vectors time conjecture\\nstates that there is no algorithm that for every c ≥ 1, finds if there are at least two orthogonal\\nvectors in a set of n Boolean vectors of length c log n in O˜(n2−Ω(1)) time. We discuss the space\\nvariants of these conjectures in Section 7. However, we are unable to connect these conjectures and\\nthe previous ones. This is perhaps not surprising as the connection between SETH and the other\\nconjectures even in the time perspective is very loose (see, for example, discussions in [5,25]).\\nBoolean Matrix Multiplication. Another problem which receives a lot of attention in the context\\nof conditional time lower bounds is calculating Boolean Matrix Multiplication (BMM). We give a\\ndata structure variant of this well-known problem. We then demonstrate the connection between\\nthis problem and the problems of SetDisjointness and k-Reachability.\\nApplications. Finally, armed with the space variants of many well-known conditional time lower\\nbounds, we apply this conditional space lower bounds to some static and dynamic problems. This\\ngives interesting space lower bound results on these important problems which sometimes also\\nadmits clear space-time tradeoff. We believe that this is just a glimpse of space lower bounds that\\ncan be achieved based on our new framework and that many other interesting results are expected\\nto follow this promising route.\\nFigure 1 in Appendix A presents a sketch of the results in this paper.\\n2 Set Intersection Hardness Conjectures\\nWe first give formal definitions of the SetDisjointness problem and its enumeration variant:\\nProblem 1 (SetDisjointness Problem). Preprocess a family F of m sets, all from universe U , with\\ntotal size N =\\n∑\\nS∈F |S| so that given two query sets S, S′ ∈ F one can determine if S ∩ S′ = ∅.\\nProblem 2 (SetIntersection Problem). Preprocess a family F of m sets, all from universe U , with\\ntotal size N =\\n∑\\nS∈F |S| so that given two query sets S, S′ ∈ F one can enumerate the set S ∩ S′.\\nConjectures. The SetDisjointness problem was regarded as a problem that admits space hardness.\\nThe hardness conjecture of the SetDisjointness problem has received several closely related formu-\\nlations. One such formulation, given by Paˇtras¸cu and Roditty [38], is as follows:\\nConjecture 1. SetDisjointness Conjecture [Formulation 1]. Any data structure for the SetDis-\\njointness problem where |U | = logcm for a large enough constant c and with a constant query time\\nmust use Ω˜(m2) space.\\nAnother formulation is implicitly suggested in Cohen and Porat [18]:\\n5\\nConjecture 2. SetDisjointness Conjecture [Formulation 2]. Any data structure for the SetDis-\\njointness problem with constant query time must use Ω˜(N2) space.\\nThere is an important distinction between the two formulations, which is related to the sparsity\\nof SetDisjointness instances. This distinction follows from the following upper bound: store anm×m\\nmatrix of the answers to all possible queries, and then queries will cost constant time. The first\\nformulation of the SetDisjointness conjecture states that if we want constant (or poly-logaritmic)\\nquery time, then this is the best we can do. At a first glance this makes the second formulation,\\nwhose bounds are in terms of N and not m, look rather weak. In particular, why would we ever be\\ninterested in a data structure that uses O(N2) space when we can use one with O(m2) space? The\\nanswer is that the two conjectures are the same if the sets are very sparse, and so at least in terms\\nof N , if one were to require a constant query time then by the second formulation the space must\\nbe at least Ω(N2) (which happens in the very sparse case).\\nNevertheless, we present a more general conjecture, which in particular captures a tradeoff curve\\nbetween the space usage and query time. This formulation captures the difficulty that is commonly\\nbelieved to arise from the SetDisjointness problem, and matches the upper bounds of Cohen and\\nPorat [18] (see also [32]).\\nConjecture 3. Strong SetDisjointness Conjecture. Any data structure for the SetDisjointness\\nproblem that answers queries in T time must use S = Ω˜(N\\n2\\nT 2\\n) space.\\nFor example, a natural question to ask is “what is the smallest query time possible with lin-\\near space?”. This question is addressed, at least from a lower bound perspective, by the Strong\\nSetDisjointness conjecture.\\nConjecture 4. Strong SetIntersection Conjecture. Any data structure for the SetIntersection\\nproblem that answers queries in O(T + op) time, where op is the size of the output of the query,\\nmust use S = Ω˜(N\\n2\\nT ) space.\\n3 3SUM-Indexing Hardness Conjectures\\nIn the classic 3SUM problem we are given an integer array A of size n and we wish to decide\\nwhether there are 3 distinct integers in A which sum up to zero. Gajentaan and Overmars [23]\\nshowed that an equivalent formulation of this problem receives 3 integer arrays A1, A2, and A3,\\neach of size n, and the goal is to decide if there is a triplet x1 ∈ A1, x2 ∈ A2, and x3 ∈ A3 that sum\\nup to zero.\\nWe consider the data structure variant of this problem which is formally defined as follows:\\nProblem 3 (3SUM-Indexing Problem). Preprocess two integer arrays A1 and A2, each of length n,\\nso that given a query integer z we can decide whether there are x ∈ A1 and y ∈ A2 such that\\nz = x+ y.\\nIt is straightforward to maintain all possible O(n2) sums of pairs in quadratic space, and then\\nanswer a query in O˜(1) time. On the other extreme, if one does not wish to utilize more than linear\\nspace then one can sort the arrays separately during preprocssing time, and then a query can be\\nanswered in O˜(n) time by scanning both of the sorted arrays in parallel and in opposite directions.\\nWe introduce two conjectures with regards to the 3SUM-Indexing problem, which serve as natural\\ncandidates for proving polynomial space lower bounds.\\n6\\nConjecture 5. 3SUM-Indexing Conjecture: There is no solution for the 3SUM-Indexing problem\\nwith truly subquadratic space and O˜(1) query time.\\nConjecture 6. Strong 3SUM-Indexing Conjecture: There is no solution for the 3SUM-Indexing\\nproblem with truly subquadratic space and truly sublinear query time.\\nNotice that one can solve the classic 3SUM problem using a data structure for 3SUM-Indexing\\nby preprocessing A1 and A2, and answering n 3SUM-Indexing queries on all of the values in A3.\\nNext, we prove theorems that show tight connections between the 3SUM-Indexing conjectures\\nand the SetDisjointness conjectures. We note that the proofs of the first two theorems are similar\\nto the proofs of [31], but with space interpretation.\\nTheorem 1. The Strong 3SUM-Indexing Conjecture implies the Strong SetDisjointness Conjecture.\\nProof. A family H of hash functions from [u]→ [m] is called linear if for any h ∈ H and any x, x′ ∈\\n[u], h(x) + h(x′) = h(x + x′) + ch (modm), where ch is some integer that depends only on h. H is\\ncalled almost linear if for any h ∈ H and any x, x′ ∈ [u], either h(x)+h(x′) = h(x+x′)+ch (modm),\\nor h(x) + h(x′) = h(x+ x′) + ch + 1 (modm).\\nGiven a hash function h ∈ H we say that a value i ∈ m is heavy for set S = {x1, . . . , xn} ⊂ [u]\\nif |{x ∈ S : h(x) = i}| > 3nm . H is called almost balanced if for any set S = {x1, . . . , xn} ⊂ [u], the\\nexpected number of elements from S that are hashed to heavy values is O(m). Kopelowitz et al.\\nshowed in [31] that a family of hash functions obtained from the construction of Dietzfelbinger [20]\\nis almost-linear, almost-balanced, and pair-wise independent. In order to reduce clutter in the proof\\nhere we assume the existence of linear, almost-balanced, and pair-wise independent families of hash\\nfunctions. Using the family of hash functions of Dietzfelbinger [20] will only affect multiplicative\\nconstants.\\nWe reduce an instance of the 3SUM-Indexing problem to an instance of the SetDisjointness\\nproblem as follows. Let R = nγ for some constant 0 < γ < 1. Let Q = (5n/R)2. Without loss of\\ngenerality we assume that\\n√\\nQ is an integer. We pick a random hash function h1 : U → [R] from\\na family that is linear and almost-balanced. Using h1 we create R buckets B1, . . . ,BR such that\\nBi = {x ∈ A1 : h1(x) = i}, and another R buckets C1, . . . , CR such that Ci = {x ∈ A2 : h1(x) = i}.\\nSince h1 is almost-balanced, the expected number of elements from A1 and A2 that are mapped\\nto buckets of size greater than 3n/R is O(R). We use O(R) space to maintain this list explicitly,\\ntogether with a lookup table for the elements in A1 and A2.\\nNext, we pick a random hash function h2 : U → [Q] where h2 is chosen from a pair-wise\\nindependent and linear family. For each bucket we create\\n√\\nQ shifted sets as follows: for each\\n0 ≤ j < √Q let Bi,j = {h2(x)− j ·\\n√\\nQ (modQ) |x ∈ Bi} and Ci,j = {−h2(x) + j (modQ) |x ∈ Ci}.\\nThese sets are all preprocessed into a data structure for the SetDisjointness problem.\\nNext, we answer a 3SUM-Indexing query z by utilizing the linearity of h1 and h2, which implies\\nthat if there exist x ∈ A1 and y ∈ A2 such that x+ y = z then h1(x)+h1(y) = h1(z)+ ch1 (modR)\\nand h2(x) + h2(y) = h2(z) + ch2 (modQ).\\nThus, if x ∈ Bi then y must be in Ch1(z)+ch1−i(modR). For each i ∈ [R] we would like to in-\\ntersect Bi with Ch1(z)+ch1−i(modR) in order to find candidate pairs of x and y. Denote by h\\n↑\\n2(z) =\\n⌊h2(z)+ch1√\\nQ\\n⌋ and h↓2(z) = h2(z) + ch2(mod\\n√\\nQ). Due to the almost-linearity of h2, if the sets Bi and\\nCh1(z)+ch1−i(modR)+ z are not disjoint then the sets Bi,h↑2(z) and Ch1(z)+ch1−i(modR),h↓2(z) are not dis-\\njoint (but the reverse is not necessarily true). Thus, if B\\ni,h↑2(z)\\n∩C\\nh1(z)+ch1−i(modR),h\\n↓\\n2(z)\\n= ∅ then there\\n7\\nis no candidate pair in Bi and Ch1(z)+ch1−i(modR)+z. However, if Bi,h↑2(z)∩Ch1(z)+ch1−i(modR),h↓2(z) 6= ∅\\nthen it is possible that this is due to a 3SUM-Indexing solution, but we may have false positives.\\nNotice that the number of set pairs whose intersection we need to examine is O(R) since z is given.\\nOnce we pick i (R choices) the rest is implicit.\\nSet z and let k = h2(z). Since h2 is pair-wise independent and linear then for any pair x, y ∈ U\\nwhere x 6= y we have that if x+ y 6= z then Pr[h2(x) + h2(y) = k + ch2(modR)] = Pr[h2(x+ y) =\\nh2(z) + ch2(modR)] =\\n1\\nQ . Since each bucket contains at most 3n/R elements, the probability of a\\nfalse positive due to two buckets Bi and Cj is not greater than (3nR )2 1Q = 925 . In order to reduce\\nthe probability of a false positive to be polynomially small, we repeat the process with O(log n)\\ndifferent choices of h2 functions (but using the same h1). This blows up the number of sets by a\\nfactor of O(log n), but not the universe. If the sets intersect under all O(log n) choices of h2 then we\\ncan spend O(n/R) time to find x and y within buckets Bi and Cj, which are either a 3SUM-Indexing\\nsolution (and the algorithm halts), or a false positive, which only occurs with probability 1/poly(n).\\nTo summarize, we create a total of O(R\\n√\\nQ log n) sets, each of size at most 3n/R. Thus, the\\ntotal size of the SetDisjointness instance is N = O˜(n2/R). For a query, we perform O˜(R) queries on\\nthe SetDisjointness structure, and spend another O(R · nR · 1poly(n)) = O(1) expected time to verify\\nthat we did not hit a false positive. Furthermore, we spend O(R) time to check possible solutions\\ncontaining one of the expected O(R) elements from buckets with too many elements by using the\\nlookup tables. If we denote by T (N) and S(N) the query time and space usage, respectively, of the\\nSetDisjointness data structure on N elements (in our case N = O˜(n2−γ)), then the query time of the\\nreduction becomes t3SI = O˜(R · T (n2/R)) time and the space usage is s3SI = O˜(S(n2/R) + O(n)).\\nSince we may assume that S(N) = Ω(N), we have that s3SI = O˜(S(N)).\\nBy the Strong 3SUM-Indexing Conjecture, either s3SI = Ω˜(n\\n2) or t3SI = Ω˜(n), which means that\\neither S(N) = Ω˜(N\\n2\\n2−γ ) or T (N) = Ω˜(N\\n1−γ\\n2−γ ). For any constant ǫ > 0, if the SetDisjointness data\\nstructure uses Θ˜(N\\n2\\n2−γ\\n−ǫ) space, then S(N) · (T (N))2 = Ω˜(N 22−γ−ǫ+ 2−2γ2−γ ) = Ω˜(N2−ǫ). Since this\\nholds for any ǫ > 0 it must be that S(N) · (T (N))2 = Ω˜(N2). ⊓⊔\\nTheorem 2. The Strong 3SUM-Indexing Conjecture implies the Strong SetIntersection Conjecture.\\nProof. The proof follows the same structure as the proof of Theorem 1, but here we set Q =\\n(n1+δ/R), where δ > 0 is a constant. Furthermore, we preprocess the buckets using a SetIntersection\\ndata structure, and if two sets intersect then instead of repeating the whole process with different\\nchoices of h2 (in order to reduce the probability of a false positive), we use the SetIntersection data\\nstructure to report all of the elements in an intersection, and verify them all directly.\\nAs before, set z and let k = h2(z). Since h2 is pair-wise independent and linear then for any\\npair x, y ∈ U where x 6= y we have that if x + y 6= z then Pr[h2(x) + h2(y) = k + ch2(modR)] =\\nPr[h2(x+ y) = h2(z) + ch2(modR)] =\\n1\\nQ . We now bound the expected output size from all of the\\nintersections. Since each pair of buckets imply at most (3nR )\\n2 pairs of elements, the expected size\\nof their intersection is E[|h2(Bi)− k ∩ h2(Cj)|] = (3nR )2 1Q = O(n\\n1−δ\\nR ). Thus, the expected size of the\\noutput of all of the O(R) intersections is O(R n\\nRnδ\\n) = O(n1−δ). For each pair in an intersection we\\ncan verify in constant time if together with z they form a solution.\\nTo summarize, we create a total of O(R\\n√\\nQ) sets, each of size at most 3n/R. Thus, the total\\nsize of the SetIntersection instance is N = O˜(n2/R). For a query, we perform O˜(R) queries on the\\nSetIntersection structure. Furthermore, we spend O(R) time to check possible solutions containing\\none of the expected O(R) elements from buckets with too many elements by using the lookup tables.\\n8\\nIf we denote by T (N) and S(N) the query time and space usage, respectively, of the SetIntersection\\ndata structure on N elements (in our case N = O˜(R\\n√\\nQn/R) = O˜(n\\n3+δ−γ\\n2 )), then the query time of\\nthe reduction becomes t3SI = O˜(R·T (N)+n1−δ) time and the space usage is s3SI = O˜(S(N)+O(n)).\\nSince we may assume that S(N) = Ω(N), we have that s3SI = O˜(S(N)).\\nBy the Strong 3SUM-Indexing conjecture, either s3SI = Ω˜(n\\n2) or t3SI = Ω˜(n), which means that\\neither S(N) = Ω˜(N\\n4\\n3+δ−γ ) or T (N) = Ω˜(N\\n2−2γ\\n3+δ−γ ). For any constant ǫ > 0, if the SetIntersection\\ndata structure uses Θ˜(N\\n4\\n3+δ−γ\\n−ǫ) space, then S(N)·T (N) = Ω˜(N 43+δ−γ−ǫ+ 2−2γ3+δ−γ ) = Ω˜(N2− 2δ3+δ−γ−ǫ).\\nSince this holds for any ǫ > 0 and any δ > 0 it must be that S(N) · T (N) = Ω˜(N2). ⊓⊔\\nTheorem 3. The SetDisjointness Conjecture implies the 3SUM-Indexing Conjecture.\\nProof. Given an instance of SetDisjointness, we construct an instance of 3SUM-Indexing as follows.\\nDenote with M the value of the largest element in the SetDisjointness instance. Notice that we may\\nassume that M ≤ N (otherwise we can use a straightforward renaming). For every element x ∈ U\\nthat is contained in at least one of the sets we create two integers xA and xB, which are represented\\nby 2⌈logm⌉+ ⌈logN⌉+ 3 bits each (recall that m is the number of sets).\\nThe ⌈logN⌉ least significant bits in xA represent the value of x. The following bit is a zero.\\nThe following ⌈logm⌉ bits in xA represent the index of the set containing x, and the rest of the\\n2+ ⌈logm⌉ are all set to zero. The ⌈logN⌉ least significant bits in xB represent the value of M −x.\\nThe following 2 + ⌈logm⌉ are all set to zero. The following ⌈logm⌉ bits in xB represent the index\\nof the set containing x, and the last bit is set to zero. Finally, the integer xA is added to A1 of the\\n3SUM-Indexing instance, while the integer xB is added to A2.\\nWe have created two sets of n ≤M integers. We then preprocess them to answer 3SUM-Indexing\\nqueries. Now, to answer a SetDisjointness query on sets Si and Sj, we query the 3SUM-Indexing data\\nstructure with an integer z which is determined as follows. The ⌈logN⌉ least significant bits in z\\nrepresent the value of M . The following bit is a zero. The following ⌈logm⌉ bits represent the index\\ni and are followed by a zero. The next ⌈logm⌉ bits represent the index j and the last bit is set to\\nzero.\\nIt is straightforward to verify that there exists a solution to the 3SUM-Indexing problem on z\\nif and only if the sets Si and Sj are not disjoint. Therefore, if there is a solution to the 3SUM-\\nIndexing problem with less than Ω˜(n2) space and constant query time then there is a solution for\\nthe SetDisjointness problem which refutes the SetDisjointness Conjecture. ⊓⊔\\n4 Parameterized Generalization:\\nk-Set Intersection and (k+1)-SUM\\nTwo parameterized generalizations of the SetDisjointness and 3SUM-Indexing problems are formally\\ndefined as follows:\\nProblem 4 (k-SetDisjointness Problem). Preprocess a family F of m sets, all from universe U , with\\ntotal size N =\\n∑\\nS∈F |S| so that given k query sets S1, S2, . . . , Sk ∈ F one can quickly determine if\\n∩ki=1Si = ∅.\\nProblem 5 ((k+1)-SUM-Indexing Problem). Preprocess k integer arrays A1, A2, . . . , Ak, each of length\\nn, so that given a query integer z we can decide if there is x1 ∈ A1, x2 ∈ A2, . . . , xk ∈ Ak such that\\nz =\\n∑k\\ni=1 xi.\\n9\\nIt turn out that a natural generalization of the data structure of Cohen and Porat [18] leads to\\na data structure for k-SetDisjointness as shown in the following lemma.\\nLemma 1. There exists a data structure for the k-SetDisjointness problem where the query time is\\nT and the space usage is S = O((N/T )k).\\nProof. We call the f largest sets in F large sets. The rest of the sets are called small sets. In\\nthe preprocessing stage we explicitly maintain a k-dimensional table with the answers for all k-\\nSetDisjointness queries where all k sets are large sets. The space needed for such a table is S = fk.\\nMoreover, for each set (large or small) we maintain a look-up table that supports disjointness queries\\n(with this set) in constant time. Since there are f large sets and the total number of elements is\\nN , the size of each of the small sets is at most N/f .\\nGiven a k-SetDisjointness query, if all of the query sets are large then we look up the answer\\nin the k-dimensional table. If at least one of the sets is small then using a brute-force search we\\nlook-up each of the at most O(N/f) elements in each of the other k− 1 sets. Thus, the total query\\ntime is bounded by O(kN/f), and the space usage is S = O(fk). The rest follows. ⊓⊔\\nNotice that for the case of k = 2 in Lemma 1 we obtain the same tradeoff of Cohen and Porat [18]\\nfor SetDisjointness. The following conjecture suggests that the upper bound of Lemma 1 is the best\\npossible.\\nConjecture 7. Strong k-SetDisjointness Conjecture. Any data structure for the k-SetDisjointness\\nproblem that answers queries in T time must use S = Ω˜(N\\nk\\nT k\\n) space.\\nSimilarly, a natural generalization of the Strong 3SUM-Indexing conjecture is the following.\\nConjecture 8. Strong (k+1)-SUM-Indexing Conjecture. There is no solution for the (k+1)-\\nSUM-Indexing problem with O˜(nk−Ω(1)) space and truly sublinear query time.\\nWe also consider some weaker conjectures, similar to the SetDisjointness and 3SUM-Indexing\\nconjectures.\\nConjecture 9. k-SetDisjointness Conjecture. Any data structure for the k-SetDisjointness problem\\nthat answers queries in constant time must use Ω˜(Nk) space.\\nConjecture 10. (k+1)-SUM-Indexing Conjecture. There is no solution for the (k+1)-SUM-Indexing\\nproblem with O˜(nk−Ω(1)) space and constant query time.\\nSimilar to Theorem 3, we prove the following relationship between the k-SetDisjointness conjec-\\nture and the (k+1)-SUM-Indexing conjecture.\\nTheorem 4. The k-SetDisjointness conjecture implies the (k+1)-SUM-Indexing conjecture\\nProof. Given an instance of k-SetDisjointness, we construct an instance of (k+1)-SUM-Indexing as\\nfollows. Denote by M the value of the largest element in the SetDisjointness instance. Notice that\\nwe may assume that M ≤ N (otherwise we use a straightforward renaming). For every element\\nx ∈ U that is contained in at least one of the sets we create k integers x1, x2, ..., xk, where each\\ninteger is represented by k⌈logm⌉+ (k − 1)⌈logN⌉+ 2k − 1 bits.\\nFor integer xi, if i > 1 the (k− 1)⌈logN⌉+ k− 1 least significant bits are all set to zero, except\\nfor the bits in indices (i − 2)(⌈logN⌉ + 1) + 1, ..., (i − 1)(⌈logN⌉ + 1) that represent the value of\\n10\\nx. If i = 1 the value of the bits in the indices (j − 1)(⌈logN⌉ + 1) + 1, ..., j(⌈logN⌉ + 1) is set to\\nM − x for all 1 ≤ j ≤ k − 1. The k⌈logm⌉+ k following bits are all set to zero, except for the bits\\nin indices (i− 1)(⌈logm⌉+1)+ 1, ..., i(⌈logm⌉+1) which represent the index of the set containing\\nx.\\nWe now create an instance of (k+1)-SUM-Indexing where the jth input array Aj is the set\\nof integers xj for all x ∈ U that is contained in at least one set of our family. Thus, the size\\nof each array is at most N . Now, given a k-SetDisjointness query (i1, i2, ..., ik) we must decide if\\nSi1 ∩ Si2 ∩ ...∩ Sik = ∅. To answer this query we will query the instance of (k+1)-SUM-Indexing we\\nhave created with an integer z whose binary representation is as follows: In the (k−1)⌈logN⌉+k−1\\nleast significant bits the value of the bits in the indices (j − 1)(⌈logN⌉+ 1) + 1, ..., j(⌈logN⌉+ 1)\\nis set to M for all 1 ≤ j ≤ k − 1. In the k⌈logm⌉ + k following bits, the bits at locations (j −\\n1)(⌈logm⌉+ 1) + 1, ..., j(⌈logm⌉+ 1) represent ij (for 1 ≤ j ≤ k). The rest of the bits are padding\\nzero bits (in between representations of various ijs and Ms).\\nIf Si1 ∩Si2 ∩ ...∩Sik 6= ∅ then by our construction it is straightforward to verify that the (k+1)-\\nSUM-Indexing query on z will return that there is a solution. If Si1 ∩Si2 ∩ ...∩Sik = ∅ then at least\\nfor one j ∈ [k− 1] the sum of values in the bits in indices (j− 1)(⌈logN⌉+1)+1, ..., j(⌈logN⌉+1)\\nin the (k − 1)⌈logN⌉ + k − 1 least significant bits will not be M . This is because we can view\\neach block of ⌈logN⌉+ 1 bits in the (k− 1)⌈logN⌉+ k − 1 least significant bits as solving a linear\\nequation. This equation is of the form M − x1 + xi = M for every block i − 1 where 2 ≤ i ≤ k.\\nThe solution of each of these equations is x1 = xi for all 2 ≤ i ≤ k. Consequently, a solution can be\\nfound only if there is a specific x which is contained in all of the k sets. Therefore, we get a correct\\nanswer to a k-SetDisjointness query by answering a (k+1)-SUM-Indexing query.\\nConsequently, if for some specific constant k there is a solution to the (k+1)-SUM-Indexing\\nproblem with less than Ω˜(nk) space and constant query time, then with this reduction we refute\\nthe k-SetDisjointness conjecture. ⊓⊔\\n5 Directed Reachability Oracles as a Generalization of Set Disjointness\\nConjecture\\nAn open question which was stated by Paˇtras¸cu in [37] asks if it is possible to preprocess a sparse\\ndirected graph in less than Ω(n2) space so that Reachability queries (given two query vertices u\\nand v decide whether there is a path from u to v or not) can be answered efficiently. A partial\\nanswer, given in [37], states that for constant query time truly superlinear space is necessary. In\\nthe undirected case the question is trivial and one can answer queries in constant time using linear\\nspace. This is also possible for planar directed graphs (see Holm et al. [27]).\\nWe now show that Reachability oracles for sparse graphs can serve as a generalization of the\\nSetDisjointness conjecture. We define the following parameterized version of Reachability. In the\\nk-Reachability problem the goal is to preprocess a directed sparse graph G = (V,E) so that given\\na pair of distinct vertices u, v ∈ V one can quickly answer whether there is a path from u to v\\nconsisting of at most k edges. We prove that 2-Reachability and SetDisjointness are tightly connected.\\nLemma 2. There is a linear time reduction from SetDisjointness to 2-Reachability and vice versa\\nwhich preserves the size of the instance.\\nProof. Given a graph G = (V,E) as an instance for 2-Reachability, we construct a corresponding\\ninstance of SetDisjointness as follows. For each vertex v we create the sets Vin = {u|(u, v) ∈ E} and\\n11\\nVout = {u|(v, u) ∈ E} ∪ {v}. We have 2n sets and 2m + n elements in all of them (|V | = n and\\n|E| = m). Now, a query u, v is reduced to determining if the sets Uout and Vin are disjoint or not.\\nNotice, that the construction is done in linear time and preserves the size of the instance. In the\\nopposite direction, we are given m sets S1, S2, ..., Sm having N elements in total e1, e2, ..., eN . We\\ncan create an instance of 2-Reachability in the following way. For each set Si we create a vertex vi.\\nMoreover, for each element ej we create a vertex uj . Then, for each element ej in a set si we create\\ntwo directed edges (vi, uj) and (uj , vi). These vertices and edges define a directed graph, which is\\npreprocessed for 2-Reachability queries. It is straightforward to verify that the disjointness of Si and\\nSj is equivalent to determining if there is a path of length at most 2 edges from vi to vj . Moreover,\\nthe construction is done in linear time and preserves the size of the instance. ⊓⊔\\nFurthermore, we consider k-Reachability for k ≥ 3. First we show an upper bound on the tradeoff\\nbetween space and query time for solving k-Reachability.\\nLemma 3. There exists a data structure for k-Reachability with S space and T query time such\\nthat ST 2/(k−1) = O(n2).\\nProof. Let α > 0 be an integer parameter to be set later. Given a directed graph G = (V,E), we\\ncall vertex v ∈ V a heavy vertex if deg(v) > α and a vertex u ∈ V a light vertex if deg(u) ≤ α.\\nNotice that the number of heavy vertices is at most n/α. For all heavy vertices in V we maintain\\na matrix containing the answers to any k-Reachability query between two heavy vertices. This uses\\nO(n2/α2) space.\\nNext, we recursively construct a data structure for (k-1)-Reachability. Given a query u, v, if both\\nvertices are heavy then the answer is obtained from the matrix. Otherwise, either u or v is light\\nvertex. Without loss of generality, say u is a light vertex. We consider each vertex w ∈ Nout(u)\\n(Nout(u) = {v|(u, v) ∈ E}) and query the (k-1)-Reachability data structure with the pair w, v. Since\\nu is a light node, there are no more than α queries. One of the queries returns a positive answer if\\nand only if there exists a path of length at most k from u to v.\\nDenote by S(k, n) the space used by our k-Reachability oracle on a graph with n vertices and\\ndenote by Q(k, n) the corresponding query time. In our construction we have S(k, n) = n2/α2 +\\nS(k− 1, n) and Q(k, n) = αQ(k− 1, n)+O(1). For k = 1 it is easy to construct a linear space data\\nstructure using hashing so that queries can be answered in constant time. Thus, S = S(k, n) =\\nO((k − 1)n2/α2) and T = Q(k, n) = O(αk−1). ⊓⊔\\nNotice that for the case of k = 2 the upper bounds from Lemma 3 exactly match the tradeoff of\\nthe Strong SetDisjointness Conjecture (ST 2 = O˜(n2)). We expand this conjecture by considering the\\ntightness of our upper bound for k-Reachability, which then leads to some interesting consequences\\nwith regard to distance oracles.\\nConjecture 11. Directed k-Reachability Conjecture. Any data structure for the k-Reachability\\nproblem with query time T must use S = Ω˜( n\\n2\\nT 2/(k−1)\\n) space.\\nNotice that when k is non-constant then by our upper bound Ω˜(n2) space is necessary indepen-\\ndent of the query time. This fits nicely with what is currently known about the general question\\nof Reachability oracles: either we spend n2 space and answer queries in constant time or we do no\\npreprocessing and then answer queries in linear time. This leads to the following conjecture.\\nConjecture 12. Directed Reachability Hypothesis. Any data structure for the Reachability\\nproblem must either use Ω˜(n2) space, or linear query time.\\n12\\nThe conjecture states that in the general case of Reachability there is no full tradeoff between\\nspace and query time. We believe the conjecture is true even if the path is limited to lengths of\\nsome non-constant number of edges.\\n6 Distance Oracles and Directed Reachability\\nThere are known lower bounds for constant query time distance oracles based on the SetDisjointness\\nhypothesis. Specifically, Cohen and Porat [18] showed that stretch-less-than-2 oracles need Ω(n2)\\nspace for constant queries. Patrascu et al. [39] showed a conditional space lower bound of Ω(m5/3)\\nfor constant-time stretch-2 oracles. Applying the Strong SetDisjointness conjecture to the same\\nargument as in [18] we can prove that for stretch-less-than-2 oracles the tradeoff between S (the\\nspace for the oracle) and T (the query time) is by S × T 2 = Ω(n2).\\nRecent effort was taken toward constructing compact distance oracles where we allow non-\\nconstant query time. For stretch-2 and stretch-3 Agarwal et al. [10] [9] achieves a space-time tradeoff\\nof S × T = O(n2) and S × T 2 = O(n2), respectively, for sparse graphs. Agarwal [8] also showed\\nmany other results for stretch-2 and below. Specifically, Agarwal showed that for any integer k a\\nstretch-(1+1/k) oracle exhibits the following space-time tradeoff: S × T 1/k = O(n2). Agarwal also\\nshowed a stretch-(1+1/(k+0.5)) oracle that exhibits the following tradeoff: S × T 1/(k+1) = O(n2).\\nFinally, Agarwal gave a stretch-(5/3) oracle that achieves a space-time tradeoff of S × T = O(n2).\\nUnfortunately, no lower bounds are known for non-constant query time.\\nConditioned on the directed k-Reachability conjecture we prove the following lower bound.\\nLemma 4. Assume the directed k-Reachability conjecture holds. Then stretch-less-than-(1 + 2/k)\\ndistance oracles with query time T must use S × T 2/(k−1) = Ω˜(n2) space.\\nProof. Given a graph G = (V,E) for which we want to preprocess for k-Reachability, we create a\\nlayered graph with k layers where each layer consists of a copy of all vertices of V . Each pair of\\nneighboring layers is connected by a copy of all edges in E. We omit all directions from the edges.\\nFor every fixed integer k, the layered graph has O(|V |) vertices and O(|E|) edges. Next, notice\\nthat if we construct a distance oracle that can distinguish between pairs of vertices of distance at\\nmost k and pairs of vertices of distance at least k + 2, then we can answer k-Reachability queries.\\nConsequently, assuming the k-Reachability conjecture we have that S×T 2/(k−1) = Ω(n2) for stretch-\\nless-than-(1+2/k) distance oracles (For k = 2 this is exactly the result we get by the SetDisjointness\\nhypothesis). ⊓⊔\\nNotice, that the stretch-(5/3) oracle shown by Agarwal [8] achieves a space-time tradeoff of\\nS ×T = O(n2). Our lower bound is very close to this upper bound since it applies for any distance\\noracle with stretch-less-than-(5/3), by setting k = 3.\\n7 SETH and Orthogonal Vectors Space Conjectures\\nSolving SAT using O(2n) time where n is number of variables in the formula can be easily done\\nusing only O(n) space. However, the question is how can we use space in the case that we have only\\na partial assignment of R variables and we would like to quickly figure out whether this partial\\nassignment can be completed to a full satisfying assignment or not. On one end, by using just O(n)\\nspace we can answer queries in O(2n−R) time. On the other end, we can save the answers to all\\n13\\npossible queries using O(2R) space. It is not clear if there is some sort of a tradeoff in between these\\ntwo. A related problem is the problem of Orthogonal Vectors (OV). In this problem one is given\\na collection of n vectors of length O(log n) and need to answer if there are two of them which are\\northogonal to one another. A reduction from SETH to OV was shown in [41]. By this reduction\\ngiven a k-CNF formula of n variables one can transform it using O(2ǫn) time to O(2ǫn) instances\\nof OV in which the vectors are of length 2f(k, ǫ) log n (for any ǫ > 0, where f(k, ǫ)n is the number\\nof clauses of each sparse formula represented by one instance of OV). This reduction leads to the\\nfollowing conjecture regarding OV, which is based on SETH: There is no algorithm that, for every\\nc ≥ 1, solves the OV problem on n boolean vectors of length c log n in O˜(n2−Ω(1)) time.\\nWe can consider a data structure variant of the OV problem, which we call OV indexing. Given\\na list of n boolean vectors of length c log n we should preprocess them and create a suitable data\\nstructure. Then, we answer queries of the following form: Given a vector v, is there a vector in the\\nlist which is orthogonal to v?\\nWe state the following conjecture which is the space variant of the well-studied OV (time)\\nconjecture:\\nConjecture 13. Orthogonal Vectors Indexing Hypothesis: There is no algorithm for every\\nc ≥ 1 that solves the OV indexing problem with O˜(n2−Ω(1)) space and truly sublinear query time.\\nWe note that we believe that the last conjecture is true even if we allow superpolynomial\\npreprocessing time. Moreover, it seems that it also may be true even for some constant c slightly\\nlarger than 2.\\n8 Space Requirements for Boolean Matrix Multiplication\\nBoolean Matrix Multiplication(BMM) is one of the most fundamental problems in Theoretical\\nComputer Science. The question of whether computing the Boolean product of two Boolean matrices\\nof size n × n is possible in O(n2) time is one of the most intriguing open problems. Moreover,\\nfinding a combinatorial algorithm for BMM taking O(n3−ǫ) time for some ǫ > 0 is considered to\\nbe impossible to do with current algorithmic techniques.\\nWe focus on the following data structure version of BMM, preprocess two n×n Boolean matrices\\nA and B, such that given a query (i, j) we can quickly return the value of ci,j where C = {ci,j}\\nis the Boolean produce A and B. Since storing all possible answers to queries will require θ(n2)\\nspace in the worst case, we focus on the more interesting scenario where we have only O(n2−Ω(1))\\nspace to store the outcome of the preprocessing stage. In case the input matrices are dense (the\\nnumber of ones and the number of zeroes are both θ(n2)) it seems that this can be hard to achieve\\nas storing the input matrices alone will take θ(n2) space. So we consider a complexity model, which\\nwe call the read-only input model, in which storing the input is for free (say on read-only memory),\\nand the space usage of the data structure is only related to the additional space used. We now\\ndemonstrate that BMM in the read-only input model is equivalent to SetDisjointness.\\nLemma 5. BMM in the read-only input model and SetDisjointness are equivalent.\\nProof. Given an instance of SetDisjointness let e1, ..., eN denote the elements in an input instance.\\nWe construct an instance of BMM as follows. Assume without loss of generality that all sets are\\nnot empty, and so m ≤ N . Row i in matrix A represents a set Si while each column j represents\\nelement ej . An entry ai,j equals 1 if ej ∈ Si and equals zero otherwise. We also set B = AT . We\\n14\\nalso pad each of the matrices with zeroes so their size will be N × N . Clearly, ci,j in matrix C,\\nwhich is the product of A and B, is an indicator whether Si ∩ Sj = ∅.\\nIn the opposite direction, given two matrices A and B having m ones we view each row i of A\\nas a characteristic vector of a set Si (the elements in the set correspond to the ones in that row)\\nand each column j of B as a characteristic vector of a set Sj+n (the elements in the set corresponds\\nto the ones in that column). Thus, the instance of SetDisjointness that have been created consists\\nof 2n set with O(m) elements. The value of an element ci,j in the product of A and B can be\\ndetermined by the intersection of Si and Sj+n. ⊓⊔\\nAnother interesting connection between BMM and the other problems discussed in this paper\\nis the connection to the problem of calculating the transitive closure of a graph, which is the\\ngeneral directed reachability mentioned above. It is well-known that BMM and transitive closure\\nare equivalent in terms of time as shown by Fischer and Meyer [22]. But what happens if we consider\\nspace? It is easy to see that BMM can be reduced to transitive-closure (directed reachability) even\\nin terms of space. However, the opposite direction is not clear as the reduction for time involves\\nrecursive squaring, which cannot be implemented efficiently in terms of space.\\nAnother fascinating variant of BMM is the one in which an n × n matrix A is input for pre-\\nprocessing and afterwards we need to calculate the result of multiplying it by a given query vector\\nv. This can be seen as the space variant of the celebrated OMV (online matrix-vector) problem\\ndiscussed by Henzinger et al. [25]. It is interesting to see if one can make use of a data structure so\\nthat n consecutive vector queries can be answered in O˜(n3−Ω(1)) time.\\n9 Applications\\nWe now provide applications of our rich framework for proving conditional space lower bounds. In\\nthe following subsections we consider both static and dynamic problems.\\n9.1 Static Problems\\nEdge Triangles The first example we consider is in regards to triangles. In a problem that is called\\nedge triangles detection, we are given a graph G = (V,E) to preprocess and then we are given an\\nedge (v, u) as a query and need to answer whether (u, v) belongs to a triangle. In a reporting variant\\nof this problem, called edge triangles we need not only to answer if (u, v) belongs to a triangle but\\nalso report all triangles it belongs to. This problem was considered in [12].\\nIt can be easily shown that these problems are equivalent to SetDisjointness and SetIntersection.\\nWe just construct a set Sv per each vertex v containing all its neighbors. Querying if there is a\\ntriangle containing the edge (u, v) is equivalent to asking if Sv ∩ Su is empty or not. Considering\\nthe reporting variant, reporting all triangles containing (u, v) is thus equivalent to finding all the\\nelements in Sv ∩ Su. Therefore, we get the following results:\\nTheorem 5. Assume the Strong SetDisjointness conjecture. Suppose there is a data structure for\\nedge triangles detection problem for a graph G = (V,E), with S space and query time T . Then\\nS = Ω˜(|E|2/T 2).\\nTheorem 6. Assume the Strong SetIntersection conjecture. Suppose there is a data structure for\\nedge triangles problem for a graph G = (V,E), with S space and query time O(T + op) time, where\\nop is the size of the output of the query. Then S = Ω˜(|E|2/T ).\\n15\\nHistogram Indexing A histogram, also called a Parikh vector, of a string T over alphabet Σ\\nis a |Σ|-length vector containing the character count of T . For example, for T = aaccbacab the\\nhistogram is v(T ) = (4, 2, 3). In the histogram indexing problem we preprocess an N -length string\\nT to support the following queries: given a query histogram v, return whether there is a substring\\nT ′ of T such that v(T ′) = v.\\nThis problem has received much attention in the recent years. The case where the alphabet size\\nis 2 (binary alphabet) was especially studied. A simple algorithm for this case solves the problem in\\nO(N2) preprocessing time and constant query time. There was a concentrated effort to reduce the\\nquadratic preprocessing time for some years. However, an algorithm with preprocessing time that\\nis O(N2−ǫ) for some ǫ > 0 was unknown until a recent breakthrough by Chan and Lewenstein [16].\\nThey showed an algorithm with O(N1.859) preprocessing time and constant query time. For alphabet\\nsize ℓ they obtained an algorithm with O˜(N2−δ) preprocessing time and O˜(N2/3+δ(ℓ+13)/6) query\\ntime for 0 ≤ δ ≤ 1. Regarding space complexity, it is well known how to solve histogram indexing\\nfor binary alphabet using linear space and constant query time. For alphabet size ℓ, Kociumaka\\net al. [30] presented a data structure with O˜(N2−δ) space and O˜(N δ(2ℓ−1)) query time. Chan and\\nLewenstein [16] improved their result and showed a solution by a data structure using O˜(N2−δ)\\nspace with only O˜(N δ(ℓ+1)/2) query time.\\nAmir et al. [11] proved conditional lower bound on the tradeoff between the preprocessing and\\nquery time of the histogram indexing problem. Very recently, their lower bound was improved and\\ngeneralized by Goldstein et al. [24]. Following the reduction by Goldstein et al. [24] and utilizing\\nour framework for conditional space lower bounds, we obtain the following lower bound on the\\ntradeoff between the space and query time of histogram indexing:\\nTheorem 7. Assume the Strong 3SUM-Indexing conjecture holds. The histogram indexing problem\\nfor a string of length N and constant alphabet size ℓ ≥ 3 cannot be solved with O(N2− 2(1−α)ℓ−1−α−Ω(1))\\nspace and O(N1−\\n1+α(ℓ−3)\\nℓ−1−α\\n−Ω(1)) query time, for any 0 ≤ α ≤ 1.\\nProof. We use the same reduction as in [24]. This time it will be used to reduce an instance\\nof 3SUM-Indexing (on 2n numbers) to histogram indexing, instead of reducing from an instance\\nof 3SUM. The space consumed by the reduction is dominated by the space needed to prepare a\\nhistogram indexing instance with string length N = O(n\\nℓ−2−α\\nℓ−3 ) for histogram queries. The number\\nof histogram queries we do for each query number z of the 3SUM-Indexing instance is O(nα). The\\nquery time is dominated by the time required by these queries. Let S(N, ℓ) denote the space required\\nby a data structure for histogram indexing on N -length string over alphabet size ℓ and let Q(N, ℓ)\\ndenote the query time for the same parameters. Assuming the strong 3SUM-Indexing conjecture and\\nfollowing our reduction, we have that S(N, ℓ) = O(n2−Ω(1)) and Q(N, ℓ) = O(n1−α−Ω(1)). Plugging\\nin the value of n in terms of N we get the required lower bound. ⊓⊔\\nIf we plug in the previous theorem δ = 2(1−α)ℓ−1−α , we get that if the strong 3SUM-Indexing conjecture\\nis true we cannot have a solution for histogram indexing with O˜(N2−δ) space and O˜(N δ(ℓ−2)/2) query\\ntime. This lower bound is very close to the upper bound obtained by Chan and Lewenstein [16] as\\nthere is only a gap of 32δ in the power of N in the query time. Moreover, if the value of δ becomes\\nclose to 0 (so the value of α is close to 1) the upper bound and the lower bound get even closer\\nto each other. This is very interesting, as it means that to get truly subquadratic space solution\\nfor histogram indexing for alphabet size greater than 2, we will have to spend polynomial query\\n16\\ntime. This is in stark contrast to the simple linear space solution for histogram indexing over binary\\nalphabets that supports queries in constant time.\\nFollowing reductions presented in [31], from SetIntersection or SetDisjointness to several other\\nproblems, we are able to show that based on the Strong SetDisjointness conjecture, the same prob-\\nlems admit a space/query time lower bounds. For sake of completeness, we reproduce these reduc-\\ntions in the next three subsections and show that they admit the space lower bounds as needed.\\nDistance Oracles for Colors Let P be a set of points in some metric with distance function\\nd(·, ·), where each point p ∈ P has some associated colors C(p) ⊂ [ℓ]. For c ∈ [ℓ] we denote by\\nP (c) the set of points from P with color c. We generalize d so that the distance between a point p\\nand a color c is denoted by d(p, c) = minq∈P (c){d(p, q)}. In the (Approximate) Distance Oracles for\\nVertex-Labeled Graphs problem [17,26] we are interested in preprocessing P so that given a query\\nof a point q and a color c we can return d(q, c) (or some approximation). We further generalize d\\nso that the distance between two colors c and c′ is denoted by d(c, c′) = minp∈P (c){d(p, c′)}. In the\\nDistance Oracle for Colors problem we are interested in preprocessing P so that given two query\\ncolors c and c′ we can return d(c, c′). In the Approximate Distance Oracle for Colors problem we\\nare interested in preprocessing P and some constant α > 1 so that given two query colors c and c′\\nwe can return some value dˆ such that d(c, c′) ≤ dˆ ≤ αd(c, c′).\\nWe show evidence of the hardness of the Distance Oracle for Colors problem and the Approxi-\\nmate Distance Oracle for Colors problem by focusing on the 1-D case.\\nTheorem 8. Assume the Strong SetDisjointness conjecture. Suppose there is a 1-D Distance Oracle\\nfor Colors with constant stretch α ≥ 1 for an input array of size N with S space and query time T .\\nThen S = Ω˜(N2/T 2).\\nProof. We reduce SetDisjointness to the Colored Distance problem as follows. For each set Si we\\ndefine a unique color ci. For an element e ∈ U (U is the universe of the elements in our sets) let\\n|e| denote the number of sets containing e and notice that ∑e∈U |e| = N . Since each element in U\\nappears in at most m sets, we partition U into Θ(logm) parts where the ith part Pi contains all of\\nthe elements e ∈ U such that 2i−1 < |e| ≤ 2i. An array Xi is constructed from Pi = {e1, · · · e|Pi|}\\nby assigning an interval Ij = [fj, ℓj ] in Xi to each ej ∈ Pi such that no two intervals overlap.\\nEvery interval Ij contains all the colors of sets that contain ej . This implies that |Ij| = |ej | ≤ 2i.\\nFurthermore, for each ej and ej+1 we separate Ij from Ij+1 with a dummy color d listed 2\\ni + 1\\ntimes at locations [ℓj + 1, fj+1 − 1].\\nWe can now simulate a SetDisjointness query on subsets (Si, Sj) by performing a colored distance\\nquery on colors ci and cj in each of the Θ(logm) arrays. There exists a Pi for which the two points\\nreturned from the query are at distance strictly less than 2i+1 if and only if there is an element in\\nU that is contained in both Si and Sj. The space usage is O˜(S) and the query time is O˜(T ). The\\nrest follows directly from the Strong SetDisjointness conjecture.\\nFinally, notice that the lower bound also holds for the approximate case, as for any constant α\\nthe reduction can overcome the α approximation by separating intervals using α2i + 1 listings of\\nd. ⊓⊔\\nDocument Retrieval Problems with Multiple Patterns In the Document Retrieval prob-\\nlem [35] we are interested in preprocessing a collection of documents X = {D1, · · · ,Dk} where\\n17\\nN =\\n∑\\nD∈X |D|, so that given a pattern P we can quickly report all of the documents that contain\\nP . Typically, we are interested in run time that depends on the number of documents that contain\\nP and not in the total number of occurrences of P in the entire collection of documents. In the\\nTwo Patterns Document Retrieval problem we are given two patterns P1 and P2 during query time,\\nand wish to report all of the documents that contain both P1 and P2. We consider two versions of\\nthe Two Patterns Document Retrieval problem. In the decision version we are only interested in\\ndetecting if there exists a document that contains both patterns. In the reporting version we are\\ninterested in enumerating all documents that contain both patterns.\\nAll known solutions for the Two Patterns Document Retrieval problem with non trivial prepro-\\ncessing use at least Ω(\\n√\\nN) time per query [18,28,29,35]. In a recent paper, Larsen, Munro, Nielsen,\\nand Thankachan [33] show lower bounds for the Two Patterns Document Retrieval problem con-\\nditioned on the hardness of boolean matrix multiplication.\\nIt is straightforward to see that the appropriate versions of the two pattern document retrieval\\nproblem solve the corresponding versions of the SetDisjointness and SetIntersection problems. In\\nparticular, this can be obtained by creating an alphabet Σ = F (one character for each set),\\nand for each e ∈ U we create a document that contains the characters corresponding to the sets\\nthat contain e. The intersection between Si and Sj directly corresponds to all the documents that\\ncontain both a and b. Thus, all of the lower bound tradeoffs for intersection problems are lower\\nbound tradeoffs for the two pattern document retrieval problem.\\nTheorem 9. Assume the Strong SetDisjointness conjecture. Suppose there is a data structure for\\nthe decision version of the Two Patterns Document Retrieval problem for a collection of documents\\nX where N =\\n∑\\nD∈X |D|, with S space and query time T . Then S = Ω˜(N2/T 2).\\nTheorem 10. Assume the Strong SetIntersection conjecture. Suppose there is a data structure for\\nthe reporting version of the Two Patterns Document Retrieval problem for a collection of documents\\nX where N =\\n∑\\nD∈X |D|, with S space and query time O(T +op) where op is the size of the output.\\nThen S = Ω˜(N2/T ).\\nForbidden Pattern Document Retrieval In the Forbidden Pattern Document Retrieval prob-\\nlem [21] we are also interested in preprocessing the collection of documents but this time given a\\nquery P+ and P− we are interested in reporting all of the documents that contain P+ and do not\\ncontain P−. Here too we consider a decision version and a reporting version.\\nAll known solutions for the Forbidden Pattern Document Retrieval problem with non trivial\\npreprocessing use at least Ω(\\n√\\nN) time per query [21,29]. In a recent paper, Larsen, Munro, Nielsen,\\nand Thankachan [33] show lower bounds for the Forbidden Pattern Document Retrieval problem\\nconditioned on the hardness of boolean matrix multiplication.\\nTheorem 11. Assume the Strong 3SUM-Indexing conjecture. Suppose there is a data structure\\nfor the decision version of the Forbidden Pattern Document Retrieval problem for a collection of\\ndocuments X where N =\\n∑\\nD∈X |D|, with S space and query time T . Then S = Ω˜(N2/T 4).\\nProof. We will make use of the hard instance of SetDisjointness that was used in order to prove\\nTheorem 1, and reduce this specific hard instance to the decision version of the Forbidden Pattern\\nDocument Retrieval problem. Recall that the size of this hard instance is O˜(n2−γ), the universe\\nsize is O(n2−2γ), the number of sets is O˜(n), and we need to perform O˜(nγ) SetDisjointness queries\\nin order to answer one 3SUM-Indexing query.\\n18\\nSimilar to the proof of Theorem 9 we set Σ = F (one character for each set). However, this\\ntime for each e we create a document that contains all the characters corresponding to sets Bi,j\\nthat contain e and all the characters corresponding to sets Ci,j that do not contain e.\\nThe reason that we prove our lower bound based on the Strong 3SUM-Indexing conjecture and\\nnot on the Strong SetDisjointness conjecture is because the size of our instance can become rather\\nlarge relative to N (as the number of sets that do not contain an element can be extremely large).\\nThus, the size of the Forbidden Pattern Document Retrieval instance is N = θ(n3−2γ), and the\\nnumber of queries to answer is θ(nγ). Notice that the size of the instance enforces γ to be strictly\\nlarger than 1/2. By the Strong 3SUM-Indexing conjecture, either S = s3SI = Ω˜(n\\n2) = Ω˜(N\\n2\\n3−2γ ) or\\nO(nγT ) ≥ t3SI ≥ Ω˜(n), and so T ≥ Ω˜(N\\n1−γ\\n3−2γ ). For any constant ǫ > 0, if the Forbidden Pattern\\nDocument Retrieval data structure uses Θ˜(N\\n2\\n3−2γ\\n−ǫ\\n) space, then S · T 4 = Ω˜(N 23−2γ−ǫ+ 4−4γ3−2γ ) =\\nΩ˜(N2−ǫ). Since this holds for any ǫ > 0 it must be that S · T 4 = Ω˜(N2). ⊓⊔\\nNotice that if we only allow linear space then we obtain a query time lower bound of Ω(N\\n1\\n4\\n−o(1)).\\nTheorem 12. Assume the Strong 3SUM-Indexing conjecture. Suppose there is a data structure\\nfor the reporting version of the Forbidden Pattern Document Retrieval problem for a collection of\\ndocuments X where N =\\n∑\\nD∈X |D|, with S space and query time O(T + op) where op is the size\\nof the output. Then S = Ω˜(N2/T ).\\nProof. Our proof is similar to the proof of Theorem 11, only this time we use the hard instance\\nof SetIntersection from Theorem 2. So the number of queries is O˜(nγ), the size of the universe is\\nO(n1+δ−γ), the number of sets is O˜(n\\n1+δ+γ\\n2 ), and the total size of the output is Θ(n2−δ).\\nThus, the size of the Forbidden Pattern Document Retrieval instance isN = Θ(n1+δ−γn\\n1+δ+γ\\n2 ) =\\nΘ(n\\n3+3δ−γ\\n2 ), and the number of queries to answer is θ(nγ). Notice that the size of the instance en-\\nforces 3δ− γ < 1. By the Strong 3SUM-Indexing conjecture, either S = s3SI = Ω˜(n2) = Ω˜(N\\n4\\n3+3δ−γ )\\nor O(nγT ) ≥ t3SI ≥ Ω˜(n), and so T ≥ Ω˜(N\\n2−2γ\\n3+3δ−γ ). For any constant ǫ > 0, if the Forbidden Pattern\\nDocument Retrieval data structure uses Θ˜(N\\n4\\n3+3δ−γ\\n−ǫ\\n) space, then S ·T = Ω˜(N 43+3δ−γ−ǫ+ 2−2γ3+3δ−γ ) =\\nΩ˜(N\\n2− 6δ\\n3+3δ−γ\\n−ǫ\\n). Since this holds for any ǫ > 0 and since we can make 6δ3+3δ−γ as small as we like,\\nit must be that S · T = Ω˜(N2). ⊓⊔\\n9.2 Dynamic Problems\\nWe show space lower bounds on dynamic problems. Lower bounds for these problems from the time\\nperspective were considered by Abboud and Vassilevska-Williams [5]. The first dynamic problem\\nwe consider is st-SubConn which is defined as follows. Given an undirected graph G = (V,E), two\\nfixed vertices s and t and a set S ⊆ V , answer whether s and t are connected using vertices form\\nS only. Vertices can be added or removed from S.\\nThe SetDisjointness problem can be reduced to st-SubConn. Given an instance of SetDisjointness\\nwe create an undirected graph G = (V,E) as follows. We first create two unique vertices s and t.\\nThen, for each set Si we create two vertices vi and ui and for each element ej we create a vertex wj .\\nMoreover, we define E = {(vi, wj)|ej ∈ Si} ∪ {(ui, wj)|ej ∈ Si} ∪ {(s, vi)|1 ≤ i ≤ m} ∪ {(ui, t)|1 ≤\\ni ≤ m}. Initially the set S contains s and t and all the wis. Given a query (i, j) asking about the\\nemptiness of Si ∩ Sj, we add vi and uj to the set S. Then, we ask if s and t are connected, if so we\\n19\\nknow that Si ∩ Sj is not empty as the only way to get from s to t is following vi and uj and some\\nnode representing a common element of Si and Sj . If s and t are not connected then it is clear\\nthat the intersection is empty. After the query we remove the two vertices we have added so other\\nqueries can be handled properly. By this construction we get the following result:\\nTheorem 13. Assume the Strong SetDisjointness conjecture. Suppose there is a data structure for\\nst-SubConn problem for a graph G = (V,E), with S space and update and query time T . Then\\nS = Ω˜(|E|2/T 2).\\nThere are other dynamic problems that st-SubConn can be efficiently reduced to, as shown by\\nAbboud and Vssilevska-Williams [5]. This includes the following 3 problems:\\nProblem 6. (s,t)-Reachability (st-Reach). Maintain a directed graph G = (V,E) subject to\\nedge insertions and deletions, so that queries about the reachability of fixed vertices s and t can be\\nanswered quickly.\\nProblem 7. Bipartite Perfect Matching (BPMatch). Preprocess and maintain undirected bi-\\npartite graph G = (V,E) subject to edge insertions and deletions, so that we can quickly answer if\\nthe graph has perfect matching.\\nProblem 8. Strong Connectivity (SC). Preprocess and maintain directed graph G = (V,E)\\nsubject to edge insertions and deletions, so that we can quickly answer if the graph is strongly\\nconnected\\nUsing our last theorem and the reductions by Abboud and Vassilevska-Williams [5], noting that\\nthey do not effect the space usage, we get the following:\\nTheorem 14. Assume the Strong SetDisjointness conjecture. Suppose there is a data structure for\\nst-Reach/ BPMatch/ SC problem for a graph G = (V,E), with S space and update and query time\\nT . Then S = Ω˜(|E|2/T 2).\\nWe can get better lower bound for these 3 problems on sparse graphs based on the directed\\nreachability conjecture. Given a sparse graph G = (V,E) as an instance of directed reachability we\\ncan reduce it to an instance of st-Reach by just adding to special nodes s and t to the graph. Then,\\nwe can answer queries of the form ”Is v reachable from u?” by inserting two edges (s, v) and (u, t)\\nand asking if t is reachable from s. After the query we can restore the initial state by deleting these\\ntwo edges. Thus, by using the reductions from st-Reach to BPMatch and SC as shown in [5], we\\nget the following hardness result:\\nTheorem 15. Assume the Directed Reachability conjecture. Any data structure for the st-Reach/\\nBPMatch/ SC problem on sparse graphs can not have O˜(n1−Ω(1)) update and query time and\\nO˜(n2−Ω(1)) space.\\nReferences\\n1. Amir Abboud, Arturs Backurs, Thomas Deuholm Hansen, Virginia Vassilevska Williams, and Or Zamir. Subtree\\nisomorphism revisited. In Proc. of 27th ACM-SIAM Symposium on Discrete Algorithms, SODA, pages 1256–1271,\\n2016.\\n20\\n2. Amir Abboud, Arturs Backurs, and Virginia Vassilevska Williams. If the current clique algorithms are optimal,\\nso is Valiant’s parser. 55th IEEE Annual Symposium on Foundations of Computer Science, FOCS, pages 98–117,\\n2015.\\n3. Amir Abboud, Arturs Backurs, and Virginia Vassilevska Williams. Quadratic-time hardness of LCS and other\\nsequence similarity measures. 55th IEEE Annual Symposium on Foundations of Computer Science, FOCS, pages\\n59–78, 2015.\\n4. Amir Abboud, Fabrizio Grandoni, and Virginia Vassilevska Williams. Subcubic equivalences between graph\\ncentrality problems, APSP and diameter. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium\\non Discrete Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015, pages 1681–1697, 2015.\\n5. Amir Abboud and Virginia Vassilevska Williams. Popular conjectures imply strong lower bounds for dynamic\\nproblems. In 55th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2014, Philadelphia,\\nPA, USA, October 18-21, 2014, pages 434–443, 2014.\\n6. Amir Abboud, Virginia Vassilevska Williams, and Oren Weimann. Consequences of faster alignment of sequences.\\nIn Automata, Languages, and Programming - 41st International Colloquium, ICALP 2014, Copenhagen, Den-\\nmark, July 8-11, 2014, Proceedings, Part I, pages 39–51, 2014.\\n7. Amir Abboud, Virginia Vassilevska Williams, and Huacheng Yu. Matching triangles and basing hardness on an\\nextremely popular conjecture. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of\\nComputing, STOC 2015, Portland, OR, USA, June 14-17, 2015, pages 41–50, 2015.\\n8. Rachit Agarwal. The space-stretch-time tradeoff in distance oracles. In Algorithms - ESA 2014 - 22th Annual\\nEuropean Symposium on Algorithms, Wroclaw, Poland, September 8-10, 2014. Proceedings, pages 49–60, 2014.\\n9. Rachit Agarwal, Brighten Godfrey, and Sariel Har-Peled. Faster approximate distance queries and compact\\nrouting in sparse graphs. CoRR, abs/1201.2703, 2012.\\n10. Rachit Agarwal, Philip Brighten Godfrey, and Sariel Har-Peled. Approximate distance queries and compact rout-\\ning in sparse graphs. In INFOCOM 2011. 30th IEEE International Conference on Computer Communications,\\npages 1754–1762, 2011.\\n11. Amihood Amir, Timothy M. Chan, Moshe Lewenstein, and Noa Lewenstein. On hardness of jumbled indexing. In\\nAutomata, Languages, and Programming - 41st International Colloquium, ICALP 2014, Copenhagen, Denmark,\\nJuly 8-11, 2014, Proceedings, Part I, pages 114–125, 2014.\\n12. Amihood Amir, Tsvi Kopelowitz, Avivit Levy, Seth Pettie, Ely Porat, and B. Riva Shalom. Mind the gap. CoRR,\\nabs/1503.07563, 2015.\\n13. Arturs Backurs and Piotr Indyk. Edit distance cannot be computed in strongly subquadratic time (unless SETH\\nis false). In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC 2015,\\nPortland, OR, USA, June 14-17, 2015, pages 51–58, 2015.\\n14. Karl Bringmann. Why walking the dog takes time: Frechet distance has no strongly subquadratic algorithms\\nunless SETH fails. In 55th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2014, Philadel-\\nphia, PA, USA, October 18-21, 2014, pages 661–670, 2014.\\n15. Karl Bringmann and Marvin Ku¨nnemann. Quadratic conditional lower bounds for string problems and dynamic\\ntime warping. 55th IEEE Annual Symposium on Foundations of Computer Science, FOCS, 2015.\\n16. Timothy M. Chan and Moshe Lewenstein. Clustered integer 3SUM via additive combinatorics. In Proceedings\\nof the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC 2015, Portland, OR, USA,\\nJune 14-17, 2015, pages 31–40, 2015.\\n17. Shiri Chechik. Improved distance oracles and spanners for vertex-labeled graphs. In ESA 2012, pages 325–336,\\n2012.\\n18. Hagai Cohen and Ely Porat. Fast set intersection and two-patterns matching. Theor. Comput. Sci., 411(40-\\n42):3795–3800, 2010.\\n19. Hagai Cohen and Ely Porat. On the hardness of distance oracle for sparse graph. CoRR, abs/1006.1117, 2010.\\n20. M. Dietzfelbinger. Universal hashing and k-wise independent random variables via integer arithmetic without\\nprimes. In Proceedings 13th Annual Symposium on Theoretical Aspects of Computer Science (STACS), pages\\n569–580, 1996.\\n21. Johannes Fischer, Travis Gagie, Tsvi Kopelowitz, Moshe Lewenstein, Veli Ma¨kinen, Leena Salmela, and Niko\\nVa¨lima¨ki. Forbidden patterns. In LATIN, pages 327–337, 2012.\\n22. Michael J. Fischer and Albert R. Meyer. Boolean matrix multiplication and transitive closure. In 12th Annual\\nSymposium on Switching and Automata Theory, East Lansing, Michigan, USA, October 13-15, 1971, pages 129–\\n131, 1971.\\n23. A. Gajentaan and M. H. Overmars. On a class of O(n2) problems in computational geometry. Comput. Geom.,\\n5:165–185, 1995.\\n21\\n24. Isaac Goldstein, Tsvi Kopelowitz, Moshe Lewenstein, and Ely Porat. How hard is it to find (honest) witnesses?\\nIn European Symposium on Algorithms, ESA 2016, pages 45:1–45:16, 2016.\\n25. Monika Henzinger, Sebastian Krinninger, Danupon Nanongkai, and Thatchaphol Saranurak. Unifying and\\nstrengthening hardness for dynamic problems via the online matrix-vector multiplication conjecture. In Pro-\\nceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC 2015, Portland, OR,\\nUSA, June 14-17, 2015, pages 21–30, 2015.\\n26. Danny Hermelin, Avivit Levy, Oren Weimann, and Raphael Yuster. Distance oracles for vertex-labeled graphs.\\nIn Automata, Languages, and Programming - 38th International Colloquium, ICALP (2), pages 490–501, 2011.\\n27. Jacob Holm, Eva Rotenberg, and Mikkel Thorup. Planar reachability in linear space and constant time. CoRR,\\nabs/1411.5867, 2014.\\n28. Wing-Kai Hon, Rahul Shah, Sharma V. Thankachan, and Jeffrey Scott Vitter. String retrieval for multi-pattern\\nqueries. In SPIRE, pages 55–66, 2010.\\n29. Wing-Kai Hon, Rahul Shah, Sharma V. Thankachan, and Jeffrey Scott Vitter. Document listing for queries with\\nexcluded pattern. In CPM, pages 185–195, 2012.\\n30. Tomasz Kociumaka, Jakub Radoszewski, and Wojciech Rytter. Efficient indexes for jumbled pattern matching\\nwith constant-sized alphabet. In ESA, pages 625–636, 2013.\\n31. Tsvi Kopelowitz, Seth Pettie, and Ely Porat. Higher lower bounds from the 3SUM conjecture. In To appear in\\n27th ACM-SIAM Symposium on Discrete Algorithms (SODA) 2016.\\n32. Tsvi Kopelowitz, Seth Pettie, and Ely Porat. Dynamic set intersection. In Proceedings 14th Int’l Symposium on\\nAlgorithms and Data Structures (WADS), pages 470–481, 2015.\\n33. Kasper Green Larsen, J. Ian Munro, Jesper Sindahl Nielsen, and Sharma V. Thankachan. On hardness of several\\nstring indexing problems. In CPM, pages 242–251, 2014.\\n34. Kasper Green Larsen, J. Ian Munro, Jesper Sindahl Nielsen, and Sharma V. Thankachan. On hardness of several\\nstring indexing problems. Theor. Comput. Sci., 582:74–82, 2015.\\n35. S. Muthukrishnan. Efficient algorithms for document retrieval problems. In SODA, pages 657–666, 2002.\\n36. Mihai Patrascu. Towards polynomial lower bounds for dynamic problems. In Proceedings of the 42nd ACM\\nSymposium on Theory of Computing, STOC 2010, Cambridge, Massachusetts, USA, 5-8 June 2010, pages 603–\\n610, 2010.\\n37. Mihai Patrascu. Unifying the landscape of cell-probe lower bounds. SIAM J. Comput., 40(3):827–847, 2011.\\n38. Mihai Patrascu and Liam Roditty. Distance oracles beyond the Thorup-Zwick bound. SIAM J. Comput.,\\n43(1):300–311, 2014.\\n39. Mihai Patrascu, Liam Roditty, and Mikkel Thorup. A new infinity of distance oracles for sparse graphs. In 53rd\\nAnnual IEEE Symposium on Foundations of Computer Science, FOCS 2012, New Brunswick, NJ, USA, October\\n20-23, 2012, pages 738–747, 2012.\\n40. Mihai Patrascu and Ryan Williams. On the possibility of faster SAT algorithms. In Proceedings of the Twenty-\\nFirst Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2010, Austin, Texas, USA, January 17-19,\\n2010, pages 1065–1075, 2010.\\n41. Ryan Williams. A new algorithm for optimal 2-constraint satisfaction and its implications. Theor. Comput. Sci.,\\n348(2-3):357–365, 2005.\\n42. Virginia Vassilevska Williams and Ryan Williams. Subcubic equivalences between path, matrix and triangle\\nproblems. In 51th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2010, October 23-26,\\n2010, Las Vegas, Nevada, USA, pages 645–654, 2010.\\n22\\nAppendix\\nA Sketch of the Main Results\\nStrong 3SUM Indexing 3SUM Indexing\\nStrong Set Disjointness Set Disjointness\\nk-Reachability\\nStrong (k+1)-SUM Indexing (k+1)-SUM Indexing\\nStrong k-Set Disjointness k-Set Disjointness\\nLess-than-(1+2/k) Distance Oracles\\n2-Reachability\\nStatic ProblemsDynamic Problems\\nDirected Reachability\\nOrthogonal Vecors and SETH\\nBoolean Matrix Multiplication\\nStrong Set Intersection\\nFig. 1. Space conjectures and the connections between them as shown in this paper. Rectangles represent conjectures,\\nwhile problems shown to be hard based on these conjectures are represented by ovals. Full arrow represents a reduction\\nbetween two problems which also means an implication in the case of conjectures. Dotted arrow means a generalization\\nwhich is also an implication, while dashed line means a generalization with no (known) reduction.\\n23\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd1d'), 'authors': 'Beame Paul, Bhattacharya Anup, Bhattacharya Anup, Bishnu Arijit, Müller Moritz, Sipser Michael, Thurley Marc, Timothy, Timothy, Williams Richard Ryan, Williams Virginia Vassilevska', 'year': '2017', 'title': 'Fine-Grained Reductions from Approximate Counting to Decision', 'full_text': 'arXiv:1707.04609v3  [cs.DS]  23 Nov 2020Fine-grained reductionsfrom approximate counting to decisionHOLGER DELL, Goethe University Frankfurt, Germany, IT University of Copenhagen, Denmark, and Ba-sic Algorithms Research Copenhagen (BARC), DenmarkJOHN LAPINSKAS, University of Bristol, UKIn this paper, we introduce a general framework for fine-grained reductions of approximate counting prob-lems to their decision versions. (Thus we use an oracle that decides whether any witness exists to multi-plicatively approximate the number of witnesses with minimal overhead.) This mirrors a foundational resultof Sipser (STOC 1983) and Stockmeyer (SICOMP 1985) in the polynomial-time setting, and a similar resultof Müller (IWPEC 2006) in the FPT setting. Using our framework, we obtain such reductions for some ofthe most important problems in fine-grained complexity: the Orthogonal Vectors problem, 3SUM, and theNegative-Weight Triangle problem (which is closely related to All-Pairs Shortest Path). While all these prob-lems have simple algorithms over which it is conjectured that no polynomial improvement is possible, ourreductions would remain interesting even if these conjectures were proved; they have only polylogarithmicoverhead, and can therefore be applied to subpolynomial improvements such as the =3/exp(Θ(√log=))-timealgorithm for the Negative-Weight Triangle problem due to Williams (STOC 2014). Our framework is alsogeneral enough to apply to versions of the problems for which more efficient algorithms are known. For ex-ample, the Orthogonal Vectors problem over GF (<)3 for constant< can be solved in time = · poly(3) by aresult of Williams and Yu (SODA 2014); our result implies that we can approximately count the number oforthogonal pairs with essentially the same running time.We also provide a fine-grained reduction from approximate #SAT to SAT. Suppose the Strong ExponentialTime Hypothesis (SETH) is false, so that for some 1 < 2 < 2 and all : there is an $ (2=)-time algorithm for:-SAT. Then we prove that for all : , there is an $ ((2 + > (1))=)-time algorithm for approximate #:-SAT. Inparticular, our result implies that the Exponential Time Hypothesis (ETH) is equivalent to the seemingly-weaker statement that there is no algorithm to approximate #3-SAT to within a factor of 1+Y in time 2> (=)/Y2(taking Y > 0 as part of the input).CCS Concepts: • Theory of computation→ Problems, reductions and completeness; Graph algorithms anal-ysis; • Mathematics of computing → Graph algorithms.Additional Key Words and Phrases: Fine-grained complexity, Approximate Counting, Satisfiability1 INTRODUCTIONIt is clearly at least as hard to count objects as it is to decide their existence, and often it is harder. Fora concrete example, there is a polynomial-time algorithm to find a perfect matching in a bipartitegraph if one exists, but computing the exact number of all perfect matchings is a #P-completeproblem [32], which means that solving it in polynomial time would collapse the polynomial-timehierarchy [30]. However, the situation changes substantially if we consider approximate ratherthan exact counting. For all real Y with 0 < Y < 1, we say that G ∈ R is an Y-approximation to# ∈ R if |G − # | ≤ Y# holds. Since the approximation guarantee is multiplicative, computingan Y-approximation to # is at least as hard as deciding whether # > 0 holds. In fact, these twotasks are often roughly equally hard, and indeed this is true for our example: Jerrum, Sinclair, andVigoda [20] proved that an Y-approximation to the number of perfectmatchings in a bipartite graphcan be computed in polynomial time. While there is a polynomial-time algorithm to find perfectmatchings in bipartite graphs and one to approximately count them, there is still an importantAuthors’ addresses: Holger Dell, Goethe University Frankfurt, Frankfurt, Germany, IT University of Copenhagen, Copen-hagen, Denmark, Basic Algorithms Research Copenhagen (BARC), Copenhagen, Denmark, dell@cs.uni-frankfurt.de; JohnLapinskas, University of Bristol, Bristol, UK, john.lapinskas@bristol.ac.uk.2 Holger Dell and John Lapinskasdiscrepancy: The former algorithm runs in quasi-linear time while the latter runs in time Y−2 ·$̃ (=10).This paper is concernedwith fine-grained complexity, in which one considers the exact runningtime of an algorithm rather than broad categories such as polynomial time, FPT time, or subex-ponential time. Reductions that solve an approximate counting problem by means of an oraclefor its decision version have been studied already in various different contexts. Sipser [27] andStockmeyer [28] proved implicitly that every problem in #P has a polynomial-time randomisedY-approximation algorithm that has access to an NP-oracle; the result is later made explicit byValiant and Vazirani [33]. In parameterised complexity, Müller [22] proved an analogue of thisresult for the W-hierarchy: In particular, for every problem in #W[1], there is a randomised algo-rithm that has access to some W[1]-oracle, runs in time 5 (:) · poly(=, Y−1) for some computable5 : N → N, and outputs an Y-approximation to the problem. Finally, in the exponential-timesetting, Thurley [29] proposed a reduction for :-SAT that implies: If there is an $∗ (2(1−X)=)-timealgorithm for :-SAT for some X > 0, then there is an Y-approximation algorithm for #:-SAT thatruns in time Y−2 ·$∗ (2(1−X/2)=). (This reduction was later improved by Schmitt and Wanka [25].)Such results are an important foundation of the wider complexity theory of approximate countinginitiated by Dyer, Goldberg, Greenhill and Jerrum [12]. However, all of these reductions introducesignificant overheads to the running time — they are not fine-grained.Perhaps the most important polynomial-time problems in fine-grained complexity are orthog-onal vectors (OV), 3SUM, and all-pairs shortest paths (APSP). All three problems admit well-studied notions of hardness, in the sense that many problems reduce to them or are equivalentto them under fine-grained reductions, and they are not known to reduce to one another. See Vas-silevska Williams [34] for a recent survey. It is not clear what a “canonical” counting version ofAPSP should be, but it is equivalent to the Negative-Weight Triangle problem (NWT) under sub-cubic reductions [35], so we consider this instead. We give highly efficient fine-grained reductionsfrom approximate counting to decision for all three problems. All of these results are immediatecorollaries of an algorithmwhich counts edges in a bipartite graph to which it has limited oracle ac-cess; this algorithm has several additional applications, including some new approximate countingalgorithms for related problems. We discuss our edge-counting framework further in Section 1.1,and describe its applications in Section 1.2 together with a detailed overview of the literature.The most important exponential-time problem in fine-grained complexity is unequivocally SAT.We provide a fine-grained reduction from approximate #:-SAT to $ (: log2 :)-SAT as : → ∞; asa corollary, we show that if the Strong Exponential Time Hypothesis (SETH) is false, then the sav-ings from decision :-SAT as : → ∞ can be passed on to approximate #:-SAT with subexponentialoverhead. Our reduction also implies that the Exponential Time Hypothesis (ETH) is equivalentto an approximate counting version. We discuss the reduction and its corollaries further in Sec-tion 1.3.1.1 Approximately Counting Edges in Bipartite GraphsLet \\x1c be a bipartite graph with \\x1c = (* ,+ , \\x1a). We consider a computation model where the algo-rithm is given* and+ , and can access the edges of the graph only via its adjacency oracle and itsindependence oracle:• The adjacency oracle of \\x1c is the function adj\\x1c : * × + → {0, 1} such that adj\\x1c (D, E) = 1 ifand only if (D, E) ∈ \\x1a.• The independence oracle of \\x1c is the function ind\\x1c : 2*∪+ → {0, 1} such that ind\\x1c (() = 1 ifand only if ( is an independent set in \\x1c .Fine-grained reductions from approximate counting to decision 3Of course, the adjacency oracle can be simulated with the independence oracle by querying sets oftwo vertices. We distinguish them here, because we wish to think of independence queries as veryexpensive, and we will use them only polylogarithmically often. Our main result is as follows:Theorem 1. There is a randomised algorithm A which, given a rational number Y with 0 <Y < 1 and oracle access to an =-vertex bipartite graph \\x1c , outputs an Y-approximation of |\\x1a (\\x1c) |with probability at least 2/3. Moreover, A runs in time Y−2 ·$ (= log4 = log log=) and makes at mostY−2 ·$ (log5 = log log=) calls to the independence oracle.We prove this result in Section 4. Note that since oracle calls are constant time operations, theadjacency oracle is called at most Y−2 · $̃ (=) times. Moreover, a polynomial factor of Y−1 in therunning time is to be expected, since the exact value of |\\x1a (\\x1c) | can be recovered by taking Y =1/(2=2).In independent work, Beame et al. [3] obtain a result similar to Theorem 1, with an overallrunning time of Y−4 ·$̃ (1) but with no further bound on the number of independence queries used.Thus their result outperforms Theorem 1 when independence queries are fast, and underperformswhen they are slow. In all our applications, independence queries are so slow as to dominate ourrunning times; thus substituting Beame et al.’s result for Theorem 1 would yield worse algorithms.While Theorem 1 is not able to deal with the non-bipartite case at all, Beame et al. [3] presenta second algorithm in their paper that is able to approximately count edges in general graphsby using $̃ (=2/3) queries to the independence oracle. Recently, Chen, Levi, and Waingarten [10]improve the number of queries to $̃ (√=) and prove unconditionally that this is optimal for generalgraphs.In work subsequent to this paper, using a different technique, the authors and Meeks [11] wereable to remove the adjacency queries from Theorem 1 while retaining a bound of Y−2 log$ (1) = forthe number of independence queries. Moreover, they generalise the theorem to:-partite :-uniformhypergraphs, where the bound on the number of queries is at most Y−2 log$ (:) =, and extend theresult to cover approximately-uniform sampling. This generalisation has consequences for thefine-grained complexity of problems that do not directly correspond to bipartite graphs, such asapproximately counting graph motifs. Independently, Bhattacharya et al. [4, 5] generalise Theo-rem 1 to :-partite :-uniform hypergraphs, obtaining a somewhat weaker bound of Y−4 log$ (:) =on the number of queries. Moreover, Bishnu et al. [6] use the generalised oracle to solve variousdecision problems in parameterised complexity.1.2 Corollaries for Problems in PAs described in Section 1, the problems Orthogonal Vectors (OV), 3SUM, and Negative-Weight Tri-angle (NWT) are central players in the field of fine-grained complexity. All three problems havesimple polynomial-time exhaustive-search algorithms over which it is conjectured that no trulypolynomial improvement is possible. The same exhaustive search algorithms also solve the canon-ical counting versions of these problems. Nevertheless it is possible that the decision version hasfaster algorithms while the exact counting version does not. Our results imply that any improve-ment to decision algorithms transfers to the approximate counting version of the problem as well,up to polylogarithmic factors in the running time.In fact, for OV [1] and NWT [37], non-trivial (subpolynomial) improvements over exhaustivesearch algorithms are already known. Our results transfer these improvements to approximatecounting. In the case of the standard version of OV, this turns out to be uninteresting as the deran-domisation of [1] due to Chan and Williams [9] already solves the exact counting version. How-ever in the case of NWT, we are not aware of improved algorithms for the counting version; usingour reduction, we obtain such an algorithm for approximate counting. Our reductions also apply4 Holger Dell and John Lapinskasto several variants of the three central problems, yielding more new algorithms. Notably, for onevariant of OV we obtain a quasilinear-time approximate counting algorithm, but all exact countingalgorithms require quadratic time under SETH. In the following, we state our results formally.1.2.1 OV. In the orthogonal vectors problem OV, we are given two lists \\x16 and \\x17 of zero-onevectors over R3 , and must determine whether there exists an orthogonal pair (0, 1) ∈ \\x16 × \\x17. In#OV, we must instead determine the number of orthogonal pairs. Writing = = |\\x16| + |\\x17 |, it is easy tosee that OV and #OV can both be solved in$ (=23) operations by iterating over all pairs. The low-dimension OV conjecture [14, 36] asserts that in the case where3 = l (log=), there is no randomisedalgorithm that solves OV in time $ (=2−X ), for any constant X > 0. This conjecture is implied bythe Strong Exponential Time Hypothesis (SETH) [36], and Abboud, Williams, and Yu [1] provedthat it fails when 3 = $ (log=).To reduce the approximate version of #OV to OV, we model the instance as a bipartite graph andapply the edge estimation algorithm from Theorem 1. Indeed, the list \\x16 becomes the left side ofthe graph, \\x17 the right side, and each orthogonal pair (0, 1) becomes an edge. Then approximatelycounting orthogonal pairs reduces to estimating the number of edges in this graph, adjacencyqueries take time $ (3) and correspond to computing the inner product of two vectors, and inde-pendence queries are simulated by invoking the assumed decision algorithm for OV. In this way,in Section 5.2 we obtain the following structural complexity result as a corollary to Theorem 1.Theorem 2. If OV with = vectors in 3 dimensions has a randomised algorithm that runs in time) (=,3), then there is a randomised Y-approximation algorithm for #OV that runs in time ) (=,3) ·Y−2$ (log6 = log log=).In particular, if Y−1 is at most polylogarithmic in =, Theorem 2 implies we can Y-approximate#OV with only polylogarithmic overhead over decision.While OV has a non-trivial algorithm [1] with running time =2−1/$ (log(3/log=)) as we mentionedin Section 1.2, it has already been adapted into an exact #OV algorithm with the same runningtime [9], so Theorem 2 does not yield a new algorithm at the moment. However, any further im-provement for the decision version of the problemwill immediately translate to a new approximatecounting algorithm.Interestingly, there is a variant of OV for which our method does yield a new algorithm; inthis variant, the real zero-one vectors are replaced by arbitrary vectors over a finite field or overthe integers modulo<. Even though Williams and Yu [39] did not consider the counting versionand their algorithms do not seem to generalise to counting, we can nevertheless use their decisionalgorithm as a black box to obtain an efficient approximate counting algorithm as a corollary toTheorem 1.Theorem 3. Let< = ?: be a constant prime power. There is a randomised Y-approximation algo-rithm for #OV over GF (<)3 with running time Y−23 (?−1): · $̃ (=), and for #OV over (Z/<Z)3 withrunning time Y−23<−1 · $̃ (=).If Y−1 and 3 are at most polylogarithmic in =, and< is constant, these algorithms run in quasilin-ear time. Note that under SETH, any exact counting algorithm for #OV over (Z/<Z)3 requires timeΩ(=2−> (1) ) [38]; we have therefore proved a separation between approximate and exact counting.(As an aside, this implies that the factor of Y−2 in the running time of Theorem 1 cannot be droppedto Y−1/2+> (1) under SETH.) Williams and Yu showed that their algorithm’s dependence on 3 is closeto best possible under SETH, and this hardness result of course applies to approximate countingas well.Fine-grained reductions from approximate counting to decision 51.2.2 3SUM. In the 3SUM problem, we are given three integer lists \\x16, \\x17, and \\x18 of total length =and must decide whether there exists a tuple (0, 1,2) ∈ \\x16 × \\x17 × \\x18 with 0 + 1 = 2 . One popularextension is 3SUM+, due to Vassilevska Williams and Williams [35], which asks for 3SUM to besolved for all inputs (\\x16, \\x17, 2) with 2 ∈ \\x18 . However, as we are specifically concerned with countingproblems, we instead consider the problem #3SUM, where we must compute the total number ofsolution tuples (0, 1, 2).It is easy to see that 3SUM and #3SUM can be solved in $̃ (=2) operations by sorting \\x18 anditerating over all pairs in \\x16 × \\x17, and it is conjectured [13, 23] that 3SUM admits no $ (=2−X )-timerandomised algorithm for any constant X > 0. This approach is also how we model instancesof 3SUM as a bipartite graph in order to do approximate counting. Joining two vertices 0 and 1whenever 0 + 1 ∈ \\x18 , adjacency queries can be answered efficiently by binary search on the now-sorted list\\x18 , and independence queries on a set ( ⊆ \\x16∪\\x17 can be answered by the assumed decisionalgorithm. Analogous to Theorem 2, in Section 5.1 we obtain the following structural result for3SUM as a corollary to Theorem 1.Theorem 4. If 3SUMwith = integers has a randomised algorithm that runs in time) (=), then thereis a randomised Y-approximation algorithm for #3SUM that runs in time) (=) ·Y−2$ (log6 = log log=).Thus if Y−1 is at most polylogarithmic in =, then the approximate counting algorithm in The-orem 4 has only polylogarithmic overhead over decision. Independently of whether or not the3SUM conjecture is true, we conclude that 3SUM and, say, 12 -approximating #3SUM have the sametime complexity up to polylogarithmic factors.The fastest-known algorithm for 3SUM, due to Baran, Demaine and Pǎtraşcu [2], has runningtime$ (=2 (log log=/log=)2). Theorem 4 does not currently yield improved algorithms for approxi-mating #3SUM as the polylogarithmic speedup factor of > (log3 =) over exhaustive search is smallerthan the $ (log6 = log log=) cost in our reduction. However, Chan and Lewenstein [8] prove that3SUM has much faster algorithms when the input is restricted to instances in which elements ofone list are somewhat clustered, in a sense made explicit below. (Their algorithm also works for3SUM+, but not for #3SUM as far as we can tell.) This is an interesting special case with severalapplications, including monotone multi-dimensional 3SUM with linearly-bounded coordinates —see the introduction of [8] for an overview. Thus by using the algorithm of Chan and Lewensteinas a black box for the independence oracle, we obtain the following algorithm as a corollary toTheorem 1.Theorem 5. For all X > 0, there is a randomised Y-approximation algorithm with running timeY−2 · $̃ (=2−X/7) for instances of #3SUM with = integers such that at least one of \\x16, \\x17, or \\x18 may becovered by =1−X intervals of length =.1.2.3 NWT. In the Negative-Weight-Triangle problem, we are given an edge-weighted graph andmust decide whether the graph contains a triangle of negative total weight. Vassilevska Williamsand Williams [35] prove that NWT is equivalent to APSP under subcubic reductions. An =-vertexinstance of NWT and its natural counting version #NWT can be solved in time $ (=3) by exhaus-tively checking every possible triangle, and it is conjectured [35] that NWT admits no $ (=3−X )-time randomised algorithm for any constant X > 0.To reduce approximate #NWT to its decision version NWT, we put all vertices on one side ofthe bipartite graph and all edges on the other side. Then adjacency queries correspond to testingwhether a given vertex and edge together form a triangle of negative weight, and independencequeries can be answered by the assumed decision algorithm for NWT. Thus in Section 5.3 weobtain the following structural result for NWT as a corollary to Theorem 1.6 Holger Dell and John LapinskasTheorem 6. If NWT for =-vertex graphs has a randomised algorithm that runs in time) (=), thenthere is a randomised Y-approximation algorithm for #NWT that runs in time) (=) · Y−2$ (log6 = log log=) .Thus if Y−1 is at most polylogarithmic, our algorithm has only polylogarithmic overhead overdecision. It is known [35] that a truly subcubic algorithm for NWT implies that the negative-weighttriangles can also be enumerated in subcubic time. While an enumeration algorithm is obviouslystronger than an approximate counting algorithm, this reduction has polynomial overhead and sodoes not imply Theorem 6.Williams [37] gives an algorithmwith subpolynomial improvements over the exhaustive searchalgorithm. Using this algorithm as a black-box to answer independence queries, we obtain thefollowing algorithm as a corollary to Theorem 1.Theorem 7. There is a randomised Y-approximation algorithm for #NWT which runs in timeY−2=3/4Ω (√log=) on graphs with = vertices and polynomially bounded edge-weights.1.3 Our results for the satisfiability problemIn :-SAT we are given a :-CNF formula with = variables and must decide whether it is satisfiable.In the natural counting version #:-SAT, we must compute the number of satisfying assignments.The phenomenon that decision, approximate counting, and exact counting seem to become pro-gressively more difficult is nicely represented in the literature: The most efficient known 3-SATalgorithms run in time $ (1.308=) for decision (Hertli [15]), in time $ (1.515=) for 12 -approximatecounting (Schmitt and Wanka [25]), and in time $ (1.642=) for exact counting (Kutzkov [21]).Schmitt and Wanka’s algorithm is based on an approach of Thurley [29]. They reduce approxi-mate counting to decision in such a way that an $∗ (2(1−X: )=)-time algorithm for :-SAT is turnedinto an Y-approximation algorithm for #:-SAT that runs in time Y−2 ·$∗ (2(1−X′: )=) for some X:/2 <X ′:< X: . In the most general form of their algorithm, X′:depends on a complicated parameterisa-tion and is calculated on an ad hoc basis for : = 3 and : = 4, so no asymptotics of X ′:− X: areavailable; the slightly weaker form given in Section 4 of their paper satisfies X ′:→ X:/2 as : → ∞.Thus the exponential savings over exhaustive search go down from X: for decision to roughly X:/2for approximate counting. For example, in the extreme case that Impagliazzo and Paturi’s [17] ex-ponential time hypothesis (ETH) is false and 3-SAT can be solved in time 2> (=) , their reductionwould only yield an exponential-time algorithm for #3-SAT.Traxler [31] constructs a reduction from approximate counting to decision, in which savings of Xfor decision become X −> (1) for approximate counting, so by this metric the reduction is efficient.However, this reduction creates clauses of width Ω(log=) and so is not suitable for :-SAT when :is a constant.We adapt the Valiant–Vazirani style approach of Calabro, Impagliazzo, Kabanets, and Paturi [7]to obtain a reduction from approximate #:-SAT to : ′-SAT, with a trade-off between keeping : ′close to : versus keeping the cost of the reduction low. At the extremes, writing = for the numberof variables in the #:-SAT instance, it implies a reduction from approximate #:-SAT to :-SATwith exponential overhead 2$ (log2 :/:)= , or a reduction from approximate #:-SAT to $ (: log2 :)-SAT with subexponential overhead. We formally state this reduction as Theorem 13 in Section 3.Our reduction yields interesting structural corollaries for ETH and SETH. Recall that SETH isfalse if and only if there exists some X > 0 such that :-SAT can be solved in time$ (2(1−X)=) for allconstants : . Our reduction implies not only that SETH is equivalent to its approximate countingversion (which is also implied by [25] and [29]), but also that the exponential savings X must bethe same:Fine-grained reductions from approximate counting to decision 7Theorem 8. Let 0 < X < 1. Suppose that for all : ∈ N, there is a randomised algorithm whichruns on =-variable instances of :-SAT in time $ (2(1−X)=). Then for all X ′ > 0 and all : ∈ N, thereis a randomised Y-approximation algorithm which runs on =-variable instances of #:-SAT in timeY−2 ·$ (2(1−X+X′)=).By the sparsification lemma [18], ETH is false if and only if :-SAT can be solved in time$ (2X=)for all constant X > 0 and : . Since approximate counting always implies decision, ETH clearlyimplies its seemingly-weaker approximate counting formulation. By letting X increase to 1 in The-orem 8, we see that the converse is also true:Theorem 9. ETH is false if and only if, for every : ∈ N and X > 0, there is a randomised Y-approximation algorithm that runs on =-variable instances of #:-SAT in time Y−2 ·$ (2X=).It remains an open and interesting question whether a result analogous to Theorem 8 holdsfor fixed : , that is, whether deciding :-SAT and approximating #:-SAT have the same time com-plexity up to a subexponential factor. Even a small improvement on Theorem 13 would lead tonew algorithms for approximate #:-SAT. Indeed, for large constant : , the best-known decision, 12 -approximate counting, and exact counting algorithms (due to Paturi, Pudlák, Saks, and Zane [24],Schmitt and Wanka [25], and Impagliazzo, Matthews, and Paturi [16], respectively) all have run-ning time 2(1−Θ(1/:))=, but with progressively worse constants in the exponent. If our reductionfrom approximate #:-SAT to :-SAT could be improved so that the exponential overhead were2> (1/:) instead of 2$ ( (log:)2/:) , this would yield faster approximate counting algorithms for largebut constant : .1.4 TechniquesOur techniques for the CNF-SAT and the fine-grained results are independent from each other.CNF-SAT results. We first discuss Theorems 8 and 9, which we prove in Section 3. In the poly-nomial setting, the standard reduction from approximating #:-SAT to deciding :-SAT is due toValiant and Vazirani [33], and runs as follows. If a :-CNF formula \\x1b has at most 2X= solutions forsome X > 0, then we use a standard branching algorithm with $∗ (2X=) calls to a :-SAT-oracle toprune the search tree to size$ (2X=). Otherwise \\x1b has many solutions, and for any< ∈ N, one mayform a new formula \\x1b< by conjoining \\x1b with < independently-chosen uniformly random XORclauses. It is relatively easy to see that as long as the number SAT(\\x1b ) of satisfying assignmentsof \\x1b is substantially greater than 2< , then SAT(\\x1b<) is concentrated around 2−<SAT(\\x1b ). By choos-ing< appropriately, one may reduce SAT(\\x1b<) to below 2X= and thus compute SAT(\\x1b<) exactly,then multiply it by 2< to obtain an estimate for SAT(\\x1b ).Unfortunately, this argument requires modification in the exponential setting. If \\x1b has = vari-ables, then each uniformly random XOR has length Θ(=) and therefore cannot be expressed asa :-CNF formula without introducing Ω(=) new variables. It follows that (for example) \\x1b ⌊=/2⌋will contain Θ(=2) variables. This blowup is acceptable in a polynomial setting, but not an expo-nential one — for example, given a Θ(2=2/3)-time algorithm for :-SAT, it would yield a uselessΘ(2=4/3)-time randomised approximate counting algorithm for #:-SAT. We can afford to add onlyconstant-length XORs, which do not in general result in concentration in the number of solutions.We therefore make use of a hashing scheme developed by Calabro, Impagliazzo, Kabanets, andPaturi [7] for a related problem, that of reducing :-SAT to Unique-:-SAT. They choose a 2B-sizedsubset of [=] uniformly at random,where B is a large constant, then choose variables independentlyat random within that set. This still does not yield concentration in the number of solutions of \\x1b< ,but it turns out that the variance is sufficiently low that we can remedy this by summing overmany slightly stronger independently-chosen hashes.8 Holger Dell and John LapinskasFine-grained results. We now sketch the proof of Theorem 1, which we prove in Section 4. Givena bipartite graph with \\x1c = (* ,+ , \\x1a) and - ⊆ + , we write m(- ) for the number of edges incidentto - . For all - ⊆ + , we may halve m(- ) in expectation simply by removing half the vertices in- chosen independently at random. Moreover, if m(- ) is sufficiently small, we may use binarysearch to efficiently determine m(- ) exactly. Thus, as with Theorems 8 and 9, we might hope toimplement the classical approach of Valiant and Vazirani [33]; start with - = + (so that m(- ) =4 (\\x1c)), repeatedly approximately halve m(- ) until it is small enough to determine exactly, thenmultiply by the appropriate power of 2 and output the result.Unfortunately, this naive algorithmmay fail. For example, if the non-isolated vertices of\\x1c forma star whose central vertex lies in+ , then the new value of m(- ) is clearly not concentrated aroundits expectation; it is either unchanged or reduced to zero. In Lemma 19, we show using martingaletechniques that this is essentially the only way things can go wrong. We say - is balanced if nosingle vertex in- is incident to a large proportion of the edges in\\x1c [* ∪- ] (see Definition 18), andLemma 19 shows that if- is balanced then with high probability we can approximately halve m(- )by deleting half of - uniformly at random.We therefore proceed by finding a small set of vertices which “unbalances” - if one exists,approximately counting the edges incident to them, and removing them from - . We repeat thisprocess as necessary until - becomes balanced, then delete half of what remains. At the end, weapproximate 4 (\\x1c) by taking an appropriate linear combination of our edge counts at each stage.However, since our access to the graph is limited, it is non-trivial to find the “unbalancing” vertices.We must also show that we do not remove too many vertices in this way, as finding edges by bruteforce is computationally expensive. Our algorithm is essentially given by EdgeCounton p. 16, withsome trivial modifications as described in the proof of Theorem 1.2 PRELIMINARIES2.1 NotationWe write N for the set of all positive integers. For a positive integer =, we use [=] to denote the set{1, . . . , =}. We use log or ln to denote the base-4 logarithm, and lg to denote the base-2 logarithm.We consider graphs \\x1c to be undirected, and write 4 (\\x1c) = |\\x1a (\\x1c) |. For all E ∈ + (\\x1c), we use# (E) to denote the neighbourhood {F ∈ + (\\x1c) : {E,F} ∈ \\x1a (\\x1c)} of E . For all - ⊆ + (\\x1c), wedefine # (- ) = ⋃E∈- # (E). We define m(- ) to be the size of the edge boundary of - , that is,m(- ) = |{4 ∈ \\x1a (\\x1c) | |4 ∩ - | = 1}|. For convenience, we shall generally present bipartite graphs\\x1cas a triple (* ,+ , \\x1a) in which (* ,+ ) is a partition of + (\\x1c) and \\x1a ⊆ * ×+ .When stating quantitative bounds on running times of algorithms, we assume the standardword-RAM machine model with logarithmic-sized words. We assume that lists and functions inthe problem input are presented in the natural way, that is, as an array using at least one word perentry, and we assume that numerical values such as the edge weights in NWT are given in binary.We shall write 5 (G) = $̃ (6(G)) when for some constant 2 ∈ R, 5 (G) = $ ((log G)26(G)) as G → ∞.Similarly, we write 5 (G) = $∗ (6(G)) when for some constant 2 ∈ R, 5 (G) = $ (G26(G)) as G → ∞.We require our problem inputs to be given as finite binary strings, and write Σ∗ for the set of allsuch strings. A randomised approximation scheme for a function 5 : Σ∗ → N is a randomised algo-rithm that takes as input an instance G ∈ Σ∗ and a rational error tolerance 0 < Y < 1, and outputsa rational number I (a random variable depending on the “coin tosses” made by the algorithm)such that, for every instance G , P((1 − Y) 5 (G) ≤ I ≤ (1 + Y) 5 (G)) ≥ 2/3. All of our approximatecounting algorithms will be randomised approximation schemes.Fine-grained reductions from approximate counting to decision 92.2 Probability theoryWe use some results from probability theory, which we collate here for reference. First, we stateChebyshev’s inequality.Lemma 10. Let - be a real-valued random variable with mean ` and let C > 0. ThenP(|- − ` | ≥ C)≤ Var(- )C2. \\x03We also use the following concentration result due to McDiarmid [26].Lemma 11. Let 5 be a real function of independent random variables -1, . . . , -< , and let ` =E(5 (-1, . . . , -<)). Let 21, . . . , 2< ≥ 0 such that, for all 8 ∈ [<] and all pairs (x, x ′) differing only inthe 8th coordinate, we have |5 (x) − 5 (x ′) | ≤ 28 . Then for all C > 0,P(|5 (-1, . . . , -<) − ` | ≥ C) ≤ 24−2C2/∑<8=1 228 . \\x03Finally, we use the following Chernoff bounds, proved in (for example) Corollaries 2.3–2.4 andRemark 2.11 of Janson, Łuczak and Rucinski [19].Lemma 12. Let - be a binomial or hypergeometric random variable with mean `.(i) For all Y with 0 < Y ≤ 32 , we have P(|- − ` | ≥ Y`) ≤ 24−Y2`/3.(ii) For all C with C ≥ 7`, we have P(- ≥ C) ≤ 4−C . \\x033 FROM DECISION TO APPROXIMATE COUNTING CNF-SATIn this section we prove our results for the satisfiability of CNF formulae, formally defined asfollows.Problem :-SAT expects as input: A :-CNF formula \\x1b .Task: Decide if \\x1b is satisfiable.Problem #:-SAT expects as input: A :-CNF formula \\x1b .Task: Compute the number SAT(\\x1b ) of satisfying assignments of \\x1b .We also define a technical intermediate problem. For all B ∈ N, we say that a matrix\\x16 is B-sparseif every row of \\x16 contains at most B non-zero entries. In the following definition, : ∈ N and B ∈ Nare constants.Problem Π:,B expects as input: An =-variable Boolean formula \\x1b of the form \\x1b (x) =\\x1b ′(x) ∧ (\\x16x = b). Here \\x1b ′ is a :-CNF formula, \\x16 is an B-sparse< ×= matrix over GF (2)with 0 ≤ < ≤ =, and b ∈ GF (2)< .Task: Decide if \\x1b is satisfiable.We define the growth rate c:,B of Π:,B as the infimum over all V > 0 such that Π:,B has a ran-domised algorithm that runs in time $∗ (2V=) and outputs the correct answer with probability atleast 2/3. Our main reduction is encapsulated in the following theorem.Theorem 13. Let : ∈ N with : ≥ 2, let 0 < X < 1, and let B ≥ 120 lg2 (6/X)/X . Then there isa randomised approximation scheme for #:-SAT which, when given an =-variable formula \\x1b andapproximation error parameter Y, runs in time Y−2 ·$(2(c:,B+X)=).10 Holger Dell and John LapinskasBefore we prove this theorem, let us derive Theorems 8 and 9 as immediate corollaries. In bothcases, we use the fact that the condition \\x16x = b can be expressed as an B-CNF formula with<2B−1clauses, and thus c:,B ≤ cmax{:,B },0 holds for all constant :, B .Theorem 8 (restated). Let 0 < X < 1. Suppose that for all : ∈ N, there is a randomised algorithmwhich runs on =-variable instances of :-SAT in time $ (2(1−X)=). Then for all X ′ > 0 and all : ∈ N,there is a randomised Y-approximation algorithm which runs on =-variable instances of #:-SAT intime Y−2 ·$ (2(1−X+X′)=).Proof. Let X > 0 be as specified in the theorem statement. Then for all constant :, B ∈ N, wehave c:,B ≤ cmax{:,B },0 ≤ 1 − X . The result follows by Theorem 13 with B = 120 lg2 (6/X ′)/X ′. \\x03Theorem9 (restated). ETH is false if and only if, for every : ∈ N and X > 0, there is a randomisedY-approximation algorithm that runs on =-variable instances of #:-SAT in time Y−2 ·$ (2X=).Proof. The backward implication is immediate: Any randomised 12 -approximation scheme for#3-SAT is able to decide 3-SAT with success probability at least 2/3. For the forward implication,assume ETH is false. By the sparsification lemma [18, Lemma 10], we then have c:,0 = 0 for all : ∈N. Hence for all :, B ∈ N, we obtain c:,B ≤ cmax{:,B },0 = 0. The result now follows by Theorem 13.\\x033.1 Proof of Theorem 13Given access to an oracle that decides satisfiability queries, we can compute the exact number ofsolutions of a formula with few solutions using a standard self-reducibility argument given below(see also [29, Lemma 3.2]).Algorithm CountFew(\\x1b, 0): Given an instance \\x1b of Π:,B on = variables, 0 ∈ N, and access to anoracle for Π:,B , this algorithm computes SAT(\\x1b ) if SAT(\\x1b ) ≤ 0; otherwise it outputs FAIL.1 (Query the oracle) If \\x1b is unsatisfiable, return 0.2 (No variables left) If \\x1b contains no variables, return 1.3 (Branch and recurse) Let \\x1b0 and \\x1b1 be the formulae obtained from \\x1b by setting the first freevariable in \\x1b to 0 and 1, respectively. If CountFew(\\x1b0, 0) + CountFew(\\x1b1, 0) is at most 0, thenreturn this sum; otherwise abort the entire computation and return FAIL.Lemma 14. CountFew is correct and runs in time at most (min{0, SAT(\\x1b )} + 1) · $̃ (|\\x1b |). Moreover,each oracle query is a formula with at most = variables.Proof. The correctness of CountFew follows by induction from SAT(\\x1b ) = SAT(\\x1b0) + SAT(\\x1b1).For the running time, consider the recursion tree of CountFew on inputs \\x1b and 0. At each vertex,the algorithm takes time at most $̃ (|\\x1b |) to compute \\x1b0 and \\x1b1, and it issues a single oracle call. Forconvenience, we call the leaves of the tree at which CountFew returns 0 in Step 1 or 1 in Step 2the 0-leaves and 1-leaves, respectively. Let G be the number of 1-leaves. Each non-leaf is on thepath from some 1-leaf to the root, otherwise it would be a 0-leaf. There are at most G such paths,so there are at most =G non-leaf vertices in total. Finally, every 0-leaf has a sibling which is not a0-leaf, or its parent would be a 0-leaf, so there are at most (= + 1)G 0-leaves in total. Overall, thetree has at most 4=G vertices. An easy induction using Step 3 implies that G ≤ 20, and certainlyG ≤ SAT(\\x1b ), so the claimed running time is correct. \\x03When our input formula \\x1b has too many solutions to apply CountFew efficiently, we first reducethe number of solutions by hashing. In particular, we use the same hash functions as Calabro etal. [7]; they are based on random sparse matrices over GF (2) and formally defined as follows:Fine-grained reductions from approximate counting to decision 11Definition 15. Let B,<,= ∈ N. An (B,<,=)-hash is a random< ×= matrix\\x16 over GF (2) definedas follows. For each row 8 ∈ [<], let \\'8 be a uniformly random size-B subset of [=]. Then for all 8 ∈ [<]and all 9 ∈ \\'8 , we choose values \\x168, 9 ∈ GF (2) independently and uniformly at random, and set allother entries of \\x16 to zero.For intuition, suppose that \\x1b is an =-variable :-CNF formula, ( is the set of satisfying assign-ments of \\x1b , and |( | > 2X= holds for some small X > 0. It is easy to see that, for all <, B ∈ N anduniformly random b ∈ GF (2)< , if \\x16 is an (B,<,=)-hash, then the number - of satisfying assign-ments of \\x1b (x) ∧ (\\x16x = b) has expected value |( |/2< . (See Lemma 16.) If - were concentratedaround its expectation, then by choosing an appropriate value of<, we could reduce the numberof solutions to at most 2X=, apply CountFew to count them exactly, then multiply the result by 2< toobtain an approximation to |( |. This is the usual approach pioneered by Valiant and Vazirani [33].In the exponential setting, however, we can only afford to take B = $ (1), which means that -is not in general concentrated around its expectation. In [7], only very limited concentration wasneeded, but we require strong concentration. To achieve this, rather than counting satisfying as-signments of a single formula \\x1b (x) ∧ (\\x16x = b), we will sum over many such formulae. We firstbound the variance of an individual (B,<,=)-hash when B and ( are suitably large. Our analysishere is similar to that of Calabro et al. [7], although they are concerned with lower-bounding theprobability that at least one solution remains after hashing and do not give bounds on variance.Lemma 16. Let X ∈ R with 0 < X < 16 and let B,<, = ∈ N. Suppose< ≤ = and B ≥ 20 lg2 (1/X)/X .Let ( ⊆ GF (2)= and suppose |( | ≥ 2<+X= . Let \\x16 be an (B,<, =)-hash, and let b ∈ GF (2)< beuniformly random and independent of \\x16. Let ( ′ = {x ∈ ( : \\x16x = b}. Then E(|( ′ |) = 2−< |( | andVar(|( ′ |) ≤ |( |22X=/8−2< .Proof. For each x ∈ GF (2), let \\x1ex be the indicator variable of the event \\x16x = b . Exposing \\x16implies P(\\x1ex ) = 2−< for all x ∈ GF (2)= , and henceE(|( ′|) =∑x∈(P(\\x1ex ) = 2−< |( |.We now bound the second moment. We haveE(|( ′ |2) =∑(x,~) ∈(2E(\\x1ex \\x1e~) =∑(x,~) ∈(2P(\\x16x = \\x16~ = b)=∑(x,~) ∈(2<∏8=1P((\\x16x)8 = (\\x16~)8 = b8 ). (1)When x and ~ are fixed, the events in (1) are identically distributed and we write ?x,~ = P(a)x =a)~ = 1), where 1 ∈ {0, 1} is sampled uniformly at random and a ∈ {0, 1}= is sampled by firstsampling a size-B set \\' ⊆ {1, . . . , =} and then setting the bits a 9 uniformly for 9 ∈ \\', and a 9 = 0 for9 ∉ \\'. Using this shorthand notation, we split the sum in (1) depending on whether the Hammingdistance 3 (x,~) between the vectors is at most U= or larger, for some parameter U < 12 specifiedlater.E(|( ′ |2) =∑(x,~) ∈(2?<x,~ =∑(x,~) ∈(23 (x,~) ≤U=?<x,~ +∑(x,~) ∈(23 (x,~)>U=?<x,~ . (2)We now provide upper bounds for these two sums. For the first sum, let us write ℎ : [0, 1] → [0, 1]for the binary entropy function ℎ(U) = −U lgU − (1 − U) lg(1 − U); it is known that the Hammingball of radius U= around a binary vector x contains at most 2ℎ (U )= binary vectors ~. Thus the first12 Holger Dell and John Lapinskassum is bounded by |( |2ℎ (U )= max{?<x,~ }. To bound the maximum, note by exposing a that ?x,~ ≤ 12holds for all x,~. Thus, the first sum in (2) is bounded by |( |2ℎ (U )=−< .The second sum in (2) is at most |( |2 max{?<x,~ : 3 (x,~) > U=}, and so it remains to bound ?x,~for vectors x and ~ whose distance is more than U=. Write x\\' ∈ GF (2)\\' for the projection of x tothe coordinates of \\'. Conditioning on the event x\\' = ~\\' , we get?x,~ = P(a)x = a)~ = 1\\x0c\\x0c x\\' ≠ ~\\')· P(x\\' ≠ ~\\')+ P(a)x = a)~ = 1\\x0c\\x0c x\\' = ~\\')· P(x\\' = ~\\')≤ P(a)x = a)~ = 1\\x0c\\x0c x\\' ≠ ~\\')+ 12 · P(x\\' = ~\\'). (3)We claim that the first summand of (3) is equal to 14 and the second is bounded above by124−UB .Indeed, conditioned on x\\' ≠ ~\\' , there is a coordinate 2 ∈ \\' with x2 ≠ ~2 . Without loss of generality,assume x2 = 1 and~2 = 0. Under this conditioning, the events a)x = a)~ and a)~ = 1 are actuallyindependent, because a2 is a uniform bit that only affects the first event and 1 is a uniform bit thatonly affects the second. More precisely, after exposing \\' with x\\' ≠ ~\\' and a 9 for all 9 ∈ \\' \\\\ {2},the probability that a2 and 1 are set correctly is14 . To bound the second summand of (3), recall that3 (x,~) ≥ U= and |\\' | = B , and observeP(x\\' = ~\\')≤(=−⌈U=⌉B)(=B) ≤ (1 − ⌈U=⌉/=)B ≤ 4−UB .Putting the bounds on the terms in (3) together, we arrive at?x,~ ≤ 14 +124−UB=14 (1 + 24−UB) ≤ 14424−UB .This allows us to bound the second moment and thus the variance as well:Var(|( ′ |) = E(|( ′ |2) − E(|( ′ |)2 ≤(|( |2ℎ (U )=−< + |( |24−<4< ·24−UB)− |( |22−2< . (4)By assumption we have |( | ≥ 2<+X= , and thus |( |22−2< ≥ |( |2X=−< . Now we set U < 12 such thatℎ(U) = X holds. Since X < 16 , we have U = ℎ−1(X) ≥ X/(2 lg(6/X)) ≥ X/(4 lg(1/X)). It follows thatUB ≥ 5 lg(1/X) ≥ 2 ln(4/X), and together with (4) we get Var(|( ′ |) ≤ |( |24X2</8/22< . Since< ≤ =and X < 1/lg(4), the result follows. \\x03We now state our algorithm for Theorem 13 that reduces from approximate counting for :-SATto decision for Π:,B . In the following definition, X is a rational constant with 0 < X <13 .Algorithm ApxToDX : Given an =-variable instance \\x1b of #:-SAT, a rational number Y ∈ (0, 1), andaccess to an oracle for Π:,B for some B ≥ 40 lg2(2/X)/X , this algorithm computes a rational number Isuch that (1 − Y)SAT(\\x1b ) ≤ I ≤ (1 + Y)SAT(\\x1b ) holds with probability at least 34 .1 (Brute-force on constant-size instances)If =/lg= ≤ 8/X , solve the problem by brute force and return the result.2 (If there are few satisfying assignments, count them exactly)Let C = ⌈X=/2 + 2 lg(1/Y)⌉, and apply CountFew to \\x1b and 0 = 2C+X=/2. Return the result if it isnot equal to FAIL.3 (Try larger and larger equation systems) For each< ∈ {0, . . . , = − C}:a For each 8 ∈ {1, . . . , 2C }:• (Prepare query) Independently sample an (B,< + C, =)-hash \\x16<,8 and a uniformly randomvector bm,i ∈ GF (2)<+C . Let \\x1b<,8 = \\x1b (x) ∧ (\\x16<,8x = bm,i).Fine-grained reductions from approximate counting to decision 13• (Ask oracle using subroutine) Let I<,8 be the output of CountFew(\\x1b<,8 , 40).• (Bad randomness or< too small) If I<,8 = FAIL or if∑89=1 I<,9 > 40, then go to the next<in the outer for-loop.b (Return our estimate) Return I = 2<∑2C8=1 I<,8 .Lemma 17. ApxToDX is correct for all X ∈ (0, 13 ) and runs in time at most Y−2 ·$∗ (2X=). Moreover,the oracle is only called on instances with at most = variables.Proof. Let \\x1b be a :-CNF formula on = variables and let Y ∈ (0, 1). For the running time, notethat Step 1 takes time$ (21/X ) = $ (1), Step 2 takes time at most$∗ (0) by Lemma 14. By the samelemma, each invocation of CountFew on input \\x1b<,8 in 3 takes time$∗ (min{I<,8 , 0} + 1). Moreover,the outer loop in Step 1 is run at most = − C times, and for each fixed<, executing Step 3a in itsentirety takes time at most$∗ (0) due to the check whether∑89=1 I<,: > 40 holds. Thus the overallrunning time of the algorithm is$∗ (0) ≤ $∗ (Y−22X=) as required.It remains to prove the correctness of the algorithm. If it terminates at Step 1 or Step 2, thencorrectness is immediate from Lemma 14. Suppose not, so that =/lg= > 8/X holds, and the set (of solutions of \\x1b satisfies |( | ≥ 2C+X=/2. Let \" = max{< ∈ Z : |( | ≥ 2<+C+X=/2}, and note that0 ≤ \" ≤ = − C and |( | ≤ 2\"+C+X=/2+1 . The formulas \\x1b<,8 are oblivious to the execution of thealgorithm, so for the analysis we may view them as being sampled in advance. Let (<,8 be the setof solutions to \\x1b<,8 . For each< with 0 ≤ < ≤ \" , let E< be the following event:\\x0c\\x0c\\x0c\\x0c\\x0c2C∑8=1|(<,8 | − 2−< |( |\\x0c\\x0c\\x0c\\x0c\\x0c≤ 2−<−(C−X=/2)/2 · |( | .Thus E< implies\\x0c\\x0c\\x0c2<∑2C8=1 |(<,8 | − |( |\\x0c\\x0c\\x0c ≤ Y |( |. By Lemma 16 applied with X/2 in place of X and< + C in place of <, for all 0 ≤ < ≤ \" and 1 ≤ 8 ≤ 2C , we have E(|(<,8 |) = 2−<−C |( | andVar(|(<,8 |) ≤ |( |22X=/16−2<−2C . Since the (<,8 ’s are independent, it follows by Lemma 10 thatP(E<) ≥ 1 −2C · |( |22X=/16−2<−2C2−2<−C+X=/2 |( |2≥ 1 − 2−X=/4 ≥ 1 − 1/=2.Thus a union bound implies that, with probability at least 3/4, the event E< occurs for all< with0 ≤ < ≤ \" simultaneously. Suppose now that this happens. Then in particular, we have2C∑8=1|(\",8 | ≤ (1 + Y)2−\" |( | ≤ 2C+X=/2+2 .But then, if ApxToDX reaches iteration< = \" , none of the calls to CountFew fail in this iterationand we have I\",8 = |(\",8 | for all 8 ∈ {1, . . . , 2C }. Thus ApxToDX returns some estimate I while< ≤ \" . Moreover, since E< occurs, this estimate satisfies (1 − Y) |( | ≤ I ≤ (1 + Y) |( | as required.Thus ApxToDX behaves correctly with probability at least 3/4, and the result follows. \\x03Theorem 13 (restated). Let : ∈ N with : ≥ 2, let 0 < X < 1, and let B ≥ 120 lg2 (6/X)/X . Thenthere is a randomised approximation scheme for #:-SAT which, when given an =-variable formula \\x1band approximation error parameter Y, runs in time Y−2 ·$(2(c:,B+X)=).Proof. If Y < 2−=, then we solve the #:-SAT instance exactly by brute force in time $∗ (Y−1),so suppose Y ≥ 2−=. By the definition of c:,B , there exists a randomised algorithm for Π:,B withfailure probability at most 1/3 and running time at most$∗ (2(c:,B+X/3)=). By Lemma 12(i), for anyconstant \\x18 , by applying this algorithm lg(1/Y) ·$ (=) = $ (=2) times and outputting the majorityanswer, we may reduce the failure probability to at most Y2/\\x18=2X=/3. We apply ApxToDX/3 to \\x1b14 Holger Dell and John Lapinskasand Y, using the randomized algorithm for Π:,B in place of the Π:,B -oracle. If we take\\x18 sufficientlylarge, then by Lemma 17 and a union bound, the overall failure probability is at most 1/3, and therunning time is Y−2 ·$∗ (2(c:,B+2X/3)=) = Y−2 ·$ (2(c:,B+X)=) as required. \\x034 APPROXIMATELY COUNTING EDGES IN BIPARTITE GRAPHSIn this section, we prove our main result, Theorem 1. Recall from Section 1.1 that it consists of analgorithm that is given access to a bipartite graph via an adjacency oracle and an independenceoracle. Throughout this section, we fix \\x1c = (* ,+ , \\x1a) and Y > 0 as the input to our edge-countingalgorithm, and we define = = |* ∪+ |.4.1 Random subsets of balanced setsA set - ⊆ + is balanced if the graph \\x1c [* ,- ] is not “star-like”, with a large proportion of edgesincident to a single vertex in - . We formally define this notion, and show that if - ′ is a uniformlyrandom subset of a balanced set - , then m(- ) ≈ 2m(- ′) holds with suitably high probability.Definition 18. For any real b with 0 < b ≤ 1, a set - ⊆ + is b-balanced if every vertex in - hasdegree at most bm(- ).Lemma 19. Let - ⊆ + be a set and let- ′ ⊆ - be a random subset formed by including each vertexof - independently with probability 12 .(i) With probability at least 1 − 2 exp(−|- |/24), we have |- ′ | ≤ 34 |- |.(ii) Let W, b be reals with 0 < b ≤ 1 and 0 < W ≤ 12 . If - is b-balanced, then with probability atleast 1 − 2 exp(−2W2/b), we have(12 − W)· m(- ) ≤ m(- ′) ≤(12 + W)· m(- ) .Proof. For the first claim, note that E(|- ′ |) = |- |/2 holds, and thus by Lemma 12(i) we haveP(|- ′ | ≥ 34 · |- |)≤ P(\\x0c\\x0c|- ′ | − 12 · |- |\\x0c\\x0c ≥ 14 · |- |)≤ 24−|- |/24 .Nowwe prove the second claim. For each vertex E ∈ - , let \\x1eE be the indicator random variable ofthe event E ∈ - ′. Then m(- ′) is a function of {\\x1eE : E ∈ - }, and changing a single indicator variable\\x1eE alters m(- ′) by exactly 3 (E). Moreover, E(m(- ′)) = m(- )/2. It therefore follows by Lemma 11thatP(\\x0c\\x0cm(- ′) − 12 · m(- )\\x0c\\x0c ≥ W · m(- ))≤ 2 exp(−2W2m(- )2∑E∈- 3 (E)2). (5)Since - is b-balanced, we have∑E∈- 3 (E)2 ≤ bm(- ) ·∑E∈- 3 (E) = bm(- )2. With (5), the claimedupper bound of 2 exp(−2W2/b)on the error probability follows. \\x03In using Lemma 19, we will takeW = Θ(Y/log=) and b = Θ(W2/log log=). Tomotivate this choice,consider the following toy argument:Suppose simplistically that Lemma 19(ii) was true for all sets, not just for balanced sets, and that bcould be chosen arbitrarily. We will see later (using the SampleNeighbours algorithm defined inSection 4.2) that, if m(- ) is small, we can quickly determine it exactly. In this situation, the followingalgorithm would estimate 4 (\\x1c): start with -0 = + . Given -8 , check whether m(-8) is small enoughto determine exactly. If so, output 28m(-8). If not, form -8+1 from -8 by including each elementindependently with probability 12 . Let -C be the final set formed this way. By Lemma 19(i), wehave C = $ (log=) with high probability. By our supposed simplistic version of Lemma 19(ii), wehave m(-C ) ∈ (1±W)C m(-0)/2C = (1±W)C4 (\\x1c)/2C ; thus the algorithm gives a valid Y-approximationwhenever (1 ± W)C ⊆ (1 ± Y). We have (1 ± W)C ⊆ 1 ± 4CW for sufficiently small W , so this holds forW = $ (Y/log=) = $ (Y/C). Finally, using a union bound together with the fact that C = $ (log=)Fine-grained reductions from approximate counting to decision 15holds with high probability, Lemma 19(ii) holds at each stage with probability at least 1−$ (log=) ·exp(−2W2/b); this can be made arbitrarily large by taking b = $ (W2/log log=).Of course, Lemma 19(ii) is not true for all sets — it fails badly if \\x1c [* ,- ] is a star, for example.While the above argument does not use independence queries at all, we will need them to dealwith unbalanced sets.4.2 Estimating vertex degreesIn order to test whether a set - is balanced and thus whether taking a uniformly random subsetof - will give a good approximation of m(- ) via Lemma 19, we will efficiently approximate therelative degrees 3 (E)/|# (- ) | for all E ∈ - . To this end, we will use independence queries to uni-formly sample a random subset . ⊆ # (- ) of a given size ~. We show that, with high probability,the random variable |# (E) ∩ . |/|. | is a 12 -approximation of the relative degree unless the relativedegree is smaller than b/140, in which case |# (E) ∩. |/|. | is no larger than b/20.Lemma 20. Let - ⊆ + and let ~ ∈ N with ~ ≤ |# (- ) |. Let . ⊆ # (- ) be a uniformly-randomsize-~ subset of # (- ). Let E ∈ - be a vertex and writeX (E) = |# (E) ||# (- ) | and X̃ (E) =|# (E) ∩ . ||. | .Let b > 0. If X (E) ≥ b/140, then with probability at least 1 − 2 exp(−b~/2000), the number X̃ (E)is a 12 -approximation of X (E). On the other hand, if X (E) ≤ b/140, then with probability at least1 − 2 exp(−b~/20), we have X̃ (E) ≤ b/20.Proof. The random variable |# (E) ∩. | follows a hypergeometric distribution with mean `E =X (E) · ~. By Lemma 12(i), we haveP(\\x0c\\x0c\\x0c|# (E) ∩ . | − `E\\x0c\\x0c\\x0c ≥`E2)≤ 2 exp(−`E/12) .If X (E) ≥ b/140 and thus `E ≥ b~/140, this immediately implies the first claim. Similarly, if X (E) ≤b/140 and thus C := b20~ ≥ 7`E holds, then Lemma 12(ii) immediately implies the second claim. \\x03When we use Lemma 20, we will apply it to all $ (=) vertices in each of the $ (log=) iter-ations of the overall algorithm. So in order for a union bound to give something meaningful,we need a success probability of 1 − Ω(1/(= log=)). We will therefore set ~ = Θ(b−1 log=) =Θ(Y−2 log3 = log log=).We can sample a uniformly random set . ⊆ # (- ), using the following straightforward proce-dure. It is the only component of our algorithm that uses independence queries.Algorithm SampleNeighbours: The algorithm takes as input a set - ⊆ + and an integer ~, and itreturns a set . ⊆ * such that |# (- ) | < ~ implies . = # (- ) and |# (- ) | ≥ ~ implies that . is auniformly random size-~ subset of # (- ).1 Let D1, . . . ,D |* | be a uniformly random ordering of* and let . = ∅.2 While |. | < ~:a Find the smallest 8 with D8 ∈ # (- ) \\\\ . . To do so, we use independence queries of theform ind\\x1c (- ∪ {D1, . . . , D 9 } \\\\ . ) and perform binary search over 9 ∈ {1, . . . , |* |}.b If D8 was found, add it to . . Otherwise we have . = # (- ) and return . .3 Return . .Lemma 21. The algorithm SampleNeighbours is correct, runs in time $ (= log=), and makes atmost $ (~ log=) independence queries.16 Holger Dell and John LapinskasProof. The uniform ordering of * induces a uniform ordering of # (- ), which implies thatSampleNeighbours is correct. For the running time, note that Step 1 runs in time $ (=) (usingFisher–Yates shuffling) and each binary search runs in time $ (log=). Thus the overall runningtime is $ (= + ~ log=) = $ (= log=) and the number of independence queries is $ (~ log=). \\x03We use SampleNeighbours for two purposes: If it returns a set . of size less than ~, then . =# (- ) holds and. is small enough to compute m(- ) using the adjacency oracle for all pairs in. ×- .Otherwise the set . gives us good estimates for the relative degrees of vertices in - by Lemma 20.In particular, we shall use this to approximate the set of vertices in - of high relative degree, asencapsulated by the following definition.Definition 22. Let b ∈ R with 0 < b ≤ 1 and let - ⊆ + . We say ( ⊆ - is a b-core of - if itsatisfies the following properties:(W1) every vertex in - with degree at leastb8 · |# (- ) | is contained in ( ;(W2) every vertex in ( has degree at leastb32 · |# (- ) |.We will show in the proof of Theorem 1 that the estimates given by Lemma 20 do indeed yieldcores. We now relate cores to balancedness.Lemma 23. Let b ∈ R with 0 < b ≤ 1 and let ( be a b-core of a set - ⊆ + .(i) If |( | ≥ 32/b2, then - is b-balanced.(ii) If - \\\\ ( contains a vertex of degree at least b4 · |# (- \\\\ () |, then |# (- \\\\ () | ≤12 · |# (- ) |.Otherwise, - \\\\ ( is b4 -balanced.Proof. For the first claim, suppose |( | ≥ 32/b2. Then by (W2), at least 32/b2 vertices in -have degree at leastb32 · |# (- ) |. Hence m(- ) ≥ |# (- ) |/b holds, and every vertex E ∈ - satisfies3 (E) ≤ |# (- ) | ≤ bm(- ). Thus - is b-balanced.For the second claim, suppose E ∈ - \\\\ ( is a vertex whose degree satisfies 3 (E) ≥ b4 · |# (- \\\\ ()|.Since E ∉ ( , we also have 3 (E) ≤ b8 · |# (- ) | by (W1). Together, these facts imply |# (- \\\\ ()| ≤4b· 3 (E) ≤ 12 · |# (- ) | as required. Finally, note that |# (- \\\\ () | ≤ m(- \\\\ () holds, so if all verticesin - \\\\ ( have degree at most b4 · |# (- \\\\ () |, then - \\\\ ( isb4 -balanced by definition. \\x034.3 The Overall AlgorithmThroughout this section, we will takeW =Y800 log=, b =W25 log log==Y28 · 105 log2 = log log=, and~ =4000 log=b=32 · 108 log3 = log log=Y2.The edge counting algorithm works in $ (log=) iterations, starting with - = + . In each iteration,either |- | is roughly halved, or |# (- ) | is at least halved. We formulate the algorithm recursively.Algorithm EdgeCount(- ): This recursive algorithm takes as input a set - ⊆ + and returns an Y-approximation to m(- ) with suitably high probability. (Recall that the input graph\\x1c = (* ,+ , \\x1a) andthe allowed error Y > 0 have already been defined globally.)1 Use SampleNeighbours(-,~) to sample a uniformly random. ⊆ # (- ) of size min{~, |# (- ) |}.2 If |- | ≤ 24 log= or |. | < ~, then compute m(- ) using adjacency queries on * × - or . × - ,respectively. (if |. | < ~, then . = # (- ) holds by the properties of SampleNeighbours)Fine-grained reductions from approximate counting to decision 173 For all E ∈ - , compute X̃ (E) = |# (E)∩. ||. | using adjacency queries on . × - .(w.h.p. each X̃ (E) is a 12 -approximation to X (E) if X (E) ≥ b/140)4 Let ( = {E ∈ - : X̃ (E) ≥ b16 }. (w.h.p. this is a b-core)5 If X̃ (E) ≤ 12b holds for all E ∈ - , or if |( | ≥ 32/b2 holds: (w.h.p. - is now b-balanced)a Let - ′ be a uniformly random subset of - . (w.h.p. - ′ is at most 34 the size of - )b Recursively compute 2 · EdgeCount(- ′), and return this number.6 Otherwise, independently and uniformly sample 3|* | log=/W2 pairs from * × ( , and use theadjacency oracle to determine the number / of these pairs which are edges in \\x1c . Let m̃(() :=/W2 |( |/3 log=. (w.h.p. m̃(() ∈ (1 ± W)m(().)7 Return EdgeCount(- \\\\ () + m̃((). (w.h.p. either # (- \\\\ () is half the size of # (- ), or - \\\\ ( isb/4-balanced.)We are ready to formally prove our main result.Theorem 1 (restated). There is a randomised algorithm A which, given a rational number Ywith 0 < Y < 1 and oracle access to an =-vertex bipartite graph \\x1c , outputs an Y-approximation of|\\x1a (\\x1c) | with probability at least 2/3. Moreover, A runs in time Y−2 ·$ (= log4 = log log=) and makesat most Y−2 ·$ (log5 = log log=) calls to the independence oracle.Proof. We may assume without loss of generality that = ≥ 105; otherwise, we simply solvethe problem in $ (1) time by brute force using the adjacency oracle. Note that each iteration ofEdgeCount makes at most one recursive call, so its recursion tree is a path. An iteration is anexecution of EdgeCount up to a recursive call. We first make a minor modification to EdgeCount:adding a global counter to ensure that we perform at most C = ⌊100 log=⌋ iterations, otherwisehalting with an output of TIMEOUT.We are very unlikely to reach this depth, but this modificationwill allow us to bound the running time deterministically (as required by Theorem 1). Having doneso, we claim that running EdgeCount on input + has the claimed properties.We first bound the running time for each iteration. By Lemma 21, Step 1 runs in time$ (= log=)and makes at most$ (~ log=) independence queries; this step is the only one that makes indepen-dence queries at all. Step 2 takes time at most $ (= log=) if |- | ≤ 24 log= or time $ (~=) other-wise. Likewise, not counting the recursive calls, Step 3, Step 4, and Step 5 take time $ (~=), andStep 6 and Step 7 take time$ (= log=/W2) = Y−2$ (= log3 =). There are$ (log=) total iterations, and~ = Y−2Θ(log3 = log log=), so the overall worst-case running time of the algorithm on input + is$ (~= log=) = Y−2$ (= log4 = log log=), and it makes at most $ (~ log2 =) = Y−2$ (log5 = log log=)queries to the independence oracle.Next, we argue that the success probability is at least 2/3. To reason about this, we define thefollowing events at each recursion depth 1 ≤ 8 ≤ C of the algorithm:F1(8) Either Step 3 is not executed at depth 8 , or each X̃ (E) computed indeed either 12 -approximatesX (E) (if X (E) ≥ b/140) or satisfies X̃ (E) ≤ b/20 (otherwise).F2(8) Either Step 5a is not executed at depth 8 , or |- ′ | ≤ 34 |- | holds and the number 2m(- ′) is a2W-approximation of m(- ).F3(8) Either Step 6 is not executed at depth 8 , or m̃(() is a W-approximation to m(().Thus F1(8), F2(8) and F3(8) vacuously occur if the algorithm terminates before reaching depth 8 .We write F (8) = F1(8) ∩ F2(8) ∩ F3(8), and F =⋂C8=1 F (8). We will now show that Pr(F ) ≥ 2/3.Each time Step 3 is executed, the set . returned by SampleNeighbours in Step 1 has size~ = |. | ≤ |# (- ) |, and thus this set is a uniformly random size-~ subset of # (- ). Lemma 20applies and shows that each event F1(8) fails to occur for an individual E with probability at most18 Holger Dell and John Lapinskasexp(−b~/2000). By our choice of ~, this is precisely 1/=2. Since there are at most = vertices E ,Pr(F1(8) fails) ≤ 1/=. (6)Conditioned on F1(8), we claim that the set ( defined in Step 4 is a b-core. If X (E) ≥ b/8, thenX̃ (E) is a valid 12 -approximation to X (E), so X̃ (E) ≥ b/16 and thus E is added to ( ; this implies that(W1) holds. Conversely, if X (E) < b/32, then either X̃ (E) is a 12 -approximation of X (E) (in whichcase X̃ (E) < b/16 and thus E is not added to () or X̃ (E) ≤ b/20 (in which case again E is not addedto (); this implies that (W2) holds.We now claim that if Step 5a is executed, again conditioned on F1(8), then - is b-balanced.Suppose Step 5a is executed; therefore either X̃ (E) ≤ 12b holds for all E ∈ - or |( | ≥ 32/b2. If|( | ≥ 32/b2, then - is b-balanced by Lemma 23(i), so suppose X̃ (E) ≤ 12 b for all E ∈ - . Since F1(8)occurs, for all E ∈ - , either X̃ (E) is a 12 -approximation for X (E) or X (E) < b/140. In the former case,X (E) ≤ 2X̃ (E) ≤ b , so X (E) ≤ b in both cases and so - is b-balanced as claimed.It follows that conditioned on F1(8), each time Step 5a is executed, |- | ≥ 24 log= and - is b-balanced. Thus Lemma 19(i) and (ii) apply, so F2(8) fails with probability at most 2 exp(−|- |/24) +2 exp(−2W2/b). By our choice of b , it follows thatPr(F2(8) fails | F1(8)) ≤2=+ 2log10 =. (7)Finally, conditioned on F1(8), each time Step 6 is executed, / is a binomial variable with mean` = 3m(() log=/W2 |( |. It follows by Lemma 12(i) that for all 8 ,Pr(F3(8) fails | F1(8)) = Pr(| m̃(() − m(() | > Wm(()) = Pr(|/ − ` | > W`)≤ 24−W2`/3 = 24−m (() log=/ |( | .Since F1(8) occurs, ( is a b-core (as shown above); thus by (W2), every vertex in ( has positivedegree, and in particular m(() ≥ |( |. Thus conditioned on F1(8), F3(8) fails with probability atmost 2/=. In conjunction with (6) and (7), this impliesPr(F (8) fails) ≤ 5=+ 2log10 =.Since = ≥ 105 and C ≤ 100 log=, this is at most 1/3C . It follows by a union bound over all 1 ≤ 8 ≤ Cthat F occurs with probability at least 2/3, as claimed.Let us now show that conditioned on F , we do not output TIMEOUT. We claim that in everyother iteration, we multiply either |# (- ) | or |- | by a factor of at most 34 . Since F2(8) occurs forall 8 , it is clear that |- | is multiplied by a factor of at most 34 if the algorithm recurses in Step 5b. Ifthe algorithm recurses in Step 7, then by Lemma 23(ii), either we reduce |# (- ) | by at least half, orthe set - \\\\ ( is b/4-balanced. In the first case we are done, in the second case it may be that - \\\\ (is not significantly smaller than - . However, as - \\\\ ( is b/4-balanced, the condition X̃ (E) ≤ b/2 ismet for all E ∈ - \\\\( in the very next iteration of the algorithm (where the input is - \\\\(), and then- \\\\ ( is multiplied by a factor of at most 34 . Since initially we have |- | ≤ = and |# (- ) | ≤ =, thenumber of iterations is thus at most 4 log 43= < C as required.It remains to prove that conditioned on F , the function call EdgeCount(+ ) returns an Y-approxi-mation for |\\x1a (\\x1c) | = m(+ ). Let C ′ ≤ C be the total number of iterations; we will prove inductivelythat for all 0 ≤ 8 ≤ C ′ − 1, we have EdgeCount(-C′−8) ∈ (1 ± 2W)8m(-C′−8). In the last iteration, thealgorithm computes m(-C′) exactly, so the claim is immediate for 8 = 0. If the algorithm in iterationFine-grained reductions from approximate counting to decision 19C ′ − 8 recurses in Step 5b, then since F2(C ′ − 8) occurs, we haveEdgeCount(-C′−8 ) = 2 · EdgeCount(-C′−8+1) ∈ (1 ± 2W)8−1m(-C′−8+1) ⊆ (1 ± 2W)8m(-C′−8),as required. If instead it recurses in Step 7, then since F3(C ′ − 8) occurs, we haveEdgeCount(-C′−8) = EdgeCount(-C′−8+1) + m̃(()∈ (1 ± 2W)8−1m(-C′−8+1) + (1 ± W)m(()⊆ (1 ± 2W)8(m(-C′−8+1) + m(-C′−8+1 \\\\ -C′−8))= (1 ± 2W)8m(-C′−8).Thus the claim holds, and in particularEdgeCount(+ ) = EdgeCount(-1) ∈ (1 ± 2W)C′−1m(-1) ⊆ (1 ± 2W)C4 (\\x1c).Since (1 − 2W)C ≥ 1 − 2CW and (1 + 2W)C ≤ 42WC ≤ 1 + 8CW , it follows that EdgeCount(+ ) is a 8CW-approximation of |\\x1a (\\x1c) |. Since C ≤ 100 log=, by our choice of W , this is an Y-approximation. \\x035 APPLICATIONS FOR POLYNOMIAL-TIME PROBLEMS5.1 3SUMWe formally define the problems as follows.Problem 3SUM expects as input: Three lists \\x16, \\x17 and \\x18 of integers.Task: Decide whether there exists a tuple (0, 1, 2) ∈ \\x16 × \\x17 ×\\x18 such that 0 + 1 = 2 .Problem #3SUM expects as input: Three lists \\x16, \\x17 and\\x18 of integers.Task: Count the number of tuples (0, 1, 2) ∈ \\x16 × \\x17 ×\\x18 such that 0 + 1 = 2 .Theorem 4 (restated). If 3SUM with = integers has a randomised algorithm that runs in time) (=), then there is a randomised Y-approximation algorithm for #3SUM that runs in time ) (=) ·Y−2$ (log6 = log log=).Proof. First we note that any bounded-error randomised algorithm for 3SUM must read a con-stant proportion of the entries in \\x16, \\x17 and\\x18 , so we can assume ) (=) = Ω(=).Let (\\x16, \\x17,\\x18) be an instance of #3SUM and let 0 < Y < 1. If Y ≤ 1=, then we use exhaustive searchto solve the problem exactly in time $ (=3) = $ (Y−2) (=)). In the following, we assume Y > 1= . Let\\x1a = {(0, 1) ∈ \\x16 × \\x17 : 0 + 1 ∈ \\x18}, and let \\x1c = (\\x16, \\x17, \\x1a). We will proceed by sorting the set \\x18 in$ (= log=) time, then applying the algorithm of Theorem 1 to \\x1c and Y.We can evaluate adj\\x1c (0, 1) in time$ (log=) using binary search on\\x18 . Moreover, for all- ⊆ \\x16∪\\x17,we have ind\\x1c (- ) = 1 if and only if (- ∩ \\x16,- ∩ \\x17,\\x18) is a ‘no’ instance of 3SUM, so ind\\x1c can beevaluated by solving a single instance of 3SUM, which takes$ (=) time to prepare. As in the proofof Theorem 13, we solve the instance by invoking the assumed randomised decision algorithm100 log= times and outputting the majority answer. The overall algorithm is given by Theorem 1.As this algorithm makes at most Y−2 · $ (log6 =) ≤ $ (=2 log6 =) queries to ind\\x1c , the probabilitythat at least one of them is answered incorrectly by the boosted randomised procedure remainsnegligible, at most $ (1/=) by Lemma 12(i), which is in particular at most 1/3 as required. Theoverall running time is:$ (= log=)sort\\x18+ Y−2$ (= log4 = log log=)# queries to adj\\x1c· $ (log=)binary search+ Y−2$ (log5 = log log=)# queries to ind\\x1c· ($ (=) +) (=) log=)prepare and solve query.20 Holger Dell and John LapinskasWe have constructed an Y-approximation algorithm for 3SUM that has the claimed running time.\\x03Theorem 5 (restated). For all X > 0, there is a randomised Y-approximation algorithm withrunning time Y−2 · $̃ (=2−X/7) for instances of #3SUM with = integers such that at least one of \\x16, \\x17, or\\x18 may be covered by =1−X intervals of length =.Proof. Say a set ( ⊆ Z is (=, X)-clustered if it can be covered by atmost=1−X intervals of length=;note that it can be checked in quasilinear time whether a set is (=, X)-clustered. Let (\\x16, \\x17,\\x18) be aninstance of #3SUM in which at least one of\\x16, \\x17 or\\x18 is (=, X)-clustered. By negating and permutingsets if necessary, wemay assume that\\x18 is (=, X)-clustered. Exactly as in the proof of Theorem4, anyrandomised) (=)-time algorithm for 3SUM on such instances yields a) (=) · Y−2$ (log6 = log log=)-time randomised approximation scheme. (In particular, note that (- ∩ \\x16,- ∩ \\x17,\\x18) remains aninstance of the restricted problem.) Chan and Lewenstein [8, Corollary 4.3] provide a randomised$ (=2−X/7)-time algorithm for 3SUM on such instances, so the result follows. \\x035.2 Orthogonal VectorsWe formally define the problems as follows.Problem OV expects as input: Two lists \\x16 and \\x17 of zero-one vectors in R3 .Task: Decide whether there exists a pair (u, v) ∈ \\x16 × \\x17 such that ∑38=1 u8v8 = 0.Problem #OV expects as input: Two lists \\x16 and \\x17 of zero-one vectors in R3 .Task: Count the number of pairs (u, v) ∈ \\x16 × \\x17 such that ∑38=1 u8v8 = 0.Theorem 2 (restated). If OV with = vectors in 3 dimensions has a randomised algorithm thatruns in time) (=,3), then there is a randomised Y-approximation algorithm for #OV that runs in time) (=,3) · Y−2$ (log6 = log log=).Proof. Let (\\x16, \\x17) be an instance of #OV and let 0 < Y < 1. If Y ≤ =−2 then we can solve theproblem exactly in time $ (=2) = $ (Y−1), so suppose Y > =−2. Let \\x1a = {(0, 1) ∈ \\x16 × \\x17 : 〈0, 1〉 = 0},and let\\x1c = (\\x16, \\x17, \\x1a) be a bipartite graph. We will proceed by applying the algorithm of Theorem 1to \\x1c and Y.We can evaluate adj\\x1c in$ (3) time by calculating the inner product. Moreover, for all- ⊆ \\x16∪\\x17,ind\\x1c (- ) = 1 if and only if (\\x16∩-, \\x17∩- ) is a ‘no’ instance of OV, so ind\\x1c can be evaluated by solvinga single instance of OV which takes $ (=3) time to prepare. As in the proof of Theorem 4, we doso by invoking our randomised decision algorithm 100 log= times and outputting the majorityanswer. Our overall running time is thenY−2 ·$ (= log4 = log log=) ·$ (3) + Y−2 · ($ (=3) +) (=,3) log=) ·$ (log5 = log log=).Since any randomised algorithm for OV must examine a constant proportion of the coordinatesof vectors in \\x16 and \\x17, we have ) (=, 3) = Ω(=3), so the result follows. \\x03In the following definitions, R is a constant finite ring.Problem OV(R) expects as input: Two lists \\x16 and \\x17 of vectors in R3 .Task: Decide whether there exists a pair (u, v) ∈ \\x16 × \\x17 such that ∑38=1 u8v8 = 0R .Fine-grained reductions from approximate counting to decision 21Problem #OV(R) expects as input: Two lists \\x16 and \\x17 of vectors in R3 .Task: Count the number of pairs (u, v) ∈ \\x16 × \\x17 such that ∑38=1 u8v8 = 0R .Theorem 3 (restated). Let< = ?: be a constant prime power. There is a randomised Y-approxi-mation algorithm for #OV over GF (<)3 with running time Y−23 (?−1): · $̃ (=), and for #OV over(Z/<Z)3 with running time Y−23<−1 · $̃ (=).Proof. Exactly as in the proof of Theorem 2, any randomised) (=,3)-time algorithm for OV(R)yields a) (=, 3) · Y−2$ (log6 = log log=)-time randomised approximation scheme for #OV(R). (Notethat R is finite and part of the problem specification, so arithmetic operations require only $ (1)time.) The result therefore follows fromTheorems 1.6 and 1.3 (respectively) ofWilliams and Yu [39].\\x035.3 Negative-Weight TrianglesWe formally define the problems as follows.Problem NWT expects as input: A tripartite graph\\x1c and a symmetric functionF :+ (\\x1c)2 → Z.Task: Decide whether there exists a triangle 012 in \\x1c such that F (0, 1) + F (1, 2) +F (2, 0) < 0.Problem #NWT expects as input: A tripartite graph \\x1c and a symmetric function F :+ (\\x1c)2 → Z.Task: Count the number of triangles 012 in \\x1c such thatF (0, 1) +F (1, 2) +F (2, 0) < 0.Theorem 6 (restated). IfNWT for =-vertex graphs has a randomised algorithm that runs in time) (=), then there is a randomised Y-approximation algorithm for #NWT that runs in time) (=) · Y−2$ (log6 = log log=) .Proof. Let (\\x1c,F) be an instance of #NWT, let \\x16, \\x17 and \\x18 be the vertex classes of \\x1c , and let0 < Y < 1. If Y ≤ =−3 then we can solve the problem exactly in time $ (=3) = $ (Y−1), so supposeY > =−3. Let* = \\x16, let + = {4 ∈ \\x1a (\\x1c) : 4 ⊆ \\x17 ∪\\x18}, and let\\x1a ={(0, {1, 2}) ∈ * ×+ : {0, 1}, {0, 2} ∈ \\x1a (\\x1c) andF (0, 1) +F (1, 2) +F (2, 0) < 0}.Let \\x1d = (* ,+ , \\x1a), so that \\x1d is a bipartite graph. We will proceed by applying the algorithm ofTheorem 1 to \\x1d and Y.We can evaluate adj\\x1d in $ (1) time by summing the appropriate weights. Moreover, for all - ⊆* ∪+ , define a graph\\x1c- by + (\\x1c- ) = (- ∩ \\x16) ∪ \\x17 ∪\\x18 and\\x1a (\\x1c- ) ={4 ∈ \\x1a (\\x1c) : 4 ∩ - ∩ \\x16 ≠ ∅ or 4 ∈ - ∩+}.LetF- = F |+ (\\x1c- )2 . Then for all - ⊆ * ∪+ , ind\\x1d (- ) = 1 if and only if (\\x1c- ,F- ) is a ‘no’ instanceof NWT, so ind\\x1c can be evaluated by solving a single instance of NWTwhich takes$ (=2) time toprepare. As in the proof of Theorem 4, we do so by invoking our randomised decision algorithm100 log= times and outputting the majority answer. Our overall running time is thenY−2 ·$ (=2 log4 = log log=) ·$ (1) + Y−2 · ($ (=2) +) (=) log=) ·$ (log5 = log log=).22 Holger Dell and John LapinskasIf \\x1c is a complete tripartite graph, then any randomised algorithm for NWT must examine aconstant proportion of the edges of \\x1c , so we have ) (=) = Ω(=2) and the result follows. \\x03In order to approximate algorithm for #NWT, we will reduce to APSP and apply the algorithmof Williams [37]. We formally define APSP as follows.Problem APSP expects as input: A directed graph\\x1c and a functionF : \\x1a (\\x1c) → Z suchthat \\x1c contains no negative-weight cycles underF .Task: Output the matrix \\x16 such that for all D, E ∈ + (\\x1c), \\x16D,E is the minimum weight ofany path from D to E in \\x1c .Theorem 7 (restated). There is a randomised Y-approximation algorithm for #NWT which runsin time Y−2=3/4Ω (√log=) on graphs with = vertices and polynomially bounded edge-weights.Proof. ByWilliams [37, Theorem1.1], an=-vertex instance ofAPSPwith polynomially boundededge weights can be solved in time =3/4Ω (√log=) . There is a well-known reduction from NWT toAPSP with only constant overhead, which we give explicitly in the following paragraph. Theo-rem 6 then implies the existence of an Y-approximation algorithm for #NWT with running timeY−2=3/4Ω (√log=) , noting that the polylogarithmic overhead is subsumed into the 4Ω (√log=) term.It remains only to reduce NWT to APSP. Let (\\x1c,F) be an instance of NWT, writing\\x1c = (+ , \\x1a).Form an instance (\\x1c ′,F ′) of APSP as follows. Let + (\\x1c ′) = (+ × [3]), and let\\x1a (\\x1c ′) =⋃8 ∈{1,2}⋃{D,E}∈\\x1a{((D, 8), (E, 8 + 1)), ((E, 8), (D, 8 + 1))}.Let F ′({(D, 8), (E, 8 + 1)}) = F (D, E) for all {(D, 8), (E, 8 + 1)} ∈ \\x1a (\\x1c ′). Thus for all {D, E} ∈ \\x1a, eachpath (D, 1) (F, 2) (E, 3) from (D, 1) to (E, 3) in \\x1c ′ corresponds exactly to the triangle DEF in \\x1c , andDEF ’s weight is the length of the corresponding path plus F (D, E). Let \\x16 be the output of APSPon \\x1c ′. Then from the discussion above, (\\x1c,F) is a ‘yes’ instance of NWT if and only if for some{D, E} ∈ \\x1a (\\x1c), we have \\x16(D,1),(E,3) +F (D, E) < 0. This can be checked in $ (=2) time. \\x03ACKNOWLEDGMENTSWe thank Rahul Santhanam and Ryan Williams for some valuable discussions.Part of this work was donewhile the authors were visiting the Simons Institute for the Theory ofComputing. The research leading to these results has received funding from the European ResearchCouncil (ERC) under the European Union’s Seventh Framework Programme (FP7/2007–2013) ERCgrant agreement no. 334828. The paper reflects only the authors’ views and not the views of theERC or the European Commission. The European Union is not liable for any use that may be madeof the information contained therein.REFERENCES[1] Amir Abboud, Richard Ryan Williams, and Huacheng Yu. 2015. More Applications of the Polynomial Method to Al-gorithm Design. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015,San Diego, CA, USA, January 4-6, 2015, Piotr Indyk (Ed.). SIAM, 218–230. https://doi.org/10.1137/1.9781611973730.17[2] Ilya Baran, Erik D. Demaine, and Mihai Patrascu. 2008. Subquadratic Algorithms for 3SUM. Algorithmica 50, 4 (2008),584–596. https://doi.org/10.1007/s00453-007-9036-3[3] Paul Beame, Sariel Har-Peled, Sivaramakrishnan Natarajan Ramamoorthy, Cyrus Rashtchian, and Makrand Sinha.2018. Edge Estimation with Independent Set Oracles. In 9th Innovations in Theoretical Computer Science Conference,ITCS 2018, January 11-14, 2018, Cambridge, MA, USA (LIPIcs, Vol. 94), Anna R. Karlin (Ed.). Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 38:1–38:21. https://doi.org/10.4230/LIPIcs.ITCS.2018.38Fine-grained reductions from approximate counting to decision 23[4] Anup Bhattacharya, Arijit Bishnu, Arijit Ghosh, and Gopinath Mishra. 2018. Triangle Estimation using Polylogarith-mic Queries. CoRR abs/1808.00691 (2018). arXiv:1808.00691 http://arxiv.org/abs/1808.00691[5] Anup Bhattacharya, Arijit Bishnu, Arijit Ghosh, and Gopinath Mishra. 2019. Hyperedge Estimation using Polyloga-rithmic Subset Queries. CoRR abs/1908.04196 (2019). arXiv:1908.04196 http://arxiv.org/abs/1908.04196[6] Arijit Bishnu, Arijit Ghosh, Sudeshna Kolay, Gopinath Mishra, and Saket Saurabh. 2018. ParameterizedQuery Complexity of Hitting Set Using Stability of Sunflowers. In 29th International Symposium on Algo-rithms and Computation, ISAAC 2018, December 16-19, 2018, Jiaoxi, Yilan, Taiwan (LIPIcs, Vol. 123), Wen-LianHsu, Der-Tsai Lee, and Chung-Shou Liao (Eds.). Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 25:1–25:12.https://doi.org/10.4230/LIPIcs.ISAAC.2018.25[7] Chris Calabro, Russell Impagliazzo, Valentine Kabanets, and Ramamohan Paturi. 2008. The complex-ity of Unique k-SAT: An Isolation Lemma for k-CNFs. J. Comput. Syst. Sci. 74, 3 (2008), 386–393.https://doi.org/10.1016/j.jcss.2007.06.015[8] Timothy M. Chan and Moshe Lewenstein. 2015. Clustered Integer 3SUM via Additive Combinatorics. In Proceedingsof the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC 2015, Portland, OR, USA, June 14-17,2015, Rocco A. Servedio and Ronitt Rubinfeld (Eds.). ACM, 31–40. https://doi.org/10.1145/2746539.2746568[9] Timothy M. Chan and Richard Ryan Williams. 2016. Deterministic APSP, Orthogonal Vectors, and More: QuicklyDerandomizing Razborov-Smolensky. In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Dis-crete Algorithms, SODA 2016, Arlington, VA, USA, January 10-12, 2016, Robert Krauthgamer (Ed.). SIAM, 1246–1255.https://doi.org/10.1137/1.9781611974331.ch87[10] Xi Chen, Amit Levi, and Erik Waingarten. 2020. Nearly optimal edge estimation with independent set queries. InProceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms, SODA 2020, Salt Lake City, UT, USA, January5-8, 2020, Shuchi Chawla (Ed.). SIAM, 2916–2935. https://doi.org/10.1137/1.9781611975994.177[11] Holger Dell, John Lapinskas, and Kitty Meeks. 2020. Approximately counting and sampling small wit-nesses using a colourful decision oracle. In Proceedings of the 2020 ACM-SIAM Symposium on Discrete Al-gorithms, SODA 2020, Salt Lake City, UT, USA, January 5-8, 2020, Shuchi Chawla (Ed.). SIAM, 2201–2211.https://doi.org/10.1137/1.9781611975994.135[12] Martin E. Dyer, Leslie Ann Goldberg, Catherine S. Greenhill, and Mark Jerrum. 2004. The Relative Complexity ofApproximate Counting Problems. Algorithmica 38, 3 (2004), 471–500. https://doi.org/10.1007/s00453-003-1073-y[13] Anka Gajentaan and Mark H. Overmars. 1995. On a Class of O(n2) Problems in Computational Geometry. Comput.Geom. 5 (1995), 165–185. https://doi.org/10.1016/0925-7721(95)00022-2[14] Jiawei Gao, Russell Impagliazzo, Antonina Kolokolova, and Richard Ryan Williams. 2017. Completeness for First-Order Properties on Sparse Structures with Algorithmic Applications. In Proceedings of the Twenty-Eighth AnnualACM-SIAM Symposium on Discrete Algorithms, SODA 2017, Barcelona, Spain, Hotel Porta Fira, January 16-19, Philip N.Klein (Ed.). SIAM, 2162–2181. https://doi.org/10.1137/1.9781611974782.141[15] Timon Hertli. 2014. 3-SAT Faster and Simpler - Unique-SAT Bounds for PPSZ Hold in General. SIAM J. Comput. 43,2 (2014), 718–729. https://doi.org/10.1137/120868177[16] Russell Impagliazzo, William Matthews, and Ramamohan Paturi. 2012. A satisfiability algorithm for AC0. In Proceed-ings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2012, Kyoto, Japan, January17-19, 2012, Yuval Rabani (Ed.). SIAM, 961–972. https://doi.org/10.1137/1.9781611973099.77[17] Russell Impagliazzo and Ramamohan Paturi. 2001. On the Complexity of k-SAT. J. Comput. Syst. Sci. 62, 2 (2001),367–375. https://doi.org/10.1006/jcss.2000.1727[18] Russell Impagliazzo, Ramamohan Paturi, and Francis Zane. 2001. Which Problems Have Strongly Exponential Com-plexity? J. Comput. Syst. Sci. 63, 4 (2001), 512–530. https://doi.org/10.1006/jcss.2001.1774[19] Svante Janson, Tomasz Luczak, and Andrzej Rucinski. 2000. Random graphs. Wiley.https://doi.org/10.1002/9781118032718[20] Mark Jerrum, Alistair Sinclair, and Eric Vigoda. 2004. A polynomial-time approximation algorithm for the permanentof a matrix with nonnegative entries. J. ACM 51, 4 (2004), 671–697. https://doi.org/10.1145/1008731.1008738[21] Konstantin Kutzkov. 2007. New upper bound for the #3-SAT problem. Inf. Process. Lett. 105, 1 (2007), 1–5.https://doi.org/10.1016/j.ipl.2007.06.017[22] Moritz Müller. 2006. Randomized Approximations of Parameterized Counting Problems. In Parameterized and Ex-act Computation, Second International Workshop, IWPEC 2006, Zürich, Switzerland, September 13-15, 2006, Proceedings(Lecture Notes in Computer Science, Vol. 4169), Hans L. Bodlaender and Michael A. Langston (Eds.). Springer, 50–59.https://doi.org/10.1007/11847250_5[23] Mihai Patrascu. 2010. Towards polynomial lower bounds for dynamic problems. In Proceedings of the 42nd ACMSymposium on Theory of Computing, STOC 2010, Cambridge, Massachusetts, USA, 5-8 June 2010, Leonard J. Schulman(Ed.). ACM, 603–610. https://doi.org/10.1145/1806689.180677224 Holger Dell and John Lapinskas[24] Ramamohan Paturi, Pavel Pudlák, Michael E. Saks, and Francis Zane. 2005. An improved exponential-time algorithmfor k-SAT. J. ACM 52, 3 (2005), 337–364. https://doi.org/10.1145/1066100.1066101[25] Manuel Schmitt and Rolf Wanka. 2013. Exploiting independent subformulas: A faster approximation scheme for#k-SAT. Inf. Process. Lett. 113, 9 (2013), 337–344. https://doi.org/10.1016/j.ipl.2013.02.013[26] Johannes Siemons (Ed.). 1989. Surveys in Combinatorics, 1989. Cambridge University Press.https://doi.org/10.1017/cbo9781107359949[27] Michael Sipser. 1983. A Complexity Theoretic Approach to Randomness. In Proceedings of the 15th Annual ACMSymposium on Theory of Computing, 25-27 April, 1983, Boston, Massachusetts, USA, David S. Johnson, Ronald Fagin,Michael L. Fredman, David Harel, Richard M. Karp, Nancy A. Lynch, Christos H. Papadimitriou, Ronald L. Rivest,Walter L. Ruzzo, and Joel I. Seiferas (Eds.). ACM, 330–335. https://doi.org/10.1145/800061.808762[28] Larry J. Stockmeyer. 1985. On Approximation Algorithms for #P. SIAM J. Comput. 14, 4 (1985), 849–861.https://doi.org/10.1137/0214060[29] Marc Thurley. 2012. An Approximation Algorithm for #k-SAT. In 29th International Symposium on The-oretical Aspects of Computer Science, STACS 2012, February 29th - March 3rd, 2012, Paris, France (LIPIcs,Vol. 14), Christoph Dürr and Thomas Wilke (Eds.). Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 78–87.https://doi.org/10.4230/LIPIcs.STACS.2012.78[30] Seinosuke Toda. 1991. PP is as Hard as the Polynomial-Time Hierarchy. SIAM J. Comput. 20, 5 (1991), 865–877.https://doi.org/10.1137/0220053[31] Patrick Traxler. 2016. The Relative Exponential Time Complexity of Approximate Counting Satisfying Assignments.Algorithmica 75, 2 (2016), 339–362. https://doi.org/10.1007/s00453-016-0134-y[32] Leslie G. Valiant. 1979. The Complexity of Computing the Permanent. Theor. Comput. Sci. 8 (1979), 189–201.https://doi.org/10.1016/0304-3975(79)90044-6[33] Leslie G. Valiant and Vijay V. Vazirani. 1986. NP is as Easy as Detecting Unique Solutions. Theor. Comput. Sci. 47, 3(1986), 85–93. https://doi.org/10.1016/0304-3975(86)90135-0[34] Virginia Vassilevska Williams. 2015. Hardness of Easy Problems: Basing Hardness on Popular Conjectures such asthe Strong Exponential Time Hypothesis (Invited Talk). In 10th International Symposium on Parameterized and ExactComputation, IPEC 2015, September 16-18, 2015, Patras, Greece (LIPIcs, Vol. 43), Thore Husfeldt and Iyad A. Kanj (Eds.).Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 17–29. https://doi.org/10.4230/LIPIcs.IPEC.2015.17[35] Virginia Vassilevska Williams and Richard Ryan Williams. 2018. Subcubic Equivalences Between Path, Matrix, andTriangle Problems. J. ACM 65, 5 (2018), 27:1–27:38. https://doi.org/10.1145/3186893[36] Richard Ryan Williams. 2005. A new algorithm for optimal 2-constraint satisfaction and its implications. Theor.Comput. Sci. 348, 2-3 (2005), 357–365. https://doi.org/10.1016/j.tcs.2005.09.023[37] Richard Ryan Williams. 2014. Faster all-pairs shortest paths via circuit complexity. In Symposium on Theoryof Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, David B. Shmoys (Ed.). ACM, 664–673.https://doi.org/10.1145/2591796.2591811[38] Richard Ryan Williams. 2018. Counting Solutions to Polynomial Systems via Reductions. In 1st Symposium on Sim-plicity in Algorithms, SOSA 2018, January 7-10, 2018, New Orleans, LA, USA (OASICS, Vol. 61), Raimund Seidel (Ed.).Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 6:1–6:15. https://doi.org/10.4230/OASIcs.SOSA.2018.6[39] Richard Ryan Williams and Huacheng Yu. 2014. Finding orthogonal vectors in discrete structures. In Proceedings ofthe Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2014, Portland, Oregon, USA, January5-7, 2014, Chandra Chekuri (Ed.). SIAM, 1867–1877. https://doi.org/10.1137/1.9781611973402.135'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd1e'), 'authors': 'Bhattacharya, Sayan, Henzinger, Monika, Nanongkai, Danupon', 'year': '2016', 'title': 'New deterministic approximation algorithms for fully dynamic matching', 'full_text': \"  \\n \\n \\n \\nwarwick.ac.uk/lib-publications \\n \\n \\n \\n \\n \\nOriginal citation: \\nBhattacharya, Sayan, Henzinger, Monika and Nanongkai, Danupon (2016) New deterministic \\napproximation algorithms for fully dynamic matching. In: 48th Annual ACM Symposium on \\nTheory of Computing, Cambridge, MA, USA, 19-21 Jun 2016 . Published in: Proceedings of \\nthe forty-eighth annual ACM symposium on Theory of Computing pp. 398-411. \\n \\nPermanent WRAP URL: \\nhttp://wrap.warwick.ac.uk/97555                \\n \\nCopyright and reuse: \\nThe Warwick Research Archive Portal (WRAP) makes this work by researchers of the \\nUniversity of Warwick available open access under the following conditions.  Copyright © \\nand all moral rights to the version of the paper presented here belong to the individual \\nauthor(s) and/or other copyright owners.  To the extent reasonable and practicable the \\nmaterial made available in WRAP has been checked for eligibility before being made \\navailable. \\n \\nCopies of full items can be used for personal research or study, educational, or not-for  profit \\npurposes without prior permission or charge.  Provided that the authors, title and full \\nbibliographic details are credited, a hyperlink and/or URL is given for the original metadata \\npage and the content is not changed in any way. \\n \\nPublisher’s statement: \\n© ACM, 2016. This is the author's version of the work. It is posted here by permission of \\nACM for your personal use. Not for redistribution. The definitive version was published in \\nProceedings of the forty-eighth annual ACM symposium on Theory of Computing pp. 398-\\n411. (2016) http://doi.acm.org/10.1145/10.1145/2897518.2897568  \\n \\nA note on versions: \\nThe version presented here may differ from the published version or, version of record, if \\nyou wish to cite this item you are advised to consult the publisher’s version.  Please see the \\n‘permanent WRAP url’ above for details on accessing the published version and note that \\naccess may require a subscription. \\n \\nFor more information, please contact the WRAP Team at: wrap@warwick.ac.uk \\n \\nNew Deterministic Approximation Algorithms for Fully\\nDynamic Matching\\n[Extended Abstract]\\nSayan Bhattacharya\\nIMSc, Chennai\\nbsayan@imsc.res.in\\nMonika Henzinger∗\\nUniversity of Vienna\\nmonika.henzinger@univie.ac.at\\nDanupon Nanongkai†\\nKTH, Stockhom\\ndanupon@kth.se\\nABSTRACT\\nWe present two deterministic dynamic algorithms for the\\nmaximum matching problem. (1) An algorithm that main-\\ntains a (2 + \\x0f)-approximate maximum matching in general\\ngraphs with O(poly(logn, 1/\\x0f)) update time. (2) An algo-\\nrithm that maintains an αK approximation of the value of\\nthe maximum matching with O(n2/K) update time in bi-\\npartite graphs, for every sufficiently large constant positive\\ninteger K. Here, 1 ≤ αK < 2 is a constant determined by\\nthe value of K. Result (1) is the first deterministic algo-\\nrithm that can maintain an o(logn)-approximate maximum\\nmatching with polylogarithmic update time, improving the\\nseminal result of Onak et al. [STOC 2010]. Its approxima-\\ntion guarantee almost matches the guarantee of the best ran-\\ndomized polylogarithmic update time algorithm [Baswana et\\nal. FOCS 2011]. Result (2) achieves a better-than-two ap-\\nproximation with arbitrarily small polynomial update time\\non bipartite graphs. Previously the best update time for this\\nproblem was O(m1/4) [Bernstein et al. ICALP 2015], where\\nm is the current number of edges in the graph.\\nCategories and Subject Descriptors\\nF.2 [Analysis of Algorithms and Problem Complex-\\nity]: Miscellaneous\\nGeneral Terms\\nAlgorithms, Theory\\nKeywords\\nData Structures, Dynamic Graph Algorithms\\n∗The research leading to this work has received funding\\nfrom the European Union’s Seventh Framework Programme\\n(FP7/2007-2013) under grant agreement number 317532\\nand from the European Research Council under the Euro-\\npean Union’s Seventh Framework Programme (FP7/2007-\\n2013)/ERC grant agreement number 340506.\\n†Support by Swedish Research Council grant 2015-04659.\\n1. INTRODUCTION\\nIn this paper, we consider the dynamic maximum car-\\ndinality matching problem. In this problem an algorithm\\nhas to quickly maintain an (integral) maximum-cardinality\\nmatching or its approximation, when the n-node input graph\\nis undergoing edge insertions and deletions. We consider\\ntwo versions of this problem: In the matching version, the\\nalgorithm has to output the change in the (approximate)\\nmatching, if any, after each edge insertion and deletion. In\\nthe value version, the algorithm only has to output the value\\nof the matching. (Note that an algorithm for the matching\\nversion can be used to solve the value version within the\\nsame time.) When stating the running time below, we give\\nthe time per update1. If not stated otherwise, these results\\nhold for both versions.\\nThe state of the art for maintaining an exact solution for\\nthe value version of this problem is a randomized O(n1.495)-\\ntime algorithm [16]. This is complemented by various hard-\\nness results which rules out polylogarithmic update time\\n[1, 8, 11]. As it is desirable for dynamic algorithms to\\nhave polylogarithmic update time, the recent work has fo-\\ncused on achieving this goal by allowing approximate solu-\\ntions. The first paper that achieved this is by Onak and Ru-\\nbinfeld [13], which gave a randomized O(1)-approximation\\nO(log2 n)-time algorithm and a deterministic O(logn) ap-\\nproximation O(log2 n)-time algorithm. As stated in the two\\nopen problems in [13], this seminal paper opened up the\\ndoors for two research directions:\\n1. Designing a (possibly randomized) polylogarithmic time\\nalgorithm with smallest approximation ratio.\\n2. Designing a deterministic polylogarithmic time algo-\\nrithm with constant approximation ratio.\\nThe second question is motivated by the fact that random-\\nized dynamic approximation algorithms only fulfill their ap-\\nproximation guarantee when used by an oblivious adversary,\\ni.e., an adversary that gives the next update without know-\\ning the outputs of the algorithm resulting from earlier up-\\ndates. This limits the usefulness of randomized dynamic\\nalgorithms. In contrast, deterministic dynamic algorithms\\nfulfill their approximation guarantee against any adversary,\\neven non-oblivous ones. Thus, they can be used, for ex-\\nample, as a “black box” by any other (potentially static)\\n1In this discussion, we ignore whether the update time is\\namortized or worst-case as this is not the focus of this paper.\\nThe update time of our algorithm is amortized.\\nalgorithm, while this is not generally the case for random-\\nized dynamic algorithms. This motivates the search for de-\\nterministic fully dynamic approximation algorithms, even\\nthough a randomized algorithm with the same approxima-\\ntion guarantee might exists.\\n(1) Up to date, the best answer to the first question is\\nthe randomized 2 approximation O(logn) update time al-\\ngorithm from [2]. It remains elusive to design a better-than-\\ntwo approximation factor with polylogarithmic update time.\\nSome recent works have focused on achieving such approxi-\\nmation factor with lowest update time possible. The current\\nbest update time is O(m1/4/\\x0f2.5) [4, 3], which is determin-\\nistic and guarantees a (3/2 + \\x0f) approximation factor.\\n(2) For the second question, deterministic polylogarithmic-\\ntime (1 + \\x0f)-approximation algorithms were known for the\\nspecial case of low arboricity graphs [12, 11, 15]. On gen-\\neral graphs, the paper [5] achieved a deterministic (3 + \\x0f)-\\napproximation polylogarithmic-time algorithm by maintain-\\ning a fractional matching; this algorithm however works only\\nfor the value version. No deterministic o(logn) approxima-\\ntion algorithm with polylogarithmic update time was known\\nfor the matching version. (There were many deterministic\\nconstant approximation algorithms with o(m) update time\\nfor the matching version (e.g. [12, 5, 7, 4, 3]). The fastest\\namong them requires O(m1/4/\\x0f2.5) update time [4].)\\nOur Results. We make progress on both versions of the\\nproblem as stated in Theorems 1 and 2.\\nTheorem 1. For every \\x0f ∈ (0, 1), there is a determin-\\nistic algorithm that maintains a (2 + \\x0f)-approximate maxi-\\nmum matching in a graph in O(poly(logn, 1/\\x0f)) update time,\\nwhere n denotes the number of nodes in the graph.\\nTheorem 1 answers Onak and Rubinfeld’s second ques-\\ntion positively. In fact, our approximation guarantee almost\\nmatches the best (2-approximation) one provided by a ran-\\ndomized algorithm [2].2 Our algorithm for Theorem 1 is\\nobtained by combining previous techniques [5, 7, 15] with\\ntwo new ideas that concern fractional matchings. First, we\\ndynamize the degree splitting process previously used in the\\nparallel and distributed algorithms literature [9] and use it\\nto reduce the size of the support of the fractional matching\\nmaintained by the algorithm of [5]. This helps us main-\\ntain an approximate integral matching cheaply using the\\nresult in [7]. This idea alone already leads to a (3 + \\x0f)-\\napproximation deterministic algorithm. Second, we improve\\nthe approximation guarantee further to (2 + \\x0f) by proving\\na new structural lemma that concerns the ratio between (i)\\nthe maximum (integral) matching in the support of a max-\\nimal fractional matching and (ii) the maximum (integral)\\nmatching in the whole graph. It was known that this ratio\\nis at least 1/3. We can improve this ratio to 1/2 with a\\nfairly simple proof (using Vizing’s theorem [17]). We note\\nthat this lemma can be used to improve the analysis of an\\nalgorithm in [5] to get the following result: There is a deter-\\nministic algorithm that maintains a (2 + \\x0f) approximation\\nto the size of the maximum matching in a general graph in\\nO(m1/3/\\x0f2) amortized update time.\\n2By combining our result with the techniques of [6] in\\na standard way, we also obtain a deterministic (4 + \\x0f)-\\napproximationO(poly lognpoly(1/\\x0f) logW )-time for the dy-\\nnamic maximum-weight matching problem, where W is the\\nratio between the largest and smallest edge weights.\\nTheorem 2. For every sufficiently large positive integral\\nconstant K, we can maintain an αK-approximation to the\\nvalue of the maximum matching3 in a bipartite graph G =\\n(V,E), where 1 ≤ αK < 2. The algorithm is deterministic\\nand has an amortized update time of O(n2/K).\\nWe consider Theorem 2 to be a step towards achieving a\\npolylogarithmic time (randomized or deterministic) fully dy-\\nnamic algorithm with an approximation ratio less than 2,\\ni.e., towards answering Onak and Rubinfeld’s first question.\\nThis is because, firstly, it shows that on bipartite graphs\\nthe better-than-two approximation factor can be achieved\\nwith arbitrarily small polynomial update time, as opposed\\nto the previous best O(m1/4) time of [3]. Secondly, it rules\\nout a natural form of hardness result and thus suggests that\\na polylogarithmic-time algorithm with better-than-two ap-\\nproximation factor exists on bipartite graphs. More pre-\\ncisely, the known hardness results (e.g. those in [14, 1, 11,\\n8]) that rule out a polylogarithmic-time α-approximation\\nalgorithm (for any α > 0) are usually in the form “assum-\\ning some conjecture, there exists a constant δ > 0 such that\\nfor any constant \\x0f > 0, there is no (1 − \\x0f)α-approximation\\nalgorithm that has nδ−\\x0f update time”; for example, for dy-\\nnamically 2-approximating graph’s diameter, this statement\\nwas proved for α = 2 and δ = 1/2 in [8], implying that\\nany better-than-two approximation algorithm for this prob-\\nlem will require an update time close to n1/2. Our result\\nin 2 implies that a similar statement cannot be proved for\\nα = 2 for the bipartite matching problem since, for any con-\\nstant δ > 0, there is a (2− \\x0f)-approximation algorithm with\\nupdate time, say, O(nδ/2) for some \\x0f > 0.\\nTo derive an algorithm for Theorem 2, we use the fact\\nthat in a bipartite graph the size of the maximum frac-\\ntional matching is the same as the size of the maximum inte-\\ngral matching. Accordingly, a maximal fractional matching\\n(which gives 2-approximation) can be augmented by a frac-\\ntional b-matching, for a carefully chosen capacity vector b,\\nto obtain a better-than-two approximate fractional match-\\ning. The idea of “augmenting a bad solution” that we use\\nhere is inspired by the approach in the streaming setting by\\nKonrad et al. [10]. But the way it is implemented is differ-\\nent as [10] focuses on using augmenting paths while we use\\nfractional b-matchings.\\nOrganization. In Section 1.1, we define some basic con-\\ncepts and notations that will be used throughout the rest of\\nthis paper. In Section 2, we give an overview of our algo-\\nrithm for Theorem 1. In Section 3, we highlight the main\\nideas behind our algorithm for Theorem 2. Finally, we con-\\nclude with some open problems in Section 4. All the missing\\ndetails can be found in the full version of the paper.\\n1.1 Notations and preliminaries\\nLet n = |V | and m = |E| respectively denote the number\\nof nodes and edges in the input graph G = (V,E). Note that\\nm changes with time, but n remains fixed. Let degv(E\\n′)\\ndenote the number of edges in a subset E′ ⊆ E that are\\nincident upon a node v ∈ V . An (integral) matching M ⊆ E\\nis a subset of edges that do not share any common endpoints.\\nThe size of a matching is the number of edges contained in\\nit. We are also interested in the concept of a fractional\\n3We can actually maintain an approximate fractional match-\\ning with the same performance bounds.\\nmatching. Towards this end, we first define the notion of\\na fractional assignment. A fractional assignment w assigns\\na weight w(e) ≥ 0 to every edge e ∈ E. We let Wv(w) =∑\\n(u,v)∈E w(u, v) denote the total weight received by a node\\nv ∈ V under w from its incident edges. Further, the support\\nof w is defined to be the subset of edges e ∈ E with w(e) >\\n0. Given two fractional assignments w,w′, we define their\\naddition (w + w′) to be a new fractional assignment that\\nassigns a weight (w + w′)(e) = w(e) + w′(e) to every edge\\ne ∈ E. We say that a fractional assignment w forms a\\nfractional matching iff we have Wv(w) ≤ 1 for all nodes\\nv ∈ V . Given any subset of edges E′ ⊆ E, we define w(E′) =∑\\ne∈E′ w(e). We define the size of a fractional matching w\\nto be w(E). Given any subset of edges E′ ⊆ E, we let\\nOptf (E\\n′) (resp. Opt(E′)) denote the maximum possible\\nsize of a fractional matching with support E′ (resp. the\\nmaximum possible size of an integral matching M ′ ⊆ E′).\\nTheorem 3 follows from the half-integrality of the matching\\npolytope in general graphs and its total unimodularity in\\nbipartite graphs.\\nTheorem 3. Consider any subset of edges E′ ⊆ E in\\nthe graph G = (V,E). We have: Opt(E′) ≤ Optf (E′) ≤\\n(3/2) ·Opt(E′). Further, if the graph G is bipartite, then we\\nhave: Optf (E\\n′) = Opt(E′).\\nNext, we recall that Gupta and Peng [7] gave a dynamic\\nalgorithm that maintains a (1 + \\x0f)-approximate maximum\\nmatching in O(\\n√\\nm/\\x0f2) update time. A simple modification\\nof their algorithm gives the following result.\\nTheorem 4. [7] If the maximum degree in a dynamic\\ngraph never exceeds some threshold d, then we can main-\\ntain a (1 + \\x0f)-approximate maximum matching in O(d/\\x0f2)\\nupdate time.\\nFinally, we say that a fractional matching w is α-maximal,\\nfor α ≥ 1, iff Wu(w) +Wv(w) ≥ 1/α for every edge (u, v) ∈\\nE. Using LP-duality and complementary slackness condi-\\ntions, one can show the following result.\\nLemma 1. We have Optf (E) ≤ 2α · w(E) for every α-\\nmaximal fractional matching w in a graph G = (V,E).\\n2. GENERAL GRAPHS\\nWe give a dynamic algorithm for maintaining an approx-\\nimate maximum matching in a general graph. We consider\\nthe following dynamic setting. Initially, the input graph is\\nempty. Subsequently, at each time-step, either an edge is in-\\nserted into the graph, or an already existing edge is deleted\\nfrom the graph. The node-set of the graph, however, re-\\nmains unchanged. Our main result in this section is stated\\nin Theorem 1. Throughout this section, we will use the no-\\ntations and concepts introduced in Section 1.1.\\n2.1 Maintaining a large fractional matching\\nOur algorithm for Theorem 1 builds upon an existing\\ndynamic data structure that maintains a large fractional\\nmatching. This data structure was developed in [5], and\\ncan be described as follows. Fix a small constant \\x0f > 0.\\nDefine L = dlog(1+\\x0f) ne, and partition the node-set V into\\nL + 1 subsets V0, . . . , VL. We say that the nodes belonging\\nto the subset Vi are in “level i”. We denote the level of a\\nnode v by `(v), i.e., v ∈ Vi iff `(v) = i. We next define the\\n“level of an edge” (u, v) to be `(u, v) = max(`(u), `(v)). In\\nother words, the level of an edge is the maximum level of\\nits endpoints. We let Ei = {e ∈ E : `(e) = i} denote the\\nset of edges at level i, and define the subgraph Gi = (V,Ei).\\nThus, note that the edge-set E is partitioned by the subsets\\nE0, . . . , EL. For each level i ∈ {0, . . . , L}, we now define a\\nfractional assignment wi with support Ei. The fractional\\nassignment wi is uniform, in the sense that it assigns the\\nsame weight wi(e) = 1/di, where di = (1+\\x0f)\\ni, to every edge\\ne ∈ Ei in its support. In contrast, wi(e) = 0 for every edge\\ne ∈ E \\\\ Ei. Throughout the rest of this section, we refer to\\nthis structure as a “hierarchical partition”.\\nTheorem 5. [5] We can maintain a hierarchical parti-\\ntion dynamically in O(logn/\\x0f2) update time. The algorithm\\nensures that the fractional assignment w =\\n∑L\\ni=0 wi is a\\n(1 + \\x0f)2-maximal matching in G = (V,E). Furthermore, the\\nalgorithm ensures that 1/(1 + \\x0f)2 ≤Wv(w) ≤ 1 for all nodes\\nv ∈ V at levels `(v) > 0.\\nCorollary 1. The fractional matching w in Theorem 5\\nis a 2(1 + \\x0f)2-approximation to Optf (E).\\nProof. Follows from Lemma 1 and Theorem 5.\\nCorollary 2. Consider the hierarchical partition in The-\\norem 5. There, we have degv(Ei) ≤ di for all nodes v ∈ V\\nand levels 0 ≤ i ≤ L.\\nProof. The corollary holds since 1 ≥Wv(w) ≥Wv(wi) =∑\\n(u,v)∈Ei wi(u, v) = (1/di) · degv(Ei).\\nAccordingly, throughout the rest of this section, we refer\\nto di as being the degree threshold for level i.\\n2.2 An overview of our approach\\nWe will now explain the main ideas that are needed to\\nprove Theorem 1. Due to space constraints, we will focus\\non getting a constant approximation in O(poly logn) update\\ntime. See the full version of the paper for the complete proof\\nof Theorem 1. First, we maintain a hierarchical partition as\\nper Theorem 5. This gives us a 2(1 + \\x0f)2-approximate max-\\nimum fractional matching (see Corollary 1). Next, we give\\na dynamic data structure that deterministically rounds this\\nfractional matching into an integral matching without losing\\ntoo much in the approximation ratio. The main challenge is\\nto ensure that the data structure has O(poly logn) update\\ntime, for otherwise one could simply use any deterministic\\nrounding algorithm that works well in the static setting.\\n2.2.1 An ideal skeleton\\nOur dynamic rounding procedure, when applied on top of\\nthe data structure used for Theorem 5, will output a low-\\ndegree subgraph that approximately preserves the size of the\\nmaximum matching. We will then extract a large integral\\nmatching from this subgraph using Theorem 4. To be more\\nspecific, recall that w is the fractional matching maintained\\nin Theorem 5. We will maintain a subset of edges E′ ⊆ E\\nin O(poly logn) update time that satisfies two properties.\\nThere is a fractional matching w′ with support E′\\nsuch that w(E) ≤ c · w′(E′) for some constant c ≥ 1. (1)\\ndegv(E\\n′) = O(poly logn) for all nodes v ∈ V. (2)\\nEquation 1, along with Corollary 1 and Theorem 3, guaran-\\ntees that the subgraph G′ = (V,E′) preserves the size of the\\nmaximum matching in G = (V,E) within a constant factor.\\nEquation 2, along with Theorem 4, guarantees that we can\\nmaintain a matching M ′ ⊆ E′ in O(poly logn/\\x0f2) update\\ntime such that Opt(E′) ≤ (1+\\x0f) · |M ′|. Setting \\x0f to be some\\nsmall constant (say, 1/3), these two observations together\\nimply that we can maintain a O(1)-approximate maximum\\nmatching M ′ ⊆ E in O(poly logn) update time.\\nTo carry out this scheme, we note that in the hierarchi-\\ncal partition the degree thresholds di = (1 + \\x0f)\\ni get smaller\\nand smaller as we get into lower levels (see Corollary 2).\\nThus, if most of the value of w(E) is coming from the lower\\nlevels (where the maximum degree is already small), then\\nwe can easily satisfy equations 1, 2. Specifically, we fix a\\nlevel 0 ≤ L′ ≤ L with degree threshold dL′ = (1 + \\x0f)L′ =\\nΘ(poly logn), and define the edge-set Y =\\n⋃L′\\nj=0Ej . We\\nalso define w+ =\\n∑\\ni>L′ wi and w\\n− =\\n∑\\ni≤L′ wi. Note that\\nw(E) = w+(E) +w−(E). Now, consider two possible cases.\\nCase 1. w−(E) ≥ (1/2) ·w(E). In other words, most of the\\nvalue of w(E) is coming from the levels [0, L′]. By Corol-\\nlary 2, we have degv(Y ) ≤\\n∑L′\\nj=0 dj ≤ (L′ + 1) · dL′ =\\nΘ(poly logn) for all nodes v ∈ V . Thus, we can simply\\nset w′ = w+ and E′ = Y to satisfy equations 1, 2.\\nCase 2. w+(E) > (1/2) ·w(E). In other words, most of the\\nvalue of w(E) is coming from the levels [L′ + 1, L]. To deal\\nwith this case, we introduce the concept of an ideal skeleton.\\nSee Definition 1. Basically, this is a subset of edges Xi ⊆ Ei\\nthat scales down the degree of every node by a factor of\\ndi/dL′ . We will later show how to maintain a structure akin\\nto an ideal skeleton in a dynamic setting. Once this is done,\\nwe can easily construct a new fractional assignment wˆi that\\nscales up the weights of the surviving edges in Xi by the\\nsame factor di/dL′ . Since wi(e) = 1/di for all edges e ∈ Ei,\\nwe set wˆi(e) = 1/dL′ for all edges e ∈ Xi. To ensure that\\nXi is the support of the fractional assignment wˆi, we set\\nwˆi(e) = 0 for all edges e ∈ E \\\\ Xi. Let X = ∪i>L′Xi and\\nwˆ =\\n∑\\ni>L′ wˆi. It is easy to check that this transformation\\npreserves the weight received by a node under the fractional\\nassignment w+, that is, we have Wv(wˆ) = Wv(w\\n+) for all\\nnodes v ∈ V . Accordingly, Lemma 2 implies that if we set\\nw′ = wˆ and E′ = X, then equations 1, 2 are satisfied.\\nDefinition 1. Consider any level i > L′. An ideal skeleton\\nat level i is a subset of edges Xi ⊆ Ei such that deg(v,Xi) =\\n(dL′/di) · deg(v,Ei) for all nodes v ∈ V . Define a fractional\\nassignment wˆi on support Xi by setting wˆi(e) = 1/dL′ for\\nall e ∈ Xi. For every other edge e ∈ E \\\\Xi, set wˆi(e) = 0.\\nFinally, define the edge-set X =\\n⋃\\ni>L′ Xi and the fractional\\nassignment wˆ =\\n∑\\nj>L′ wˆj .\\nLemma 2. We have: degv(X) = O(poly logn) for all nodes\\nv ∈ V , and wˆ(E) = w+(E). The edge-set X and the frac-\\ntional assignment wˆ are defined as per Definition 1.\\nProof. Fix any node v ∈ V . Corollary 2, Definition 1 im-\\nply that: degv(X) =\\n∑\\nj>L′ degv(Xj) =\\n∑\\nj>L′(dL′/dj) ×\\ndegv(Ej) ≤\\n∑\\nj>L′(dL′/dj)dj = (L−L′)dL′ = O(poly logn).\\nTo prove the second part, consider any level i > L′. Defi-\\nnition 1 implies that Wv(wˆi) = (1/dL′) · degv(Xi) = (1/di) ·\\ndegv(Ei) = Wv(wi). Accordingly, we infer that: Wv(wˆ) =\\n∑\\ni>L′Wv(wˆi) =\\n∑\\ni>L′Wv(wi) = Wv(w\\n+). Summing over\\nall the nodes, we get:\\n∑\\nv∈V Wv(wˆ) =\\n∑\\nv∈V Wv(w\\n+). It\\nfollows that wˆ(E) = w+(E).\\n2.2.2 A degree-splitting procedure\\nIt remains to show to how to maintain an ideal skele-\\nton. To gain some intuition, let us first consider the prob-\\nlem in a static setting. Fix any level i > L′, and let λi =\\ndi/dL′ . An ideal skeleton at level i is simply a subset of\\nedges Xi ⊆ Ei that scales down the degree of every node\\n(w.r.t. Ei) by a factor λi. Can we compute such a sub-\\nset Xi in O(|Ei| · poly log n) time? Unless we manage to\\nsolve this problem in the static setting, we cannot expect\\nto get a dynamic data structure for the same problem with\\nO(poly logn) update time. The SPLIT(Ei) subroutine de-\\nscribed below answers this question in the affirmative, albeit\\nfor λi = 2. Specifically, in linear time the subroutine outputs\\na subset of edges where the degree of each node is halved. If\\nλi > 2, then to get an ideal skeleton we need to repeatedly\\ninvoke this subroutine log2 λi times: each invocation of the\\nsubroutine reduces the degree of each node by a factor of\\ntwo, and hence in the final output the degree of each node is\\nreduced by a factor of λi.\\n4 This leads to a total runtime of\\nO(|Ei| · log2 λi) = O(|Ei| · logn) since λi = di/dL′ ≤ di ≤ n.\\nThe SPLIT(E) subroutine, where E ⊆ E. To highlight\\nthe main idea, we assume that (1) degv(E) is even for every\\nnode v ∈ V , and (2) there are an even number of edges in E .\\nHence, there exists an Euler tour on E that visits each edge\\nexactly once. We construct such an Euler tour in O(|E|)\\ntime and then collect alternating edges of this Euler tour in\\na set H. It follows that (1) H ⊆ E with |H| = |E|/2, and\\n(2) degv(|H|) = (1/2) · degv(|E|) for every node v ∈ V . The\\nsubroutine returns the set of edges H. In other words, the\\nsubroutine runs in O(|E|) time, and returns a subgraph that\\nhalves the degree of every node.\\n2.3 From ideal to approximate skeleton\\nWe now shift our attention to maintaining an ideal skele-\\nton in a dynamic setting. Specifically, we focus on the fol-\\nlowing problem: We are given an input graph Gi = (V,Ei),\\nwith |V | = n, that is undergoing a sequence of edge inser-\\ntions/deletions. The set Ei corresponds to the level i edges\\nin the hierarchical partition (see Section 2.1). We always\\nhave degv(Ei) ≤ di for all nodes v ∈ V (see Corollary 2).\\nThere is a parameter 1 ≤ λi = di/dL′ ≤ n. In O(poly log n)\\nupdate time, we want to maintain a subset of edges Xi ⊆ Ei\\nsuch that degv(Xi) = (1/λi) · degv(Ei) for all nodes v ∈ V .\\nThe basic building block of our dynamic algorithm will be\\nthe (static) subroutine SPLIT(E) from Section 2.2.2.\\nUnfortunately, we will not be able to achieve our initial\\ngoal, which was to reduce the degree of every node by exactly\\nthe factor λi in a dynamic setting. For one thing, there\\nmight be some nodes v with degv(Ei) < λi. It is clearly not\\npossible to reduce their degrees by a factor of λi (otherwise\\ntheir new degrees will be between zero and one). Further,\\nwe will need to introduce some slack in our data structures\\nif we want to ensure polylogarithmic update time.\\nWe now describe the structures that will be actually main-\\ntained by our dynamic algorithm. We maintain a partition\\n4To highlight the main idea, we assume that λi is a power\\nof 2.\\nof the node-set V into two subsets: Bi ⊆ V and Ti = V \\\\B.\\nThe nodes in Bi (resp. Ti) are called “big” (resp. “tiny”).\\nWe also maintain a subset of nodes Si ⊆ V that are called\\n“spurious”. Finally, we maintain a subset of edges Xi ⊆ Ei.\\nFix two parameters \\x0f, δ ∈ (0, 1). For technical reasons that\\nwill become clear later on, we require that:\\n\\x0f = 1/100, and δ = \\x0f2/L (3)\\nWe ensure that the following properties are satisfied.\\ndegv(Ei) ≥ \\x0fdi/L for all nodes v ∈ Bi \\\\ Si. (4)\\ndegv(Ei) ≤ 2\\x0fdi/L for all nodes v ∈ Ti \\\\ Si. (5)\\n|Si| ≤ δ · |Bi| (6)\\n(1− \\x0f)\\nλi\\n· degv(Ei) ≤ degv(Xi) ≤\\n(1 + \\x0f)\\nλi\\n· degv(Ei)\\nfor all nodes v ∈ Bi \\\\ Si. (7)\\ndegv(Xi) ≤ (1/λi) · (2\\x0fdi/L) for all nodes v ∈ Ti \\\\ Si. (8)\\ndegv(Xi) ≤ (1/λi) · di for all nodes v ∈ Si. (9)\\nEquation 4 implies that all the non-spurious big nodes\\nhave large degrees in Gi = (V,Ei). On a similar note, equa-\\ntion 5 implies that all the non-spurious tiny nodes have small\\ndegrees in Gi. Next, by equation 6, the number of spurious\\nnodes is negligibly small in comparison with the number of\\nbig nodes. By equation 7, the degrees of the non-spurious big\\nnodes are scaled by a factor that is very close to λi. Thus,\\nthe non-spurious big nodes satisfy an approximate version\\nof the degree-splitting property required by Definition 1.\\nMoving on, by equation 8, the degrees of the non-spurious\\ntiny nodes in Xi are at most (1/λi) · (2\\x0fdi/L) = 2\\x0fdL′/L.\\nSince each edge in Xi receives weight 1/dL′ under the as-\\nsignment wˆi (see Definition 1), we infer that Wv(wˆi) =\\n(1/dL′) · degv(Xi) ≤ 2\\x0f/L for all nodes v ∈ Ti \\\\ Si. Since\\nthere are at most (L − L′) relevant levels in a hierarchical\\npartition, we infer that:∑\\ni>L′:v∈Ti\\\\Si\\nWv(wˆi) ≤ L · (2\\x0f/L) = 2\\x0f (10)\\nSince for a non-spurious tiny node v ∈ Ti \\\\ Si we have\\ndegv(Ei) ≤ 2\\x0fdi/L (see equation 5) and Wv(wi) = (1/di) ·\\ndegv(Ei) ≤ 2\\x0f/L, an exactly similar argument gives us:∑\\ni>L′:v∈Ti\\\\Si\\nWv(wi) ≤ L · (2\\x0f/L) = 2\\x0f (11)\\nEquations 10, 11 have the following implication: The levels\\nwhere v is a non-spurious tiny node contribute a negligible\\namount towards the weights Wv(w\\n+) and Wv(wˆ) (see Sec-\\ntion 2.2.1). Hence, although we are no longer guaranteed\\nthat the degrees of these nodes will be scaled down exactly\\nby the factor λi, this should not cause too much of a problem\\n– the sizes of the fractional assignments w+(E) and wˆ(E)\\nshould still be close to each other as in Section 2.2.1.\\nFinally, Corollary 2 states that the degree of a node in Ei\\nis at most di. Hence, according to the definition of an ideal\\nskeleton (see Definition 1), the degree of a node in Xi ought\\nnot to exceed (1/λi) · di = dL′ . Equation 9 ensures that the\\nspurious nodes satisfy this property.\\nIf the set of edges Xi satisfies the conditions described\\nabove, then we say that we have an approximate-skeleton at\\nour disposal. This is formally stated as follows.\\nDefinition 2. Fix any level i > L′, and suppose that there\\nis a partition of the node-set V into two subsets Bi ⊆ V and\\nTi = V \\\\Bi. Further, consider another subset of nodes Si ⊆\\nV and a subset of edges Xi ⊆ Ei. The tuple (Bi, Ti, Si, Xi)\\nis an approximate-skeleton iff it satisfies equations (4) – (9).\\nOne may object at this point that we have deviated from\\nthe concept of an ideal skeleton (see Definition 1) so much\\nthat it will impact the approximation ratio of our final algo-\\nrithm. To address this concern, we now state the following\\ntheorem whose proof appears in Section 2.5.\\nTheorem 6. For each level i > L′, consider an approx-\\nimate skeleton as per Definition 2. Let X =\\n⋃\\ni>L′ Xi de-\\nnote the set of edges from these approximate-skeletons. Let\\nY =\\n⋃\\ni≤L′ Ei denote the set of edges from the remaining\\nlevels in the hierarchical partition. Then we have:\\n1. There is a fractional matching w′ on support X ∪ Y\\nsuch that w(E) ≤ O(1) · w′(X ∪ Y ). Here, w is the\\nfractional matching given by Theorem 5.\\n2. degv(X ∪ Y ) = O(poly logn) for all v ∈ V .\\nIn other words, the set of edges X∪Y satisfies equations 1, 2.\\nAs per the discussion immediately after equations 1, 2, we\\ninfer the following guarantee.\\nCorollary 3. Suppose that for each level i > L′ there is\\na dynamic algorithm that maintains an approximate-skeleton\\nin O(poly logn) update time. Then we can also maintain a\\nO(1)-approximate maximum matching in the input graph G\\nin O(poly logn) update time.\\nIt remains to show how to maintain an approximate skele-\\nton efficiently in a dynamic setting. Accordingly, we state\\nthe following theorem whose proof appears in Section 2.4.\\nTheorem 7. Consider any level i > L′. In O(poly logn)\\nupdate time, we can maintain an approximate-skeleton at\\nlevel i as per Definition 2.\\nCorollary 3 and Theorem 7 imply that we can maintain a\\nO(1)-approximate maximum matching in a dynamic graph\\nin O(poly logn) update time.\\n2.4 Maintaing an approximate skeleton: Proof\\nof Theorem 7\\nFix a level i > L′. We will show how to efficiently main-\\ntain an approximate skeleton at level i under the assump-\\ntion that λi = 2. In the full version of the paper, if λi >\\n2, then we iteratively apply the algorithm presented here\\nO(log2 λi) = O(log2(di/dL′)) = O(logn) times, and each it-\\neration reduces the degrees of the nodes by a factor of two.\\nHence, after the final iteration, we get a subgraph that is an\\napproximate skeleton as per Definition 2.\\nWe maintain the set of edges EBi = {(u, v) ∈ Ei : {u, v}∩\\nBi 6= ∅} that are incident upon the big nodes. Further, we\\nassociate a “status bit” with each node v ∈ V , denoted by\\nStatus[v] ∈ {0, 1}. We ensure that they satisfy two condi-\\ntions: (1) If Status[v] = 1, then degv(Ei) ≥ \\x0fdi/L (which\\nis the threshold for non-spurious big nodes in equation 4).\\n(2) If Status[v] = 0, then degv(Ei) ≤ 2\\x0fdi/L (which is\\nthe threshold for non-spurious tiny nodes in equation 5).\\nWhenever an edge incident upon v is inserted into (resp.\\ndeleted from) Ei, we update the status bit of v in a lazy\\nmanner (i.e., we flip the bit only if one of the two condi-\\ntions is violated). We define an “epoch” of a node v to be\\nthe time-interval between any two consecutive flips of the\\nbit Status[v]. Since there is a gap of \\x0fdi/L between the\\nthresholds in equations 4, 5, we infer that:\\nIn any epoch of a node v, at least \\x0fdi/L edge\\ninsertions/deletions incident upon v takes place in Ei. (12)\\nOur dynamic algorithm runs in “phases”. In the beginning\\nof a phase, there are no spurious nodes, i.e., we have Si = ∅.\\nDuring a phase, we handle the insertion/deletion of an edge\\nin Ei as follows.\\n2.4.1 Handling the insertion/deletion of an edge\\nConsider the insertion/deletion of an edge (u, v) in Ei. To\\nhandle this event, we first update the set Ei and the status\\nbits of u, v. If {u, v}∩Bi 6= ∅ , then we also update the edge-\\nset EBi . Next, for every endpoint x ∈ {u, v}\\\\Si, we check if\\nthe node x violates any of the equations 4, 5, 7. If yes, then\\nwe set Si ← Si ∪{x}. Finally, we check if |Si| > δ · |Bi|, and\\nif yes, then we terminate the phase by calling the subroutine\\nTERMINATE-PHASE(.).\\n2.4.2 The subroutine TERMINATE-PHASE(.)\\nWe scan through the nodes in Si. For each such node\\nv ∈ Si, if Status[v] = 1, then we set Bi ← Bi ∪ {v},\\nTi ← Ti \\\\ {v}, and ensure that all the edges (u, v) ∈ Ei\\nincident upon v are included in EBi . Else if Status[v] = 0,\\nthen we set Ti ← Ti ∪ {v}, Bi ← Bi \\\\ {v}, and ensure that\\nall the edges (u, v) ∈ Ei incident upon v are excluded from\\nEBi . Finally, we set Si ← ∅ and Xi ← SPLIT(EBi) (see\\nSection 2.2.2). From the next edge insertion/deletion in Ei,\\nwe begin a new phase.\\n2.4.3 Correctness.\\nAt the start of a phase, clearly all the properties hold.\\nThis fact, along with the observation that an edge is never\\ninserted into Xi during the middle of a phase, implies that\\nequations 8, 9 hold all the time. Whenever a node violates\\nequations 4, 5, 7, we make it spurious. Finally, whenever\\nequation 6 is violated, we terminate the phase. This ensures\\nthat all the properties hold all the time.\\n2.4.4 Analyzing the amortized update time.\\nHandling an edge insertion/deletion in Ei in the middle\\nof a phase needs O(1) update time. Just before a phase\\nends, let b and s respectively denote the number of big and\\nspurious nodes. Since a phase ends only when equation 6 is\\nviolated, we have s ≥ δ ·b. In the subroutine TERMINATE-\\nPHASE(.), updating the edge-set EBi requires O(s·di) time,\\nsince we need to go through all the s nodes in S, and for\\neach such node, we need to check all the edges incident upon\\nit (and a node can have at most di edges incident upon it\\nby Corollary 2). At this stage, the set Bi consists of at most\\n(s+b) nodes, and so the set EBi consists of at most (s+b)di\\nedges. Hence, the call to the subroutine SPLIT(EBi) takes\\nO((s+b)di) time. Accordingly, the total time taken to termi-\\nnate the phase is O((s+ b)di) = O((s+ s/δ)di) = O(sdi/δ).\\nWe thus reach the following conclusion: The total time spent\\non a given phase is equal to O(sdi/δ), where s is the num-\\nber of spurious nodes at the end of the phase. Since Si = ∅\\nin the beginning of the phase, we can also interpret s as\\nbeing the number of nodes that becomes spurious during\\nthe phase. Let C denote a counter that is initially set\\nto zero, and is incremented by one each time some node\\nbecomes spurious. From the preceding discussion, it fol-\\nlows that the total update time of our algorithm, across all\\nthe phases, is at most O(Cdi/δ). Let t be the total num-\\nber of edge insertions/deletions in Ei. We will show that\\nC = O(tL/(\\x0f2di)). This will imply an amortized update\\ntime of O((1/t) · Cdi/δ) = O(L/\\x0f2δ) = O(poly log n). The\\nlast equality holds due to equation 3.\\nNote that during a phase a node v becomes spurious be-\\ncause of one of two reasons: (1) It violated equations 4 or 5.\\nIn this case, the node’s status bit is flipped. Hence, by equa-\\ntion 12, between any two such events, at least \\x0fdi/L edge\\ninsertions/deletions occur incident upon v. (2) It violates\\nequation 7. In this event, note that in the beginning of the\\nphase we had v ∈ Bi, degv(Xi) = (1/2) ·degv(Ei) = (1/λi) ·\\ndegv(Ei) and degv(Ei) ≥ \\x0fdi/L. The former guarantee\\nholds since we set Xi ← SPLIT(EBi) at the end of the pre-\\nvious phase, whereas the latter guarantee follows from equa-\\ntion 4. On the other hand, when the node v violates equa-\\ntion 7, we find that degv(Xi) differs from (1/λi) · degv(Ei)\\nby at least (\\x0f/λi) · degv(Ei) = (\\x0f/2) · degv(Ei). Accord-\\ningly, during this time-interval (that starts at the begin-\\nning of the phase and ends when equation 7 is violated),\\nat least Ω(\\x0f2di/L) edge insertions/deletions incident upon\\nv must have taken place in Ei. To summarize, for each\\nunit increment in C, we must have Ω(\\x0f2di/L) edge inser-\\ntions/deletions in Ei. Thus, we have C = O(t/(\\x0f\\n2di/L)) =\\nO(tL/(\\x0f2di)).\\n2.5 Approximation guarantee from approxi-\\nmate skeletons: Proof of Theorem 6\\nWe devote this section to the complete proof of Theo-\\nrem 6. At a high level, the main idea behind the proof\\nremains the same as in Section 2.2.1. We will have to over-\\ncome several intricate obstacles, however, because now we\\nare dealing with the relaxed notion of an approximate skele-\\nton as defined in Section 2.3.\\nWe start by focussing on the second part of Theorem 6,\\nwhich states that the degree of every node in X ∪ Y is at\\nmost O(poly logn). This is stated and proved in Lemma 3.\\nLemma 3. Consider the subsets of edges X ⊆ E and\\nY ⊆ E as per Theorem 6. Then we have degv(X ∪ Y ) =\\nO(poly logn) for every node v ∈ V .\\nProof. We first bound the degree of a node v ∈ V in X.\\nTowards this end, consider any level i > L′. By equation 9,\\nwe have that degv(Xi) ≤ (1/λi) · di = dL′ for all spurious\\nnodes v ∈ Si. By equation 8, we have degv(Xi) ≤ (1/λi) ·\\n(2\\x0fdi/L) = 2\\x0fdL′/L ≤ dL′ for all non-spurious tiny nodes\\nv ∈ Ti \\\\ Si. Finally, by equation 7 and Corollary 2, we\\nhave that degv(Xi) ≤ ((1 + \\x0f)/λi) · degv(Ei) ≤ (1 + \\x0f) ·\\n(di/λi) = (1 + \\x0f) · dL′ for all non-spurious big nodes v ∈\\nBi \\\\ Si. By Definition 2, a node belongs to exactly one of\\nthe three subsets – Si, Ti\\\\Si and Bi\\\\Si. Combining all these\\nobservations, we get: degv(Xi) ≤ (1+\\x0f)·dL′ = O(poly log n)\\nfor all nodes v ∈ V . Now, summing over all i > L′, we get:\\ndegv(X) =\\n∑\\ni>L′ degv(Xi) ≤ (L − L′) · O(poly logn) =\\nO(poly logn) for all the nodes v ∈ V .\\nNext, we bound the degree of a node v ∈ V in Y . Note\\nthat the degree thresholds in the levels [0, L′] are all at most\\ndL′ . Specifically, for all i ≤ L′ and v ∈ V , Corollary 2\\nimplies that degv(Ei) ≤ di ≤ dL′ = O(poly logn). Hence,\\nfor every node v ∈ V , we have degv(Y ) =\\n∑\\ni≤L′ degv(Ei) ≤\\n(L′ + 1) ·O(poly logn) = O(poly logn).\\nTo summarize, the maximum degree of a node in the edge-\\nsets X and Y is O(poly logn). Hence, for every node v ∈ V ,\\nwe have: degv(X∪Y ) = degv(X)+degv(Y ) = O(poly log n).\\nThis concludes the proof of the lemma.\\nWe now focus on the first part of Theorem 6, which guar-\\nantees the existence of a large fractional matching with sup-\\nport X ∪ Y . This is stated in the lemma below. Note that\\nLemma 3 and Lemma 4 together imply Theorem 6.\\nLemma 4. Consider the subsets of edges X ⊆ E and Y ⊆\\nE as per Theorem 6. Then there exists a fractional matching\\nw′ on support X ∪ Y such that w(E) ≤ O(1) · w′(E). Here,\\nw is the fractional matching given by Theorem 5.\\nWe devote the rest of this section to the proof of Lemma 4.\\nAs in Section 2.2.1, we start by defining two fractional as-\\nsignments w+ =\\n∑\\ni>L′ wi and w\\n− =\\n∑\\ni≤L′ wi. In other\\nwords, w+ captures the fractional weights assigned to the\\nedges in levels [L′+1, L] by the hierarchical partition, whereas\\nw− captures the fractional weights assigned to the edges in\\nthe remaining levels [0, L′]. The fractional assignment w+\\nhas support ∪i>L′Ei, whereas the fractional assignment w−\\nhas support ∪i≤L′Ei = Y . We have w = w+ + w− and\\nw(E) = w+(E) + w−(E). If at least half of the value of\\nw(E) is coming from the levels [0, L′], then there is nothing\\nto prove. Specifically, suppose that w−(E) ≥ (1/2) · w(E).\\nThen we can set w′ = w− and obtain w(E) ≤ 2 · w−(E) =\\n2 · w−(Y ) = 2 · w′(Y ) = 2 · w′(X ∪ Y ). This concludes the\\nproof of the lemma. Accordingly, from this point onward,\\nwe will assume that at least half of the value of w(E) is\\ncoming from the levels i > L′. Specifically, we have:\\nw(E) ≤ 2 · w+(E) (13)\\nGiven equation 13, we will construct a fractional matching\\nwith support X whose size is within a constant factor of\\nw(E).5 We want to follow the argument applied to ideal\\nskeletons in Section 2.2.1 (see Definition 1). Accordingly,\\nfor every level i > L′ we now define a fractional assignment\\nwˆi with support Xi.\\nwˆi(e) = 1/dL′ for all edges e ∈ Xi\\n= 0 for all edges e ∈ E \\\\Xi. (14)\\nWe next define the fractional assignment wˆ.\\nwˆ =\\n∑\\ni>L′\\nwˆi (15)\\nIn Section 2.2.1 (see Lemma 2), we observed that wˆ is a\\nfractional matching with support X whose size is exactly the\\nsame as w+(E). This observation, along with equation 13,\\nwould have sufficed to conclude the proof of Lemma 4. The\\nintuition was that at every level i > L′, the degree of a\\nnode v ∈ V in Xi is exactly (1/λi) times its degree in Ei.\\nOn the other hand, the weight of an edge e ∈ Xi under\\nwˆi is exactly λi times its weight under wi. This ensured\\n5Recall that w is the fractional matching given by the hier-\\narchical partition. See Section 2.1.\\nthat the weight of node remained unchanged as we transi-\\ntioned from wi to wˆi, that is, Wv(wi) = Wv(wˆi) for all nodes\\nv ∈ V . Unfortunately, this guarantee will no longer hold for\\napproximate-skeletons. It still seems natural, however, to\\ncompare the weights a node receives under these two frac-\\ntional assignments wi and wˆi. This depends on the status\\nof the node under consideration, depending on whether the\\nnode belongs to the set Bi \\\\ Si, Ti \\\\ Si or Si (see Defini-\\ntion 2). Towards this end, we derive Claims 1, 2, 3. The\\nfirst claim states that the weight of a non-spurious big node\\nunder wˆi is very close to its weight under wi. The second\\nclaim states that the weight of a non-spurious tiny node un-\\nder wˆi is very small (less than 2\\x0f/L). The third claim states\\nthat the weight of a spurious node under wˆi is at most one.\\nClaim 1. For all i > L′ and v ∈ Bi \\\\ Si, we have:\\n(1− \\x0f) ·Wv(wi) ≤Wv(wˆi) ≤ (1 + \\x0f) ·Wv(wi).\\nProof. Fix any level i > L′ and any node v ∈ Bi \\\\ Si.\\nThe claim follows from equation 7 and the facts below:\\n(1) λi = di/dL′ .\\n(2) Wv(wˆi) = (1/dL′) · degv(Xi). See equation 14.\\n(3) Wv(wi) = (1/di) · degv(Ei). See Section 2.1.\\nClaim 2. For all levels i > L′ and non-spurious tiny\\nnodes v ∈ Ti \\\\ Si, we have Wv(wˆi) ≤ 2\\x0f/L.\\nProof. Fix any level i > L′ and any node v ∈ Ti \\\\ Si.\\nThe claim follows from equation 8 and the facts below:\\n(1) λi = di/dL′ .\\n(2) Wv(wˆi) = (1/dL′) · degv(Xi). See equation 14.\\nClaim 3. For all i > L′ and v ∈ Si, we have Wv(wˆi) ≤ 1.\\nProof. Fix any level i > L′ and any node v ∈ Si. The\\nclaim follows from equation 9 and the facts below:\\n(1) λi = di/dL′ .\\n(2) Wv(wˆi) = (1/dL′) · degv(Xi). See equation 14.\\nUnfortunately, the fractional assignment wˆ need not nec-\\nessarily be a fractional matching, the main reason being that\\nat a level i > L′ the new weight Wv(wˆi) of a spurious node\\nv ∈ Si can be much larger than its original weight Wv(wi).\\nSpecifically, Claim 3 permits that Wv(wˆi) = 1 for such a\\nnode v ∈ Si. If there exists a node v ∈ V that belongs\\nto Si at every level i > L\\n′, then we might have Wv(wˆ) =∑\\ni>L′Wv(wˆi) =\\n∑\\ni>L′ 1 = (L− L′) >> 1 ≥Wv(w).\\nTo address this concern regarding the weights of the spuri-\\nous nodes, we switch from wˆ to a new fractional assignment\\nw′′, which is defined as follows. For every level i > L′, we\\nconstruct a fractional assignment w′′i that sets to zero the\\nweight of every edge in Xi that is incident upon a spuri-\\nous node v ∈ Si. For every other edge e, the weight w′′i (e)\\nremains the same as wˆi(e). Then we set w\\n′′ =\\n∑\\ni>L′ w\\n′′\\ni .\\nw′′i (u, v) = wˆi(u, v) if (u, v) ∈ Xi and {u, v} ∩ Si = ∅\\n= 0 if (u, v) ∈ Xi and {u, v} ∩ Si 6= ∅.\\n= 0 else if (u, v) ∈ E \\\\Xi (16)\\nw′′ =\\n∑\\ni>L′\\nw′′i (17)\\nThe above transformation guarantees that Wv(w\\n′′\\ni ) = 0\\nfor every spurious node v ∈ Si at level i > L′. Thus, the ob-\\njection raised above regarding the weights of spurious nodes\\nis no longer valid for the fractional assignment w′′i . We now\\nmake three claims on the fractional assignments wˆ and w′′.\\nClaim 4 bounds the maximum weight of a node under w′′.\\nIts proof appears in Section 2.5.1.\\nClaim 4. We have Wv(w\\n′′) ≤ 1 + 3\\x0f for all v ∈ V .\\nClaim 5 states that the size of w′′ is close to the size of wˆ.\\nIts proof appears in Section 2.5.2.\\nClaim 5. We have w′′(E) ≥ wˆ(E)− 4\\x0f · w+(E).\\nClaim 6 states that the size of wˆ is within a constant factor\\nof the size of w+. Its proof appears in Section 2.5.3.\\nClaim 6. We have wˆ(E) ≥ (1/8) · w+(E).\\nCorollary 4. We have w′′(E) ≥ (1/8− 4\\x0f) · w+(E).\\nProof. Follows from Claims 5 and 6.\\nTo complete the proof of Lemma 4, we scale down the\\nweights of the edges in w′′ by a factor of (1+3\\x0f). Specifically,\\nwe define a fractional assignment w′ such that:\\nw′(e) =\\nw′′(e)\\n(1 + 3\\x0f)\\nfor all edges e ∈ E.\\nSince w′′ has support X, the fractional assignment w′ also\\nhas support X, that is, w′(e) = 0 for all edges e ∈ E \\\\ X.\\nClaim 4 implies that Wv(w\\n′) = Wv(w′′)/(1 + 3\\x0f) ≤ 1 for all\\nnodes v ∈ V . Thus, w′ is fractional matching on support\\nX. Since the edge-weights are scaled down by a factor of\\n(1 + 3\\x0f), Corollary 4 implies that:\\nw′(E) =\\nw′′(E)\\n(1 + 3\\x0f)\\n≥ (1/8− 4\\x0f)\\n(1 + 3\\x0f)\\n· w+(E). (18)\\nEquations 13 and 18 imply that w(E) ≤ O(1) · w′(E). This\\nconcludes the proof of Lemma 4.\\n2.5.1 Proof of Claim 4\\nThroughout the proof, we fix any given node v ∈ V . We\\nwill show that Wv(w\\n′′) ≤ 1 + 3\\x0f. We start by making a\\nsimple observation:\\nWv(w\\n′′\\ni ) ≤Wv(wˆi) for all levels i > L′. (19)\\nEquation 19 holds since we get the fractional assignment w′′i\\nfrom wˆi by setting some edge-weights to zero and keeping\\nthe remaining edge-weights unchanged (see equation 16).\\nBy Definition 2, at every level i > L′ the node v is part\\nof exactly one of the three subsets – Ti \\\\ Si, Bi \\\\ Si and\\nSi. Accordingly, we can classify the levels into three types\\ndepending on which of these subsets v belongs to at that\\nlevel. Further, recall that Wv(w\\n′′) =\\n∑\\ni>L′Wv(w\\n′′\\ni ). We\\nwill separately bound the contributions from each type of\\nlevels towards the node-weight Wv(w\\n′′).\\nWe first bound the contribution towards Wv(w\\n′′) from all\\nthe levels i > L′ where v ∈ Ti \\\\ Si.\\nClaim 7. We have:∑\\ni>L′:v∈Ti\\\\Si\\nWv(w\\n′′\\ni ) ≤ 2\\x0f.\\nProof. Claim 2 implies that:∑\\ni>L′:v∈Ti\\\\Si\\nWv(wˆi) ≤\\n∑\\ni>L′:v∈Ti\\\\Si\\n(2\\x0f/L) ≤ 2\\x0f. (20)\\nThe claim follows from equations 19 and 20.\\nWe next bound the contribution towards Wv(w\\n′′) from all\\nthe levels i > L′ where v ∈ Bi \\\\ Si.\\nClaim 8. We have:∑\\ni>L′:v∈Bi\\\\Si\\nWv(w\\n′′\\ni ) ≤ 1 + \\x0f.\\nProof. Let LHS =\\n∑\\ni>L′:v∈Bi\\\\Si Wv(w\\n′′\\ni ). We have:\\nLHS ≤\\n∑\\ni>L′:v∈Bi\\\\Si\\nWv(wˆi) (21)\\n≤\\n∑\\ni>L′:v∈Bi\\\\Si\\n(1 + \\x0f) ·Wv(wi) (22)\\n≤ (1 + \\x0f) ·\\nL∑\\ni=0\\nWv(wi) = (1 + \\x0f) ·Wv(w)\\n≤ (1 + \\x0f) (23)\\nEquation 21 holds because of equation 19. Equation 22 fol-\\nlows from Claim 1. Finally, equation 23 holds since w is a\\nfractional matching (see Section 2.1).\\nFinally, we bound the contribution towards Wv(w\\n′′) from\\nall the levels i > L′ where v ∈ Si.\\nClaim 9. We have:∑\\ni>L′:v∈Si\\nWv(w\\n′′\\ni ) = 0.\\nProof. Consider any level i > L′ where v ∈ Si. By equa-\\ntion 16, every edge in Xi incident upon v has zero weight\\nunder w′′i , and hence Wv(w\\n′′\\ni ) = 0. The claim follows.\\nAdding up the bounds given by Claims 7, 8 and 9, we get:\\nWv(w\\n′′) =\\n∑\\ni>L′\\nWv(w\\n′′\\ni )\\n=\\n∑\\ni>L′:v∈Ti\\\\Si\\nWv(w\\n′′\\ni ) +\\n∑\\ni>L′:v∈Bi\\\\Si\\nWv(w\\n′′\\ni )\\n+\\n∑\\ni>L′:v∈Si\\nWv(w\\n′′\\ni )\\n≤ 2\\x0f+ (1 + \\x0f) + 0 = 1 + 3\\x0f.\\nThis concludes the proof of Claim 4.\\n2.5.2 Proof of Claim 5\\nFor any given fractional assignment, the some of the node-\\nweights is two times the sum of the edge-weights (since each\\nedge has two endpoints). Keeping this in mind, instead of\\nrelating the sum of the edge-weights under the fractional\\nassignments w′′, wˆ and w+ as stated in Claim 5, we will\\nattempt to relate the sum of the node-weights under w′′, wˆ\\nand w+.\\nAs we switch from the fractional assignment wˆi to the frac-\\ntional assignment w′′i at some level i > L\\n′, all we do is to set\\nto zero the weight of any edge incident upon a spurious node\\nin Si. Hence, intuitively, the difference between the sum of\\nthe node-weights under w′′ =\\n∑\\ni>L′ w\\n′′\\ni and wˆ =\\n∑\\ni>L′ wˆi\\nshould be bounded by the sum of the weights of the spurious\\nnodes across all the levels i > L′. This is formally stated in\\nthe claim below.\\nClaim 10. We have:∑\\nv∈V\\nWv(w\\n′′) ≥\\n∑\\nv∈V\\nWv(wˆ)−\\n∑\\ni>L′\\n∑\\nv∈Si\\n2 ·Wv(wˆi).\\nProof. The left hand side (LHS) of the inequality is ex-\\nactly equal to two times the sum of the edge-weights under\\nw′′. Similarly, the first sum in the right hand side (RHS)\\nis exactly equal to two times the sum of the edge-weights\\nunder wˆ. Finally, we can also express the second sum in the\\nRHS as the sum of certain edge-weights under wˆ.\\nConsider any edge (x, y) ∈ E. We will show that the\\ncontribution of this edge towards the LHS is at least its\\ncontribution towards the RHS, thereby proving the claim.\\nCase 1. (x, y) /∈ Xi for all i > L′. Then the edge (x, y)\\ncontributes zero to the left hand side (LHS) and zero to the\\nright hand side (RHS).\\nCase 2. (x, y) ∈ Xi at some level i > L′, but none of\\nthe endpoints of the edge is spurious, that is, {x, y} ∩ Si =\\n∅. In this case, by equation 16, the edge (x, y) contributes\\n2 · w′′i (x, y) to the LHS, 2 · wˆi(x, y) to the first sum in the\\nRHS, and zero to the second sum in the RHS. Further, we\\nhave w′′i (x, y) = wˆi(x, y). Hence, the edge makes exactly\\nthe same contribution towards the LHS and the RHS.\\nCase 3. (x, y) ∈ Xi at some level i > L′, and at least one\\nendpoint of the edge is spurious, that is, {x, y} ∩ Si 6= ∅. In\\nthis case, by equation 16, the edge (x, y) contributes zero\\nto the LHS, 2 · wˆ(x, y) to the first sum in the RHS, and\\nat least 2 · wˆ(x, y) to the second sum in the RHS. Hence,\\nthe net contribution towards the RHS is at most zero. In\\nother words, the contribution towards the LHS is at least\\nthe contribution towards the RHS.\\nAt every level i > L′, we will now bound the sum of the\\nweights of the spurious nodes v ∈ Si under wˆ by the sum of\\nthe node-weights under wi. We will use the fact that each\\nspurious node gets weight at most one (see Claim 3), which\\nimplies that\\n∑\\nv∈Si Wv(wˆi) ≤ |Si|. By equation 6, we will\\nupper bound the number of spurious nodes by the number of\\nnon-spurious big nodes. Finally, by equation 7, we will infer\\nthat each non-spurious big node has sufficiently large degree\\nin Ei, and hence its weight under wi is also sufficiently large.\\nClaim 11. For every level i > L′, we have:∑\\nv∈Si\\nWv(wˆi) ≤ (2δL/\\x0f) ·\\n∑\\nv∈V\\nWv(wi).\\nProof. Fix any level i > L′. Claim 3 states thatWv(wˆi) ≤\\n1 for all nodes v ∈ Si. Hence, we get:∑\\nv∈Si\\nWv(wˆi) ≤ |Si| (24)\\nEquation 6 implies that |Si| ≤ δ · |Bi| ≤ δ · (|Bi \\\\ Si|+ |Si|).\\nRearranging the terms, we get: |Si| ≤ δ1−δ · |Bi \\\\ Si|. Since\\nδ < 1/2 (see equation 3), we have:\\n|Si| ≤ 2δ · |Bi \\\\ Si| (25)\\nFrom equations 24 and 25, we get:∑\\nv∈Si\\nWv(wˆi) ≤ 2δ · |Bi \\\\ Si| (26)\\nNow, equation 4 states that degv(Ei) ≥ (\\x0fdi/L) for all\\nnodes v ∈ Bi \\\\ Si. Further, in the hierarchical partition\\nwe have Wv(wi) = (1/di) · degv(Ei) for all nodes v ∈ V (see\\nSection 2.1). Combining these two observations, we get:\\nWv(wi) ≥ \\x0f/L for all nodes v ∈ Bi \\\\ Si. Summing over all\\nnodes v ∈ V , we get:∑\\nv∈V\\nWv(wi) ≥\\n∑\\nv∈Bi\\\\Si\\nWv(wi) ≥ (\\x0f/L) · |Bi \\\\ Si| (27)\\nThe claim follows from equations 26 and 27.\\nCorollary 5. We have:∑\\ni>L′\\n∑\\nv∈Si\\nWv(wˆi) ≤ (2δL/\\x0f) ·\\n∑\\nv∈V\\nWv(w\\n+).\\nProof. Follows from summing Claim 11 over all levels\\ni > L′, and noting that since w+ =\\n∑\\ni>L′ wi, we have\\nWv(w\\n+) =\\n∑\\ni>L′Wv(wi) for all nodes v ∈ V .\\nFrom Claim 10 and Corollary 5, we get:∑\\nv∈V\\nWv(w\\n′′) ≥\\n∑\\nv∈V\\nWv(wˆ)− (4δL/\\x0f) ·\\n∑\\nv∈V\\nWv(w\\n+) (28)\\nSince δ = \\x0f2/L (see equation 3) and since the sum of\\nthe node-weights in a fractional assignment is exactly two\\ntimes the sum of the edge-weights, Claim 5 follows from\\nequation 28.\\n2.5.3 Proof of Claim 6\\nEvery edge (u, v) ∈ X = ∪i>L′Xi has at least one end-\\npoint at a level i > L′ (see Definition 2). In other words,\\nevery edge in X has at least one endpoint in the set V ∗ as\\ndefined below.\\nDefinition 3. Define V ∗ = {v ∈ V : `(v) > L′} to be the\\nset of all nodes at levels strictly greater than L′.\\nThus, under any given fractional assignment, the sum of\\nthe node-weights in V ∗ is within a factor of 2 of the sum of\\nthe edge-weights inX. Since both the fractional assignments\\nwˆ and w+ have support X, we get the following claim.\\nClaim 12. We have:\\n2 · w+(E) ≥\\n∑\\nv∈V ∗\\nWv(w\\n+) ≥ w+(E).\\n2 · wˆ(E) ≥\\n∑\\nv∈V ∗\\nWv(wˆ) ≥ wˆ(E).\\nSince we want to compare the sums of the edge-weights\\nunder wˆ and w+, by Claim 12 it suffices to focus on the\\nsum of the node-weights in V ∗ instead. Accordingly, we\\nfirst lower bound the sum\\n∑\\nv∈V ∗Wv(wˆ) in Claim 13. In\\nthe proof, we only use the fact that for each level i > L′,\\nthe weight of a node v ∈ Bi \\\\ Si remains roughly the same\\nunder the fractional assignments wˆi and wi (see Claim 1).\\nClaim 13. We have:∑\\nv∈V ∗\\nWv(wˆ) ≥ (1− \\x0f) ·\\n∑\\nv∈V ∗\\n∑\\ni>L′:v∈Bi\\\\Si\\nWv(wi).\\nProof. Fix any node v ∈ V ∗. By Claim 1, we have:\\nWv(wˆi) ≥ (1 − \\x0f) ·Wv(wi) at each level i > L′ where v ∈\\nBi \\\\ Si. Summing over all such levels, we get:∑\\ni>L′:v∈Bi\\\\Si\\nWv(wˆi) ≥ (1− \\x0f) ·\\n∑\\ni>L′:v∈Bi\\\\Si\\nWv(wi) (29)\\nSince wˆ =\\n∑\\ni>L′ wˆi, we have:\\nWv(wˆ) ≥\\n∑\\ni>L′:v∈Bi\\\\Si\\nWv(wˆi).\\nHence, equation 29 implies that:\\nWv(wˆ) ≥ (1− \\x0f) ·\\n∑\\ni>L′:v∈Bi\\\\Si\\nWv(wi).\\nWe now sum the above inequality over all nodes v ∈ V ∗.\\nIt remains to lower bound the right hand side (RHS) in\\nClaim 13 by\\n∑\\nv∈V ∗Wv(w\\n+). Say that a level i > L′ is of\\nType I, II or III for a node v ∈ V ∗ if v belongs to Bi \\\\ Si,\\nSi or Ti \\\\ Si respectively. By Definition 2, for every node\\nv ∈ V ∗, the set of levels i > L′ is partitioned into these\\nthree types. The sum in the RHS of Claim 13 gives the\\ncontribution of the type I levels towards\\n∑\\nv∈V ∗Wv(w\\n+). In\\nClaims 14 and 15, we respectively show that the type II and\\ntype III levels make negligible contributions towards the sum∑\\nv∈V ∗Wv(w\\n+). Note that the sum of these contributions\\nfrom the type I, type II and type III levels exactly equals∑\\nv∈V ∗Wv(w\\n+). Specifically, we have:∑\\nv∈V ∗\\n∑\\ni>L′:v∈Bi\\\\Si\\nWv(wi) +\\n∑\\nv∈V ∗\\n∑\\ni>L′:v∈Si\\nWv(wi)\\n+\\n∑\\nv∈V ∗\\n∑\\ni>L′:v∈Ti\\\\Si\\nWv(wi) =\\n∑\\nv∈V ∗\\nWv(w\\n+) (30)\\nHence, equation 30, Claim 14 and Claim 15 lead to the\\nfollowing lower bound on the right hand side of Claim 13.\\nCorollary 6. We have:∑\\nv∈V ∗\\n∑\\ni>L′:v∈Bi\\\\Si\\nWv(wi) ≥ (1− 40\\x0f) ·\\n∑\\nv∈V ∗\\nWv(w\\n+).\\nFrom Claim 13, Corollary 6 and equation 3, we get:∑\\nv∈V ∗\\nWv(wˆ) ≥ (1/4) ·\\n∑\\nv∈V ∗\\nWv(w\\n+) (31)\\nFinally, from Claim 12 and equation 31, we infer that:\\nwˆ(E) ≥ (1/2) ·\\n∑\\nv∈V ∗\\nWv(wˆ) ≥ (1/8) ·\\n∑\\nv∈V ∗\\nWv(w\\n+)\\n≥ (1/8) · w+(E)\\nThis concludes the proof of Claim 6. Accordingly, we devote\\nthe rest of this section to the proofs of Claims 14 and 15.\\nClaim 14. We have:∑\\nv∈V ∗\\n∑\\ni>L′:v∈Si\\nWv(wi) ≤ 8\\x0f ·\\n∑\\nv∈V ∗\\nWv(w\\n+).\\nProof. The proof of this claim is very similar to the proof\\nof Claim 11 and Corollary 5. Going through that proof,\\none can verify the following upper bound on the number of\\nspurious nodes across all levels i > L′.∑\\ni>L′\\n|Si| ≤ (2δL/\\x0f) ·\\n∑\\nv∈V\\nWv(w\\n+) (32)\\nSince each wi is a fractional matching (see Section 2.1), we\\nhave Wv(wi) ≤ 1 for all nodes v ∈ V and all levels i > L′.\\nHence, we get:∑\\nv∈V ∗\\n∑\\ni>L′:v∈Si\\nWv(wi) ≤\\n∑\\ni>L′\\n|Si| (33)\\nFrom equations 32 and 33, we infer that:∑\\nv∈V ∗\\n∑\\ni>L′:v∈Si\\nWv(wi) ≤ (2δL/\\x0f) ·\\n∑\\nv∈V\\nWv(w\\n+) (34)\\nSince the sum of the node-weights under any fractional as-\\nsignment is equal to twice the sum of the edge-weights,\\nClaim 12 implies that:∑\\nv∈V\\nWv(w\\n+) = 2 · w+(E) ≤ 4 ·\\n∑\\nv∈V ∗\\nWv(w\\n+) (35)\\nClaim 14 follows from equations 3, 34 and 35.\\nClaim 15. We have:∑\\nv∈V ∗\\n∑\\ni>L′:v∈Ti\\\\Si\\nWv(wi) ≤ 32\\x0f ·\\n∑\\nv∈V ∗\\nWv(w\\n+).\\nProof. Fix any node v ∈ V ∗. By equation 5, we have\\ndegv(Ei) ≤ (2\\x0fdi/L) at each level i > L′ where v ∈ Ti \\\\ Si.\\nFurther, the fractional matching wi assigns a weight 1/di to\\nevery edge in its support Ei (see Section 2.1). Combining\\nthese two observations, we get: Wv(wi) = (1/di)·degv(Ei) ≤\\n2\\x0f/L at each level i > L′ where v ∈ Ti \\\\ Si. Summing over\\nall such levels, we get:∑\\ni>L′:v∈Ti\\\\Si\\nWv(wi) ≤ 2\\x0f (36)\\nIf we sum equation 36 over all v ∈ V ∗, then we get:∑\\nv∈V ∗\\n∑\\ni>L′:v∈Ti\\\\Si\\nWv(wi) ≤ 2\\x0f · |V ∗| (37)\\nA node v ∈ V ∗ has level `(v) > L′. Hence, all the edges\\nincident upon this node also have level at least L′ + 1.\\nThis implies that such a node v receives zero weight from\\nthe fractional assignment w− =\\n∑\\ni≤L′ wi, for any edge in\\nthe support of w− is at level at most L′. Thus, we have:\\nWv(w) = Wv(w\\n+) +Wv(w\\n−) = Wv(w+) for such a node v.\\nNow, applying Theorem 5, we get:\\n1/(1 + \\x0f)2 ≤Wv(w+) for all nodes v ∈ V ∗. (38)\\nSumming equation 38 over all nodes v ∈ V ∗ and multiplying\\nboth sides by (1 + \\x0f)2, we get:\\n|V ∗| ≤ (1 + \\x0f)2 ·\\n∑\\nv∈V ∗\\nWv(w\\n+) (39)\\nSince (1 + \\x0f)2 ≤ 4 and V ∗ ⊆ V , equations 37, 39 imply that:∑\\nv∈V ∗\\n∑\\ni>L′:v∈Ti\\\\Si\\nWv(wi) ≤ 8\\x0f ·\\n∑\\nv∈V\\nWv(w\\n+) (40)\\nThe claim follows from equations 35 and 40.\\n3. BIPARTITE GRAPHS\\nIdeally, we would like to present a dynamic algorithm on\\nbipartite graphs that proves Theorem 2. Due to space con-\\nstraints, however, we will only prove a weaker result stated\\nin Theorem 8 and defer the complete proof of Theorem 2\\nto the full version. Throughout this section, we will use the\\nnotations and concepts introduced in Section 1.1.\\nTheorem 8. There is a randomized dynamic algorithm\\nthat maintains a 1.976 approximation to the maximum match-\\ning in a bipartite graph in O(\\n√\\nn logn) expected update time.\\nIn Section 3.1, we present a result from [5] which shows\\nhow to maintain a (2 + \\x0f)-approximation to the maximum\\nmatching in bipartite graphs in O(\\n√\\nn/\\x0f2) update time. In\\nSection 3.2, we build upon this result and prove Theorem 8.\\nIn Section 3.3, we allude to the extensions that lead us to\\nthe proof of Theorem 2 in the full version of the paper.\\n3.1 (2+\\x0f)-approximation in O(√n/\\x0f2) update time\\nThe first step is to define the concept of a kernel. Setting\\n\\x0f = 0, d = 1 in Definition 4, we note that the kernel edges in\\na (0, 1)-kernel forms a maximal matching – a matching where\\nevery unmatched edge has at least one matched endpoint.\\nFor general d, we note that the kernel edges in a (0, d)-kernel\\nforms a maximal d-matching – which is a maximal subset of\\nedges where each node has degree at most d. In Lemma 5\\nand Corollary 7, we show that the kernel edges in any (\\x0f, d)-\\nkernel preserves the size of the maximum matching within\\na factor of 2/(1 − \\x0f). Since d is the maximum degree of\\na node in an (\\x0f, d)-kernel, a (1 + \\x0f)-approximation to the\\nmaximum matching within a kernel can be maintained in\\nO(d/\\x0f2) update time using Theorem 4. Lemma 6 shows\\nthat the set of kernel edges themselves can be maintained in\\nO(n/(\\x0fd)) update time. Setting d =\\n√\\nn and combining all\\nthese observations, we get our main result in Corollary 8.\\nDefinition 4. Fix any \\x0f ∈ (0, 1), d ∈ [1, n]. Consider any\\nsubset of nodes T ⊂ V in the graph G = (V,E), and any\\nsubset of edges H ⊆ E. The pair (T,H) is called an (\\x0f, d)-\\nkernel of G iff: (1) degv(H) ≤ d for all nodes v ∈ V , (2)\\ndegv(H) ≥ (1− \\x0f)d for all nodes v ∈ T , and (3) every edge\\n(u, v) ∈ E with both endpoints u, v ∈ V \\\\ T is part of the\\nsubset H. We define the set of nodes T c = V \\\\ T , and say\\nthat the nodes in T (resp. T c) are“tight”(resp. “non-tight”).\\nThe edges in H are called “kernel edges”.\\nLemma 5. Consider any integral matching M ⊆ E and\\nlet (T,H) be any (\\x0f, d)-kernel of G = (V,E) as per Defini-\\ntion 4. Then there is a fractional matching w′′ in G with\\nsupport H such that\\n∑\\nv∈V Wv(w\\n′′) ≥ (1− \\x0f) · |M |.\\nThe proof of Lemma 5 appears in Section 3.1.1.\\nCorollary 7. Consider any (\\x0f, d)-kernel as per Defini-\\ntion 4. We have Opt(H) ≥ (1/2) · (1− \\x0f) ·Opt(E).\\nProof. Let M ⊆ E be a maximum cardinality matching\\nin G = (V,E). Let w′′ be a fractional matching with support\\nH as per Lemma 5. Since in a bipartite graph the size of\\nthe maximum cardinality matching is the same as the size of\\nthe maximum fractional matching (see Theorem 3), we get:\\nOpt(H) = Optf (H) ≥ w′′(H) = (1/2) ·\\n∑\\nv∈V Wv(w\\n′′) ≥\\n(1/2) · (1− \\x0f) · |M | = (1/2) · (1− \\x0f) ·Opt(E).\\nLemma 6. In the dynamic setting, an (\\x0f, d)-kernel can be\\nmaintained in O(n/(\\x0fd)) amortized update time.\\nProof. (Sketch) When an edge (u, v) is inserted into the\\ngraph, we simply check if both its endpoints are non-tight.\\nIf yes, then we insert (u, v) into H. Next, for each endpoint\\nx ∈ {u, v}, we check if degx(H) has now become equal to d\\ndue to this edge insertion. If yes, then we delete the node\\nx from T c and insert it into T . All these operations can be\\nperformed in constant time.\\nNow, consider the deletion of an edge (u, v). If both u, v\\nare non-tight, then we have nothing else to do. Otherwise,\\nfor each tight endpoint x ∈ {u, v}, we check if degx(H) has\\nnow fallen below the threshold (1 − \\x0f)d due to this edge\\ndeletion. If yes, then we might need to change the status\\nof the node x from tight to non-tight. Accordingly, we scan\\nthrough all the edges in E that are incident upon x, and try\\nto insert as many of them into H as possible. This step takes\\nΘ(n) time in the worst case since the degree of the node x\\ncan be Θ(n). However, the algorithm ensures that this event\\noccurs only after \\x0fd edges incident upon x are deleted from\\nE. This is true since we have a slack of \\x0fd between the\\nlargest and smallest possible degrees of a tight node. Thus,\\nwe get an amortized update time of O(n/(\\x0fd)).\\nCorollary 8. In a bipartite graph, one can maintain a\\n(2+6\\x0f)-approximation to the size of the maximum matching\\nin O(\\n√\\nn/\\x0f2) amortized update time.\\nProof. (Sketch) We set d =\\n√\\nn and maintain an (\\x0f, d)-\\nkernel (T,H) as per Lemma 6. This takes O(\\n√\\nn/\\x0f) update\\ntime. Next, we note that the maximum degree of a node\\nin H is d =\\n√\\nn (see Definition 4). Accordingly, we can ap-\\nply Theorem 4 to maintain a (1 + \\x0f)-approximate maximum\\nmatching MH ⊆ H in O(√n/\\x0f2) update time. Hence, by\\nCorollary 7, this matching MH is a 2(1+\\x0f)/(1−\\x0f) ≤ (2+6\\x0f)-\\napproximation to the maximum matching inG = (V,E).\\n3.1.1 Proof of Lemma 5\\nFirst, define a fractional assignment w as follows. For\\nevery edge (u, v) ∈ H incident on a tight node, we set w(e) =\\n1/d, and for every other edge (u, v) ∈ E, set w(u, v) = 0.\\nSince each node v ∈ V has degv(H) ≤ d, it is easy to check\\nthat Wv(w) ≤ 1 for all nodes v ∈ V . In other words, w\\nforms a fractional matching in G.\\nNext, we define another fractional assignment w′. First,\\nfor every node v ∈ T c, we define a “capacity” b(v) = 1 −\\nWv(w) ∈ [0, 1]. Next, for every edge (u, v) ∈ H ∩M whose\\nboth endpoints are non-tight, set w′(u, v) = min(b(u), b(v)).\\nFor every other edge (u, v) ∈ E, set w′(u, v) = 0.\\nWe finally define w′′ = w + w′. Clearly, the fractional\\nassignment w′′ has support H, since for every edge (u, v) ∈\\nE \\\\ H, we have w(u, v) = w′(u, v) = 0. Hence, the lemma\\nfollows from Claims 16 and 17.\\nClaim 16. We have Wv(w\\n′′) ≤ 1 for all nodes v ∈ V ,\\nthat is, w′′ is a fractional matching in G.\\nProof. If a node v is tight, that is, v ∈ T , then we have\\nWv(w\\n′′) = Wv(w) + Wv(w′) = Wv(w) ≤ 1. Hence, for\\nthe rest of the proof, consider any node from the remaining\\nsubset v ∈ T c = V \\\\T . There are two cases to consider here.\\nCase 1. If v is not matched in M , then we have Wv(w\\n′) = 0,\\nand hence Wv(w\\n′′) = Wv(w) +Wv(w′) = Wv(w) ≤ 1.\\nCase 2. If v is matched in M , then let u be its mate, i.e.,\\n(u, v) ∈ M . Here, we have Wv(w′) = w′(u, v) = min(1 −\\nWu(w), 1−Wv(w)) ≤ 1−Wv(w). This implies thatWv(w′′) =\\nWv(w) +Wv(w\\n′) ≤ 1. This concludes the proof.\\nClaim 17. We have\\n∑\\nv∈V Wv(w\\n′′) ≥ (1− \\x0f) · |M |.\\nProof. Throughout the proof, fix any edge (u, v) ∈ M .\\nWe will show that Wu(w\\n′′) +Wv(w′′) ≥ (1− \\x0f). The claim\\nwill then follow if we sum over all the edges in M .\\nCase 1. The edge (u, v) has at least one tight endpoint. Let\\nu ∈ T . In this case, we have Wu(w′′)+Wv(w′′) ≥Wu(w′′) =\\nWu(w) +Wu(w\\n′) ≥Wu(w) = (1/d) · degu(H) ≥ (1− \\x0f).\\nCase 2. Both the endpoints of (u, v) are non-tight. Without\\nany loss of generality, let Wu(w) ≥ Wv(w). In this case, we\\nhave Wu(w\\n′′) + Wv(w′′) ≥ Wu(w′′) = Wu(w) + Wu(w′) =\\nWu(w) +w\\n′(u, v) = Wu(w) + min(1−Wu(w), 1−Wv(w)) =\\nWu(w) + (1−Wu(w)) = 1. This concludes the proof.\\n3.2 Better than 2-approximation\\nThe approximation guarantee derived in Section 3.1 fol-\\nlows from Claim 17. Looking back at the proof of this\\nclaim, we observe that we actually proved a stronger state-\\nment: Any matching M ⊆ E satisfies the property that\\nWu(w\\n′′)+Wv(w′′) ≥ (1−\\x0f) for all matched edges (u, v) ∈M ,\\nwhere w′′ is a fractional matching with support H that\\ndepends on M . In the right hand side of this inequality,\\nif we replace the term (1 − \\x0f) by anything larger than 1,\\nthen we will get a better than 2 approximation (see the\\nproof of Corollary 7). The reason it was not possible to do\\nso in Section 3.1 is as follows. Consider a matched edge\\n(u, v) ∈ M with u ∈ T and v ∈ T c. Since u is tight,\\nwe have 1 − \\x0f ≤ Wu(w) = Wv(w′′) ≤ 1. Suppose that\\nWu(w\\n′′) = 1 − \\x0f. In contrast, it might well be the case\\nthat Wv(w) is very close to being zero (which will happen\\nif degv(H) is very small). Let Wv(w) ≤ \\x0f. Also note that\\nWv(w\\n′′) = Wv(w) + Wv(w′) = Wv(w) ≤ \\x0f since no edge\\nthat gets nonzero weight under w′ can be incident on v\\n(for v is already incident upon an edge in M whose other\\nendpoint is tight). Hence, in this instance we will have\\nWu(w\\n′′) +Wv(w′′) ≤ (1− \\x0f) + \\x0f = 1, where (u, v) ∈M is a\\nmatched edge with one tight and one non-tight endpoint.\\nThe above discussion suggests that we ought to “throw\\nin” some additional edges into our kernel – edges whose one\\nendpoint is tight and the other endpoint is non-tight with\\na very small degree in H. Accordingly, we introduce the\\nnotion of residual edges in Section 3.2.1. We show that the\\nunion of the kernel edges and the residual edges preserves\\nthe size of the maximum matching within a factor of strictly\\nless than 2. Throughout the rest of this section, we set the\\nvalues of two parameters δ, \\x0f as follows.\\nδ = 1/20, \\x0f = 1/2000 (41)\\n3.2.1 The main framework: Residual edges\\nWe maintain an (\\x0f, d)-skeleton (T,H) as in Section 3.1.\\nWe further partition the set of non-tight nodes T c = V \\\\ T\\ninto two subsets: B ⊆ T c and S = T c \\\\B. The set of nodes\\nin B (resp. S) are called “big” (resp. “small”). They satisfy\\nthe following degree-thresholds: (1) degv(H) ≤ 2δd/(1− δ)\\nfor all small nodes v ∈ S, and (2) degv(H) ≥ (2δ−\\x0f)d/(1−δ)\\nfor all big nodes v ∈ B. Let Er = {(u, v) ∈ E : u ∈ T, v ∈ S}\\nbe the subset of edges joining the tight and the small nodes.\\nWe maintain a maximal subset of edges Mr ⊆ Er subject\\nto the following constraints: (1) degv(M\\nr) ≤ 1 for all tight\\nnodes v ∈ T and (2) degv(Mr) ≤ 2 for all small nodes v ∈ S.\\nThe edges in Mr are called the “residual edges”. The degree\\nof a node in Mr is called its “residual degree”. The corollary\\nbelow follows from the maximality of the set Mr ⊆ Er.\\nCorollary 9. If an edge (u, v) ∈ Er with u ∈ T , v ∈ S\\nis not in Mr, then either degv(M\\nr) = 1 or degu(M\\nr) = 2.\\nLemma 7. We can maintain the set of kernel edges H\\nand the residual edges Mr in O(n logn/(\\x0fd)) update time.\\nProof. (Sketch) We maintain an (\\x0f, d)-kernel as per the\\nproof of Lemma 6. We maintain the node-setsB,S ⊆ T c and\\nthe edge-set Er in the same lazy manner: A node changes\\nits status only after Ω(\\x0fd) edges incident upon it are either\\ninserted into or deleted from G (since δ is a constant), and\\nwhen that happens we might need to make Θ(n) edge inser-\\ntions/deletions in Er. This gives the same amortized update\\ntime of O(n/(\\x0fd)) for maintaining the edge-set Er.\\nIn order to maintain the set of residual edges Mr ⊆ Er,\\nwe use a simple modification of the dynamic algorithm of\\nBaswana et al. [2] that maintains a maximal matching in\\nO(logn) update time. This holds since Mr is a maximal\\nb-matching in Er where each small node can have at most\\ntwo matched edges incident upon it, and each tight node can\\nhave at most one matched edge incident upon it.\\nLemma 8. Fix any (\\x0f, d) kernel (T,H) as in Section 3.1,\\nany set of residual edges Mr as in Section 3.2.1, and any\\nmatching M ⊆ E. Then we have a fractional matching w′′\\non support H∪Mr such that∑v∈V Wv(w′′) ≥ (1+δ/4)·|M |.\\nRoadmap for the rest of this section. The statement of\\nLemma 8 above is similar to that of Lemma 5 in Section 3.1.\\nHence, using a similar argument as in Corollary 7, we in-\\nfer that the set of edges Mr ∪ H preserves the size of the\\nmaximum matching within a factor of 2/(1 + δ/4). Since\\ndegv(M\\nr ∪ H) = degv(H) + degv(Mr) ≤ d + 2 for all\\nnodes v ∈ V (see Definition 4), we can maintain a (1 + \\x0f)-\\napproximate maximum matching in H ∪ Mr using Theo-\\nrem 4 in O(d/\\x0f2) update time. This matching will give a\\n2(1+\\x0f)/(1+δ/4) = 1.976-approximation to the size of max-\\nimum matching in G (see equation 41). The total update\\ntime is O(d/\\x0f2 + n logn/(\\x0fd)), which becomes O(\\n√\\nn logn)\\nif we set d =\\n√\\nn and plug in the value of \\x0f. This concludes\\nthe proof of Theorem 8.\\nIt remains to prove Lemma 8, which is done in Section 3.2.2.\\n3.2.2 Proof of Lemma 8\\nWe will define four fractional assignments w,wr, w′, w′′. It\\nmight be instructive to contrast the definitions of the frac-\\ntional assignments w,w′ and w′′ here with Section 3.1.1.\\nThe fractional assignment w: Set w(e) = (1 − δ)/d for ev-\\nery edge e ∈ H incident upon a tight node. Set w(e) = 0\\nfor every other edge e ∈ E. Hence, we have Wv(w) =\\n((1 − δ)/d) · degv(H) for all nodes v ∈ V . Accordingly,\\nrecalling the bounds on degv(H) for various types of nodes\\n(see Definition 4, Section 3.2.1), we get:\\nWv(w) ≤ (1− δ) for all nodes v ∈ V. (42)\\nWv(w) ≤ 2δ for all small nodes v ∈ S. (43)\\nWv(w) ≥ (1− δ)(1− \\x0f) for all tight nodes v ∈ T. (44)\\nWv(w) ≥ 2δ − \\x0f for all big nodes v ∈ B. (45)\\nThe fractional assignment wr: Set wr(e) = δ for every resid-\\nual edge e ∈Mr. Set wr(e) = 0 for every other edge e ∈ E.\\nThe fractional assignment w′: For every node v ∈ T c, we\\ndefine a “capacity” b(v) as follows. If v ∈ B ⊆ T c, then\\nb(v) = 1 − Wv(w). Else if v ∈ S = T c \\\\ B, then b(v) =\\n1− 2δ −Wv(w). Hence, equations 42, 43 imply that:\\nb(v) ≥ δ for all big nodes v ∈ B. (46)\\nb(v) ≥ 1− 4δ for all small nodes v ∈ S. (47)\\nFor every edge (u, v) ∈ M with u, v ∈ T c = V \\\\ T , we\\nset w′(u, v) = min(b(u), b(v)). For every other edge e ∈ E,\\nwe set w′(e) = 0. By Definition 4, every edge whose both\\nendpoints are non-tight is a kernel edge. Hence, an edge gets\\nnonzero weight under w′ only if it is part of the kernel.\\nThe fractional assignment w′′: Define w′′ = w + wr + w′.\\nRoadmap for the rest of the proof. Each of the fractional\\nassignments w,wr, w′ assigns zero weight to every edge e ∈\\nE \\\\ (H ∪Mr). Hence, the fractional assignment w′′ = w +\\nwr +w′ has support H ∪Mr. In Claim 18, we show that w′′\\nis a fractional matching in G. Moving on, in Definition 5,\\nwe partition the set of matched edges in M into two parts.\\nThe subset M1 ⊆ M consists of those matched edges that\\nhave one tight and one small endpoints, and the subsetM2 =\\nM\\\\M1 consists of the remaining edges. In Claims 19 and 20,\\nwe relate the node-weights under w,w′, wr with the sizes of\\nthe matchings M1 and M2. Adding up the bounds from\\nClaims 19 and 20, Corollary 10 lower bounds the sum of\\nthe node-weights under w′′ by the size of the matching M .\\nFinally, Lemma 8 follows from Claim 18 and Corollary 10.\\nClaim 18. We have Wv(w\\n′′) ≤ 1 for all nodes v ∈ V .\\nProof. Consider any node v ∈ V . By equation 42, we\\nhave: Wv(w) ≤ 1 − δ. Also note that Wv(wr) ≤ δ for all\\ntight nodes v ∈ T , Wv(wr) ≤ 2δ for all small nodes v ∈ S,\\nand Wv(w\\nr) = 0 for all big nodes v ∈ B. This holds since\\nthe degree (among the edges in Mr) of a tight, small and big\\nnode is at most one, two and zero respectively. Next, note\\nthat for all nodes v ∈ T c, we have Wv(w′) ≤ b(v). This holds\\nsince there is at most one edge in M ∩ H incident upon v\\n(since M is a matching). So at most one edge incident upon\\nv gets a nonzero weight under w′, and the weight of this edge\\nis at most b(v). Finally, note that every edge with nonzero\\nweight under w′ has both the endpoints in T c. Hence, we\\nhave Wv(w\\n′) = 0 for all tight nodes v ∈ T . To complete the\\nproof, we now consider three possible cases.\\nCase 1. v ∈ T . Here, Wv(w′′) = Wv(w) + Wv(wr) +\\nWv(w\\n′) = Wv(w) +Wv(wr) ≤Wv(w) + δ ≤ (1− δ) + δ = 1.\\nCase 2. v ∈ S. Here, Wv(w′′) = Wv(w) + Wv(wr) +\\nWv(w\\n′) ≤ Wv(w) + 2δ + b(v) = Wv(w) + 2δ + (1 − 2δ −\\nWv(w)) = 1.\\nCase 3. v ∈ B. Here, Wv(w′′) = Wv(w) + Wv(wr) +\\nWv(w\\n′) = Wv(w) + Wv(w′) ≤ Wv(w) + b(v) = Wv(w) +\\n(1−Wv(w)) = 1.\\nDefinition 5. Partition the set of edges in M into two\\nparts: M1 = {(u, v) ∈M : u ∈ T, v ∈ S} and M2 = M \\\\M1.\\nClaim 19. Recall Definition 5. We have:∑\\n(u,v)∈M2\\nWu(w + w\\n′) +Wv(w + w\\n′) ≥ (1 + δ/4) · |M2|.\\nProof. Fix any edge (u, v) ∈M2, and let LHS = Wu(w+\\nw′) +Wv(w+w′). We will show that LHS ≥ (1 + δ/4). The\\nclaim will then follow if we sum over all such edges M2. We\\nrecall equation 41 and consider four possible cases.\\nCase 1. Both endpoints are tight, that is, u, v ∈ T . Here,\\nfrom equation 44 we get: LHS ≥ 2·(1−δ−\\x0f+δ\\x0f) ≥ (1+δ/4).\\nCase 2. One endpoint is tight, and one endpoint is big, that\\nis, u ∈ T , v ∈ B. Here, from equations 44, 45 we get:\\nLHS ≥ (1− δ − \\x0f+ δ\\x0f) + (2δ − \\x0f) ≥ (1 + δ − 2\\x0f) ≥ 1 + δ/4.\\nCase 3. Both endpoints are non-tight, that is, u, v ∈ B ∪ S.\\nWithout any loss of generality, let b(u) ≥ b(v). Note that\\n(u, v) ∈ H since both u, v ∈ T c, and hence (u, v) ∩M2 ∩H.\\nThus, we have Wu(w\\n′) = Wv(w′) = w′(u, v) = b(v) since\\nat most one matched edge can be incident upon a node.\\nNow, either v ∈ B or v ∈ S. In the former case, from\\nequation 47 we get: LHS ≥ Wu(w′) + Wv(w′) = 2 · b(v) ≥\\n2(1−4δ) ≥ (1+δ/4). In the latter case, from equation 46 we\\nget: LHS ≥ (Wv(w)+Wv(w′))+Wu(w′) = (b(v)+Wv(w))+\\nb(v) = 1 + b(v) ≥ 1 + δ ≥ 1 + δ/4.\\nClaim 20. Recall Definition 5. We have:∑\\n(u,v)∈M1\\n(Wu(w) +Wv(w)) +\\n∑\\nv∈V\\nWv(w\\nr) ≥ (1 + δ/4) · |M1|.\\nProof. Every edge (u, v) ∈M1 has one endpoint u ∈ T .\\nThus, Applying equation 44 we get: Wu(w) + Wv(w) ≥\\nWu(w) ≥ 1− δ − \\x0f. Summing over all such edges, we get:∑\\n(u,v)∈M1\\nWu(w) +Wv(w) ≥ (1− δ − \\x0f) · |M1| (48)\\nRecall that degu(M\\nr) ≤ 1 for every tight node u ∈ T . Ac-\\ncordingly, we classify each tight node as being either “full”\\n(in which case degu(M\\nr) = 1) or “deficient” (in which case\\ndegu(M\\nr) = 0). Further, recall that each edge (u, v) ∈ M1\\nhas one tight and one small endpoints. We say that an edge\\n(x, y) ∈ M1 is deficient if the tight endpoint of the edge\\nis deficient. Now, consider any deficient edge (x, y) ∈ M1\\nwhere x ∈ T and y ∈ S. Since degx(Mr) = 0, it follows\\nthat (x, y) ∈ Er \\\\Mr. From the maximality of Mr, we infer\\nthat degy(M\\nr) = 2. Accordingly, there must be two edges\\n(x′, y), (x′′, y) ∈ Mr with x′, x′′ ∈ T . It follows that both\\nthe nodes x′, x′′ are full. We say that the tight nodes x′, x′′\\nare conjugates of the deficient edge (x, y) ∈ M1. In other\\nwords, we have shown that every deficient edge in M1 has\\ntwo conjugate tight nodes. Further, the same tight node x′\\ncannot be a conjugate of two different deficient edges in M1,\\nfor otherwise each of those deficient edges will contribute one\\ntowards degx′(M\\nr), and we will get degx′(M\\nr) ≥ 2, which is\\na contradiction. Thus, a simple counting argument implies\\nthat the number of conjugate tight nodes is exactly twice the\\nnumber of deficient matched edges in M1. Let D(M1), F, C\\nrespectively denote the set of deficient matched edges in M1,\\nthe set of full tight nodes and the set of conjugate tight\\nnodes. Thus, we get:\\nT ⊇ F ⊇ C, D(M1) ⊆M1, and |C| = 2 · |D(M1)| (49)\\nNow, either |D(M1)| ≤ (1/3)·|M1| or |D(M1)| > (1/3)·|M1|.\\nIn the former case, at least a (2/3)rd fraction of the edges\\nin M1 are not deficient, and each such edge has one tight\\nendpoint that is full. Thus, we get |F | ≥ (2/3) · |M1|. In\\nthe latter case, from equation 49 we get |F | ≥ |C| = 2 ·\\n|D(M1)| > (2/3) · |M1|. Thus, in either case we have |F | ≥\\n(2/3) · |M1|. Since each node v ∈ F ⊆ T has degv(Mr) = 1,\\nand since each edge e ∈Mr has weight wr(e) = δ, it follows\\nthat Wv(w\\nr) = δ for all nodes v ∈ F ⊆ T . Hence, we get∑\\nv∈T Wv(w\\nr) ≥ δ · |F | ≥ (2δ/3) · |M1|. Next, we note that\\neach edge in Mr contributes the same amount δ towards\\nthe weights of both its endpoints – one tight and the other\\nsmall. Thus, we have:∑\\nv∈S\\nWv(w\\nr) =\\n∑\\nv∈T\\nWv(w\\nr) ≥ (2δ/3) · |M1|.\\nSince B ∪ S ⊆ V and B ∩ S = ∅, we get:∑\\nv∈V\\nWv(w\\nr) ≥\\n∑\\nv∈B∪S\\nWv(w\\nr) ≥ (4δ/3) · |M1|.\\nThis inequality, along with equation 48, gives us:∑\\n(u,v)∈M1\\n(Wu(w) +Wv(w)) +\\n∑\\nv∈V\\nWv(w\\nr)\\n≥ (1− δ − \\x0f) · |M1|+ (4δ/3) · |M1| = (1 + δ/3− \\x0f) · |M1|\\n≥ (1 + δ/4) · |M1|.\\nThe last inequality follows from equation 41.\\nCorollary 10. We have:∑\\nv∈V\\nWv(w\\n′′) ≥ (1 + δ/4) · |M |.\\nProof. Since |M | = |M1| + |M2|, the corollary follows\\nfrom adding the inequalities stated in Claims 19 and 20,\\nand noting that no node-weight under w′′ is counted twice\\nin the left hand side.\\n3.3 Extensions\\nWe gave a randomized algorithm for maximum bipartite\\nmatching that maintains a better than 2 approximation in\\nO(\\n√\\nn logn) update time. In the full version of the paper, we\\nderandomize this scheme using the following idea. Instead\\nof applying the randomized maximal matching algorithm\\nfrom [2] for maintaining the set of residual edges Mr, we\\nmaintain a residual fractional matching using the determin-\\nistic algorithm from [5] (see Theorem 5). To carry out the\\napproximation guarantee analysis, we have to change the\\nproof of Lemma 8 (specifically, the proof of Claim 20).\\nTo get arbitrarily small polynomial update time, we main-\\ntain a partition of the node-set into multiple levels. The top\\nlevel consists of all the tight nodes (see Definition 4). We\\nnext consider the subgraph induced by the non-tight nodes.\\nEach edge in this subgraph is a kernel edge (see Definition 4).\\nIntuitively, we split the node-set of this subgraph again into\\ntwo parts by defining a kernel within this subgraph. The\\ntight nodes we get in this iteration forms the next level in\\nour partition of V . We keep doing this for K levels, where\\nK is a sufficiently large integer. We show that (a) this struc-\\nture can be maintained in O(n2/K) update time, and (b) by\\ncombining the fractional matchings from all these levels, we\\ncan get an αK approximate maximum fractional matching\\nin G, where 1 ≤ αK < 2. By Theorem 3, this gives αK-\\napproximation to the size of the maximum integral matching\\nin G. See the full version of the paper for the details.\\n4. OPEN PROBLEMS\\nIn this paper, we presented two deterministic dynamic al-\\ngorithms for maximum matching. Our first algorithm main-\\ntains a (2+ \\x0f)-approximate maximum matching in a general\\ngraph in O(poly(logn, 1/\\x0f)) update time. The exponent hid-\\nden in the polylogorithmic factor of the update time, how-\\never, is rather huge. It will be interesting to bring down\\nthe update time of this algorithm to O(logn/\\x0f2) without\\nincreasing the approximation factor. This will match the\\nupdate time in [5] for maintaining a fractional matching.\\nWe also showed how to maintain a better than 2 approx-\\nimation to the size of the maximum matching on bipartite\\ngraphs in O(n2/K) update time, for every sufficiently large\\ninteger K. The approximation ratio approaches 2 as K be-\\ncomes large. The main open problem here is to design a\\ndynamic algorithm that gives better than 2 approximation\\nin polylogarithmic update time. This remains open even on\\nbipartite graphs and even if one allows randomization.\\n5. REFERENCES\\n[1] A. Abboud and V. Vassilevska Williams. Popular\\nconjectures imply strong lower bounds for dynamic\\nproblems. In FOCS, 2014.\\n[2] S. Baswana, M. Gupta, and S. Sen. Fully dynamic\\nmaximal matching in O(logn) update time. In FOCS,\\n2011.\\n[3] A. Bernstein and C. Stein. Fully dynamic matching in\\nbipartite graphs. In ICALP, 2015.\\n[4] A. Bernstein and C. Stein. Faster fully dynamic\\nmatchings with small approximation ratios. In SODA,\\n2016.\\n[5] S. Bhattacharya, M. Henzinger, and G. F. Italiano.\\nDeterministic fully dynamic data structures for vertex\\ncover and matching. CoRR, abs/1412.1318, 2014.\\nAnnounced at SODA 2015.\\n[6] M. Crouch and D. S. Stubbs. Improved streaming\\nalgorithms for weighted matching, via unweighted\\nmatching. In APPROX/RANDOM, 2014.\\n[7] M. Gupta and R. Peng. Fully dynamic\\n(1 + \\x0f)-approximate matchings. In FOCS, 2013.\\n[8] M. Henzinger, S. Krinninger, D. Nanongkai, and\\nT. Saranurak. Unifying and strengthening hardness for\\ndynamic problems via the online matrix-vector\\nmultiplication conjecture. In STOC, 2015.\\n[9] A. Israeli and Y. Shiloach. An improved parallel\\nalgorithm for maximal matching. Information\\nProcessing Letters, 22:57–60, 1986.\\n[10] C. Konrad, F. Magniez, and C. Mathieu. Maximum\\nmatching in semi-streaming with few passes. In\\nAPPROX-RANDOM, 2012.\\n[11] T. Kopelowitz, S. Pettie, and E. Porat. Higher lower\\nbounds from the 3SUM conjecture. In SODA, 2016.\\n[12] O. Neiman and S. Solomon. Simple deterministic\\nalgorithms for fully dynamic maximal matching. In\\nSTOC, 2013.\\n[13] K. Onak and R. Rubinfeld. Maintaining a large\\nmatching and a small vertex cover. In STOC, 2010.\\n[14] M. Patrascu. Towards polynomial lower bounds for\\ndynamic problems. In STOC, 2010.\\n[15] D. Peleg and S. Solomon. Dynamic\\n(1 + \\x0f)-approximate matchings: A density-sensitive\\napproach. In SODA, 2016.\\n[16] P. Sankowski. Faster dynamic matchings and vertex\\nconnectivity. In SODA, 2007.\\n[17] V. G. Vizing. The chromatic class of a multigraph.\\nKibernetika, 3:29–39, 1965.\\n\"}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd1f'), 'authors': 'Bringmann, K., Wellnitz, P.', 'year': '2020', 'title': 'On Near-Linear-Time Algorithms for Dense Subset Sum', 'full_text': 'On Near-Linear-Time Algorithms for Dense Subset Sum\\nKarl Bringmann\\nSaarland University and Max Planck Institute for Informatics,\\nSaarland Informatics Campus, Saarbrücken, Germany\\nbringmann@cs.uni-saarland.de\\nPhilip Wellnitz\\nMax Planck Institute for Informatics,\\nSaarland Informatics Campus, Saarbrücken, Germany\\nwellnitz@mpi-inf.mpg.de\\nAbstract\\nIn the Subset Sum problem we are given a set of n positive integers X and a target t and are asked whether some\\nsubset of X sums to t. Natural parameters for this problem that have been studied in the literature are n and t as\\nwell as the maximum input number mxX and the sum of all input numbers ΣX . In this paper we study the dense\\ncase of Subset Sum, where all these parameters are polynomial in n. In this regime, standard pseudo-polynomial\\nalgorithms solve Subset Sum in polynomial time nO(1).\\nOur main question is: When can dense Subset Sum be solved in near-linear time Õ(n)? We provide an\\nessentially complete dichotomy by designing improved algorithms and proving conditional lower bounds, thereby\\ndetermining essentially all settings of the parameters n, t,mxX ,ΣX for which dense Subset Sum is in time Õ(n).\\nFor notational convenience we assume without loss of generality that t ≥ mxX (as larger numbers can be ignored)\\nand t ≤ ΣX/2 (using symmetry). Then our dichotomy reads as follows:\\nBy reviving and improving an additive-combinatorics-based approach by Galil and Margalit [SICOMP’91], we\\nshow that Subset Sum is in near-linear time Õ(n) if t\\x1d mxXΣX/n2.\\nWe prove a matching conditional lower bound: If Subset Sum is in near-linear time for any setting with\\nt\\x1c mxXΣX/n2, then the Strong Exponential Time Hypothesis and the Strong k-Sum Hypothesis fail.\\nWe also generalize our algorithm from sets to multi-sets, albeit with non-matching upper and lower bounds.\\nKeywords and phrases Subset sum, fine-grained complexity theory, additive combinatorics\\nFunding Karl Bringmann: This work is part of the project TIPEA that has received funding from the European\\nResearch Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant\\nagreement No. 850979).\\nar\\nX\\niv\\n:2\\n01\\n0.\\n09\\n09\\n6v\\n1 \\n [\\ncs\\n.D\\nS]\\n  1\\n8 \\nO\\nct\\n 2\\n02\\n0\\n1 On Near-Linear-Time Algorithms for Dense Subset Sum\\n1 Introduction\\nIn the Subset Sum problem we are given a (multi-)set X of n positive integers and a target t and want to\\ndecide whether some subset of X sums to t. Subset Sum is the most fundamental NP-hard problem at the\\nintersection of theoretical computer science, mathematical optimization, and operations research. It draws\\nmuch of its motivation from being a hard special case of many other problems, e.g., Knapsack and Integer\\nProgramming. An additional modern motivation is lattice-based crypto, which is partly based upon\\naverage-case hardness of a variant of Subset Sum (specifically the Short Integer Solution problem [3]).\\nAlgorithms for Subset Sum have been studied for many decades (see, e.g., the monograph [29]), and\\nthe literature on this problem is still flourishing (see, e.g., [8, 9, 11, 28, 31]). Maybe the most well-known\\nalgorithm for Subset Sum is Bellman’s pseudopolynomial O(nt)-time algorithm [10]. This was recently\\nimproved to randomized time1 Õ(n+ t) [11]; for further logfactor improvements see [28]. Using the modern\\ntoolset of fine-grained complexity theory, this improved running time was shown to be near-optimal,\\nspecifically any t1−ε2o(n)-time algorithm would violate the Strong Exponential Time Hypothesis [2], and a\\nsimilar lower bound holds under the Set Cover Hypothesis [19]. This essentially settles the time complexity\\nwith respect to parameters n, t. Alternative parameters for Subset Sum are the maximum input number,\\nwhich we denote by mxX , and the sum of all input numbers, which we denote by ΣX . Studying Subset\\nSum with respect to these parameters has been a significant effort in theoretical computer science and\\noptimization, as illustrated by Table 1. In particular, it is known that Subset Sum can be solved in time\\nO(n ·mxX) [38] or Õ(ΣX) [31]. The most crucial open problem in this line of research is whether Subset\\nSum can be solved in time Õ(n+ mxX), see [8].\\nThis open problem illustrates that we are far from a complete understanding of the complexity of\\nSubset Sum with respect to the combined parameters n, t,mxX ,ΣX . A line of work from around 1990\\n[15, 16, 22, 25, 26] suggests that this complexity is fairly complicated, as it lead to the following result.\\nTheorem 1.1 (Galil and Margalit [26]). Given a set X of n positive integers and a target t ≤ ΣX/2, if2\\nt\\x1d mxXΣX/n2 then Subset Sum can be solved in time Õ(n+ mx2X/n2).\\nFor understanding the complexity of Subset Sum with respect to the parameters n, t,mxX ,ΣX , Galil and\\nMargalit’s algorithm provides a highly non-trivial upper bound in a complicated regime. In their paper\\nthey argue that their approach must fail outside their feasible regime. Nevertheless one can wonder: Is the\\noptimal time complexity of Subset Sum really so complicated, or is this an artefact of Galil and Margalit’s\\napproach? In particular, can we show a lower bound that establishes their regime to be natural? Note that\\nGalil and Margalit discovered a non-trivial regime in which Subset Sum can be solved in near-linear time\\nÕ(n), namely when ΣX/2 ≥ t\\x1d mxXΣX/n2 and mxX = Õ(n3/2). One can wonder: Can this near-linear\\ntime regime be extended? What is the largest possible regime in which Subset Sum is in near-linear time?\\nIn this paper, we provide answers to all of these questions.\\nFirst, let us discuss the details of Galil and Margalit’s result. Note that the assumption t ≤ ΣX/2 is\\nwithout loss of generality: For t > ΣX the problem is trivial, and for ΣX/2 < t ≤ ΣX any subset Y ⊆ X\\nsums to t if and only if X \\\\ Y sums to ΣX − t, so the inputs (X, t) and (X,ΣX − t) are equivalent. In\\nparticular, an alternative formulation is that this algorithm solves Subset Sum very efficiently when the\\ntarget lies in a feasible interval centered around ΣX/2. Note that the feasible interval is only non-empty if\\n1 We write Õ(T ) for any function that is bounded by O(T logc T ) for some c > 0.\\n2 We use the notation “f \\x1d g” only in the informal overview of our results. We mostly use it to hide polylogarithmic\\nfactors, sometimes also to hide subpolynomial factors. Formally, for functions f, g and a property P , we write “if\\nf \\x1d g then P” if the following statement is true: For any ε > 0 there exists C > 0 such that f ≥ C · nε · g implies P .\\nK. Bringmann and P. Wellnitz 2\\nReference Running Time Comments\\nBellman [10] O(nt)\\nPisinger [39] O(nt/w) RAM model with cells of w bits\\nPisinger [38] O(nmxX)\\nKlinz and Woeginger [30] O(Σ3/2X )\\nEppstein [20], Serang [44] Õ(nmxX) data structure\\nLokshtanov and Nederlof [36] O(n3t) polynomial space, see also [11]\\nKoiliaris and Xu [31] Õ(\\n√\\nnt+ n)\\nKoiliaris and Xu [31] Õ(t5/4 + n)\\nKoiliaris and Xu [31] Õ(ΣX)\\nBringmann [11] Õ(t+ n) randomized\\nJin and Wu [28] Õ(t+ n) randomized, improved logfactors\\nTable 1 Short survey of pseudopolynomial-time algorithms for Subset Sum on multi-sets. The input consists of a\\nmulti-set X of positive integers and a target number t. We write n for the size of X, mxX for the maximum number\\nin X, and ΣX for the sum of all numbers in X. This table does not contain the line of work [15, 16, 22, 25, 26]\\nleading to Galil and Margalit’s algorithm (Theorem 1.1), because these algorithms only work on sets.\\nmxX \\x1c n2, so this result only applies to the dense setting of Subset Sum, where all parameters t,mxX ,ΣX\\nare polynomial in n. Moreover, while all previously mentioned algorithms also work when X is a multi-set,\\nGalil and Margalit’s algorithm really requires X to be a set. Under these strict conditions, their algorithm\\ngives a highly non-trivial result, that uses structural insights from additive combinatorics about arithmetic\\nprogressions in the set of all subset sums. Since it discovers a regime where Subset Sum can be solved\\nsurprisingly fast, it has for instance recently found use in an approximation algorithm for the Partition\\nproblem [37].\\nWe remark that the conference version of Galil and Margalit’s paper [26] claims the result as stated in\\nTheorem 1.1, but does not contain all proof details. The journal version of their paper [25] only proves a\\nweaker result, assuming the stricter condition t \\x1d mx1/2X ΣX/n. Nevertheless, their conference version\\nwas recently cited and used in [37]. It would therefore be desirable to have an accessible full proof of\\nTheorem 1.1. In any case, we will compare the results of this paper with Theorem 1.1.\\n1.1 Our Contribution\\nIn this paper, we study Subset Sum in the dense regime, where the parameters t,mxX ,ΣX are all\\nbounded by a polynomial in n. In this regime, any pseudopolynomial-time algorithm solves Subset Sum\\nin polynomial time poly(n). Our main result is an essentially complete dichotomy that determines all\\nsettings of the parameters n, t,mxX ,ΣX where Subset Sum can be solved in near-linear time Õ(n).\\nWe start by discussing the case where X is a set (not a multi-set).\\nAlgorithm\\nGalil and Margalit discovered a non-trivial regime where Subset Sum can be solved in near-linear time Õ(n),\\nnamely when ΣX/2 ≥ t \\x1d mxXΣX/n2 and mxX = Õ(n3/2). We extend the near-linear-time regime,\\nspecifically we remove the restriction mxX = Õ(n3/2) from their regime. We achieve this by following the\\n3 On Near-Linear-Time Algorithms for Dense Subset Sum\\nsame high-level approach as Galil and Margalit, but exchanging almost all parts of the algorithm in order\\nto improve the running time. Moreover, we provide a full proof which we think is easily accessible.\\nTheorem 1.2. Given a set X of n positive integers and a target t ≤ ΣX/2, if t\\x1d mxXΣX/n2 then\\nSubset Sum can be solved in time Õ(n).\\nOur \\x1d-notation hides the same number of logfactors and comparable constants in Theorems 1.1\\nand 1.2. We also remark that the recent trend of additive-combinatorics-based algorithm design typically\\nleads to improved, but nasty running times [17, 13, 37], so our clean running time of Õ(n) is an exception.\\nConditional Lower Bound\\nWe prove a lower bound based on the standard Strong Exponential Time Hypothesis [27, 14] from\\nfine-grained complexity theory. Alternatively, our bound can be based on the (less standard) Strong\\nk-Sum Hypothesis [6, 1]. For details on these hypotheses, see Section 5.\\nTheorem 1.3 (Informal). Subset Sum requires time (mxXΣX/(nt))1−o(1), unless the Strong Exponential\\nTime Hypothesis and the Strong k-Sum Hypothesis both fail. This even holds when X must be a set.\\nMore precisely, we prove this lower bound for any parameter setting of n, t,mxX ,ΣX (similar to [12]).\\nSpecifically, for the parameters t,mxX ,ΣX we fix corresponding exponents τ, ξ, σ ∈ R and focus on\\ninstances with t = Θ(nτ ), mxX = Θ(nξ), and ΣX = Θ(nσ). Some settings of τ, ξ, σ are trivial, in the\\nsense that they admit no (or only finitely many) instances; we ignore such settings. For each non-trivial\\nparameter setting, we prove a conditional lower bound of (mxXΣX/(nt))1−o(1). This shows that we did\\nnot miss any setting in which Subset Sum admits algorithms running in time (mxXΣX/(nt))1−Ω(1).\\nNote that for t\\x1c mxXΣX/n2 we obtain a super-linear lower bound. This complements our Theorem 1.2,\\nwhich runs in near-linear time Õ(n) if t\\x1d mxXΣX/n2. In particular, we obtain an essentially complete\\ndichotomy of near-linear-time settings, except for leaving open settings with t ≈ mxXΣX/n2. That is, we\\ndetermined the largest possible regime in which dense Subset Sum is in near-linear time.\\nAlso note that our lower bound establishes the regime t\\x1d mxXΣX/n2 to be natural, and not just an\\nartefact of the algorithmic approach, so the optimal time complexity of Subset Sum is indeed complicated!\\nMulti-Sets\\nFinally, we provide a generalization of our algorithm (and thus also Galil and Margalit’s result) to\\nmulti-sets. For a multi-set X, we denote by µX the largest multiplicity of any number in X.\\nTheorem 1.4. Given a multi-set X of n positive integers and a target t ≤ ΣX/2, if t\\x1d µXmxXΣX/n2\\nthen Subset Sum can be solved in time Õ(n).\\nFor constant multiplicity µX = O(1) this yields the same result as for sets. For larger multiplicities,\\nwe pay a factor µX in the feasibility bound. Since any set is also a multi-set, the lower bound from\\nTheorem 1.3 also applies here. However, for µX \\x1d 1 we no longer obtain matching regimes.\\n1.2 Organization\\nAfter formalizing some notation in Section 2, we give a technical overview of our results in Section 3. We\\npresent our algorithm in Section 4, and our conditional lower bound in Section 5. Finally, we conclude\\nwith open problems in Section 6.\\nK. Bringmann and P. Wellnitz 4\\n2 Notation\\nWe write [n ] := {1, . . . , n} and [ ` . . r ] := {`, . . . , r}. Further, for a set A and an integer d, we write\\nA mod d := {a mod d | a ∈ A}.\\nThroughout the paper we let X denote a finite non-empty multi-set of positive integers (or multi-set\\nfor short). For an integer x, we write µ(x;X) to denote the multiplicity of x in X. A number that does\\nnot appear in X has multiplicity 0. We use the same notation for multi-sets and sets, e.g., a subset Y ⊆ X\\nis a multi-set with µ(x;Y ) ≤ µ(x;X) for all x. We write suppX to denote the support of X, that is, the\\nset of all distinct integers contained in the multi-set X.\\nWe associate the following relevant objects to a multi-set X:\\nSize |X|: The number of elements of X, counted with multiplicity, that is, |X| :=\\n∑\\nx∈N µ(x;X).\\nMaximum mxX : The maximum element of X, that is, mxX := max{x ∈ N | µ(x;X) > 0}.\\nMultiplicity µX : The maximum multiplicity of X, that is, µX := max{µ(x;X) | x ∈ N}.\\nSum ΣX : The sum of all elements of X, that is, ΣX :=\\n∑\\nx∈N x · µ(x;X).\\nSet of all subset sums SX : The set containing all sums of subsets of X, that is, SX := {ΣY | Y ⊆ X}.\\nThe Subset Sum problem now reads as follows.\\nProblem 2.1 (Subset Sum). Given a (multi-)set X and an integer t, decide whether t ∈ SX .\\nFor a multi-set X and an integer k ≥ 1 we write kX := {kx | x ∈ X}, that is, every number in X is\\nmultiplied by k. Similarly, if every number in X is divisible by k, then we write X/k := {x/k | x ∈ X}.\\n3 Technical Overview\\n3.1 Technical Overview of the Algorithm\\nWe follow the same high-level approach as Galil and Margalit [26]. However, we replace essentially every\\npart of their algorithm to obtain our improved running time as well as a generalization to multi-sets.\\nOur goal is to design a near-linear time algorithm for Subset Sum in the regime t\\x1d µXmxXΣX/|X|2.\\nRecall that without loss of generality we can assume t ≤ ΣX/2, by using symmetry. Combining these\\ninequalities, we will in particular assume |X|2 \\x1d µXmxX . We formalize this assumption as follows.\\nDefinition 3.1 (Density). We say that a multi-set X is δ-dense if it satisfies |X|2 ≥ δ · µX ·mxX .\\nIf (almost) all numbers in X are divisible by the same integer d > 1, then it may be that not all\\nremainders modulo d are attainable by subsets sums. For this reason, we introduce the following notion.\\nDefinition 3.2 (Almost Divisor). We write X(d) := X ∩ dZ to denote the multi-set of all numbers in X\\nthat are divisible by d. Further, we write X(d) := X \\\\X(d) to denote the multi-set of all numbers in X\\nnot divisible by d. We say an integer d > 1 is an α-almost divisor of X if |X(d)| ≤ α · µX · ΣX/|X|2.\\nUsing the above definitions, we can cleanly split our proof into a structural part and an algorithmic\\npart. We first formulate these two parts and show how they fit together to solve Subset Sum. We then\\ndiscuss the proofs of our structural and algorithmic part in Sections 3.1.1 and 3.1.2 below.\\nIn the structural part we establish that dense and almost-divisor-free sets generate all possible subset\\nsums apart from a short prefix and suffix. Note that any X satisfies SX ⊆ [ 0 . . ΣX ].\\n5 On Near-Linear-Time Algorithms for Dense Subset Sum\\nTheorem 3.3 (Structural Part, Informal). If X is Θ̃(1)-dense3 and has no Θ̃(1)-almost divisor, then\\nthere exists λX = Θ̃(µXmxXΣX/|X|2) such that [λX . . ΣX − λX ] ⊆ SX .\\nOur algorithmic part is a reduction of the general case to the setting of Theorem 3.3. This is achieved\\nby repeatedly removing almost divisors (i.e., finding an almost divisor d and replacing X by X(d)/d).\\nTheorem 3.4 (Algorithmic Part, Informal). Given an Θ̃(1)-dense multiset X of size n, in time Õ(n) we\\ncan compute an integer d ≥ 1 such that X ′ := X(d)/d is Θ̃(1)-dense and has no Θ̃(1)-almost divisor.\\nBy combining these two components, we show that in the regime t \\x1d µXmxXΣX/|X|2 the Subset\\nSum problem is characterized by its behaviour modulo d.\\nTheorem 3.5 (Combination I, Informal). Let X be an Θ̃(1)-dense multi-set, and let d be as in Theorem 3.4.\\nThen for any t ≤ ΣX/2 with t\\x1d µXmxXΣX/|X|2 we have\\nt ∈ SX if and only if t mod d ∈ SX mod d.\\nProof Sketch. In one direction, if t mod d 6∈ SX mod d, then clearly t is not a subset sum of X. In the\\nother direction, if t mod d ∈ SX mod d, then there is a subset Y ⊆ X summing to t modulo d. We can\\nassume that Y ⊆ X(d), since numbers divisible by d do not help for this purpose. The remaining target\\nt′ = t− ΣY is divisible by d. Since we assume t to be large and by arguing about the size of Y , we can\\nshow that t′/d lies in the feasible interval of Theorem 3.3 applied to X(d)/d. Thus, some subset of X(d)/d\\nsums to t′/d, meaning some subset of X(d) sums to t′. Together, we have found a subset of X summing\\nto t. Hence, we can decide whether t is a subset sum of X by deciding the same modulo d.\\nUsing the above structural insight for algorithm design yields the following result.\\nTheorem 3.6 (Combination II, Informal). We can preprocess a given Θ̃(1)-dense multi-set X of size n\\nin time Õ(n). Given a query t ≤ ΣX/2 with t\\x1d µXmxXΣX/n2 we can then decide t ∈ SX in time O(1).\\nIn particular, given a multi-set X of size n and a target t ≤ ΣX/2 with t\\x1d µXmxXΣX/n2 we can\\ndecide whether t ∈ SX in time Õ(n).\\nProof Sketch. The preprocessing has two steps: (1) Computing the number d from Theorem 3.4. This\\ncan be done in time Õ(n) by Theorem 3.4. (2) Solving Subset Sum modulo d, that is, computing the set\\nSX mod d. Here we use a recent algorithm by Axiotis et al. [8, 7] that runs in time Õ(n+ d), which can\\nbe bounded by Õ(n) in our context using the density assumption.\\nOn query t it suffices to check whether t mod d lies in the precomputed set SX mod d, by Theorem 3.5.\\nFor the second formulation, we argue that the assumptions t ≤ ΣX/2 and t ≥ Θ̃(µXmxXΣX/n2)\\nimply that X is Θ̃(1)-dense. Therefore, the first formulation implies the second.\\nIt remains to describe the two main components: the structural part and the algorithmic part.\\n3 In this technical overview we present informal versions of our intermediate theorems. In particular, we write Θ̃(1) to\\nhide a sufficiently large polylogarithmic factor C logC(n). These factors are made precise later in the paper.\\nK. Bringmann and P. Wellnitz 6\\n3.1.1 Structural Part\\nRecall that in the structural part we analyze the setting of dense and almost-divisor-free multisets.\\nTheorem 3.3 (Structural Part, Informal). If X is Θ̃(1)-dense and has no Θ̃(1)-almost divisor, then\\nthere exists λX = Θ̃(µXmxXΣX/|X|2) such that [λX . . ΣX − λX ] ⊆ SX .\\nNote that the density assumption implies λX = o(ΣX), so the interval [λX . . ΣX − λX ] is non-trivial.\\nAlso note that if all numbers in X were even, then all subset sums would be even, so we cannot have\\n[λX . . ΣX − λX ] ⊆ SX — therefore it is natural to exclude almost divisors.\\nTheorem 3.3 is an existential result about an arithmetic progression (of stepsize 1) in the set of all\\nsubset sums, and thus belongs to the realm of additive combinatorics, see [47] for an overview. Arithmetic\\nprogressions in the set of all subsets sums have been studied at least since early work of Alon [4], see also\\nthe literature by Erdős, Freiman, Sárközy, Szemerédi, and others [5, 21, 23, 33, 34, 35, 42, 43, 46]. A\\nresult of this type is also implicit in the algorithm by Galil and Margalit [25], but only applies to sets. The\\nmain novelty of Theorem 3.3 over previous work in additive combinatorics is that we consider multi-sets\\nwith a bound on the multiplicity µX , which has not been explicitly studied before.\\nProof Sketch. The proof is an elementary, but involved construction using arguments that are standard\\nin the additive combinatorics community, but not in theoretical computer science.\\nThe multi-set X is partitioned into three suitable subsets A ∪R ∪G such that:\\nSA contains a long arithmetic progression of small step size s and small starting value. To construct A,\\nwe adapt the proof of a result by Sárközy [43], to generalize it from sets to multi-sets.\\nR generates all remainders modulo s, that is, SR mod s = Zs. To construct R, it suffices to pick any s\\nnumbers in X(s). (We discuss later how to avoid that all numbers in X(s) are already picked by A.)\\nThe remaining elements G = X \\\\ (A ∪R) still have a large sum.\\nUsing this partitioning, for any target t ∈ [λX . . ΣX − λX ] we construct a set summing to t as follows.\\nWe first greedily pick elements from G that sum to a number t′ = t − Θ(λX). We then pick elements\\nfrom R that sum to t− t′ modulo s. It remains to add the right multiple of s. This number appears as an\\nelement of the arithmetic progression guaranteed by A, so we pick the corresponding subset of A.\\nRemark 3.7. Our proof of Theorem 3.3 is constructive and yields a polynomial-time algorithm. However,\\nwe currently do not know how to obtain near-linear time preprocessing and solution reconstruction.\\nFortunately, for the decision version of Subset Sum an existential result suffices.\\n3.1.2 Algorithmic Part\\nRecall that our algorithmic part is a reduction to the almost-divisor-free setting.\\nTheorem 3.4 (Algorithmic Part, Informal). Given an Θ̃(1)-dense multiset X of size n, in time Õ(n) we\\ncan compute an integer d ≥ 1 such that X ′ := X(d)/d is Θ̃(1)-dense and has no Θ̃(1)-almost divisor.\\nA similar result is implicit in the algorithm by Galil and Margalit [25]. However, their running time is\\nÕ(n+ (mxX/n)2), which ranges from Õ(n) to Õ(n2) in our near-linear-time regime. The main difference\\nis that they compute d using more or less brute force, specifically the bottleneck of their running time is\\nto test for every integer 1 < d ≤ mxX/n and for each of the O(mxX/n) smallest elements x ∈ X whether\\nd divides x. In contrast, we read off almost divisors from the prime factorizations of the numbers in X.\\nAnother difference is that they construct d by a direct method, while we iteratively construct d = d1 · · · di.\\n7 On Near-Linear-Time Algorithms for Dense Subset Sum\\nProof Sketch. Consider the following iterative procedure. Initialize X0 := X and i = 1. While Xi−1 has\\nan almost divisor, we pick any almost divisor di of Xi−1, and we continue with Xi := Xi−1(di)/di. The\\nfinal set Xi = X(d1 · · · di)/(d1 · · · di) has no almost divisor, so we return d := d1 · · · di.\\nWe need to show that the resulting set Xi is Θ̃(1)-dense. The key step here is to establish the size\\nbound |Xi| = Ω(n). This allows us to control all relevant parameters of Xi. We thus obtain existence of a\\nnumber d with the claimed properties.\\nIt remains to show that this procedure can be implemented to run in time Õ(n). The number of\\niterations is O(logn), since the product d1 · · · di grows exponentially with i. Therefore, the running time\\nis dominated by the time to find an almost divisor di, if there exists one. We observe that if there exists\\nan almost divisor, then there exists one that is a prime number. It would thus be helpful to know the\\nprime factorizations of all numbers in X. Indeed, from these prime factorizations we could read off\\nall primes that divide sufficiently many elements of X, so we could infer all prime almost divisors. As\\nit turns out (see Theorem 3.8 below), we can simultaneously factorize all numbers in X in total time\\nÕ(n+√mxX). This can be bounded by Õ(n) using that X is Θ̃(1)-dense. It follows that our procedure\\ncan be implemented in time Õ(n).\\nThe above algorithm crucially relies on computing the prime factorization of all input numbers.\\nTheorem 3.8. The prime factorization of n given numbers in [ s ] can be computed in time Õ(n+\\n√\\ns).\\nIn the proof of Theorem 3.4, we use this algorithm for s = O(n2), where it runs in time Õ(n). From\\nthe literature (see, e.g., [18]), we know three alternatives to our algorithm, which are all worse for us:\\nAfter constructing the Sieve of Eratosthenes on [ s ] in time Õ(s), we can determine all prime factors\\nof a number in [ s ] in time O(log s). This yields a total running time of Õ(s+ n).\\nThe prime factorization of a number in [ s ] can be computed in expected time so(1) (more precisely,\\ntime 2O((log s)1/2(log log s)1/2) for rigorously analyzed algorithms [32], and time 2O((log s)1/3(log log s)2/3)\\nfor heuristics, see [41]). Running this for each of n input numbers takes expected time n · so(1). For\\ns = O(n2), we improve upon this running time by a factor so(1), and our algorithm is deterministic.\\nThe fastest known deterministic factorization algorithms are due to Pollard [40] and Strassen [45] and\\nfactorize a number in [ s ] in time Õ(s1/4). Running this for all n input numbers takes time Õ(n · s1/4).\\nFor s ≤ n2, we improve this running time by a factor s1/4.\\nProof Sketch. Suppose we want to factorize m1, . . . ,mn ∈ [ s ]. Let p1, . . . , p` denote all primes below\\n√\\ns.\\nTheir product P = p1 · · · p` is an Õ(\\n√\\ns)-bit number. We compute P in a bottom-up tree-like fashion;\\nthis takes time Õ(\\n√\\ns). Similarly, we compute M = m1 · · ·mn in a bottom-up tree-like fashion; this takes\\ntime Õ(n). We can now compute P mod M . Then we iterate over the same tree as for M in a top-down\\nmanner, starting from the value P mod M at the root and computing the values P mod mj at the leaves;\\nthis again can be done in time Õ(n). From these values we compute the greatest common divisor of P and\\nmj as gcd(P,mj) = gcd(P mod mj ,mj). Observe that mj >\\n√\\ns is prime if and only if gcd(P,mj) = 1,\\nso we can now filter out primes.\\nFor composites, we repeat the above procedure once with the left half of the primes p1, . . . , p`/2 and\\nonce with the right half p`/2+1, . . . , p`. We can infer which composites mj have a prime factor among the\\nleft half, and which have a prime factor among the right half. We then recurse on these halves. In the\\nbase case we find prime factors.\\n3.2 Technical Overview of the Conditional Lower Bound\\nOur goal in the lower bound is to show that Subset Sum cannot be solved in near-linear time for\\nt\\x1c mxXΣX/n2, in the case where X is a set. To this end, we present a conditional lower bound in the\\nK. Bringmann and P. Wellnitz 8\\nrealm of fine-grained complexity theory.\\nWe start by defining parameter settings: For the parameters t,mxX ,ΣX we fix corresponding exponents\\nτ, ξ, σ ∈ R+ and consider Subset Sum instances (X, t) satisfying t = Θ(nτ ), mxX = Θ(nξ), and ΣX = Θ(nσ).\\nWe call the family of all these instances a parameter setting and denote it by Subset Sum(τ, ξ, σ). Note\\nthat some choices of the exponents τ, ξ, σ are contradictory, leading to trivial parameter settings that\\nconsist of only finitely many instances or that can otherwise be solved trivially. For example, if X is a set\\nof size n then ΣX ≥\\n∑n\\ni=1 i = Θ(n2), so parameter settings with σ < 2 are trivial. Accordingly, we call a\\nparameter setting (τ, ξ, σ) non-trivial if it satisfies the inequalities σ ≥ 2 as well as 1 ≤ ξ ≤ τ ≤ σ ≤ 1 + ξ;\\nfor a justification of each one of these restrictions see Section 5.2.\\nFor every non-trivial parameter setting Subset Sum(τ, ξ, σ), we prove a conditional lower bound ruling\\nout running time O((mxXΣX/(nt))1−ε) for any ε > 0 (see Theorem 5.10). In particular, for any non-trivial\\nparameter setting with t \\x1c mxXΣX/n2 this yields a super-linear lower bound. Note that our use of\\nparameter settings ensures that we did not miss any setting in which Subset Sum admits a near-linear\\ntime algorithm.\\nOur lower bound is conditional on assumptions from fine-grained complexity theory. Specifically, it\\nholds under the Strong Exponential Time Hypothesis [27, 14], which is the most standard assumption from\\nfine-grained complexity [48] and essentially states that the Satisfiability problem requires time 2n−o(n).\\nAlternatively, our lower bound also follows from the Strong k-SUM hypothesis; see Section 5.1.2 for a\\ndiscussion. To obtain a uniform lower bound under both of these hypotheses, we introduce the following\\nintermediate hypothesis:\\nFor any α, ε > 0 there exists k ≥ 3 such that given a set Z ⊆ {1, . . . , U} of size |Z| ≤ Uα and given\\na target T , no algorithm decides whether any k numbers in Z sum to T in time O(U1−ε).\\nWe first show that this intermediate hypothesis is implied both by the Strong Exponential Time\\nHypothesis (via a reduction from [2]) and by the Strong k-SUM hypothesis (which is easy to prove). Then\\nwe show that the intermediate hypothesis implies our desired conditional lower bound for every non-trivial\\nparameter setting of Subset Sum. For this step, we design a reduction that starts from a k-Sum instance\\nand constructs an equivalent Subset Sum instance. This is in principle an easy task. However, here we are\\nin a fine-grained setting, where we cannot afford any polynomial overhead and thus have to be very careful.\\nSpecifically, as we want to prove a conditional lower bound for each non-trivial parameter setting, we need\\nto design a family of reductions that is parameterized by (τ, ξ, σ). Ensuring the conditions t = Θ(nτ ),\\nmxX = Θ(nξ), and ΣX = Θ(nσ) in the constructed Subset Sum instance (X, t) requires several ideas on\\nhow to “pack” numbers (in fact, this task is so complicated that for the case of multi-sets we were not\\nable to prove a tight conditional lower bound).\\n4 The Algorithm\\n4.1 Precise Theorem Statements and Combination\\nRecall that our algorithm has two main components, the algorithmic part and the structural part. We\\nnow present formal statements of these parts.\\nTheorem 4.1 (Algorithmic Part, Formal Version of Theorem 3.4). Let δ, α be functions of n with δ ≥ 1\\nand 16α ≤ δ. Given an δ-dense multiset X of size n, in time Õ(n) we can compute an integer d ≥ 1 such\\nthat X ′ := X(d)/d is δ-dense and has no α-almost divisor. Moreover, we have the following additional\\nproperties:\\n1. d ≤ 4µXΣX/|X|2,\\n9 On Near-Linear-Time Algorithms for Dense Subset Sum\\n2. d = O(n),\\n3. |X ′| ≥ 0.75 |X|,\\n4. ΣX′ ≥ 0.75 ΣX/d.\\nTheorem 4.2 (Structural Part, Formal Version of Theorem 3.3). Let X be a multi-set and set\\nCδ := 1699200 · log(2n) log2(2µX),\\nCα := 42480 · log(2µX),\\nCλ := 169920 · log(2µX).\\nIf X is Cδ-dense and has no Cα-almost divisor, then for λX := Cλ · µXmxXΣX/|X|2 we have\\n[λX . . ΣX − λX ] ⊆ SX .\\nWe next show how to combine these theorems to solve Subset Sum. We use notation as in Theorem 4.2.\\nTheorem 4.3 (Combination I, Formal Version of Theorem 3.5). Given a Cδ-dense multi-set X, in time\\nÕ(n) we can compute an integer d ≥ 1 such that for any t ≤ ΣX/2 with t ≥ (4 + 2Cλ)µXmxXΣX/|X|2:\\nt ∈ SX if and only if t mod d ∈ SX mod d.\\nProof. The easy direction does not depend on the choice of d: If t mod d 6∈ SX mod d, then clearly t is\\nnot a subset sum of X.\\nFor the more difficult direction, first apply Theorem 4.1 with δ := Cδ and α := Cα to compute an integer\\nd ≥ 1 such thatX ′ = X(d)/d has no Cα-almost divisor and is Cδ-dense (note that Theorem 4.1 is applicable\\nsince 16Cα ≤ Cδ). Then Theorem 4.2 is applicable to X ′ and shows that [λX′ . . ΣX′ − λX′ ] ⊆ SX′ .\\nNote that if t mod d ∈ SX mod d, then there is a subset Y ⊆ X summing to t modulo d. We choose\\na minimal such set Y , that is, we pick any Y ⊆ X such that ΣY mod d = t mod d, but for any proper\\nsubset Y ′ ( Y we have ΣY ′ mod d 6= t mod d. We claim that (i) Y ⊆ X(d) and (ii) t′ := (t− ΣY )/d lies\\nin the feasible interval [λX′ . . ΣX′ − λX′ ]. Assuming these two claims, by Theorem 4.2 there exists a set\\nZ ′ ⊆ X ′ with ΣZ′ = t′. This corresponds to a set Z ⊆ X(d) with ΣZ = dt′ = t− ΣY . Since Y and Z are\\nsubsets of disjoint parts of X, their union Y ∪ Z is a subset of X summing to t. This proves the desired\\nstatement: if t mod d ∈ SX mod d then t ∈ SX . In the following we prove the two remaining claims.\\nClaim 4.4. We have (1) Y ⊆ X(d), (2) |Y | ≤ d, and (3) ΣY ≤ 4µXmxXΣX/|X|2.\\nProof. (1) If Y 6⊆ X(d), then we can replace Y by Y ∩X(d) without changing the value of ΣY mod d, as\\nthis change only removes numbers divisible by d. Hence, by minimality of Y we have Y ⊆ X(d).\\nTo see (2), write Y = {y1, . . . , y`} and consider the prefix sums (y1 + . . .+ yi) mod d. If ` > d, then\\nby the pigeonhole principle there exist i < j with the same remainder\\ny1 + . . .+ yi ≡ y1 + . . .+ yj (mod d).\\nIt follows that yi+1 + . . .+ yj ≡ 0 (mod d), so we can remove {yi+1, . . . , yj} from Y without changing the\\nvalue of ΣY mod d. As this violates the minimality of Y , we obtain ` ≤ d.\\nFor (3), using Theorem 4.1.1, the inequality |Y | ≤ d implies ΣY ≤ d ·mxX ≤ 4µXmxXΣX/|X|2.\\nThe remaining target t− ΣY is divisible by d. It remains to prove that t′ := (t− ΣY )/d lies in the\\nfeasible interval of Theorem 4.2 applied to X ′ = X(d)/d:\\nK. Bringmann and P. Wellnitz 10\\nClaim 4.5. We have t′ ∈ [λX′ . . ΣX′ − λX′ ].\\nProof. Using the inequality |X ′| ≥ 0.75 |X| from Theorem 4.1.3, the bound (1/0.75)2 < 2, and the easy\\nfacts µX′ ≤ µX , mxX′ ≤ mxX/d, and ΣX′ ≤ ΣX/d, we bound\\nλX′ =\\nCλµX′mxX′ΣX′\\n|X ′|2\\n≤ 2CλµXmxXΣX\\nd|X|2\\n.\\nBy the assumption t ≥ (4 + 2Cλ)µXmxXΣX/|X|2 and Claim 4.4.(3), it follows that t′ = (t−ΣY )/d ≥ λX′ .\\nFor the other direction, we use that X is Cδ-dense and thus also 8Cλ-dense, which gives µXmxX/|X|2 ≤\\n1/(8Cλ). Therefore, we can further bound\\nλX′ ≤\\n2CλµXmxXΣX\\nd|X|2\\n≤ ΣX4d .\\nFrom Theorem 4.1.4 we have ΣX′ ≥ 0.75 ΣX/d, and thus ΣX′ − λX′ ≥ 0.5 ΣX/d. Finally, we use the\\nassumption t ≤ ΣX/2 to obtain\\nt′ = t− ΣY\\nd\\n≤ t\\nd\\n≤ ΣX2d ≤ ΣX\\n′ − λX′ .\\nThis finishes the proof of t′ ∈ [λX′ . . ΣX′ − λX′ ].\\nWe thus proved the two remaining claims, finishing the proof.\\nThis leads to the following formal version of our final algorithm. We use notation as in Theorem 4.2.\\nTheorem 4.6 (Combination II, Formal Version of Theorem 3.6). We can preprocess a given Cδ-dense\\nmulti-set X of size n in time Õ(n). Given a query t ≤ ΣX/2 with t ≥ (4 + 2Cλ)µXmxXΣX/n2 we can\\nthen decide whether t ∈ SX in time O(1).\\nIn particular, given a multi-set X of size n and a target t ≤ ΣX/2 with4 t ≥ 0.5CδµXmxXΣX/n2, we\\ncan decide whether t ∈ SX in time Õ(n).\\nProof. In the preprocessing, we first run the algorithm from Theorem 4.3 to compute the number d.\\nThen we solve Subset Sum modulo d, that is, we compute the set SX mod d. To this end, we use a recent\\nalgorithm by Axiotis et al. [8, 7] that runs in time Õ(n+ d). By Theorem 4.1.2 we have d = O(n), so the\\nrunning time can be bounded by Õ(n).\\nOn query t, we check whether t mod d lies in the precomputed set SX mod d. If so, we return “t ∈ SX”,\\nif not, we return “t 6∈ SX”. This runs in time O(1).\\nFor the second formulation, note that ΣX/2 ≥ t ≥ 0.5CδµXmxXΣX/n2 implies n2 ≥ CδµXmxX .\\nHence, the multi-set X is Cδ-dense. Since also 0.5Cδ ≥ 4 + 2Cλ, the first formulation applies and we can\\ndecide whether t ∈ SX in Õ(n) preprocessing time plus O(1) query time.\\nIt remains to prove the two main components: the structural part and the algorithmic part. After\\nsome preparations in Section 4.2, we will prove the algorithmic part in Section 4.3 and the structural part\\nin Section 4.4.\\n4 Note that by definition of Cδ, the requirement on t is t ≥ 849600 · log(2n) log2(2µX) · µXmxXΣX/n2. We did not\\noptimize constant factors.\\n11 On Near-Linear-Time Algorithms for Dense Subset Sum\\n4.2 Preparations\\nWe start with some observations about our notion of density. Recall the following definition.\\nDefinition 3.1 (Density). We say that a multi-set X is δ-dense if it satisfies |X|2 ≥ δ · µX ·mxX .\\nLemma 4.7. For any δ-dense multi-set X of size n, we have µX · ΣX/n2 ≤ n/δ.\\nProof. By definition of δ-density, we have n2 ≥ δ · µX ·mxX . Combining this with the trivial inequality\\nΣX ≤ mxX · n and rearranging yields the claim.\\nNext we show that large subsets of dense multi-sets are dense as well.\\nLemma 4.8. For any κ ≥ 1 and any δ-dense multi-set X, any subset Y ⊆ X of size |Y | ≥ |X|/κ is\\nδ/κ2-dense.\\nProof. Using the size assumption |Y | ≥ |X|/κ, the definition of δ-density, and the trivial facts µY ≤ µX\\nand mxY ≤ mxX , we obtain\\nκ2 · |Y |2 ≥ |X|2 ≥ δ · µX ·mxX ≥ δ · µY ·mxY .\\nThis yields the claimed inequality after rearranging.\\nLastly, we show that dividing all numbers in a set increases the density of a set.\\nLemma 4.9. Let d ≥ 1 be an integer, and let X be a δ-dense multi-set of positive integers divisible\\nby d. Then the multi-set X/d is dδ-dense.\\nProof. By definition of δ-density, we have\\n|X/d|2 = |X|2 ≥ δ · µX ·mxX = δ · µX/d · dmxX/d,\\nwhere we used the facts that the multi-sets X and X/d have the same number of elements and the same\\nmultiplicity, while mxX = d ·mxX/d. In total, this proves that the multi-set X/d is dδ-dense.\\n4.3 Algorithmic Part\\nIn this section, we first design an algorithm for prime factorization (proving Theorem 3.8 in Section 4.3.1),\\nthen use this to find almost divisors (Section 4.3.2), and finally present a proof of the algorithmic part\\n(proving Theorem 4.1 in Section 4.3.3).\\n4.3.1 Prime Factorization\\nIn this section, we show that n given numbers in [ s ] can be factorized in total time Õ(n+\\n√\\ns), proving\\nTheorem 3.8. We start by describing the following subroutine.\\nLemma 4.10 (Decision Subroutine). Given a set of integers M ⊆ [ s ] and a set of prime numbers\\nP ⊆ [ s ], in time Õ((|M |+ |P |) log s) we can compute all m ∈M that are divisible by some p ∈ P , that\\nis, we can compute the set M ′ := {m ∈M | ∃p ∈ P : p divides m}.\\nK. Bringmann and P. Wellnitz 12\\nProof. Observe that an integer m is divisible by a prime p if and only if their greatest common\\ndivisor satisfies gcd(m, p) > 1. More generally, m is divisible by some p ∈ P if and only if we have\\ngcd\\n(\\nm,\\n∏\\np∈P p\\n)\\n> 1. This is the check that we will use in our algorithm. However, note that\\n∏\\np∈P p is\\nan Ω(|P |)-bit number, and thus a direct computation of gcd\\n(\\nm,\\n∏\\np∈P p\\n)\\nrequires time Ω̃(|P |). Repeating\\nthis operation for all m ∈M would require time Ω̃(|M | · |P |), which we want to avoid.\\nIn the following we make use of efficient algorithms for multiplication and division with remainder,\\nthat is, we use that the usual arithmetic operations on b-bit numbers take time Õ(b).\\nFor a set S = {s1, . . . , sn}, we denote by TS a balanced binary tree with n leaves corresponding to\\nthe elements s1, . . . , sn. Each node of TS corresponds to a subset I = {si, . . . , sj} ⊆ S. The root of TS\\ncorresponds to the set S. A node corresponding to a set I ⊆ S of size |I| = 1 is a leaf and has no children.\\nA node corresponding to a set I ⊆ S of size |I| > 1 is an internal node and has two children, corresponding\\nto the two parts of a balanced partitioning I = I1 ∪ I2. We denote these children by leftchild(I) = I1 and\\nrightchild(I) = I2. Moreover, we denote the parent relation by parent(I1) = parent(I2) = I. We write\\nV (TS) for the family of all subsets I forming the nodes of TS .\\nWe construct the trees TM and TP in time Õ(|M |+ |P |).\\nFor each I ∈ V (TM ) we define Π(I) :=\\n∏\\nm∈I m. In particular, at any leaf I = {m} we have Π(I) = m,\\nand at the root we have Π(M) =\\n∏\\nm∈M m. We compute the numbers Π(I) by traversing the tree TM\\nbottom-up, that is, for any internal node I we compute Π(I) := Π(leftchild(I)) ·Π(rightchild(I)).\\nTo analyze the running time to compute all numbers Π(I), note that the total bit length of the numbers\\nΠ(I) on any fixed level of TM is O(|M | log s). Using an efficient multiplication algorithm, we can thus\\nperform all operations on a fixed level in total time Õ(|M | log s). Over all O(log |M |) levels of TM , the\\nrunning time is still bounded by Õ(|M | log s).\\nIn the same way, we compute the numbers Π(J) =\\n∏\\np∈J p for all nodes J ∈ V (TP ). In particular, at\\nthe root of TP we compute Π(P ) =\\n∏\\np∈P p. This takes time Õ(|P | log s).\\nNow we compute the number Π(P ) mod Π(M). Since the combined bit length of Π(P ) and Π(M) is\\nO((|M |+ |P |) log s), this takes time Õ((|M |+ |P |) log s).\\nNext we compute the numbers R(I) := Π(P ) mod Π(I) for all I ∈ V (TM ), by traversing the tree TM\\ntop-down. At the root we use the already computed number Π(P ) mod Π(M). At an internal node I we\\ncompute\\nR(I) := R(parent(I)) mod Π(I).\\nNote that both R(parent(I)) and Π(I) are already computed when we evaluate R(I).\\nTo analyze the running time to compute all numbers R(I), we note that R(I) ≤ Π(I), so the total\\nbit length of the numbers R(I) is bounded by the total bit length of the numbers Π(I). Thus, the same\\nanalysis as before shows that computing the numbers R(I) takes total time Õ(|M | log s).\\nNote that for the leaves of TM we have now computed the numbers R({m}) = Π(P ) mod m for all\\nm ∈M . Since m and R({m}) have bit length O(log s), we can compute their greatest common divisor\\ngcd(m,R({m})) in time Õ(log s). In total, this takes time Õ(|M | log s).\\nFinally, we use the identity gcd(a, b) = gcd(a, b mod a) to observe that\\ngcd\\n(\\nm,\\n∏\\np∈P\\np\\n)\\n= gcd\\n(\\nm,Π(P )\\n)\\n= gcd\\n(\\nm,R({m})\\n)\\n.\\nBy our initial discussion, a number m ∈M is divisible by some p ∈ P if and only if gcd(m,\\n∏\\np∈P p) > 1.\\nHence, we can determine all m ∈M that are divisible by some p ∈ P in total time Õ((|M |+ |P |) log s).\\nNext we adapt the above decision subroutine to obtain a search subroutine.\\n13 On Near-Linear-Time Algorithms for Dense Subset Sum\\nLemma 4.11 (Search Subroutine). Given a set of integers M ⊆ [ s ] and a set of prime numbers\\nP ⊆ [ s ], in time Õ((|M |+ |P |) log2 s) we can compute for all numbers m ∈M all prime factors among\\nthe primes P , that is, we can compute the set F := {(m, p) | m ∈M, p ∈ P, p divides m}.\\nProof. On instance (M,P ), we first partition in a balanced way P = P1 ∪ P2. Then we call the decision\\nsubroutine from Lemma 4.10 twice, to compute the sets M1 and M2 with\\nMi := {m ∈M | ∃p ∈ Pi : p divides m} for i ∈ {1, 2}.\\nFinally, we recursively solve the instances (M1, P1) and (M2, P2). (We ignore recursive calls with M = ∅.)\\nIn the base case, we have P = {p}. In this case, all m ∈M are divisible by p, so we print the pairs\\n{(m, p) | m ∈M}.\\nCorrectness of this algorithm is immediate. To analyze its running time, note that the recursion\\ndepth is O(log |P |), since P is split in a balanced way. Further, the total size of P ′ over all recursive\\ncalls (M ′, P ′) on a fixed level of recursion is O(|P |). Moreover, note that a number m ∈ [ s ] has O(log s)\\nprime factors. Since for each recursive call (M ′, P ′) each number m ∈ M ′ has a prime factor in P ′,\\nit follows that each number m ∈ M appears in O(log s) recursive calls on a fixed level of recursion.\\nThus, the total size of M ′ over all recursive calls (M ′, P ′) on a fixed level of recursion is O(|M | log s).\\nPlugging these bounds into the running time Õ((|M |+ |P |) log s) of Lemma 4.10 yields a total time of\\nÕ((|M | log s+ |P |) log s) = Õ((|M |+ |P |) log2 s) per level. The same time bound also holds in total over\\nall O(log |P |) levels.\\nOur main prime factorization algorithm now follows easily.\\nTheorem 3.8. The prime factorization of n given numbers in [ s ] can be computed in time Õ(n+\\n√\\ns).\\nProof. Let M ⊆ [ s ] be the set of n given numbers. Denote by P the set of all prime numbers less than\\nor equal to\\n√\\ns, and note that P can be computed in time Õ(\\n√\\ns), e.g., by using the Sieve of Eratosthenes.\\nWe run Lemma 4.11 on (M,P ) to obtain the set F := {(m, p) | m ∈M, p ∈ P, p divides m}. From this\\nset, we can compute the prime factorization of each m ∈M efficiently as follows. Fix m ∈M . For any\\n(m, p) ∈ F , determine the largest exponent e = e(m, p) such that pe divides m. This determines all prime\\nfactors of m that are less than or equal to\\n√\\ns. Since m ∈ [ s ], the number m has at most one prime factor\\ngreater than\\n√\\ns. We determine this potentially missing prime factor as q := m/\\n∏\\np∈P p\\ne(m,p). If q > 1,\\nthen q is the single large prime factor of m, and if q = 1, then all prime factors of m are less than or equal\\nto\\n√\\ns. The running time is dominated by the call to Lemma 4.11, which takes time\\nÕ((|M |+ |P |) log2 s) = Õ((n+\\n√\\ns) log2 s) = Õ(n+\\n√\\ns).\\n4.3.2 Finding an Almost Divisor\\nIn this section, we use the prime factorization algorithm from the last section to find almost divisors.\\nTheorem 4.12 (Finding Almost Divisors). Given α > 0 and a multi-set X of size n, we can decide\\nwhether X has an α-almost divisor, and compute an α-almost divisor if it exists, in time Õ(n+√mxX).\\nRecall the definition of almost divisors:\\nDefinition 3.2 (Almost Divisor). We write X(d) := X ∩ dZ to denote the multi-set of all numbers in X\\nthat are divisible by d. Further, we write X(d) := X \\\\X(d) to denote the multi-set of all numbers in X\\nnot divisible by d. We say an integer d > 1 is an α-almost divisor of X if |X(d)| ≤ α · µX · ΣX/|X|2.\\nK. Bringmann and P. Wellnitz 14\\nAlgorithm 1 Reduction to the almost-divisor-free setting, see Theorem 4.1.\\n1 AlmostDivisorFreeSubset(α, X)\\n2 X0 ← X; i← 1;\\n3 while Xi−1 has an α-almost divisor di do\\n4 Xi ← Xi−1(di)/di;\\n5 i← i+ 1;\\n6 d← d1 · · · di;\\n7 return (d,Xi);\\nWe first observe that any proper divisor d′ of an almost divisor d is also an almost divisor.\\nLemma 4.13. If d is an α-almost divisor of a multi-set X, then any divisor d′ > 1 of d is also an\\nα-almost divisor of X.\\nProof. Since any number divisible by d is also divisible by d′, we have |X(d′)| ≥ |X(d)|, or, equivalently,\\n|X(d′)| ≤ |X(d)|. By definition of α-almost divisor, we obtain\\n|X(d′)| ≤ |X(d)| ≤ αµXΣX/|X|2,\\nso also d′ is an α-almost divisor.\\nThe above lemma shows that if X has an α-almost divisor, then it also has a prime α-almost divisor,\\nthat is, it has an α-almost divisor that is a prime number. This suggests the following approach.\\nProof of Theorem 4.12. We use Theorem 3.8 to compute the prime factorization of all numbers in X\\nin total time Õ(n+√mxX). From the prime factorizations we can infer for each prime p (that divides\\nsome x ∈ X) the number of x ∈ X that are divisible by p. In particular, we can determine whether some\\nprime p divides at least n− αµXΣX/n2 numbers in X. If such a prime p exists, then p is an α-almost\\ndivisor of X. If no such prime p exists, then the set X has no prime α-almost divisor, so by Lemma 4.13\\nthe set X has no α-almost divisor. This proves Theorem 4.12.\\n4.3.3 Proof of the Algorithmic Part\\nWe are now ready to prove our algorithmic part.\\nTheorem 4.1 (Algorithmic Part, Formal Version of Theorem 3.4). Let δ, α be functions of n with δ ≥ 1\\nand 16α ≤ δ. Given an δ-dense multiset X of size n, in time Õ(n) we can compute an integer d ≥ 1 such\\nthat X ′ := X(d)/d is δ-dense and has no α-almost divisor. Moreover, we have the following additional\\nproperties:\\n1. d ≤ 4µXΣX/|X|2,\\n2. d = O(n),\\n3. |X ′| ≥ 0.75 |X|,\\n4. ΣX′ ≥ 0.75 ΣX/d.\\nConsider Algorithm 1, which iterative removes almost divisors. We start with X0 = X. While Xi−1\\nhas an α-almost divisor di, we continue with Xi := Xi−1(di)/di, that is, we remove all numbers not\\ndivisible by di from Xi−1 and divide the remaining numbers by di. The final multi-set Xi has no α-almost\\ndivisor. We return d = d1 · · · di.\\n15 On Near-Linear-Time Algorithms for Dense Subset Sum\\nWe first analyze the running time of this algorithm. By Theorem 4.12, we can find almost divisors\\nin time Õ(n + √mxX). This dominates the running time of one iteration of Algorithm 1. Note that\\nthe number of iterations is bounded by O(logmxX), since mxXi = mxX/(d1 · · · di) and d1 · · · di ≥ 2i.\\nTherefore, the total running time of Algorithm 1 is Õ(n+√mxX). Rearranging the definition of δ-density\\nyields mxX ≤ n2/(δµX). Since µX ≥ 1 and X is δ-dense for δ ≥ 1, we obtain mxX = O(n2). Hence, the\\nrunning time is Õ(n+√mxX) = Õ(n), as claimed in Theorem 4.1.\\nIn the following we analyze correctness of Algorithm 1, that is, we show that it ensures the properties\\nclaimed in Theorem 4.1. We will denote by Xi any intermediate multi-set of Algorithm 1, for i ≥ 0.\\nObserve that Xi contains all numbers in X that are divisible by d1 · · · di, divided by d1 · · · di. That is,\\nXi = X(d1 · · · di)/(d1 · · · di).\\nIn particular, Algorithm 1 returns the multi-set X(d)/d. Since X(d1 · · · di) ⊆ X, we obtain the easy facts\\nmxXi ≤ mxX/(d1 · · · di),\\nΣXi ≤ ΣX/(d1 · · · di), (1)\\nµXi ≤ µX .\\nThe key property in our analysis is the size |Xi|, and how it compares to n = |X|.\\nClaim 4.14. For any i ≥ 0, we have\\n|Xi| ≥ n−\\n4α · µXΣX\\nn2\\n≥\\n(\\n1− 4α\\nδ\\n)\\nn ≥ 34n.\\nProof. Since di+1 is an α-almost divisor of Xi, at most α · µXiΣXi/|Xi|2 numbers in Xi are not divisible\\nby di. For Xi+1 = Xi(di+1)/di+1 we can thus bound\\n|Xi+1| = |Xi(di+1)| ≥ |Xi| −\\nα · µXiΣXi\\n|Xi|2\\n.\\nUsing the easy facts (1), we obtain\\n|Xi+1| ≥ |Xi| −\\nα · µXΣX\\nd1 . . . di|Xi|2\\n≥ |Xi| −\\nα · µXΣX\\n2i|Xi|2\\n. (2)\\nWe use this inequality to inductively prove that\\n|Xi| ≥ n−\\n(\\n1− 12i\\n)4α · µXΣX\\nn2\\n. (3)\\nLet us first argue that this inequality implies the main claim. Using 1 − 1/2i ≤ 1, we obtain the first\\nclaimed inequality\\n|Xi| ≥ n−\\n4α · µXΣX\\nn2\\n.\\nSince X is δ-dense, Observation 4.7 yields µXΣX ≤ n3/δ. Plugging this in, we obtain the second inequality\\n|Xi| ≥ n ·\\n(\\n1− 4α\\nδ\\n)\\n.\\nThe last inequality |Xi| ≥ 34n now follows from the assumption 16α ≤ δ of Theorem 4.1.\\nK. Bringmann and P. Wellnitz 16\\nIt remains to prove inequality (3) by induction. The inductive base is i = 0 with X0 = X and thus\\n|X0| = n. For the inductive step, assume that the induction hypothesis (3) holds for Xi. As shown above,\\nthe inductive hypothesis for Xi implies |Xi| ≥ 34n. Plugging this bound into the recurrence (2) yields\\n|Xi+1| ≥ |Xi| −\\n(4/3)2α · µXΣX\\n2in2 ≥ |Xi| −\\n4α · µXΣX\\n2i+1n2 .\\nUsing the induction hypothesis (3) again, we obtain\\n|Xi+1| ≥ n−\\n(\\n1− 12i\\n)4α · µXΣX\\nn2\\n− 4α · µXΣX2i+1n2 = n−\\n(\\n1− 12i+1\\n)4α · µXΣX\\nn2\\n.\\nThis finishes the inductive step, and thus the proof of the claim.\\nThe claimed properties of the multi-set X ′ := X(d)/d computed by Algorithm 1 now easily follow\\nfrom Claim 4.14, as we show in the following.\\nClaim 4.15. X ′ is δ-dense.\\nProof. By Claim 4.14 we have |X(d)| = |X ′| ≥ 34 |X|. Since X is δ-dense, Observation 4.8 implies that\\nX(d) ⊆ X is ( 34 )\\n2δ-dense; in particular it is δ/2-dense. Now Observation 4.9 implies that X ′ = X(d)/d is\\ndδ/2-dense. If d > 1 then dδ/2 ≥ δ, so X ′ is δ-dense. If d = 1, then X ′ = X, which is δ-dense.\\nThis finishes the proof of the main statement of Theorem 4.1. It remains to verify the four additional\\nproperties.\\nClaim 4.16. We have |X ′| ≥ 0.75n.\\nProof. Follows directly from Claim 4.14.\\nClaim 4.17. We have ΣX′ ≥ 0.75 ΣX/d.\\nProof. We can bound the sum of the removed elements by\\nΣ\\nX(d) ≤ |X(d)| ·mxX ≤\\n4α · µXΣX\\n|X|2\\n·mxX ,\\nwhere we used the first inequality of Claim 4.14. Using that X is δ-dense, we obtain\\nΣ\\nX(d) ≤\\n4α\\nδ\\nΣX .\\nBy the assumption 16α ≤ δ from Theorem 4.1, we obtain Σ\\nX(d) ≤ ΣX/4. Finally, we note that\\nΣX′ = ΣX(d)/d =\\n1\\nd\\n(\\nΣX − ΣX(d)\\n)\\n≥ 3ΣX4d .\\nClaim 4.18 (Compare [25, Lemma 3.10]). We have d ≤ 4µXΣX/n2.\\nProof. Sort the numbers in X(d) = {x1 ≤ · · · ≤ x|X(d)|} and define a function f(z) := xdze. Note that, as\\nthe numbers in X(d) are divisible by d and each number appears at most µX times in X(d), we can lower\\n17 On Near-Linear-Time Algorithms for Dense Subset Sum\\nbound the value of the function f at z by f(z) = xdze ≥ ddze/µXe · d ≥ zd/µX . Now, we can write the\\nsum ΣX(d) as the integral of the function f from 0 to |X(d)|:\\nΣX ≥ ΣX(d) =\\n|X(d)|∫\\n0\\nf(z) dz ≥ d\\nµX\\n·\\n|X(d)|∫\\n0\\nz dz = d\\nµX\\n· |X(d)|\\n2\\n2 .\\nUsing |X(d)| = |X ′| ≥ 34n from Claim 4.14, we obtain\\nΣX ≥\\nd\\nµX\\n· n\\n2\\n(4/3)2 · 2 ≥\\ndn2\\n4µX\\n.\\nRearranging now yields the claim.\\nClaim 4.19. We have d = O(n).\\nProof. In the preceeding claim we showed that d ≤ 4µXΣX/n2. Using the trivial inequality ΣX ≤ n ·mxX ,\\nwe obtain d ≤ 4µXmxX/n. Using that X is δ-dense for δ ≥ 1, we now obtain d ≤ 4n/δ = O(n).\\nThe above claims verify all claimed properties and thus finish the proof of Theorem 4.1.\\n4.4 Structural Part\\nIn this section, we first show how to find a small subset R ⊆ X that generates all remainders modulo all\\nsmall numbers d (Section 4.4.1). Then we construct long arithmetic progressions (Section 4.4.2). We use\\nthese tools to obtain a decomposition of X (Section 4.4.3), which then yields the structural part (proving\\nTheorem 4.2 in Section 4.4.4).\\nThroughout this section, for multi-sets X,Y we write X + Y to denote their sumset (the sumset is a\\nset, that is, each distinct sum appears only once in the sumset):\\nX + Y := {x+ y | x ∈ X, y ∈ Y }.\\nFurther, we write Xh := X + · · ·+X for the iterated sumset containing all sums of h (not necessarily\\ndistinct) elements of X. Similarly, we write X≤h :=\\n⋃bhc\\nj=1X\\nj . Note that the objects X + Y,Xh, and X≤h\\nare sets, not multi-sets.\\n4.4.1 Generating All Remainders\\nTheorem 4.20 (Compare [25, Theorem 3.4]). Let δ, α ≥ 1. Let X be a δ-dense multi-set of size n that\\nhas no α-almost divisor. Then there exists a subset R ⊆ X such that\\n|R| ≤ |X| · 8α log(2n)/δ,\\nΣR ≤ ΣX · 8α log(2n)/δ, and\\nfor any integer 1 < d ≤ α · µXΣX/n2 the multi-set R contains at least d numbers not divisible by d,\\nthat is, |R(d)| ≥ d.\\nProof. If 8α log(2n) ≥ δ then we can simply set R = X. The first two claims hold trivially, and the third\\nclaim holds because X has no α-almost divisor, which implies |X(d)| ≥ α · µXΣX/n2 ≥ d. Therefore,\\nfrom now on we can assume 8α log(2n) < δ. In particular, we have\\nδ > 8α. (4)\\nK. Bringmann and P. Wellnitz 18\\nSet τ := dα · µXΣX/n2e. By α ≥ 1 and the easy fact ΣX ≥\\n∑n\\ni=1di/µXe ≥\\n1\\n2n\\n2/µX we have\\nα · µXΣX/n2 ≥ 1/2 and thus\\nα · µXΣX/n2 ≤ τ ≤ 2α · µXΣX/n2.\\nSince X is δ-dense, Observation 4.7 and inequality (4) now imply that\\nτ ≤ 2α · n/δ < n/2.\\nWe start by picking an arbitrary subset R′ ⊆ X of size 2τ . This is possible because τ < n/2.\\nClaim 4.21. Let P be the set of primes p with p ≤ τ and |R′(p)| < τ . Then we have |P | ≤ 2 logmxX .\\nProof. Consider the prime factorization of the numbers in R′, that is, consider the set\\nPFR′ = {(r, p) | r ∈ R′, prime p divides r}.\\nSince any integer m ≥ 1 has at most logm prime factors, we have |PFR′ | ≤ |R′| · logmxX . On the other\\nhand, for each p ∈ P we have |R′(p)| ≥ τ = |R′|/2, so PFR′ contains at least |R′|/2 pairs of the form (r, p).\\nHence, we have\\n|P | · |R\\n′|\\n2 ≤ |PFR\\n′ | ≤ |R′| logmxX ,\\nwhich yields the claimed bound |P | ≤ 2 logmxX .\\nFor any p ∈ P , we let Rp ⊆ X(p) be an arbitrary subset of size τ . This exists by the assumption that X\\nhas no α-almost divisor.\\nFinally, we construct the multi-set R ⊆ X as\\nR := R′ ∪\\n⋃\\np∈P\\nRp.\\n(To be precise, we set µ(x;R) := max{µ(x;R′),max{µ(x;Rp) | p ∈ P}}, ensuring that R is a subset of X.)\\nWe show that R satisfies the claimed properties. For the third property, consider any integer 1 < d ≤ τ ,\\nand let p be any prime factor of d. Note that we have |R(d)| ≥ |R(p)|, since any number divisible by d is\\nalso divisible by p. If p ∈ P , then we obtain\\n|R(p)| ≥ |Rp(p)| = |Rp| = τ ≥ d.\\nIf p 6∈ P , then by construction of P we have\\n|R(p)| ≥ |R′(p)| ≥ τ ≥ d.\\nIn any case, we have |R(d)| ≥ d, so we proved the third claim.\\nFor the first two claims, note that\\n|R| ≤ |R′|+\\n∑\\np∈P\\n|Rp| = 2τ + |P | · τ ≤ 2(1 + logmxX)τ ≤ 4 log(2mxX) · α · µXΣX/n2.\\nSince X is δ-dense for δ ≥ 1, we have mxX ≤ n2, so we can further bound\\n|R| ≤ 8 log(2n) · α · µXΣX/n2.\\n19 On Near-Linear-Time Algorithms for Dense Subset Sum\\nMoreover, Observation 4.7 now yields\\n|R| ≤ 8 log(2n) · α · n/δ,\\nproving the first claim. We similarly bound ΣR by\\nΣR ≤ |R| ·mxR ≤ 8 log(2n) · α · µXmxXΣX/n2.\\nUsing that X is δ-dense, we finally obtain the second claim\\nΣR ≤\\n8α log(2n)\\nδ\\nΣX .\\nNext we show that the set R constructed in the above Lemma 4.20 generates all remainders modulo\\nany small integer d. More precisely, with notation as in Lemma 4.20, the following theorem implies that\\nSRmod d = Zd holds for any 1 < d ≤ α · µXΣX/|X|2.\\nTheorem 4.22 (Compare [25, Lemma 3.3]). Let X be a multi-set and let τ be an integer. Suppose that\\nfor any 1 < d ≤ τ the multi-set X contains d numbers not divisible by d, that is, |X(d)| ≥ d. Then for\\nany 1 ≤ d ≤ τ the set SX is d-complete, that is, SX mod d = Zd.\\nProof. We perform induction on d. For d = 1 the statement is trivial.\\nSo consider a number 1 < d ≤ τ . By assumption we have we have |X(d)| ≥ d. We denote the elements\\nof the multi-set X(d) by x1, . . . , xr, where r = |X(d)| ≥ d.\\nLet Ci := S{x1,...,xi}mod d denote the set of remainders that can be obtained from the elements\\nx1, . . . , xi. In other words, we construct the following sequence of sets:\\nC0 := {0},\\nCi := (Ci−1 + {0, xi}) mod d.\\nObserve that we have SX mod d = SX(d) mod d = Cr, since numbers divisible by d do not yield new\\nremainders modulo d. Furthermore, we have\\n1 = |C0| ≤ |C1| ≤ . . . ≤ |Cr| ≤ d.\\nSince r ≥ d, by the pigeonhole principle we have |Ci−1| = |Ci| for some i. For this i, for any number\\nc ∈ Ci−1 also the number (c+ xi) mod d is contained in the set Ci−1. More generally, for any positive\\ninteger k also the number (c+ kxi) mod d is contained in the set Ci−1. Now we use that the numbers\\nkxi mod d form the subgroup gZd/g of Zd, where g := gcd(xi, d). It thus follows that for any c ∈ Ci−1\\nand any integer k also the number (c+ kg) mod d is in Ci−1. We call this property g-symmetry.\\nNote that we can write\\nSX mod d = (Ci−1 + S{xi,...,xr}) mod d.\\nFrom this, we see that the g-symmetry of Ci−1 extends to SX mod d. More precisely, for any c ∈ SX mod d\\nand any integer k, also (c+ kg) mod d is in SX mod d.\\nMoreover, since xi ∈ X(d) is not divisible by d, we have g = gcd(xi, d) < d. Therefore, by induction\\nhypothesis SX is g-complete.\\nCombining g-symmetry and g-completeness proves that SX is d-complete. Indeed, for any remainder\\nz ∈ Zd, since SX is g-complete there is a subset sum y ∈ SX with y ≡ z (mod g). Equivalently, we can\\nwrite z − y = kg for some integer k. Since y mod d is in SX mod d, by the g-symmetry property also\\n(y + kg) mod d = z mod d is in SX mod d. Since z was arbitrary, the set SX is d-complete.\\nK. Bringmann and P. Wellnitz 20\\n4.4.2 Long Arithmetic Progressions\\nIn this paper, an arithmetic progression is a set P of the form {a+ s, a+ 2s, . . . , a+ms}. We call m the\\nlength of P and s the step size of P.\\nProving existence of a long arithmetic progression in a set SX has a long tradition, e.g., consider the\\nfollowing result by Sárközy [43] (more precisely, we present a variant with improved constants from [34]).\\nTheorem 4.23 ([43, 34]). Let X be a set of n positive integers.\\nFor every integer 4mxX ≤ m ≤ n2/(12 log(4mxX/n)) the set SX contains an arithmetic progression P\\nof length m and step size s ≤ 4mxX/n. Moreover, every element of P can be obtained as the sum of at\\nmost 6m/n distinct elements of X.\\nNote that this theorem assumes X to be a set. Unfortunately, such a result is not readily available for\\nmulti-sets with prescribed multiplicity µX .\\nWe remark that one could naively use Theorem 4.23 on multi-sets by ignoring the multiplicities\\nand working on the support suppX . However, this loses a factor of |X|/|suppX | ≤ µX in the size, and\\nthus changes the density. In particular, this approach would require us to start with an Ω(µX)-dense\\nmulti-set X. We will avoid this additional factor µX , and only pay factors of the form polylog(µX).\\nThe main result of this section is a theorem similar to Theorem 4.23 that works for Ω(log(n) log2(µX))-\\ndense multi-sets. We prove this result by following and suitably adapting the proof by Sárközy [43].\\nTheorem 4.24. Let X be a multi-set of size n. For every integer m with\\n2mxX ≤ m ≤\\nn2\\n33984µX log(2n) log2(2µX)\\n,\\nthe set SX contains an arithmetic progression P of length m and step size s ≤ 4248µXmxX log(2µX)/n.\\nMoreover, every element of P can be obtained as the sum of at most 4248mµX log(2µX)/n distinct5\\nelements of X, and we have mxP ≤ 4248mµXmxX log(2µX)/n.\\nThe proof of Theorem 4.24 proceeds similar as in [43]; we present it here for completeness. Similar\\nto [43], we rely on the following result of [42].\\nTheorem 4.25 ([42]). Let X ⊆ [m ] be a set of n positive integers and let k be a positive integer with\\nn >\\nm\\nk\\n+ 1.\\nThen there is an integer 1 ≤ h < 118k such that the set Xh contains an arithmetic progression P of\\nlength m.\\nWe will need a slight adaptation of the above theorem.\\nLemma 4.26 (Variant of Theorem 4.25). Let X ⊆ [m ] be a set of n positive integers. Then the set\\nX≤354m/n =\\n⋃b354m/nc\\nj=1 X\\nj contains an arithmetic progression P of length m.\\n5 Here, distinct means that any integer x ∈ X may be chosen up to its multiplicity µ(x;X) times. Thus, the elements\\nchosen from X are distinct, but the corresponding integers might not.\\n21 On Near-Linear-Time Algorithms for Dense Subset Sum\\nProof. Recall that we assume all our sets to be non-empty. If n = 1 then we can write X = {x}. In this\\ncase, the set X≤m contains the arithmetic progression {x, 2x, . . . ,mx} of length m.\\nIf n ≥ 2, then we set k := bm/(n−1)c+1. Observe that 2 ≤ n ≤ m implies k ≤ m/(n−1)+1 ≤ 3m/n.\\nMoreover, note that we have k > m/(n− 1) or, equivalently, n > m/k + 1. Therefore, Theorem 4.25 is\\napplicable for k and shows that the set X≤118k contains an arithmetic progression of length m. Finally,\\nnote that X≤118k ⊆ X≤354m/n since k ≤ 3m/n.\\nIn order to use Theorem 4.26, we need to take care of two things. First, the arithmetic progression\\nobtained in Theorem 4.26 lies in X≤h, which may use elements from the set X multiple times, and thus\\ndoes not correspond to subset sums. Second, Theorem 4.26 assumes X to be a set. (While it may seem\\nas if both issues dissolve for multi-sets X, this is the case only for multi-sets with a multiplicity of at\\nleast 354m/n for every single element.)\\nWe tackle these two issues separately (and as in [43]): We consider a set of integers where every\\nelement can be obtained as a sum of two elements of X in many different, disjoint ways (see Lemmas 4.33\\nand 4.34). In order to obtain such a set, we first ensure that in our multi-set every number appears equally\\noften, that is, our multi-set has a uniform multiplicity (see Lemma 4.28).\\nDefinition 4.27 (Uniformity). We call a multi-set X uniform if every x ∈ X has multiplicity µX in X.\\nLemma 4.28. Let X be a δ-dense multi-set of size n. For any integer 0 ≤ r ≤ logµX , we define a\\nsubset Xr ⊆ X by picking 2r copies of every number with multiplicity in [2r, 2r+1), that is, for any x ∈ N\\nwe set\\nµ(x;Xr) :=\\n{\\n2r, if 2r ≤ µ(x;X) < 2r+1,\\n0, otherwise.\\nThere exists an integer 0 ≤ r ≤ logµX such that the multi-set Xr is δ/(4 log2(2µX))-dense and has size\\n|Xr| ≥\\nn\\n2 log(2µX)\\n.\\nProof. The proof is indirect. Assume that each of the sets Xr has size |Xr| < n/(2 log(2µX)). By\\nconstruction, at least every second element of X appears in some subset Xr. We thus have\\nn\\n2 ≤\\nblogµXc∑\\nr=0\\n|Xr| <\\nblogµXc∑\\nr=0\\nn\\n2 log(2µX)\\n≤ n2 ,\\nwhich yields the desired contradiction. Hence, there exists a subset Xr ⊆ X of size |Xr| ≥ n/(2 log(2µX)).\\nSince X is δ-dense, by Observation 4.8 we obtain that Xr is δ/(4 log2(2µX))-dense.\\nUsing Lemma 4.28, at the cost of some log(µX)-factors, we may assume that the given multi-set X is\\nuniform in the sense of Definition 4.27.\\nWe next turn to the sumset X +X of a uniform multi-set X.\\nDefinition 4.29 (Number of Representations). For a set S and an integer z, we define fS(z) as the\\nnumber of representations of z as the sum of two numbers in S, that is,\\nfS(z) := |{(x, x′) ∈ S × S | x+ x′ = z}|.\\nFor a uniform multi-set X and an integer z, we extend this notation by defining\\nfX(z) := µX · fsuppX (z).\\nK. Bringmann and P. Wellnitz 22\\nWe start by proving basic properties of the function fX .\\nLemma 4.30. For any uniform multi-set X of size n, the function fX satisfies all of the following:\\n1. For any integer z we have fX(z) ≤ n,\\n2. For any integer z > 2mxX we have fX(z) = 0,\\n3. The sum of all values of fX is\\n∑\\nz fX(z) = n2/µX .\\n4. Any integer z can be written in at least bfX(z)/2c disjoint ways as the sum of two elements of X, that\\nis, there exist distinct6 elements x1, x′1, . . . , xk, x′k ∈ X with xi + x′i = z for all i and k ≥ bfX(z)/2c.\\nProof. (1) Let S := suppX . In the definition of fS(z), after choosing x ∈ X we must set x′ = z − x.\\nThus, there are only |S| options to choose from, resulting in the inequality fS(z) ≤ |S|. For the uniform\\nmulti-set X, we thus obtain fX(z) = µX · fS(z) ≤ µX · |S| = |X|.\\n(2) Follows from the fact that the sum of two elements of X is at most 2 mxX .\\n(3) Let S := suppX . Note that every pair x, x′ ∈ S contributes to exactly one function value of fS ,\\nnamely fS(x+ x′). Hence, we have\\n∑\\nz fS(z) = |S|2. Since X is uniform, we have |S| = n/µX . Therefore,∑\\nz\\nfX(z) =\\n∑\\nz\\nµXfS(z) = µX |S|2 = n2/µX .\\n(4) Set S := suppX and let x, x′ ∈ S with x+ x′ = z. First consider the case x 6= x′. In this case, the\\npairs (x, x′) and (x′, x) contribute 2 to the value fS(z), so they contribute 2µX to the value fX(z). Note\\nthat we can form µX many disjoint pairs between the µX copies of x in X and the µX copies of x′ in X.\\nThat is, the number of constructed pairs is half of the contribution to fX(z).\\nLet us turn to the case x = x′, that is, x = z/2. In this case, the pair (x, x′) contributes 1 to the value\\nfS(z), so it contributes µX to the value fX(z). Note that we can form bµX/2c many disjoint pairs among\\nthe µX copies of x in X. Again this is half of the contribution to fX(z), but now rounded down. In total,\\nwe have constructed bfX(z)/2c disjoint pairs summing to z.\\nNext we consider buckets of numbers with an almost uniform number of representations. Informally,\\nbucket Bv,X contains all integers z that can be written in Ω(v) many disjoint ways as the sum of two\\nelements of X.\\nDefinition 4.31 (Buckets). For any uniform multi-set X and any integer v ≥ 1, we define the bucket\\nBv,X := {z ∈ N | fX(z) ≥ v}.\\nOur goal is to apply Theorem 4.26 to an appropriate bucket Bv,X . We list some simple observations.\\nLemma 4.32. Let X be a uniform multi-set and let v ≥ 1 be an integer. Then we have\\n1. |Bv,X| ≤ 2 mxX .\\n2. If v > |X| then the set Bv,X is empty.\\nProof. Both properties follow immediately from Lemma 4.30. For the first we use that the function fX is\\nzero on all integers larger than 2 mxX . For the second we use that fX is bounded from above by |X|.\\nThe buckets Bv,X can, in a limited way, remedy the multiple use of the same element in a sum. While\\nin general for a multi-set X the set Xh may contain numbers that are no subset sums of X, we show that\\nfor sufficiently small h all numbers in Bhv,X also correspond to subset sums of X. In the proof of this\\nstatement, we use the large number of representations guaranteed by the definition of Bv,X to avoid any\\nmultiple use of the same number.\\n6 Here again distinct means that any x ∈ X may appear up to µ(x;X) times in this sequence.\\n23 On Near-Linear-Time Algorithms for Dense Subset Sum\\nLemma 4.33 (Compare [43, Lemma 2]). Let X be a uniform multi-set. For any integers v ≥ 1 and\\n1 ≤ h ≤ (v− 1)/4, every number in the set Bhv,X can be represented as a sum of 2h distinct elements of X.\\nProof. The proof is by induction on h. We define B0v,X := {0} to make the base case h = 0 trivial.\\nFor h ≥ 1, consider any w ∈ Bhv,X and write w = z + w′ for some z ∈ Bv,X and w′ ∈ B\\nh−1\\nv,X . By the\\ninduction hypothesis, we can represent w′ as a sum x1 + . . .+ x2h−2 of distinct elements of X. By the\\ndefinition of Bv,X we have fX(z) ≥ v, so by Lemma 4.30.4 we can find bv/2c ≥ (v − 1)/2 ≥ 2h disjoint\\nrepresentations of the number z as the sum of two numbers in X. By the pigeonhole principle, at least\\none of these 2h many representation of z does not contain any of the 2h− 2 many numbers x1, . . . , x2h−2.\\nWe pick such a representation z = x2h−1 + x2h to obtain a representation of w as a sum x1 + . . .+ x2h of\\n2h distinct elements of X.\\nThe above lemma shows that plugging the set Bv,X into Theorem 4.26 yields knowledge about the\\nsubset sums of X, despite the formulation of Theorem 4.26 allowing to pick summands multiple times. In\\norder to use Theorem 4.26 effectively, we need to pick a bucket Bv,X of large size. We next prove existence\\nof such a bucket.\\nLemma 4.34 (Compare [43, Lemma 1]). Let X be a uniform multi-set of size n that is 7-dense. Then\\nthere is an integer 1 < v ≤ n such that the set Bv,X satisfies\\n|Bv,X | ≥\\nn\\n3µX\\n+ n\\n2\\n3 v µX log(2n)\\n.\\nProof. The proof is indirect. Assume that for every integer 1 < v ≤ n we have\\n|Bv,X | <\\nn\\n3µX\\n+ n\\n2\\n3 v µX log(2n)\\n.\\nFrom the construction of the buckets Bv,X , we observe the following identity:∑\\nz\\nfX(z) =\\n∑\\nv≥1\\nv ·\\n(\\n|Bv,X | − |Bv+1,X |\\n)\\n.\\nBy telescoping this sum and by using\\n∑\\nz fX(z) = n2/µX from Lemma 4.30.3, we arrive at\\nn2\\nµX\\n=\\n∑\\nv≥1\\n|Bv,X |.\\nWe bound the right hand side by using |Bv,X | = 0 for v > n (by Lemma 4.32.2), the assumed upper bound\\nfor 1 < v ≤ n, and |Bv,X | ≤ 2mxX for v = 1 (by Lemma 4.32.1). This yields\\nn2\\nµX\\n≤ 2mxX +\\nn∑\\nv=2\\n(\\nn\\n3µX\\n+ n\\n2\\n3 v µX log(2n)\\n)\\n≤ 2mxX +\\nn2\\n3µX\\n+ n\\n2\\n3µX log(2n)\\n·\\nn∑\\nv=1\\n1\\nv\\n.\\nWe now use the standard fact\\n∑n\\nv=1 1/v ≤ 1 + log(n) = log(2n) to obtain\\nn2\\nµX\\n≤ 2mxX +\\n2n2\\n3µX\\n.\\nFinally, we use that the multi-setX is 7-dense, so that n2 ≥ 7µXmxX , which yields the desired contradiction\\nn2\\nµX\\n≤\\n(2\\n7 +\\n2\\n3\\n)\\n· n\\n2\\nµX\\n.\\nK. Bringmann and P. Wellnitz 24\\nCombining Lemmas 4.26, 4.28, 4.33, and 4.34 we now prove the main theorem of this section.\\nTheorem 4.24. Let X be a multi-set of size n. For every integer m with\\n2mxX ≤ m ≤\\nn2\\n33984µX log(2n) log2(2µX)\\n,\\nthe set SX contains an arithmetic progression P of length m and step size s ≤ 4248µXmxX log(2µX)/n.\\nMoreover, every element of P can be obtained as the sum of at most 4248mµX log(2µX)/n distinct7\\nelements of X, and we have mxP ≤ 4248mµXmxX log(2µX)/n.\\nProof. In order for the theorem statement to be non-trivial we must have\\n2mxX ≤\\nn2\\n33984µX log(2n) log2(2µX)\\n.\\nRearranging this shows that X must be δ-dense for\\nδ := 67968 log(2n) log2(2µX).\\nWe first apply Lemma 4.28 to obtain a subset X ′ := Xr ⊆ X such that X ′ is a uniform multi-set of size n′\\nthat is δ′-dense, where\\nn ≥ n′ ≥ n2 log(2µX)\\n, δ′ = δ\\n4 log2(2µX)\\n≥ 7.\\nNext, we apply Lemma 4.34 to obtain an integer 1 < v ≤ n′ such that\\n|Bv,X′ | ≥\\nn′\\n3µX\\n+ n\\n′2\\n3 v µX log(2n)\\n.\\nBy Lemma 4.30.2, for any m ≥ 2mxX we have Bv,X′ ⊆ [m ]. Thus, Theorem 4.26 yields that the set\\nB≤hv,X′ contains an arithmetic progression P of length m, for h := 354m/|Bv,X′ |. Using our bounds on\\n|Bv,X′ | and n′, we obtain\\nh = 354m\\n|Bv,X′ |\\n≤ 354m · 3 v µX log(2n)\\nn′2\\n≤ 354m · 12 v µX log(2n) log\\n2(2µX)\\nn2\\n≤ v8 ,\\nwhere the last step uses the assumption on m. Since v > 1 is an integer, we have v − 1 ≥ v/2, so we can\\nfurther bound\\nh ≤ v − 14 .\\nTherefore, Lemma 4.33 is applicable and implies that the arithmetic progression P also appears as a\\nsubset of SX . Moreover, Lemma 4.33 shows that every element of P can be written as the sum of at most\\n2h distinct elements of X. We now bound differently from before:\\nh = 354m\\n|Bv,X′ |\\n≤ 354m · 3µX\\nn′\\n≤ 354m · 6µX log(2µX)\\nn\\n.\\n7 Here, distinct means that any integer x ∈ X may be chosen up to its multiplicity µ(x;X) times. Thus, the elements\\nchosen from X are distinct, but the corresponding integers might not.\\n25 On Near-Linear-Time Algorithms for Dense Subset Sum\\nThis shows that every element of P can be obtained as the sum of at most 2h ≤ 4248mµX log(2µX)/n\\ndistinct elements of X. In particular, we obtain\\nmxP ≤ 2h ·mxX ≤ 4248mµXmxX log(2µX)/n. (5)\\nDenote by s the step size of the arithmetic progression P. Then we have m · s ≤ mxP (here we use\\nthat P is of the form {a+ s, a+ 2s, . . . , a+m · s}). Together with inequality (5), this yields\\ns ≤ 4248µXmxX log(2µX)/n.\\n4.4.3 Constructing a Decomposition\\nWe use the tools from the last two sections to decompose X as follows.\\nTheorem 4.35. Let X be a δ-dense multi-set of size n that has no α-almost divisor, where\\nδ = 1699200 log(2n) log2(2µX),\\nα = 42480 log(2µX).\\nThere exists a partitioning X = R ∪A ∪G and an integer s ≤ 42480 · µXΣX log(2µX)/n2 such that\\nthe set SR is s-complete, that is, SR mod s = Zs,\\nthe set SA contains an arithmetic progression P of length 2mxX and step size s satisfying mxP ≤\\n84960µXmxXΣX log(2µX)/n2,\\nthe multi-set G has sum ΣG ≥ ΣX/2.\\nProof. Given the multi-set X, we first use Lemma 4.20 to obtain the subset R ⊆ X. From the remaining\\nelements X \\\\R we pick the smallest bn/4c elements to form the set A. We call the remaining set G :=\\n(X\\\\R)\\\\A. This yields the partitioningX = R∪A∪G (in the sense of µ(x;X) = µ(x;R)+µ(x;A)+µ(x;G)).\\nIn the following we analyze the properties of this decomposition.\\nClaim 4.36. The multi-set R satisfies:\\n|R| ≤ n/4,\\nΣR ≤ ΣX/4, and\\nR is d-complete for any d ≤ α · µXΣX/n2 = 42480 · µXΣX log(2µX)/n2.\\nProof. The first two properties follow directly from Lemma 4.20 and the inequality δ ≥ 32α log(2n), which\\nfollows from the definitions of δ and α.\\nLemma 4.20 also shows that for any 1 < d ≤ α · µXΣX/n2 we have |R(d)| ≥ d. The third claim now\\nfollows from Lemma 4.22.\\nClaim 4.37. The multi-set A satisfies:\\nn/5 ≤ |A| ≤ n/4,\\nmxA ≤ 2ΣX/n,\\nSA contains an arithmetic progression P of length 2mxX and step size\\ns ≤ 42480 · µXΣX\\nn2\\nlog(2µX).\\nThe arithmetic progression P moreover satisfies\\nmxP ≤ 84960 ·\\nµXmxXΣX\\nn2\\nlog(2µX).\\nK. Bringmann and P. Wellnitz 26\\nProof. By definition of δ-density we obtain n2 ≥ δ ≥ 225 and thus n ≥ 15. This implies |A| = bn/4c ≥\\n(n− 3)/4 ≥ n/5, which proves the first claim.\\nSince R picks at most n/4 elements from X and A picks the bn/4c many smallest remaining elements,\\nit follows that every elements in A is bounded from above by the median of X. Since X contains at\\nleast n/2 elements that are larger than or equal to the median, the median is bounded from above by\\nΣX/(n/2) = 2ΣX/n. Hence, we have mxA ≤ 2ΣX/n.\\nFor the last claim, we apply Theorem 4.24 to the multi-set A and m := 2mxX . Let us check the\\npreconditions of this theorem. We clearly have m = 2mxX ≥ 2mxA. Moreover, we have\\n|A|2\\n33984µA log(2|A|) log2(2µA)\\n≥ n\\n2\\n52 · 33984µX log(2n) log2(2µX)\\n≥ 2mxX = m,\\nwhere we used the assumption that X is δ-dense for δ ≥ 1699200 log(2n) log2(2µX). Thus, Theorem 4.24\\nis applicable to (A,m) and yields an arithmetic progression P in SA of length m and step size\\ns ≤ 4248 · µAmxA\\n|A|\\nlog(2µA) ≤ 5 · 2 · 4248 ·\\nµXΣX\\nn2\\nlog(2µX) = 42480 ·\\nµXΣX\\nn2\\nlog(2µX),\\nwhere we used the properties |A| ≥ n/5 and mxA ≤ 2ΣX/n. Moreover, from Theorem 4.24 we also obtain\\nmxP ≤ 4248 ·\\nmµAmxA\\n|A|\\nlog(2µA) ≤ 5 ·2 ·4248 ·2 ·\\nmxXµXΣX\\nn2\\nlog(2µX) = 84960 ·\\nµXmxXΣX\\nn2\\nlog(2µX).\\nClaim 4.38. The multi-set G satisfies ΣG ≥ ΣX/2.\\nProof. Since A picks the bn/4c smallest elements of X \\\\R, and since |X \\\\R| ≥ 34n by Claim 4.36, we have\\nΣA ≤\\n|A|\\n|X \\\\R|\\n· ΣX\\\\R ≤\\n1\\n3ΣX\\\\R =\\n1\\n3(ΣX − ΣR).\\nUsing ΣR ≤ ΣX/4 from Claim 4.36, we obtain\\nΣA + ΣR ≤\\n1\\n3(ΣX − ΣR) + ΣR =\\n1\\n3ΣX +\\n2\\n3ΣR ≤\\n(1\\n3 +\\n1\\n6\\n)\\nΣX =\\nΣX\\n2 .\\nTherefore, ΣG = ΣX − ΣR − ΣA ≥ ΣX/2.\\nNote that since the multi-set R is d-complete for each small d, in particular R is also s-complete.\\nHence, Claims 4.36, 4.37, and 4.38 finish the proof of Theorem 4.35.\\n4.4.4 Proof of the Structual Part\\nFinally, we are ready to prove the structural part.\\nTheorem 4.2 (Structural Part, Formal Version of Theorem 3.3). Let X be a multi-set and set\\nCδ := 1699200 · log(2n) log2(2µX),\\nCα := 42480 · log(2µX),\\nCλ := 169920 · log(2µX).\\nIf X is Cδ-dense and has no Cα-almost divisor, then for λX := Cλ · µXmxXΣX/|X|2 we have\\n[λX . . ΣX − λX ] ⊆ SX .\\n27 On Near-Linear-Time Algorithms for Dense Subset Sum\\nProof. We want to show that any target number t ∈ [λX . . ΣX − λX ] is also a subset sum of X. By\\nsymmetry, it suffices to prove the claim for t ≤ ΣX/2.\\nWe construct the partitioning X = R∪A∪G from Theorem 4.35. We denote the arithmetic progression\\nP ⊆ SA by P = {a+ s, a+ 2s, . . . , a+ 2mxXs}.\\nWe construct a subset summing to t as follows. First, we pick a subset G′ ⊆ G by greedily adding\\nelements until\\nt− a− s · (mxX + 1)−mxX < ΣG′ ≤ t− a− s · (mxX + 1).\\nThis is possible because this range for ΣG′ has length mxX , and because we have t ≤ ΣX/2 ≤ ΣG and\\nt ≥ λX ≥ (84960 + 2 · 42480) · µXmxXΣX log(2µX)/n2 ≥ mxP + 2smxX ≥ a+ s · (mxX + 1).\\nNext we pick a subset R′ ⊆ R that sums to (t − ΣG′ − a) modulo s. This is possible because R\\nis s-complete. We can assume that R′ has size |R′| ≤ s, since otherwise some subset of R′ sums to 0\\nmodulo s and can be removed (the details of this argument were explained in the proof of Claim 4.4). In\\nparticular, we can assume ΣR′ ≤ s ·mxX .\\nWe thus have t− ΣG′∪R′ ≡ a (mod s) and\\nt− a− s · (mxX + 1)−mxX < ΣG′∪R′ ≤ t− a− s,\\nor, equivalently,\\na+ s ≤ t− ΣG′∪R′ < a+ s · (mxX + 1) + mxX .\\nNote that for any positive integers x, y we have (x− 1)(y− 1) ≥ 0. Rearranging this yields x+ y ≤ xy+ 1,\\nor equivalently x(y + 1) + y ≤ 2xy + 1. Using this, we obtain\\na+ s ≤ t− ΣG′∪R′ ≤ a+ 2mxXs.\\nNote that this is exactly the range of the elements of the arithmetic progression P. Moreover, since the\\nremaining target t−ΣG′∪R′ is of the form a+ks for some integer k, we can pick a subset A′ ⊆ A that gives\\nthe appropriate element of the arithmetic progression P and thus yields the desired t = ΣG′∪R′∪A′ .\\nThis finishes the proof of the structural part, and thus of the algorithm.\\n5 Fine-Grained Lower Bound\\nBefore presenting our conditional lower bound, we introduce and discuss our hardness assumptions.\\n5.1 Hardness Assumptions\\n5.1.1 Strong Exponential Time Hypothesis\\nWe first consider the classic Satisfiability problem, more precisely the k-SAT problem.\\nProblem 5.1 (k-SAT). Given a k-CNF formula ϕ on N variables and M clauses, decide whether ϕ is\\nsatisfiable, that is, decide whether there is an assignment of true or false to the variables such that ϕ is\\nsatisfied.\\nThe Strong Exponential Time Hypothesis was introduced by Impagliazzo, Paturi, and Zane and essentially\\npostulates that there is no exponential improvement over exhaustive search for the k-SAT problem. This\\nis the most widely used hardness assumption in fine-grained complexity theory [48].\\nConjecture 5.2 (Strong Exponential Time Hypothesis (SETH) [27, 14]). For any ε > 0 there is an\\ninteger k ≥ 3 such that k-SAT cannot be solved in time O(2(1−ε)N ).\\nK. Bringmann and P. Wellnitz 28\\n5.1.2 Strong k-Sum Hypothesis\\nProblem 5.3 (k-Sum). Given a set Z ⊆ [U ] of N integers and a target T , decide whether there exist\\nz1, . . . , zk ∈ Z with z1 + . . .+ zk = T .\\nThe k-Sum problem has classic algorithms running in time O(Ndk/2e) (via meet-in-the-middle) and in\\ntime Õ(U) (via Fast Fourier transform). The (standard) k-Sum Hypothesis postulates that the former\\nalgorithm cannot be improved by polynomial factors, i.e., k-Sum has no O(Ndk/2e−ε)-time algorithm for\\nany ε > 0 [24]. Note that both algorithmic approaches yield the same running time when U ≈ Ndk/2e. The\\nStrong k-Sum Hypothesis postulates that even in this special case both algorithms cannot be improved by\\npoynomial factors.\\nConjecture 5.4 (Strong k-Sum Hypothesis [6, 1]). For any k ≥ 3 and ε > 0, the k-Sum problem\\nrestricted to U = Ndk/2e cannot be solved in time O(Ndk/2e−ε).\\n5.1.3 Intermediate Hypothesis\\nIn this paper, we introduce and make use of the following hypothesis.\\nConjecture 5.5 (Intermediate Hypothesis). For any constants α, ε > 0 there exists a constant k ≥ 3\\nsuch that k-Sum restricted to N ≤ Uα cannot be solved in time O(U1−ε).\\nWe call this hypothesis “intermediate” because it not as strong as the Strong k-Sum Hypothesis.\\nIndeed, the latter implies the former.\\nLemma 5.6. The Strong k-Sum Hypothesis implies the Intermediate Hypothesis.\\nProof. We show that if the Intermediate Hypothesis fails then the Strong k-Sum Hypothesis fails.\\nIf the Intermediate Hypothesis fails, then there exist α, ε > 0 such that for all k ≥ 3 the k-Sum\\nproblem restricted to N ≤ Uα can be solved in time O(U1−ε). In particular, this holds for k := d2/αe.\\nFor this value of k, we have Uα ≥ U2/k ≥ U1/dk/2e. Hence, any k-Sum instance with U = Ndk/2e satisfies\\nN ≤ Uα. In particular, using the assumed algorithm, k-Sum restricted to U = Ndk/2e can be solved in\\ntime O(U1−ε) = O(Ndk/2e−ε′) for ε′ := ε · dk/2e > 0, so the Strong k-SUM Hypothesis fails.\\nMoreover, the Intermediate Hypothesis also follows from the Strong Exponential Time Hypothesis.\\nLemma 5.7. SETH implies the Intermediate Hypothesis.\\nThis follows from the following theorem by Abboud et al. [2].\\nTheorem 5.8 ([2]). Assuming SETH, for any ε > 0 there exists δ > 0 such that for any k the k-Sum\\nproblem is not in time O(T 1−εNδk).\\nProof of Lemma 5.7. We show that if the Intermediate Hypothesis fails then SETH fails.\\nIf the Intermediate Hypothesis fails, then there exist α, ε > 0 such that for any k ≥ 3 we can solve\\nk-Sum restricted to N ≤ Uα in time O(U1−ε). We claim that, without any restriction on N , we can then\\nsolve k-Sum in time O(U1−ε +N1/αpolylogN). Indeed, for N ≤ Uα we assumed running time O(U1−ε),\\nand for N > Uα using the standard algorithm based on Fast Fourier Transform we solve k-Sum in time\\nÕ(U) = Õ(N1/α). We roughly bound this time by O(U1−ε +N1/αpolylogN) = O(U1−εN2/α).\\n29 On Near-Linear-Time Algorithms for Dense Subset Sum\\nNote that for k-Sum we can assume without loss of generality that U ≤ T , since input numbers\\nlarger than T can be ignored. Therefore, we can bound the running time of our k-Sum algorithm by\\nO(U1−εN2/α) = O(T 1−εN2/α).\\nNow it is useful to consider the contraposition of Theorem 5.8: If there exists ε > 0 such that for\\nall δ > 0 there exists k such that k-Sum is in time O(T 1−εNδk), then SETH fails. We note that our\\nO(T 1−εN2/α)-time algorithm for k-Sum satisfies the precondition of this statement, by picking the same\\nvalue for ε and setting k := d2/(αδ)e. Hence, we showed that SETH fails.\\nBy the above two lemmas, if we want to prove a lower bound based on the Strong Exponential Time\\nHypothesis and the Strong k-Sum Hypothesis, then it suffices to prove a lower bound based on the\\nIntermediate Hypothesis.\\n5.2 A Lower Bound for Subset Sum\\nWe can now present our lower bound. Throughout this section we only consider the set case, so we let X\\nbe a set of size n, in particular µX = 1. Recall that in this case Subset Sum can be solved in time Õ(n) if\\nt\\x1d mxXΣX/n2 (Theorem 1.2).\\nOur goal is to show that this regime essentially characterizes all near-linear-time settings, that is,\\ndense Subset Sum is not in near-linear time for t\\x1c mxXΣX/n2. To show that we do not miss any setting,\\nwe consider a notion of parameter settings similarly as in [12]: For the parameters t,mxX ,ΣX we fix\\ncorresponding exponents τ, ξ, σ, and we focus on Subset Sum instances (X, t) that satisfy t = Θ(nτ ),\\nmxX = Θ(nξ), and ΣX = Θ(nσ). This defines a slice or parameter setting of the Subset Sum problem.\\nOur goal is to prove a conditional lower bound for each parameter setting in which the near-linear time\\nalgorithms do not apply.\\nWe note that some choices of the exponents τ, ξ, σ are contradictory, in the sense that there exist no\\n(or only finitely many) instances satisfying t = Θ(nτ ), mxX = Θ(nξ), and ΣX = Θ(nσ). Additionally,\\nsome assumptions can be made that hold without loss of generality. Specifically, we call any parameter\\nsetting (τ, ξ, σ) non-trivial if it satisfies all of the following justified inequalities:\\nξ ≥ 1: For any set X of n positive integers we have mxX ≥ n.\\nσ ≥ 2: For any set X of n positive integers we have ΣX ≥\\n∑n\\ni=1 i = Ω(n2).\\nσ ≤ 1 + ξ: Any set X satisfies ΣX ≤ n ·mxX .\\nτ ≥ ξ: Since any numbers in X larger than t can be ignored, we can assume t ≥ mxX .\\nτ ≤ σ: If t > ΣX then there is no solution, so the problem is trivial.\\nRecall that we only want to prove a super-linear lower bound in the regime t\\x1c mxXΣX/n2. We call a\\nparameter setting hard if it satisfies the corresponding inequality on the exponents:\\nτ < ξ + σ − 2.\\nOur goal is to show a lower bound of the form n1+Ω(1) for each hard non-trivial parameter setting.\\nThis discussion is summarized and formalized by the following definition.\\nDefinition 5.9. A parameter setting is a tuple (τ, ξ, σ) ∈ R3. A parameter setting is called non-trivial\\nif σ ≥ 2 and 1 ≤ ξ ≤ τ ≤ σ ≤ 1 + ξ. A parameter setting is called hard if τ < ξ + σ − 2.\\nFor a parameter setting (τ, ξ, σ) and a constant ρ ≥ 1 we define Subset Sumρ(τ, ξ, σ) as the set of\\nall Subset Sum instances (X, t) for which the quotients t/|X|τ , mxX/|X|ξ, and ΣX/|X|σ all lie in the\\ninterval [1/ρ, ρ]. In some statements we simply write Subset Sum(τ, ξ, σ) to abbreviate that there exists a\\nconstant ρ ≥ 1 such that the statement holds for Subset Sumρ(τ, ξ, σ).\\nNote that we can express the running time of an algorithm solving Subset Sum(τ, ξ, σ) either in terms\\nof n, t,mxX ,ΣX or in terms of n, nτ , nξ, nσ, both views are equivalent. Our main result of this section is:\\nK. Bringmann and P. Wellnitz 30\\nTheorem 5.10 (Lower Bound with Parameter Settings). Assuming the Intermediate Hypothesis, for any\\nnon-trivial parameter setting (τ, ξ, σ) there is a constant ρ ≥ 1 such that for any ε > 0 the problem Subset\\nSumρ(τ, ξ, σ) cannot be solved in time O\\n(\\n(mxXΣX/(nt))1−ε\\n)\\n= O\\n(\\nn(ξ+σ−τ−1)(1−ε)\\n)\\n.\\nBy Lemmas 5.6 and 5.7, the same lower bound also holds under the Strong Exponential Time\\nHypothesis and under the Strong k-Sum Hypothesis. Ignoring the notion of parameter settings, we have\\nthus shown that Subset Sum cannot be solved in time O\\n(\\n(mxXΣX/(nt))1−ε\\n)\\nfor any ε > 0, unless the\\nStrong Exponential Time Hypothesis and the Strong k-Sum Hypothesis both fail. This proves Theorem 1.3.\\nNote that Theorem 5.10 is trivial when nξ+σ−τ−1 ≤ n, since it is then subsumed by the trivial lower\\nbound of Ω(n) to read the input. Therefore, we only need to prove the theorem statement for hard\\nnon-trivial parameter settings.\\nWe prove Theorem 5.10 by a reduction from k-Sum to any hard non-trivial parameter setting of Subset\\nSum. The reduction transforms the hypothesized time complexity U1−o(1) of k-Sum into a lower bound of\\nnξ+σ−τ−1−o(1) for Subset Sum.\\nLemma 5.11 (The Reduction). Let (τ, ξ, σ) be a hard non-trivial parameter setting and fix k ≥ 3. Set\\nα := 1/(ξ + σ − τ − 1). Given an instance (Z, T ) of k-Sum with Z ⊆ [U ] and |Z| ≤ Uα, in time O(Uα)\\nwe can construct an equivalent instance (X, t) of Subset Sum(τ, ξ, σ) with |X| = Θ(Uα).\\nThis reduction easily implies Theorem 5.10.\\nProof of Theorem 5.10. Assume that some non-trivial parameter setting Subset Sum(τ, ξ, σ) can be\\nsolved in time O(n(ξ+σ−τ−1)(1−ε)) for some ε > 0. Since reading the input requires time Ω(n), we must\\nhave ξ + σ − τ − 1 > 1, which is equivalent to α := 1/(ξ + σ − τ − 1) < 1. Pick any k ≥ 3 and an\\ninstance (Z, T ) of k-Sum with Z ⊆ [U ] and |Z| ≤ Uα. Run the reduction from Lemma 5.11 to produce\\nan equivalent Subset Sum instance (X, t). The reduction itself runs in time O(Uα). Now we use the\\nassumed algorithm to solve the instance (X, t) in time O(|X|(ξ+σ−τ−1)(1−ε)) = O(U1−ε). Since (X, t) is\\nequivalent to (Z, T ), we have thus solved the given instance (Z, T ) in time O(Uα + U1−ε) = O(U1−ε′) for\\nε′ := min{ε, 1− α} > 0. This violates the Intermediate Hypothesis.\\nIt remains to design the reduction.\\n5.3 The Reduction\\nIn this section we prove Lemma 5.11.\\nProof of Lemma 5.11. We set\\nβ := ασ − α− 1 and γ := α(ξ − σ + 1).\\nGiven a k-Sum instance (Z, T ), we construct the following Subset Sum instance (X, t). Consider Figure 1\\nfor a visualization.\\nX :=X1 ∪X2 ∪X3, where\\nX1 := {8k2U1+β + z · 4kUβ + 2Uβ | z ∈ Z}\\nX2 := {Uγ · 8k2U1+β + j · 4kUβ + 1 | j ∈ [Uβ ]}\\nX3 := {8k2U1+β + j · 4kUβ | j ∈ [Uα ]}\\nt :=(k + Uβ+γ) · 8k2U1+β + (T + Σ[Uβ ]) · 4kUβ + k · 2Uβ + Uβ\\n31 On Near-Linear-Time Algorithms for Dense Subset Sum\\n2Uβ4k Uβ8k2 U1+β8k2 U1+β+γ\\n0\\n0\\n...\\n0\\nX1\\n1\\n1\\n...\\n1\\nz1\\nz2\\n...\\nz|Z|\\n1\\n1\\n...\\n1\\n0\\n0\\n...\\n0\\nX3\\n0\\n0\\n...\\n0\\n1\\n2\\n...\\nUα\\n1\\n1\\n...\\n1\\n1\\n1\\n...\\n1\\nX2\\n0\\n0\\n...\\n0\\n1\\n2\\n...\\nUβ\\nUγ\\nUγ\\n...\\nUγ\\nUβkT + Σ[Uβ ]k + Uβ+γ\\nFigure 1 An overview over the reduction from k-Sum to Subset Sum. The given k-Sum instance is (Z, T ), and\\nwe write Z = {z1, . . . , z|Z|}. Bit blocks of the constructed numbers are depicted as boxes, the value of a bit block is\\nwritten inside the corresponding box. The constructed target number is visualized at the bottom. The annotations\\nat the top represent the maximum value of the constructed numbers up to the specified point. The annotations on\\nthe right denote group of constructed numbers. We remark that the number T + Σ[Uβ ] not necessarily fits in its\\nblock, but this is the only overflow that can occur in this figure.\\nFor simplicity, here we assumed that Uα, Uβ , Uγ are integers, more precisely they should be replaced by\\ndUαe, dUβe, dUγe. For this construction to make sense we need α, β, γ ≥ 0; we will take care of these\\nbounds later.\\nWe first verify that the Subset Sum instance (X, t) is indeed equivalent to the k-Sum instance (Z, T ).\\nClaim 5.12. Any solution to the Subset Sum instance (X, t) corresponds to a solution to the k-Sum\\ninstance (Z, T ), and vice versa.\\nProof. We start with the easier direction: Any solution to (Z, T ) corresponds to a solution to (X, t). To\\nthat end, let B ⊆ Z denote a solution to the k-Sum instance (Z, T ), that is, we have ΣB = T and |B| = k.\\nConsider the set A ⊆ X defined by picking the subset of X1 corresponding to B, and picking all numbers\\nin X2, that is,\\nA := {8k2U1+β + z · 4kUβ + 2Uβ | z ∈ B} ∪ {Uγ · 8k2U1+β + j · 4kUβ + 1 | j ∈ [Uβ ]}.\\nObserve that we have\\nΣA = (k · 8k2U1+β + T · 4kUβ + k · 2Uβ) + (Uβ+γ · 8k2U1+β + Σ[Uβ ] · 4kUβ + Uβ)\\n= (k + Uβ+γ) · 8k2U1+β + (T + Σ[Uβ ]) · 4kUβ + k · 2Uβ + Uβ = t,\\nK. Bringmann and P. Wellnitz 32\\ncompleting the proof of the first direction.\\nFor the other direction, let A ⊆ X denote a solution to the Subset Sum instance (X, t), that is, we\\nhave ΣA = t. By construction, we have t ≡ Uβ (mod 2Uβ). Since all numbers in X1 ∪X3 are 0 modulo\\n2Uβ , and all Uβ many numbers in X2 are 1 modulo 2Uβ , the set A must contain all numbers in X2.\\nThus, consider the remaining set A′ := A \\\\X2. We have\\nt′ := ΣA′ = ΣA − ΣX2 = t− (Uβ+γ · 8k2U1+β + Σ[Uβ ] · 4kUβ + Uβ)\\n= k · 8k2U1+β + T · 4kUβ + k · 2Uβ . (6)\\nObserve that we have t′ ≡ k · 2Uβ (mod 4kUβ). Since the numbers in X3 are 0 modulo 4kUβ , and the\\nnumbers in X1 are 2Uβ modulo 4kUβ , it follows that |A′ ∩X1| ≡ k (mod 2k). In particular, we have\\n|A′ ∩X1| ≥ k. (7)\\nWe can assume without loss of generality that T ≤ kU . This implies\\nt′ = k · 8k2U1+β + T · 4kUβ + k · 2Uβ < (k + 1)8k2U1+β .\\nSince all numbers in X1 ∪X3 are bounded from below by 8k2U1+β , the bound on t′ implies that we can\\nchoose at most k items from X1 ∪ X3, that is, |A′| ≤ k. Together with inequality (7), it follows that\\nA′ ⊆ X1 and |A′| = k.\\nSo let B ⊆ Z be the subset corresponding to A′ ⊆ X1. Then we have\\nΣA′ = k · 8k2U1+β + ΣB · 4kUβ + k · 2Uβ .\\nComparing with (6), we obtain ΣB = T . Hence, if the Subset Sum instance (X, t) has a solution A, then\\nthe k-Sum instance (Z, T ) has a solution B. This completes the proof of the second direction and thus\\nthe proof of the claim.\\nWe next verify that α, β, γ ≥ 0, in addition to other inequalities that we will need in the following.\\nClaim 5.13. The parameters α, β, γ satisfy the following inequalities.\\n1. 0 < α < 1,\\n2. β ≥ 0,\\n3. γ ≥ 0,\\n4. β + γ ≤ α,\\n5. β ≤ α.\\nProof. (1.) Follows from the parameter setting (τ, ξ, σ) being hard. (2.) The non-triviality assumption\\nτ ≥ ξ yields σ − 1 ≥ σ + ξ − τ − 1 = 1/α. After rearranging, we obtain 0 ≤ ασ − α − 1 = β. (3.) The\\nnon-triviality assumption σ ≤ 1+ξ yields ξ−σ+1 ≥ 0. Together with α > 0 we obtain 0 ≤ α(ξ−σ+1) = γ.\\n(4.) β + γ ≤ α: The non-triviality assumption τ ≤ σ yields ξ − 1 ≤ σ+ ξ − τ − 1 = 1/α. Rearranging this,\\nwe obtain α ≥ αξ − 1 = β + γ. (5.) Follows from the preceeding inequality and γ ≥ 0.\\nIt remains to verify that the instance (X, t) belongs to the parameter setting Subset Sum(τ, ξ, σ) and\\nfulfills the claimed size bound |X| = Θ(Uα).\\nClaim 5.14. The Subset Sum instance (X, t) satisfies n := |X| = Θ(Uα), mxX = Θ(nσ), ΣX = Θ(nξ),\\nand t = Θ(nτ ).\\n33 On Near-Linear-Time Algorithms for Dense Subset Sum\\nProof. For the size of X we bound Uα ≤ |X| ≤ Uα+Uα+Uβ ≤ 3Uα, where we used β ≤ α (Claim 5.13.5).\\nNote that in the definition of X1, X2, X3 the leftmost summand is always asymptotically dominating.\\nIn particular, every x ∈ X1 ∪X3 satisfies x = Θ(U1+β) and every x ∈ X2 satisfies x = Θ(U1+β+γ). (Here\\nwe treat k as a constant, and for X1 we use Z ⊆ [U ], for X2 we use β ≤ α ≤ 1, and for X3 we use α ≤ 1.)\\nThis allows us to determine the maximum number as mxX = Θ(U1+β+γ). From the definition of β, γ\\nwe see that 1 + β + γ = αξ. Hence, mxX = Θ(Uαξ) = Θ(nξ).\\nNote that |X1| = |Z| ≤ Uα, |X2| = Uβ , and |X3| = Uα. From the resulting |X1 ∪X3| = Θ(Uα) and\\nour bounds on numbers in X1 ∪X3 and X2, we determine the sum of all numbers in X as\\nΣX = Θ(Uα · U1+β + Uβ · U1+β+γ).\\nThe inequality β + γ ≤ α (Claim 5.13.4) now yields ΣX = Θ(U1+α+β). From the definition of β, we see\\nthat ΣX = Θ(Uασ) = Θ(nσ).\\nFinally, we turn to the target t. We claim that t = Θ(Uβ+γ · U1+β), that is, t is asymptotically\\ndominated by its first summand. This is clear for almost all summands. For the summand T · 4kUβ\\nwe use T ≤ kU = O(U) to see that it is O(U1+β) and thus dominated by the first summand. For the\\nsummand Σ[Uβ ] · 4kUβ we use Σ[Uβ ] = O(U2β) to see that it is O(U3β) = O(U1+2β), since β ≤ α ≤ 1,\\nso this summand is also dominated by the first one.\\nIt remains to analyze the exponent of t = Θ(U1+2β+γ). First plugging in the definitions of β and γ,\\nand then expanding 1 = α/α = α(ξ + σ − τ − 1), we obtain\\n1 + 2β + γ = α(σ + ξ − 1)− 1 = α(σ + ξ − 1)− α(ξ + σ − τ − 1) = ατ.\\nHence, we have t = Θ(U1+2β+γ) = Θ(Uατ ) = Θ(nτ ), completing the proof of the claim.\\nFrom the size bound |X| = Θ(Uα) and the easy structure of X and t, it follows that (X, t) can be\\ncomputed in time O(Uα). Together with Claims 5.14 and 5.12, this finishes the proof of Lemma 5.11.\\n6 Conclusion and Open Problems\\nIn this paper we designed improved algorithms and lower bounds for dense Subset Sum with respect to the\\nparameters n, t,mxX ,ΣX . When the input X is a set, we showed a dichotomy into parameter settings\\nwhere Subset Sum can be solved in near-linear time Õ(n) and settings where it cannot, under standard\\nassumptions from fine-grained complexity theory. We also generalized our algorithms to multi-sets. We\\nconclude with some open problems.\\nIn the set case, our lower bound characterizes all near-linear time settings, but it does not match the\\nknown upper bounds in the super-linear regime. It would be plausible that Subset Sum can be solved in\\ntime Õ(n+ min{t,mxXΣX/(nt)}), which would match our lower bound. So far, this running time can be\\nachieved for t = Õ(\\n√\\nmxXΣX/n) [11] or t\\x1d mxXΣX/n2 (Theorem 1.2).\\nHowever, this is a hard open problem, since a matching algorithm (or a higher lower bound) would also\\nanswer the open problem from [8] whether Subset Sum can be solved in time Õ(n+mxX). Indeed, bounding\\nΣX ≤ n ·mxX and min{t,mx2X/t} ≤ mxX we obtain time Õ(n+ min{t,mxXΣX/(nt)}) = Õ(n+ mxX).\\nWe generalized our algorithm to the multi-set case, at the cost of a factor µX in the feasibility bound.\\nGeneralizing our lower bounds and gaining a similar factor µX seems complicated. We therefore leave it\\nas an open problem to determine the near-linear time regime in the case of multi-sets.\\nGalil and Margalit’s algorithm can be phrased as a data structure: We can preprocess X in time\\nÕ(n+ mx2X/n2) so that given a target t\\x1d mxXΣX/n2 we can decide whether some subset of X sums to\\nK. Bringmann and P. Wellnitz 34\\nt in time O(1). Our algorithm from Theorem 1.2 can be phrased in the same data structure setting, with\\nan improved preprocessing time of Õ(n).\\nHowever, Galil and Margalit’s data structure can even reconstruct solutions, namely after preprocessingX\\nand given t they can compute a subset Y ⊆ X summing to t, if it exists, in time Õ(|Y |). We leave it as\\nan open problem to extend our algorithm to admit this type of solution reconstruction, as we focused on\\nthe decision problem throughout this paper.\\nReferences\\n1 A. Abboud, A. Backurs, K. Bringmann, and M. Künnemann. Fine-grained complexity of analyzing compressed\\ndata: Quantifying improvements over decompress-and-solve. In FOCS, pages 192–203. IEEE, 2017.\\n2 A. Abboud, K. Bringmann, D. Hermelin, and D. Shabtay. SETH-based lower bounds for subset sum and\\nbicriteria path. In SODA, pages 41–57. SIAM, 2019.\\n3 M. Ajtai. Generating hard instances of lattice problems (extended abstract). In STOC, pages 99–108. ACM,\\n1996.\\n4 N. Alon. Subset sums. Journal of Number Theory, 27(2):196–205, 1987.\\n5 N. Alon and G. Freiman. On sums of subsets of a set of integers. Combinatorica, 8(4):297–306, 1988.\\n6 A. Amir, T. M. Chan, M. Lewenstein, and N. Lewenstein. On hardness of jumbled indexing. In ICALP,\\nvolume 8572 of LNCS, pages 114–125. Springer, 2014.\\n7 K. Axiotis, A. Backurs, K. Bringmann, C. Jin, V. Nakos, C. Tzamos, and H. Wu. Fast and simple modular\\nsubset sum. In SOSA@SODA, 2020. To appear.\\n8 K. Axiotis, A. Backurs, C. Jin, C. Tzamos, and H. Wu. Fast modular subset sum using linear sketching. In\\nSODA, pages 58–69. SIAM, 2019.\\n9 N. Bansal, S. Garg, J. Nederlof, and N. Vyas. Faster space-efficient algorithms for subset sum and k-sum. In\\nSTOC, pages 198–209. ACM, 2017.\\n10 R. E. Bellman. Dynamic Programming. Princeton University Press, 1957.\\n11 K. Bringmann. A near-linear pseudopolynomial time algorithm for subset sum. In SODA, pages 1073–1084.\\nSIAM, 2017.\\n12 K. Bringmann and M. Künnemann. Multivariate fine-grained complexity of longest common subsequence.\\nIn SODA, pages 1216–1235. SIAM, 2018.\\n13 K. Bringmann and V. Nakos. Top-k-convolution and the quest for near-linear output-sensitive subset sum.\\nIn STOC, pages 982–995. ACM, 2020.\\n14 C. Calabro, R. Impagliazzo, and R. Paturi. The complexity of satisfiability of small depth circuits. In\\nIWPEC, volume 5917 of LNCS, pages 75–85, 2009.\\n15 M. Chaimovich. New algorithm for dense subset-sum problem. Astérisque, 258:363–373, 1999.\\n16 M. Chaimovich, G. Freiman, and Z. Galil. Solving dense subset-sum problems by using analytical number\\ntheory. J. Complex., 5(3):271–282, 1989.\\n17 T. M. Chan and M. Lewenstein. Clustered integer 3SUM via additive combinatorics. In STOC, pages 31–40.\\nACM, 2015.\\n18 R. Crandall and C. B. Pomerance. Prime Numbers: A Computational Perspective, volume 182. Springer\\nScience & Business Media, 2006.\\n19 M. Cygan, H. Dell, D. Lokshtanov, D. Marx, J. Nederlof, Y. Okamoto, R. Paturi, S. Saurabh, and\\nM. Wahlström. On problems as hard as CNF-SAT. ACM Trans. Algorithms, 12(3):41:1–41:24, 2016.\\n20 D. Eppstein. Minimum range balanced cuts via dynamic subset sums. J. Algorithms, 23(2):375–385, 1997.\\n21 P. Erdős and G. Freiman. On two additive problems. Journal of Number Theory, 34(1):1–12, 1990.\\n22 G. A. Freiman. On extremal additive problems of paul erdős. Ars Combinatoria, 26:93–114, 1988.\\n23 G. A. Freiman. New analytical results in subset-sum problem. Discrete Mathematics, 114(1-3):205–217,\\n1993.\\n24 A. Gajentaan and M. H. Overmars. On a class of O(n2) problems in computational geometry. Comput.\\nGeom., 5:165–185, 1995.\\n35 On Near-Linear-Time Algorithms for Dense Subset Sum\\n25 Z. Galil and O. Margalit. An almost linear-time algorithm for the dense subset-sum problem. SIAM J.\\nComput., 20(6):1157–1189, 1991.\\n26 Z. Galil and O. Margalit. An almost linear-time algorithm for the dense subset-sum problem. In ICALP,\\nvolume 510 of LNCS, pages 719–727, 1991.\\n27 R. Impagliazzo and R. Paturi. On the Complexity of k-SAT. J. Comput. Syst. Sci., 62(2):367–375, 2001.\\n28 C. Jin and H. Wu. A simple near-linear pseudopolynomial time randomized algorithm for subset sum. In\\nSOSA@SODA, volume 69 of OASICS, pages 17:1–17:6, 2019.\\n29 H. Kellerer, U. Pferschy, and D. Pisinger. Knapsack Problems. Springer, 2004.\\n30 B. Klinz and G. J. Woeginger. A note on the bottleneck graph partition problem. Networks, 33(3):189–191,\\n1999.\\n31 K. Koiliaris and C. Xu. Faster pseudopolynomial time algorithms for subset sum. ACM Trans. Algorithms,\\n15(3):40:1–40:20, 2019.\\n32 H. W. Lenstra and C. Pomerance. A Rigorous Time Bound for Factoring Integers. J. AMS, 5(3):483–516,\\n1992.\\n33 V. F. Lev. Optimal representations by sumsets and subset sums. Journal of Number Theory, 62(1):127–143,\\n1997.\\n34 V. F. Lev. Blocks and Progressions in Subset Sum Sets. Acta Arithmetica Warszawa, 106(2):123–142, 2003.\\n35 E. Lipkin. On representation of r-th powers by subset sums. Acta Arithmetica, 52(4):353–365, 1989.\\n36 D. Lokshtanov and J. Nederlof. Saving space by algebraization. In STOC, pages 321–330. ACM, 2010.\\n37 M. Mucha, K. Wegrzycki, and M. Wlodarczyk. A subquadratic approximation scheme for partition. In\\nSODA, pages 70–88. SIAM, 2019.\\n38 D. Pisinger. Linear time algorithms for knapsack problems with bounded weights. J. Algorithms, 33(1):1–14,\\n1999.\\n39 D. Pisinger. Dynamic programming on the Word RAM. Algorithmica, 35(2):128–145, 2003.\\n40 J. M. Pollard. Theorems on Factorization and Primality Testing. Mathematical Proceedings of the Cambridge\\nPhilosophical Society, 76(3):521–528, 1974.\\n41 C. Pomerance. A Tale of Two Sieves. Notices of the AMS, 85:175, 2008.\\n42 A. Sárközy. Finite Addition Theorems, I. Journal of Number Theory, 32(1):114–130, 1989.\\n43 A. Sárközy. Finite Addition Theorems, II. Journal of Number Theory, 48(2):197–218, 1994.\\n44 O. Serang. The probabilistic convolution tree: efficient exact bayesian inference for faster LC-MS/MS protein\\ninference. PLOS ONE, 9(3), 2014.\\n45 V. Strassen. Einige Resultate über Berechnungskomplexität. Jahresbericht der Deutschen Mathematiker-\\nVereinigung, 78:1–8, 1976.\\n46 E. Szemerédi and V. Vu. Long arithmetic progressions in sumsets: thresholds and bounds. Journal AMS,\\n19(1):119–169, 2006.\\n47 T. Tao and V. H. Vu. Additive Combinatorics, volume 105. Cambridge University Press, 2006.\\n48 V. Vassilevska Williams. On some fine-grained questions in algorithms and complexity. In Proc. ICM,\\nvolume 3, pages 3431–3472. World Scientific, 2018.\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd20'), 'authors': 'Cohen-Addad, Vincent, Klein, Philip N., Mathieu, Claire', 'year': '2016', 'title': 'Local search yields approximation schemes for k-means and k-median in\\n  Euclidean and minor-free metrics', 'full_text': 'Local search yields approximation schemes for k-means and\\nk-median in Euclidean and minor-free metrics\\nVincent Cohen-Addad ˚1, Philip N. Klein :2, and Claire Mathieu ;3\\n1De´partement d’informatique, E´cole normale supe´rieure, France\\n2Brown University, United States\\n3CNRS, De´partement d’informatique, E´cole normale supe´rieure,France\\nAbstract\\nWe give the first polynomial-time approximation schemes (PTASs) for the following prob-\\nlems: (1) uniform facility location in edge-weighted planar graphs; (2) k-median and k-means\\nin edge-weighted planar graphs; (3) k-means in Euclidean space of bounded dimension. Our\\nfirst and second results extend to minor-closed families of graphs. All our results extend to\\ncost functions that are the p-th power of the shortest-path distance. The algorithm is local\\nsearch where the local neighborhood of a solution S consists of all solutions obtained from S by\\nremoving and adding 1{εOp1q centers.\\n˚Research funded by the French ANR Blanc project ANR-12-BS02-005 (RDAM)\\n:Research funded by NSF Grant CCF-10-12254 with additional support from the Radcliffe Institute of Advanced\\nStudy, Harvard University\\n;Research funded by the French ANR Blanc project ANR-12-BS02-005 (RDAM)\\nar\\nX\\niv\\n:1\\n60\\n3.\\n09\\n53\\n5v\\n2 \\n [c\\ns.D\\nS]\\n  7\\n A\\npr\\n 20\\n16\\n1 Introduction\\nIn this paper, we address three fundamental problems, facility location, k-median and k-means\\nclustering, in two settings, graphs and Euclidean spaces. The problem of approximating k-means\\nclustering in low-dimensional Euclidean space has been studied since at least 1994 [34]; since then,\\nmany researchers have given approximation schemes that are polynomial for fixed k but exponential\\nin k. Very recently, building on [22], a bicriteria polynomial-time approximation scheme has been\\ngiven [15] for k-means: it finds p1 ` \\x0fqk centers whose cost is at most 1 ` \\x0f times the cost of an\\noptimal k-means solution. As the authors of that paper point out, it remained an open question\\nwhether there is a true polynomial-time approximation scheme for k-means in the plane (where k is\\nconsidered part of the input); the best polynomial-time approximation bound known was 9` \\x0f. In\\nthis paper, we resolve this open question by giving the first polynomial-time approximation scheme\\nfor arbitrary (i.e. nonconstant) k in low-dimensional Euclidean space.\\nOur analysis of the k-means approximation scheme shows that it can also be applied to graphs\\nbelonging to a fixed nontrivial minor-closed family.1\\nFor example, for any fixed integer g, graphs embeddable on a surface of genus g form such a\\nfamily. In particular, planar graphs forms such a family. Thus we also obtain the first polynomial-\\ntime approximation scheme for k-means in planar graphs.\\nThe problems of (uncapacitated) metric facility location and k-median in graphs has similarly\\nbeen studied for many years. The first polynomial-time approximation algorithm, with a loga-\\nrithmic performance guarantee, was given by Hochbaum in 1982 [33]. The first polynomial-time\\napproximation algorithm to achieve a constant approximation ratio was given by Shmoys et al. [46]\\nin 1997. It was later improved by Jain and Vazirani [36] and by Arya et al. [7]. The current best\\napproximation algorithm for metric (uncapacitated) facility location, due to Li, has approximation\\nratio 1.488 [43]. Guha and Khuller [27] proved that there exists no polynomial-time approxi-\\nmation algorithm with approximation ratio of 1.463 for metric facility location problem unless\\nNP Ď DTIMErnOplog lognqs. The current best approximation ratio for the k-median problem is\\n1`?3 by Li and Svensson [44].\\nIn order to obtain a substantially better approximation ratio, therefore, one must restrict atten-\\ntion to special metrics. Because facility location problems often arise on the surface of the earth, it\\nis natural to consider the metrics induced by planar graphs. Researchers have been trying to find a\\npolynomial-time approximation scheme for the planar restriction for many years. An unpublished\\nmanuscript [2] by Ageev dating back at least to 2001 addressed the planar case via a straightforward\\napplication of Baker’s method [12], giving an algorithm whose performance on an instance depends\\non how much of the cost of the optimal solution is facility-opening cost. Despite the title of the\\nmanuscript, the algorithm is not an approximation scheme for instances with arbitrary weights.\\nSince then there have been no results on the problem despite efforts by several researchers in the\\narea.\\nIn this paper, we give the first polynomial-time approximation scheme for (uncapacitated uni-\\nform) facility location and k-median where the metric is that induced by a planar graph or, more\\ngenerally a graph belonging to a fixed nontrivial minor-closed family.\\n1Contracting an edge of a graph means identifying its endpoints and then removing it. A graph H is a minor\\nof graph G if H can be obtained from H by edge deletions and edge contractions. The family of planar graphs, for\\nexample, is closed under taking minors, as is the family of graphs embeddable on a surface of genus g, for any fixed\\ninteger g ą 0. We say a minor-closed family is nontrivial if it omits at least one graph.\\n1\\n1.1 Results\\nWe describe a simple and natural, and previously studied local-search algorithm for clustering\\nproblems, parameterized by the desired cluster size k, the objective function costp¨q, and a parameter\\ns governing the local-search neighborhood.\\nAlgorithm 1 Local Search for finding k clusters\\n1: Input: A metric space and associated cost function costp¨q, an n-element set C of points, error\\nparameter ε ą 0, positive integer parameters k and s\\n2: S Ð Arbitrary size-k set of points\\n3: while D S1 s.t. |S1| ď k and |S ´ S1| ` |S1 ´ S| ď s and cost(S1) ď p1´ ε{nqcostpSq\\n4: do\\n5: S Ð S1\\n6: end while\\n7: Output: S\\nWe consider two kinds of metric spaces. For any fixed positive integer d, we consider Rd\\nequipped with Euclidean distance. For any undirected edge-weighted graph G, we consider the\\nmetric completion of G, i.e. the metric space whose points are the vertices of G and where the\\ndistance between u and v is defined to be the length of the shortest u-to-v path in G with respect\\nto the given edge-weights.\\nTheorem 1.1 (Euclidean Spaces). For any fixed integers p, d ą 0, there is a constant c such that,\\nfor any 0 ă ε ă 1{2, applying Algorithm 1 to the d-dimensional Euclidean space with cost function\\ncostpSq “\\nÿ\\ncPC\\npmin\\nuPS distpc, uqq\\np\\nand s “ 1{εc yields a solution S whose cost is at most 1` ε times the minimum.\\nWhen p “ 2, the objective function is the k-means objective function. When p “ 1, the objective\\nfunction is that of k-median.\\nWhen the metric space is Rd, it is not trivial to implement an iteration of Algorithm 1. However,\\nas observed in [15] (see [34]), there is a method using an arrangement of algebraic surfaces to execute\\nan iteration in nOpdsq time. The number of iterations is Opn{εq (see [7, 22]). The running time is\\ntherefore polynomial for fixed p, d, ε. We obtain the following.\\nCorollary 1. For any integer d ą 0, there is a polynomial-time approximation scheme for the\\nk-means problem in d-dimensional Euclidean spaces.\\nAlgorithm 1 can also be applied to the metric completion of a graph.\\nTheorem 1.2 (Graphs). Let K be a nontrivial minor-closed family of edge-weighted graphs. For\\nany fixed integer p ą 0, there is a constant c with the following property. For any 0 ă ε ă 1{2, for\\nany G P K, Algorithm 1 applied to the metric completion of G with cost function\\ncostpSq “\\nÿ\\ncPC\\npmin\\nuPS distpc, uqq\\np\\nand with s “ 1{εc yields a solution S whose cost is at most 1` ε times the minimum.\\nIt is straightforward to implement Algorithm 1 applied to the metric completion of a graph. As\\nbefore, the number of iterations is Opn{εq where n is the number of clients. We therefore obtain:\\n2\\nCorollary 2. There is a polynomial-time approximation scheme for k-means and for k-median in\\nplanar graphs and in bounded-genus graphs.\\nMore generally, for any nontrivial minor-closed family of edge-weighted graphs, there is a\\npolynomial-time approximation schemes for k-means and for k-median for graphs in that family.\\nThe local-search algorithm is easily adapted to the case where we do not specify the number of\\nclusters but instead specify a per-cluster cost. This case includes a variant of the facility location\\nproblem.\\nDefinition 1.1 (Uncapacitated Uniform Facility Location). The Uncapacitated Uniform Facility\\nLocation problem is as follows: given a finite metric space, a subset C of points, and a facility\\nopening cost f , find a subset S of points that minimizes costpSq “ f |S| `řcPC minuPS distpc, uq.\\nTo address this problem, we use a simple modification of the local-search algorithm given earlier.\\nAlgorithm 2 Local Search for Uniform Facility Location\\n1: Input: A metric space and associated cost function costp¨q, an n-element set C of points, error\\nparameter ε ą 0, facility opening cost f ą 0, positive integer parameter s\\n2: S Ð Arbitrary subset of F .\\n3: while D S1 s.t. |S ´ S1| ` |S1 ´ S| ď s and cost(S1) ď p1´ ε{nq cost(S)\\n4: do\\n5: S Ð S1\\n6: end while\\n7: Output: S\\nTheorem 1.3. Fix a nontrivial minor-closed family K of graphs. There is a constant c such that,\\nwhen Algorithm 2 is applied to the metric completion of a graph in K with\\ncostpSq “ |S|f `\\nÿ\\ncPC\\npmin\\nuPS distpc, uqq\\np\\nand s “ 1{εc, the output has cost at most 1` ε times the minimum.\\nIn fact, for p “ 1, setting s “ c{ε2 suffices to achieve a 1 ` ε approximation. The theorem\\nimplies the following:\\nCorollary 3. Fix a nontrivial minor-closed family K of edge-weighted graphs. There is a polynomial-\\ntime approximation scheme for uniform uncapacitated facility location in graphs of K.\\n1.2 Related work\\nIn arbitrary metric spaces, it is NP-hard to approximate the k-median and k-means problems within\\na factor of 1` 2{e and 1` 3{e respectively, see Guha and Khuller [27] and Jain et al. [35]. In the\\ncase of Euclidean space, Guruswami and Indyk [29] showed that there is no PTAS for k-median if\\nboth k and d are part of the input. More recently, Awasthi et al. [9] showed APX-Hardness for\\nk-means if both k and d are part of the input.\\nIn Euclidean spaces, p1` εq-approximation algorithms for k-median have been proposed when\\nk or d is fixed. For example, when k is fixed, there exists different PTAS (See [11, 41, 42, 24, 31]\\nand [23] for the best known so far). When d is fixed, Arora et al. gave the first PTAS [4] for the\\nk-median problem. This result was subsequently improved to an efficient PTAS by Kolliopoulos et\\nal. [38] and Har-Peled et al. [30, 31].\\n3\\nFor the k-means problem, Kanungo et al. [37] showed that local search achieves a 9 ` ε-\\napproximation in general metrics and this remains the best known approximation guarantee so\\nfar even for fixed d. There are also a variety of results for k-means and k-median when the in-\\nput has some stability conditions (see for example [10, 8, 14, 13, 18, 40, 45]) or in the context of\\nsmoothed analysis (see for example [6, 5]).\\nLocal Search for metric k-median was first analyzed by Korupolu et al [39]. They gave a\\nbicriteria approximation using k ¨ p1 ` εq centers an achieving a cost of at most 3 ` 5{ε times the\\ncost of the optimum k-clustering. This was later improved to k ¨ p1` εq centers an achieving a cost\\nof at most 2 ` 2{ε times the cost of the optimum k-clustering by Charikar an Guha [21]. Arya et\\nal. [7] gave the first analysis showing that Local Search with a neighborhood of size 1{ε gives a 3`2ε\\napproximation to k-median. Moreover, they show that this bound is tight. As mentioned earlier,\\nKanungo et al. [37] showed that local search is a 9`ε-approximation for k-means in general metrics.\\nLocal search is a very popular algorithm for clustering and has been widely used : see [19] in the\\ncontext of parallel algorithms, [28] in the streaming model and [16] for distributed computing.\\nSee [1] for a general introduction to theory and practice of local search.\\nNote added: After we had written up our results and while we were editing the submission, we\\nnoticed a recent ArXiv paper [26] that has similar results for doubling metrics.\\n2 Techniques\\n2.1 r-divisions in minors\\nOne key ingredient in our analyses is the existence of a certain kind of decomposition of the input\\ncalled a weak r-division. The concept (in a stronger form) is due to Frederickson [25] in the context\\nof planar graphs. It is straightforward to extend it to any family of graphs with balanced separators\\nof size sublinear-polynomial. We also define a weak r-division for points in a Euclidean space, and\\nshow that such a decomposition always exists. These definitions and results are in Sections 3.1\\nand 3.2. Note that r-divisions play no role in our algorithm; only the analysis uses them.\\nChan and Har-Peled [20] showed that local search can be used to obtain a PTAS for (un-\\nweighted) maximum independent pseudo-disks in the plane, which implies the analogous result for\\nplanar graphs. More generally, Har-Peled and Quanrud [32] show that local search can be used\\nto obtain PTASs for several problems including independent set, set cover, and dominating set, in\\ngraphs with polynomial expansion. These graphs have small separators and therefore r-divisions.\\nHowever, our analysis of local search for clustering requires not only that the input graph have an\\nr-division but that a minor of the input graph have an r-division. This is not true of graphs of\\npolynomial expansion. Indeed, we show in Section 2.3 that there are low-density graphs in low-\\ndimensional space (which are therefore polynomial-expansion graphs) for which our local-search\\nalgorithm produces a solution that is worse than the optimum by at least a constant factor.\\nThus one of our technical contributions is showing how to take advantage of a property possessed\\nby nontrivial minor-closed graph families that is not possessed by polynomial-expansion graph\\nfamilies.\\n2.2 Isolation\\nIn order to obtain our approximation schemes for k-means and k-median clustering, we need another\\ntechnique. As mentioned earlier, a bicriteria approximation scheme for k-means was already known;\\nthe solution it returns has more than k centers. It seems hard to avoid an increase in the number\\n4\\nof centers in comparing a locally optimal solution to a globally optimal solution. It would help if\\nwe could show that the globally optimal solution could be modified so as to reduce the number of\\ncenters below k while only slightly increasing the cost; we could then compare the local solution to\\nthis modified global solution, and the increase in the number of centers would leave the number no\\nmore than k.\\nUnfortunately, we cannot unconditionally reduce the number of centers. However, consider a\\nglobally optimal solution G and a locally optimal solution L. A facility f in G might correspond to\\na facility ` in L in the sense that they serve almost exactly the same set of clients. In this case, we\\nsay the pair pf, `q is 1-1 isolated (the formal definition is below). Such centers do not contribute\\nmuch to the increase in cost in going from global solution to local solution, so let’s ignore them.\\nAmong the remaining centers of G, there are a substantial number that can be removed without\\nthe cost increasing much. The analysis of the local solution then proceeds as discussed above.\\nWe now give the formal definition of 1-1 isolated.\\nDefinition 2.1. Let ε ă 1{2 be a positive constant and L and G be two solutions for the k-clustering\\nproblem with parameter p. Given a facility f0 P G and a facility ` P L, we say that the pair pf, `q\\nis 1-1-isolated if most of the clients served by ` in L are served by f in G, and most of the clients\\nserved by f in G are served by ` in L : in other words,\\n|VLp`q X VGpfq| ě\\n\" p1´ εq|VLp`q|\\np1´ εq|VGpfq|\\nTheorem 2.2. Let ε ă 1{2 be a positive constant and L and G be two solutions for the k-clustering\\nproblem with exponent p. Let k¯ denote the number of facilities f of G that are not in a 1-1 isolated\\nregion. There exists a set S0 of facilities of G of size at least ε3k¯{6 that can be removed from G at\\nlow cost: costpG ´ S0q ď p1` 23p`1εqcostpGq ` 23p`1εcostpLq.\\nNote that the following theorem does not assume that L is a local optimum and G is an optimal\\nsolution. Thus we believe that this theorem can be of broader interest. We now define the concept\\nof isolated regions; 1-1-isolated regions correspond to the special case of isolated regions when L0\\nconsists of a single facility.\\nDefinition 2.1 (Isolated Region). Given a facility f0 P G and a set of facilities L0 Ď L, we say\\nthat the pair pf0,L0q is an isolated region if\\n• For each facility f 1 P L0, most of the clients served by f 1 in L are served by f0 in G: in other\\nwords, |VLpf 1q X VGpf0q| ě p1´ εq|VLpf 1q|, and\\n• Most of the clients served by f0 in G are served by facilities of L0 in L: in other words,\\n|VLpL0q X VGpf0q| ě p1´ εq|VGpf0q|;\\nFinally, if pf0,L0q is an isolated region, we say that f0 and the elements of L0 are isolated.\\n2.3 Tightness of the results\\nProposition 2.3. For any w, t, there exists an infinite family of graphs excluding Kw as a t-\\nshallow minor such that for any constant ε, local search with neighborhoods of size 1{ε might return\\na solution of cost at least 3OPT.\\nSee Figure 1 and [7] for a complete proof that local search performs badly on the instance\\ndepicted in the figure.\\n5\\nlength 0\\nlength 6\\nlength 1 ...\\nOPT\\nLocal OPT for neighborhoods\\nof size 3\\n...\\n(k-3)/4\\nk-(k-3)/4\\n}\\n}\\nFigure 1: This instance contains the complete bipartite graph K3,3 as a 1-shallow minor (and so is\\nnot planar) but no non-trivial 0-shallow minor. Clients are denoted by circle and candidate centers\\nby squares. There exists a local (for neighborhoods of size 3) optimum whose cost is at least 3OPT.\\nThis example can be generalized to handle locality for neighborhoods of size 1{ε for any constant\\nε ą 0. This is based on an example of [7] and can be extended to form a i-shallow minor graph for\\nany i “ opnq.\\nProposition 2.4. For any ρ, there exists an infinite family of graphs that are low-density graphs\\nsuch that for any constant ε, local search with neighborhoods of size 1{ε might return a solution of\\ncost at least 3OPT.\\nThe proposition follows from encoding the graph of Figure 1 as an low-density graph.\\nWe additionally remark that Awasthi et al. [9] show that the k-means problem is APX-Hard for\\nany non-constant dimension d. Moreover, Kanungo et al. [37] give an example where local search\\nreturns a solution of cost at least 9OPT.\\n3 Preliminaries\\nWe will use the following technical lemma in order to give a general proof that encompasses both\\nthe cases of k-median and k-means. Throughout the paper we assume p constant and define ε1 to\\nbe a positive constant.\\nLemma 3.1. Let p ě 0 and 0 ă ε1 ă 1{2. For any a, b, c P AY F , we have\\ndistpa, bqp ď\\n#\\np1` ε1qppdistpa, cqp ` distpc, bqp{εp1q\\n2ppdistpa, cqp ` distpc, bqpq.\\nProof. By the triangular inequality,\\ndistpa, bqp “ pdistpa, cq ` distpc, bqqp ď\\n#\\np1` ε1qpdistpa, cqp If distpc, bq ď ε1distpa, cq.\\np1` ε1qpdistpc, bqp{εp1 Otherwise.\\n6\\nMoreover, by the binomial theorem, distpa, bqp “ pdistpa, cq`distpc, bqqp ď 2ppcostpa, cqp`costpc, bqpq.\\n3.1 Graph r-division and other definitions\\nFor a graph G, we use V pGq and EpGq to denote the set of vertices of G and the set of edges of\\nG, respectively. For a subgraph H of G, the vertex boundary of H in G, denoted BGpHq, is the set\\nof vertices v such that v is in H but has an incident edge that is not in H. (We might write BpHq\\nif G is unambiguous.) A vertex in the vertex boundary of H is called a boundary vertex of H. A\\nvertex of H that is not a boundary vertex of H is called an internal vertex. We denote the set of\\ninternal vertices of H as IpHq.\\nDefinition 3.2. Let c1 and c2 be constants (depending on G). For a number r, a weak r-division\\nof a graph G (with respect to c1, c2) is a collection R of subgraphs of G, called regions, with the\\nfollowing properties.\\n1. Each edge of G is in exactly one region.\\n2. The number of regions is at most c1|V pGq|{r.\\n3. Each region contains at most r vertices.\\n4. The number of boundary vertices, summed over all regions, is at most c2|V pGq|{r1{2.\\nA family of graphs F is said to be closed under taking minor (minor-closed) if for any graph\\nG P F , for any minor H of G, we have H P F .\\nTheorem 3.3 (Frederickson [25] + Alon, Seymour, and Thomas [3]). Let K be a nontrivial minor-\\nclosed family of graphs. There exist c1, c2 such that every graph in K has a weak r-division with\\nrespect to c1, c2.\\nProof. Alon, Seymour, and Thomas [3] proved a separator theorem for the family of graphs exclud-\\ning a fixed graph as a minor. Any nontrivial minor-closed family excludes some graph as a minor\\n(else it is trivial). Frederickson [25] gave a construction for a stronger kind of r-division of a planar\\ngraph. The construction uses nothing of planar graphs except that they have such separators.\\nLet G be an undirected graph with edge-lengths. Fix an arbitrary priority ordering of the vertex\\nset V pGq. For every subset S of V pGq, we define the Voronoi partition with respect to S. For each\\nvertex v P S, the Voronoi cell with center v, denoted VSpvq, is the set of vertices that are closer to\\nv than to any other vertex in S, breaking ties in favor of the highest-priority vertex of S.\\nFact 1. For any S, for any vertex v P S, the induced subgraph GrVSpvqs is a connected subgraph\\nof G.\\nProof. Let u P VSpvq, and let p denote a v-to-u shortest path. Let w be a vertex on P . Assume\\nfor a contradiction that, for some vertex v1 P S, either the v1-to-w shortest path p1 is shorter than\\nthe shortest v-to-w path, or it is no longer and v1 has higher priority than v. Replacing the v-to-w\\nsubpath of p with p1 yields a v-to-u path that either is shorter than p or is no longer than p and\\noriginates at a higher-priority vertex than v.\\nIt follows that, for any vertex v of G, contracting the edges of the subgraph GrVSpvqs yields a\\nsingle vertex.\\n7\\nDefinition 3.4. We define GVorpSq as the graph obtained from G by contracting every edge of\\nGrVSpvqs for every vertex v P S. For each vertex v P S, we denote by vˆ the vertex of GVorpSq\\nresulting from contracting every edge of GrVSpvqs.\\nIf G belongs to a minor-closed family K then so does GVorpSq.\\n3.2 Euclidean space r-division\\nWe define analogous notions for the case of Euclidean spaces of fixed dimension d. Consider a set of\\npoints C in Rd. For a set Z of points in Rd and a bipartition C1YC2 of C, we say that Z separates\\nC1 and C2 if, in the Voronoi diagram of C Y Z, the boundaries of cells of points in C1 are disjoint\\nfrom the boundaries of cells of points in C2.\\nDefinition 3.5. Let c1 and c2 be constants. Let C be a set of points in Rd. For an integer r ą 1,\\na weak r-division of C (with respect to c1, c2) is a set of boundary points Z Ă Rd together with a\\ncollection of subsets R of C Y Z called regions, with the following properties.\\n1. R´ Z is a partition of C.\\n2. The number of regions is at most c1|C|{r.\\n3. Each region contains at most r points of C Y Z.\\n4.\\nř\\nRPR |RX Z| ď c2|C|{r1{d.\\nMoreover, for any region Ri, Ri X Z is a Voronoi separator for Ri ´ Z and pC Y Zq ´Ri.\\nThe following theorem is from [17, Theorem 3.7].\\nTheorem 3.6. [17, Theorem 3.7] Let P be a set of n points in Rd. One can compute, in expected\\nlinear time, a sphere S, and a set Z Ď S, such that\\n• |Z| ď cn1´1{d,\\n• There are most σn points of P in the sphere S and at most σn points of P not in S, and\\n• Z is a Voronoi separator of the points of P inside S from the points of P outside S.\\nHere c and σ ă 1 are constants that depends only on the dimension d.\\nFrom that theorem we can easily derive the following (see Section 8.1 for the proof):\\nTheorem 3.7. Let r be a positive integer and d be fixed. Then there exist c1, c2 such that every\\nset of points C Ă Rd has a weak r-division with respect to c1, c2.\\n3.3 Properties of the r-Divisions\\nWe present the properties of the r-divisions that we will be using for the analysis of the solution\\noutput by the local search algorithm.\\nLemma 3.8. Let G “ pV,Eq be a graph excluding a fixed minor H and F Ď V . Let Hi be a region\\nof the r-division of GVorpFq. Suppose c and v are vertices of G such that one of the vertices in tcˆ, vˆu\\nis a vertex of Hi and the other is not an internal vertex of Hi. Then there exists a vertex x P F\\nsuch that xˆ is a boundary vertex of the region Hi and distpc, xq ď distpc, vq.\\n8\\nProof. Let p be a shortest c-to-v path in G. By the conditions on cˆ and vˆ, there is some vertex\\nw of p such that wˆ is a boundary vertex of Hi. Let x be the center of the Voronoi cell whose\\ncontraction yields wˆ. By definition of Voronoi cell, distpw, xq ď distpw, vq. Therefore replacing the\\nw-to-v subpath of p with the shortest w-to-x path yields a path no longer than p.\\nWe obtain the analogous lemma for the Euclidean case, whose proof follows directly from the\\ndefinition of r-division (i.e.: the fact that Z is a Voronoi separator).\\nLemma 3.9. Let C be a set of points in Rd and Z be an r-division of C. For any two different\\nregions R1, R2, for any points c P R1, v P R2 there exists a boundary vertex x P Z X R1 such that\\ndistpc, xq ď distpc, vq.\\n4 Facility Location in minor-closed graphs: Proof of Theorem 1.3\\nAs a warm-up, we analyze Local Search for Uniform Facility Location (Algorithm 2) applied to\\nthe metric completion of an edge-weighted graph G belonging to a nontrivial minor-closed family\\nK. The proof of the k-median and k-means results (for both Euclidean and minor-closed metrics),\\ninvolve the use of Theorem 2.2 and a more complex analysis.\\nThroughout this section we consider a solution L output by Algorithm 2 (the “local” solution)\\nand a globally optimal solution G of value OPT. Let F “ L Y G. Let r “ 1{ε2. Consider the\\ngraph GVorpFq defined in Definition 3.4, and recall that each vertex of G maps to a vertex vˆ in the\\ncontracted graph GVorpFq.\\nSince G belongs to K and GVorpFq is obtained from G by contraction, it too belongs to K and\\nhence it has an r-division. Let H1, . . . Hκ be the regions of this r-division. For i “ 1, . . . , κ, define\\nVi and Bi as follows:\\nVi “ tv P F : vˆ is a vertex of Hiu\\nBi “ tv P F : vˆ is a boundary vertex of Hiu\\nThat is, Vi is the set of vertices in the union of the local solution and the global solution that map\\nvia contraction to vertices of the region Hi, and Bi is the set of vertices in the union that map to\\nboundary vertices of Hi.\\nLet G1 “ G YŤκi“1Bi.\\nFix a region Hi of the r-division of GVorpFq. We define Li “ L X Vi and G1i “ G1 X Vi. We\\nconsider the mixed solution Mi defined as follows:\\nMi “ pL´ Liq Y G1i.\\nLemma 4.1. |Mi ´ L| ` |L´Mi| ď 1{ε2.\\nProof. To obtain Mi from L, one can remove the vertices in L X Vi that are not in G1, and add\\nthe vertices in G1 X Vi that are not in L. Thus the size of the symmetric difference is at most\\n|pL Y G1q X Vi|. Since the vertices of L Y G1 are centers of Voronoi cells, these vertices all map to\\ndifferent vertices in the contracted graph GVorpFq. Therefore |pLY G1q X Vi| is at most the number\\nof vertices in region Hi, which is at most r “ 1{ε2.\\nLemma 4.2. Let c be a vertex of G and Hi a region. Then:\\nmic ´ lc ď\\n#\\ngc ´ `c if cˆ is an internal vertex of Hi\\n0 otherwise.\\n9\\nFigure 2: The diagram shows a region of the weak r-division. The blobs represent vertices of the\\nregion. Each blob is obtained by coalescing a set of vertices of the input graph. These vertices are\\nindicated by circles. The unfilled circled represent the centers of the Voronoi cells.\\nProof. First suppose cˆ is an internal vertex of Hi, and let v be the facility in Mi closest to c. If v\\nis in Vi then it is in Gi, so mic “ g1i. Suppose v is not in Vi. Then by Lemma 3.8 there is a vertex\\nx P F such that xˆ is a boundary vertex of Hi and distpc, xq ď distpc, vq. As before, x is in G1i so\\nmic ď g1c. Since g1c ď gc, this proves the claimed upper bound.\\nNow, suppose cˆ is not an internal vertex of Hi and let v be the facility in L closest to c. If v is\\nnot in Vi then it is in the mixed solution Mi, so mic “ `c. Suppose v is in Vi. Then by Lemma 3.8\\nthere is a vertex x P F such that xˆ is a boundary vertex of Hi and distpc, xq ď distpc, vq. Since x is\\nin F and xˆ is a boundary vertex of Hi, we know x is in G1 X Vi, which is G1i. Therefore x is in Mi.\\nSince distpc, xq ď distpc, vq, we obtain mic ď `c, which proves the claimed upper bound.\\nLemma 4.3.\\nκÿ\\ni“1\\n|G1i| ď |G| ` c2εp|G| ` |L|q\\nProof. Let v be a vertex of G1. For i “ 1, . . . , κ, if vˆ is an internal vertex of the region Hi then\\nv contributes only one towards the left-hand side. If vˆ is a boundary vertex of Hi then v P Bi.\\nTherefore\\nκÿ\\ni“1\\n|G1 X Vi| ď |G| `\\nκÿ\\ni“1\\n|Bi|.\\nTo finish the proof, we bound the sum in the right-hand side. Each vertex in F is the center of\\none Voronoi cell, so GVorpFq has |F | vertices. For each region Hi, there is one vertex in Bi that\\ncorresponds to each boundary vertex of Hi, so\\nřκ\\ni“1 |Bi| is the sum over all regions of the number of\\nboundary vertices of that region, which, by Property 4 of r-divisions, is at most c2|F |{r1{2, which,\\nby choice of r, is at most c2ε|F |, which in turn is at most c2εp|G| ` |L|q.\\nProof. (Proof of Theorem 1.3) Lemma 4.1 and the stopping condition of Algorithm 2 imply the\\nfollowing:\\n´ 1\\nn\\ncostpLq ď costpMiq ´ costpLq. (1)\\n10\\nWe now decompose the right-hand side. For a client c, we denote by `c, g\\n1\\nc and m\\ni\\nc the distance\\nfrom the client c to the closest facilities in L, G1 and Mi respectively. This gives\\ncostpMiq ´ costpLq “ p|G1i| ´ |Li|q ¨ f `\\nÿ\\nc\\npmic ´ `cq. (2)\\nUsing Lemma 4.2 and summing over c shows thatÿ\\nc\\npmic ´ `cq ď\\nÿ\\nc:cˆPIpHiq\\npgc ´ `cq. (3)\\nCombining Inequalities (1), (2) and (3), we obtain\\n´ 1\\nn\\ncostpLq ď p|G1i| ´ |Li|q ¨ f `\\nÿ\\nc:cˆPIpHiq\\npg1c ´ `cq (4)\\nWe next sum this inequality over all κ regions of the weak r-division and use Lemma 4.3.\\n´κ\\nn\\ncostpLq ď\\n˜\\nκÿ\\ni“1\\n|G1i| ´\\nκÿ\\ni“1\\n|Li|\\n¸\\n¨ f `\\nκÿ\\ni“1\\nÿ\\nc:cˆPIpHiq\\npgc ´ `cq\\nď p|G| ` pc2εp|G| ` |L|q ´ |L|q ¨ f `\\nÿ\\nc\\npgc ´ `cq\\n“ pp1` c2εq|G| ´ p1´ c2εq|L|q ¨ f `\\nÿ\\nc\\npgc ´ `cq\\nď p1` c2εqcostpGq ´ p1´ c2εqcostpLq\\nSince κ ď c1|F |{r ď c1ε2n, we obtain\\n´c1ε2costpLq ď p1` c2εqcostpGq ´ p1´ c2εqcostpLq\\nso\\ncostpLq ď p1´ c2ε´ c1ε2q´1p1` c2εqcostpGq\\nThis completes the proof of Theorem 1.3.\\n5 Clusters in minor-closed graphs: Proof of Theorem 1.2\\nWe prove Theorem 1.2. The proof is similar for graphs and for points lying in Rd. It builds on the\\nnotions of isolation and 1-1 isolation introduced in Section 2.2.\\nWe consider a solution L output by Algorithm 1 and an optimal solution G. Let F¯ be the set\\nof facilities of L and G that are not in a 1-1 isolated region and let k¯ “ |F¯ |.\\nWe apply Theorem 2.2 to G and L in order to find a set S0 Ă G such that costpG ´ S0q ď\\np1 ` 23p`1εqcostpGq ` 23p`1ε costpLq and |S0| ě ε3k¯{6. Let G1 “ G ´ S0. We define a subgraph\\nG1 “ pV 1, E1q of G as follows: For each isolated region pL0, f0q, for each client c P VLpL0q X VGpf0q,\\ndesignate c as a good client, and include in E1 the edges of a c-to-L0 shortest path and a c-to-f0\\nshortest path. For every nonisolated facility ` P L and every nonisolated facility f P G, for every\\nclient c P VLp`q X VGpfq, also designate c as a good client, and include in E1 the edges of a c-to-`\\nshortest path and a c-to-f0 shortest path. Let C1 be the set of clients designated as good. The\\nremaining clients are considered bad.\\n11\\nLet F “ G1 Y L. Let R1, R2, . . . be an r-division of G1VorpFq where r “ 1{ε7. Define G˚ “\\nG1 Y tboundary vertices of the r-divisionu.\\nThe vertex sets of regions are of course not disjoint—a boundary vertex is in multiple regions—\\nbut it is convenient to represent them by disjoint sets. We therefore define a ground set Ω “\\ntpv,Rq : v a vertex of G1VorpFq, R a region containing vu, and, for each region R, we define pR “\\ntpv,Rq : v a vertex of Ru. Now xR1,xR2, . . . form a partition of Ω. To allow us to go from an element\\nof Ω back to a vertex, if x “ pv,Rq we define qx “ v. Finally, define pG “ tpv,Rq P Ω : v P G˚u.\\nLet F¯ be the set of facilities of local and G that are not in 1-1 isolated regions.\\nLemma 5.1. |pG| ď |G1| ` c2ε3.5|F¯ |, where c2 is the constant in the definition of r-division.\\nProof. Consider the r-division. Each 1-1 isolated region results in a connected component of size 2\\nin G1VorpFq and so no boundary vertices arise from such connected components. By the definition of\\nr-division, the sum over regions of boundary vertices is at most c2 ¨ |n¯0|{r1{2, where n¯0 is the total\\nnumber of elements of G1 and L that are not in 1-1 isolated regions. Since r “ 1{ε7, we have that\\n|pG| ď |G1| ` c2 ¨ ε4|F¯ |.\\nLemma 5.2. |pG| ď k.\\nProof. By Theorem 2.2, we have that |G1| ď k ´ ε3k¯{12. By Lemma 5.1 we thus have\\n|pG| ď |G1| ` c2ε3.5k¯ ď k ´ ε3k¯{12` c2ε3.5k¯ ď k,\\nfor ε small enough.\\nThroughout the rest of the proof, we will bound the cost of L by the cost of G˚. We now slightly\\nabuse notations in the following way : each facility ` of L that belongs to an isolated region and\\nthat is a boundary vertex is now in G˚. We say that this facility is isolated.\\nThe following lemma first appears in [22].\\nLemma 5.3 (Balanced Partitioning). Let S “ tS1, ..., Spu and tA,Bu be partitions of some ground\\nset. Suppose |A| ě |B| and, for “ 1, . . . , p, 1{p2ε2q ď |Si| ď 1{ε2.\\nThere exists a partition that is a coarsening of S satisfying the two following properties. For\\nany part C of the coarser partition,\\n• Small Cardinality: C is the union of Op1{ε5q parts of S.\\n• Balanced: |C XA| ě |C XB|.\\nWe now apply Lemma 5.3 to the partition xR1,xR2, . . . with A “ tpv,Rq P Ω : v P Lu and\\nB “ pG. We refer to the parts of the resulting coarse partition as super-regions. Each super-region\\nR naturally corresponds to a subgraph of G1VorpFq, the subgraph induced by tv : pv,Rq P Ru, and\\nwe sometimes use R to refer to this subgraph.\\nFor a super-region R, let LpRq (resp. G˚pRq) be the set of facilities of L (resp. G˚) in the\\nsuper-region R, i.e.: the set t` | ` P L and p`, Rq P Ωu (resp. tf | f P G˚ and pf,Rq P Ωu). We\\nconsider the mixed solution\\nMR “ pL´ LpRqq Y G˚pRq.\\nLemma 5.4. |MR ´ L| ` |L´MR| “ Op1{ε12q and |MR| ď k.\\nProof. Each region of the r-division contains at most c1{ε7 facilities where c1 is the constant in the\\ndefinition of r-divisions. By Lemma 5.3, each super-region is the union of Op1{ε5q regions\\n12\\nWe now define gc to be the cost of client c in solution G˚ and lc to be the cost of client c in solution\\nL. For any client c P VGpf0q ´ VLpL0q for some isolated region pf0,L0q, define ReassignG˚ ÞÑLpcq as\\nthe cost of assigning c to the facility of L0 that is the closest to f0. We let ε1 be a positive constant\\nthat will be chosen later.\\nLemma 5.5. Consider an isolated region pf0,L0q.ÿ\\ncPVGpf0q´VLpL0q\\nReassignG˚ ÞÑLpcq ď p1` ε1qp\\nÿ\\ncPVGpf0q´VLpL0q\\ngc ` 2\\npp1` ε1qpε´p1 ε\\n1´ ε\\nÿ\\ncPVGpf0q\\npgc ` lcq,\\nProof. Consider a client c P VGpf0q´VLpL0q, and let ` denote the facility of L that is the closest to f0.\\nBy Lemma 3.1, distpc, `qp ď p1` ε1qppdistpc, f0qp` ε´p1 distp`, f0qpq “ p1` ε1qppgc` ε´p1 distp`, f0qpq.\\nSumming over c P VGpf0q ´ VLpL0q,ÿ\\ncPVGpf0q´VLpL0q\\nReassignL ÞÑG˚pcq ď p1`ε1qp\\nÿ\\ncPVGpf0q´VLpL0q\\ngc`p1`ε1qpε´p1 |VGpf0q´VLpL0q|distp`, f0qp.\\nTo upper bound distp`, f0qp, we use an averaging argument. For each client c1 P VLp`q X VGpf0q, let\\nLpcq be the facility of L0 that serves it in L. By Lemma 3.1 we have distp`, f0qp ď 2ppdistp`, c1qp `\\ndistpc1, f0qpq ď 2ppdistpLpcq, c1qp ` distpc1, f0qpq “ 2pplc1 ` gc1q, thus\\ndistp`, f0qp ď 2\\np\\n|VLp`q X VGpf0q|\\nÿ\\ncPVLp`qXVGpf0q\\nplc ` gcq.\\nSubstituting, we have that\\nř\\ncPVGpf0q´VLpL0q\\nReassignL ÞÑG˚pcq is at most\\np1` ε1qp\\nÿ\\ncPVGpf0q´VLpL0q\\ngc ` 2pp1` ε1qpε´p1\\n|VGpf0q ´ VLpL0q|\\n|VLp`q X VGpf0q|\\nÿ\\ncPVLp`qXVGpf0q\\nplc ` gcq.\\nBy definition of isolated regions, VGpf0q´VLpL0q ď ε|VGpf0q| and |VGpf0q´VLpL0q| ě p1´εq|VGpf0q|,\\nso the ratio is at most ε{p1´ εq. Summing over ` P L0 proves the Lemma.\\nSimilarly, for any client c P VLpL0q´VGpf0q for some isolated region pf0,L0q, define ReassignLÞÑG˚\\nas the cost of assigning c to f0.\\nLemma 5.6. Consider an isolated region pf0,L0q.ÿ\\ncPVLpL0q´VGpf0q\\nReassignL ÞÑG˚pcq ď p1` ε1qp\\nÿ\\ncPVLpL0q´VGpf0q\\nlc ` 2\\npp1` ε1qpε´p1 ε\\n1´ ε\\nÿ\\ncPVGpf0q\\npgc ` lcq.\\nProof. Consider a client c P VLpL0q ´ VGpf0q, and let ` denote the facility serving it in L. By\\nLemma 3.1, distpc, f0qp ď p1 ` ε1qppdistpc, `qp ` ε´p1 distp`, f0qpq “ p1 ` ε1qpp`c ` ε´p1 distp`, f0qpq.\\nSumming over c P VLp`q ´ VGpf0q,\\nÿ\\ncPVLp`q´VGpf0q\\nReassignL ÞÑG˚pcq ď p1` ε1qp\\n¨˝ ÿ\\ncPVLp`q´VGpf0q\\nlc ` ε´p1 |VLp`q ´ VGpf0q|distp`, f0qp‚˛.\\n13\\nTo upper bound distp`, f0q, we use an averaging argument. For each client c1 P VLp`q X VGpf0q, by\\nLemma 3.1 we have distp`, f0qp ď 2ppdistp`, c1qp ` distpc1, f0qpq “ 2pplc1 ` gc1q, thus\\ndistp`, f0qp ď 2\\np\\n|VLp`q X VGpf0q|\\nÿ\\ncPVLp`qXVGpf0q\\nplc ` gcq.\\nSubstituting,\\nÿ\\ncPVLp`q´VGpf0q\\nReassignL ÞÑG˚pcq ď p1`ε1qp\\n¨˝ ÿ\\ncPVLp`q´VGpf0q\\nlc ` 2\\npε´p1 |VLp`q ´ VGpf0q|\\n|VLp`q X VGpf0q|\\nÿ\\ncPVLp`qXVGpf0q\\nplc ` gcq‚˛.\\nBy definition of isolated regions, |VLp`q ´ VGpf0q| ď ε|VLp`q| and |VLp`q X VGpf0q| ě p1´ εq|VLp`q|,\\nso the ratio is at most ε{p1´ εq. Summing over ` P L0 proves the Lemma.\\nLemma 5.7. Consider an isolated region pf,L0q. Let ` be a facility of L0. For any super-region\\nR, MR contains f or a facility that is at distance at most distp`, fq from f .\\nProof. Since ` and f belong to the same isolated region pf,L0q and ` P L0, they belong to the same\\nconnected component of G1VorpFq. Now consider a super-region R which does not contain `. Then\\n` P LpRq. Thus, either f P R or by Lemma 3.8, a boundary element `1 P R of the r-division is on\\nthe path from ` to f and distp`1, fq ď distp`, fq. Thus, `1 PMR, proving the lemma.\\nFor a client c and a super-region R, we define mRpcq to be the cost of c in the mixed solution\\nMR. Moreover, for each client c, we consider the facilities v and w that serve this client in solution\\nL and G˚ respectively. We define lpcq to be an arbitrary pair pv,Rq P Ω and g˚pcq to be an arbitrary\\npair pw,Rq P Ω. We slightly abuse notation and say that pv,Rq is isolated if v belongs to one of\\nthe isolated regions.\\nLemma 5.8. Let c be a good client and R a super-region. The value of mRpcq ´ lc is less than or\\nequal to: #\\ngc ´ lc if g˚pcq P R\\n0 otherwise\\nProof. Observe that if g˚pcq P R, then mRpcq ď gc and the first case holds. Now, for any super-\\nregion R S lpcq, g˚pcq, MR contains the facility serving client c in local. Thus its cost is at most\\nlc and the second case holds. Finally, assume that R contains lpcq and does not contain g˚pcq. If\\ncˆ belongs to R, then by the separation property of the r-division (see Lemmas 3.8, 3.9), g˚pcq P R\\nand mRpcq ď gc. Otherwise, cˆ R R, and so, by the separation property there must be a boundary\\nvertex of R that is closer to c than the facility that serves it in L. Therefore, we have mRpcq ď lc\\nand the second case holds.\\nWe now turn to the bad clients.\\nLemma 5.9. Let c be a bad client and R a super-region. The value of mRpcq ´ lc is less than or\\nequal to: $’’’’’’&’’’’’’%\\ngc ´ lc if `pcq P R and g˚pcq P R\\nReassignG˚ ÞÑLpcq ´ lc if `pcq P R and g˚pcq R R and g˚pcq is isolated\\ngc ´ lc if `pcq R R and g˚pcq P R and g˚pcq is not isolated\\nReassignLÞÑG˚pcq ´ lc if `pcq P R and g˚pcq R R and g˚pcq is not isolated\\n0 otherwise.\\n14\\nProof. Observe that the super-regions form a partition of the lpcq and g˚pcq. Let Rp`pcqq be the\\nregion that contains `pcq and Rpg˚pcqq be the region that contains g˚pcq. If Rp`pcqq “ Rpg˚pcqq\\nthen, the facility serving c in G˚ is in MRp`pcqq, hence mRp`pcqqpcq ď gc. Moreover for any other\\nregion R1 ‰ Rp`pcqq, we have `pcq R R1 and so the facility serving c in L is in MR1 . Therefore\\nmR1pcq ď lc.\\nThus, we consider c such that Rp`pcqq ‰ Rpg˚pcqq. Since c is bad, we have that `pcq or g˚pcq is\\nisolated. Consider the case where g˚pcq is isolated. The cost of c in solutionMRp`pcqq is, by Lemma\\n5.7, at most ReassignG˚ ÞÑLpcq satisfying the lemma. Now, for any other region R1 ‰ Rpg˚pcqq,\\nagain we have `pcq R R1 and so the facilitiy serving c in L is in MR1 . Therefore, mR1pcq ď lc.\\nTherefore, we consider the case where c is such that Rp`pcqq ‰ Rpg˚pcqq and such that g˚pcq is\\nnot isolated. Since c is bad, `pcq is isolated. Thence, by Lemma 5.7, the cost in solution MRp`pcqq\\nis at most ReassignL ÞÑG˚pcq, satisfying the Lemma. Moreover, in solution Rpg˚pcqq, the cost is at\\nmost gc. Finally, for any other region R1 ‰ Rp`pcqq,Rpg˚pcqq, `pcq R R1 and so the facilitiy serving\\nc in L is in MR1 . Therefore, mR1pcq ď lc, concluding the proof of the lemma.\\nWe now partition the clients into three sets, Λ1,Λ2,Λ3. Let Λ1 be the set of clients such that\\nthere exists a super-region R such that `pcq P R and g˚pcq R R and g˚pcq is not isolated. Let Λ2\\nbe the set of clients such that there exists a super-region R such that `pcq P R and g˚pcq R R and\\ng˚pcq is isolated. Finally let Λ3 be the remaining clients : Λ3 “ C´Λ1´Λ2. The following corollary\\nfollows directly from combining Lemmas 5.8 and 5.9 and by observing that the super-regions form\\na partition of the lpcq and g˚pcq, and by the definition of Λ1,Λ2,Λ3.\\nCorollary 4. For any client c, we have that\\nÿ\\nR\\npmRpcq ´ lcq ď\\n$’&’%\\nReassignL ÞÑG˚ ` gc ´ 2lc if c P Λ1\\nReassignG˚ ÞÑL ´ lc if c P Λ2\\ngc ´ lc if c P Λ3\\nWe now turn to the proof of Theorem 1.2.\\nProof of Theorem 1.2. By Lemma 5.4, for any super-region R the solution MR is in the local\\nneighborhood of L. By local optimality, we have\\np1´ ε{nq\\nÿ\\nc\\nlc ď\\nÿ\\nc\\nmRpcq.\\nHence,\\n´ ε\\nn\\ncostpLq ď\\nÿ\\nc\\npmRpcq ´ lcq.\\nObserve that the number of regions is at most k ď n. Thus, summing over all regions we have\\n´εcostpLq ď\\nÿ\\nR\\nÿ\\nc\\npmRpcq ´ lcq.\\nInverting summations and applying Corollary 4, we obtain\\n´εcostpLq ď\\nÿ\\ncPΛ1\\npReassignLÞÑG˚pcq ` gc ´ 2lcq `\\nÿ\\ncPΛ2\\npReassignG˚ ÞÑLpcq ´ lcq `\\nÿ\\ncPΛ3\\npgc ´ lcq.\\n15\\nBy definition of Λ1 and since each client in Λ1 is bad, applying Lemma 5.6 yields\\n´εcostpLq ď\\nÿ\\ncPΛ1\\npgc ` pp1` ε1qp ´ 2qlcq `\\nÿ\\ncPΛ2\\npReassignG˚ ÞÑLpcq ´ lcq\\n`\\nÿ\\ncPΛ3\\npgc ´ lcq ` p1` ε1q\\npε´p1 2pε\\n1´ ε pcostpLq ` costpG\\n˚qq.\\nHence, for ε small enough with respect to p and ε1, we have\\n´εcostpLq ď\\nÿ\\ncPΛ1\\npgc ´ p1´ εqlcq `\\nÿ\\ncPΛ2\\npReassignG˚ ÞÑLpcq ´ lcq\\n`\\nÿ\\ncPΛ3\\npgc ´ lcq ` p1` ε1q\\npε´p1 2pε\\n1´ ε pcostpLq ` costpG\\n˚qq.\\nNow, by definition of Λ2 and since each client in Λ2 is bad, applying Lemma 5.5 gives\\n´εcostpLq ď\\nÿ\\ncPΛ1\\npgc ´ p1´ εqlcq `\\nÿ\\ncPΛ2\\npp1` ε1qpgc ´ lcq\\n`\\nÿ\\ncPΛ3\\npgc ´ lcq ` 2\\np`1p1` ε1qpε´p1 ε\\n1´ ε pcostpLq ` costpG\\n˚qq\\nThus,\\n´εcostpLq ď\\nÿ\\nc\\npp1` ε1qpgc ´ p1´ εqlcq ` 2\\np`1p1` ε1qpε´p1 ε\\n1´ ε pcostpLq ` costpG\\n˚qq\\nď p1` ε1qpp1` 2\\np`1ε´p1 ε\\n1´ ε qpcostpG\\n˚q ´ costpLqq,\\nsince Λ1,Λ2,Λ3 is a partition of the clients. Therefore, assuming ε is small enough with respect to\\np and ε1, there exists a constant c1 such that\\np1´ c1 2ε\\n1´ ε ´ εqcostpLq ď p1` c1\\n2ε\\n1´ εqcostpG\\n˚q\\np1´ c1 2ε\\n1´ ε ´ εqcostpLq ď p1` c1\\n2ε\\n1´ εqp1` εqcostpGq ` c1εcostpLq\\nNow, observe that costpG˚q ď costpG1q since G1 Ď G˚. By Theorem 2.2, costpG1q ď p1`c1εqcostpGq`\\nc1εcostpLq. Combining concludes the proof of Theorem 1.2\\n6 Clusters in Euclidean space : Proof of Theorem 1.1\\nThe proof is similar for Rd. We explain how to modify the beginning of the proof of the graph\\ncase, the rest of the proof applies directly. We let C1 denote the set of clients that do not belong\\nto the symetric difference of VLpL0q and VGpf0q of any isolated region pL0, f0q. We call them good\\nclients; the others are bad clients. Again, we define a solution G1 by applying Theorem 2.2 to G.\\nLet F “ LY G1. We now consider each isolated region pL0, f0q, with |L0| ą 1{ε7d´ 1, and proceed\\nto an r-division of L0 Y tf0u with r “ 1{ε7d. Moreover, for the remaining facilities F of that are\\nnot in any isolated region, we proceed to an r-division of those points with r “ 1{ε7d. We denote\\n16\\nby R1, R2 . . . the subset of all the regions defined by the above r-divisions. Let Z denote the set of\\nboundary elements of all the r-divisions. Define G˚ “ G1 Y Z.\\nThe point sets of regions are not disjoint since points of Z appear in various regions. Thus, we\\nagain define a ground set Ω “ tpv,Rq : v a point of F , R a region containing vu, and, for each\\nregion R, we define pR “ tpv,Rq : v a point of Ru. Now xR1,xR2, . . . form a partition of Ω. To\\nallow us to go from an element of Ω back to a point, if x “ pv,Rq we define qx “ v. Finally, definepG “ tpv,Rq P Ω : v P G˚u.\\nWe now branch with the rest of the proof of Theorem 1.2, starting from Lemma 5.1.\\n7 Reducing the number of clusters : Proof of Theorem 2.2\\nWe recall the statement of Theorem 2.2.\\nTheorem 2.2. Let ε ă 1{2 be a positive constant and L and G be two solutions for the k-clustering\\nproblem with exponent p. Let k¯ denote the number of facilities f of G that are not in a 1-1 isolated\\nregion. There exists a set S0 of facilities of G of size at least ε3k¯{6 that can be removed from G at\\nlow cost: costpG ´ S0q ď p1` 23p`1εqcostpGq ` 23p`1εcostpLq.\\nLet ε ă 1{2 be a positive constant and L and G be two solutions for the k-clustering problem\\nwith exponent p. Observe that since ε ă 1{2, each facility of L belongs to at most one isolated\\nregion. Let G˜ denote the facilities of G that are not in an isolated region.Theorem 2.2 relies on the\\nfollowing lemma, whose proof we momentarily defer.\\nLemma 7.1. There exists a function φ : G˜ ÞÑ G such that reassigning all the clients of VGpfq to\\nφpfq for every facility f P G˜ increases the cost of G by at most 23p`1ε´2pcostpLq ` costpGqq.\\nProof of Theorem 2.2. Consider the abstract graph H where the nodes are the elements of G and\\nthere is a directed arc from f to φpfq. More formally, H “ pG, txf, φpfqy | f P G˜uq. Notice that\\nevery node of H has outdegree at most 1. Thus, there exists a coloring of the nodes of H with\\nthree colors, such that all arcs are dichromatic. Let S denote the color set with the largest number\\nof nodes of G˜. We have that S contains at least |G˜|{3 nodes of G˜.\\nArbitrarily partition S into 1{ε3 parts, each of cardinality at least ε3|G˜|{3. By Lemma 7.1 and\\nan averaging argument, there exists a part S0 such that reassigning each facility f P S0 to φpfq\\nincreases the cost by at most\\n23p`1ε´2\\nε´3 pcostpLq ` costpGqq “ 2\\n3p`1εpcostpLq ` costpGqq.\\nSince the arcs of H are dichromatic, if f P S0 then φpfq R S0. Consider the solution G ´ S0.\\nClient that belong to VGpfq for some f P S0 can be assigned in G´S0 to a facility that is no farther\\nthan φpfq. Therefore, the cost of the solution G´S0 is at most costpGq`23p`1ε ¨pcostpLq`costpGqq.\\nWe now relate |G˜| to k¯. Let kpLq1 be the number of facilities of L that belong to an isolated region\\nthat is not 1-1 isolated. Let kpGq1 be the number of facilities of G that belong to an isolated region\\nthat is not 1-1 isolated. Finally, let kpGq2 “ |G˜|. By definition, we have kpGq1`kpGq2 “ k¯ ě kpLq1.\\nNow, observe that there are at least two facilities of L per isolated region that is not 1-1 isolated.\\nThus, 2kpGq1 ď kpLq1. Hence, k¯ “ kpGq1 ` kpGq2 ď kpLq1{2 ` kpGq2. But for any kpGq2 ă kpGq1,\\nkpLq1{2`kpGq2 ă kpLq1 ď k¯. Therefore, we must have kpGq2 ě kpGq1, and so kpGq2 ě k¯{2. Thence\\nε|G˜|{3 ě εk¯{6 and the theorem follows.\\nWe now define gc to be the cost of client c in solution G˚ and lc to be the cost of client c in\\nsolution L.\\n17\\nProof of Lemma 7.1. For each facility f P G˜, we define φpfq “ argmintdistpf, f 1q | f 1 P G ´ tfuu.\\nInstead of analyzing the cost increase when reassigning clients of VGpfq to φpfq we will analyze the\\ncost increase of the following fractional assignment. First for a facility f P G, we denote by Lˆpfq\\nthe set\\nLˆpfq “ t` P L | 1 ď |VGpfq X VLp`q| ă p1´ εq|VLp`q|u. (5)\\nBy definition of isolated regions (Definition 2.1), for any f P G˜ we haveÿ\\n`PLˆpfq\\n|VLp`q X VGpfq| ą ε|VGpfq|. (6)\\nThus, we partition the clients in VGpfq into parts indexed by ` P Lˆpfq, in a such a way that the part\\nassociated to ` has size at most ε´1|VGpfq X VLp`q|. For any ` P Lˆpfq, the clients in the associated\\npart are reassigned to the facility ψp`, fq P G ´ tfu that is the closest to `.\\nWe now bound the cost increase ∆ induced by the reassignment. For each client c P VGpfq as-\\nsigned to a part associated to a facility `, the new cost for c is cost1c “ distpc, ψp`, fqqp. By the trian-\\ngular inequality and Lemma 3.1, cost1c ď 2ppdistpc, fqp`distpf, ψp`, fqqpq “ 2ppgc`distpf, ψp`, fqqpq.\\nSumming over all clients, we have that the new cost is at mostÿ\\nc\\n2pgc `\\nÿ\\nfPG˜\\nÿ\\n`PLˆpfq\\nε´1|VGpfq X VLp`q|2pdistpf, ψp`, fqqp.\\nLet ∆ “ řfPG˜ř`PLˆpfq ε´1|VGpfq X VLp`q|2pdistpf, ψp`, fqqp. By Lemma 3.1, we have\\n∆ ď\\nÿ\\nfPG˜\\nÿ\\n`PLˆpfq\\nε´1|VGpfq X VLp`q|4ppdistpf, `qp ` distp`, ψp`, fqqpq.\\nInverting summations,\\n∆ ď 4pε´1\\n¨˝ÿ\\n`PL\\nÿ\\nfPG˜:`PLˆpfq\\n|VGpfq X VLp`q|distpf, `qp `\\nÿ\\n`PL\\nÿ\\nfPG˜:`PLˆpfq\\n|VGpfq X VLp`q|distp`, ψp`, fqqp‚˛.\\nDefine ∆1 “ ř`PLřfPG˜:`PLˆpfq |VGpfq X VLp`q|distpf, `qp and ∆2 “ ř`PLřfPG˜:`PLˆpfq |VGpfq X\\nVLp`q|distp`, ψp`, fqqp.\\nWe first bound ∆1. By Lemma 3.1, we have that distpf, `qp ď 2ppdistpf, cqp ` distp`, cqpq “\\n2ppgc ` lcq for any client c P VGpfq X VLp`q. Therefore,\\n∆1 ď ε´1\\nÿ\\n`PL\\nÿ\\nfPG˜ : `PLˆpfq\\n|VGpfq X VLp`q| 2\\np\\n|VGpfq X VLp`q|\\nÿ\\ncPVGpfqXVLp`q\\npgc ` lcq\\nď 2pε´1\\nÿ\\n`PL\\nÿ\\nfPG˜ : `PLˆpfq\\nÿ\\ncPVGpfqXVLp`q\\npgc ` lcq ď 2pε´1pcostpGq ` costpLqq.\\nWe now turn to bound the cost of ∆2. Let f\\n`\\nmin be the facility of G that is the closest to `. Let\\n∆3 “ ε´1\\nÿ\\n`PL\\nÿ\\nf‰f`min:`PLˆpfq\\n|VGpfq X VLp`q|distp`, ψp`, fqqp\\n∆4 “ ε´1\\nÿ\\n`PL\\n|VGpf `minq X VLp`q|distp`, ψp`, f `minqqp.\\n18\\nFor any client c P VGpfqXVLp`q, by Lemma 3.1, distp`, ψp`, fqqp, for f ‰ f `min yields distp`, ψp`, fqqp ď\\n2ppdistp`, cqp ` distpc, ψp`, fqqpq ď 2ppdistp`, cqp ` distpc, fqpq “ 2pplc ` gcq. Thus,\\n∆3 ď ε´1\\nÿ\\n`PL\\nÿ\\nf‰f`min:`PLˆpfq\\n2p|VGpfq X VLp`q|\\n|VGpfq X VLp`q|\\nÿ\\ncPVGpfqXVLp`q\\nplc ` gcq ď 2pε´1pcostpGq ` costpLqq.\\nWe conclude by analyzing ∆4. Observe that if ` R Lˆpf `minq then we are done : the clients in\\nVGpf `minq are not reassigned through `. Thus we assume ` P Lˆpf `minq. We now apply Lemma 3.1 to\\ndistp`, ψp`, f `minqqp, for any client c P VLp`q ´ VGpf `minq we have distp`, ψp`, f `minqqp ď 2ppdistp`, cqp`\\ndistpc, ψp`, f `minqqpq ď 2pplc ` gcq, since ψp`, f `minq is the facility of G that is the second closest to `.\\nReplacing we have,\\n∆4 ď ε´1\\nÿ\\n`PL\\n2p|VGpf `minq X VLp`q|\\n|VLp`q ´ VGpf `minq|\\nÿ\\ncPVLp`q´VGpf`minq\\nplc ` gcq\\nNow, since ` P Lˆpf `minq, we have that |VGpf `minq X VLp`q|{|VLp`q ´ VGpf `minq| ď p1´ εq{ε. Therefore,\\n∆4 ď 2pp1´ εqε´2\\nÿ\\ncPVLp`q´VGpf`minq\\nplc ` gcq.\\nPutting ∆1,∆2,∆3,∆4 together we obtain that the total cost increase induced by the reassignment\\nis at most 23p`1pcostpGq ` costpLqq{ε2.\\n8 Postponed proofs\\n8.1 Proof of existence of weak r-divisions in Euclidean space\\nProof. We describe a recursive procedure to construct the set Z in the definition of weak r-division\\nof C. Assuming that |C| ą r, find a sphere S and a set Z0 satisfying Theorem 3.6. Let Z1 be the\\nresult of applying the procedure to the union of Z0 with the set of points inside C, and similarly\\nobtain Z2 from the set of points outside C. Return Z0 Y Z1 Y Z2.\\nIt is clear that the set Z together with its induced partition R of C returned by the procedure\\nsatisfies all the properties of a weak r-division except for Property 4, which requires some calcula-\\ntion. Let bpnq “ řRPR |R X Z| when the procedure is applied to a set C of size at most n, where\\nn ą p1´ σqr. If n ď r then bpnq “ 0, and if n ą r then\\nbpnq ď cn1´1{d ` max\\nαPr1´σ,σs\\nfpαn` cn1´1{dq ` fpp1´ αqn` cn1´1{dq.\\nWe show by induction that bpnq ď β n\\nr1{d ´γn1´1{d for suitable constants β, γ ą 0 to be determined.\\nWe postpone the basis of the induction until β, γ are selected.\\nBy the inductive hypothesis,\\nbpαn` cn1´1{dq ď β αn\\nr1{d\\n` β cn\\n1´1{d\\nr1{d\\n´ γα1´1{dn1´1{d\\nbpp1´ αqn` cn1´1{dq ď β p1´ αqn\\nr1{d\\n` β cn\\n1´1{d\\nr1{d\\n´ γp1´ αq1´1{dn1´1{d\\n19\\nso\\nbpnq ď\\nˆ\\nc` 2c\\nr1{d\\n˙\\nn1´1{d ` β n\\nr1{d\\n´ γ\\n”\\nα1´1{d ` p1´ αq1´1{d\\nı\\nn1´1{d (7)\\nThe function fpxq “ x1´1{d ` p1 ´ xq1´1{d is strictly concave for x P r0, 1s, as can be seen by\\ntaking its second derivative. For any α P r1 ´ σ, σs, there exists a number 0 ă µ ă 1 such that\\nα “ p1 ´ µqp1 ´ σq ` µσ. By concavity, therefore, fpαq ě p1 ´ µqfp1 ´ σq ` µfpσq. Since a\\nweighted average is at least the minimum, p1´ µqfp1´ σq ` µfpσq ě mintfp1´ σq, fpσqu. Write\\nfp1´σq “ fpσq “ 1` δ. Since f is strictly concave, δ ą 0. We choose γ “ pc` 2c{r1{dq{δ, for then\\nthe first term in Inequality 7 is bounded by γδn1´1{d, and we obtain bpnq ď β n\\nr1{d ´ γn1´1{d.\\nFor the basis of the induction, suppose n ą p1´ σqr. Then\\nβ\\nn\\nr1{d\\n´ γn1´1{d “\\n˜\\nβ\\nn1{d\\nr1{d\\n´ γ\\n¸\\nn1´1{d ě\\n˜\\nβ\\np1´ σq1{dr1{d\\nr1{d\\n´ γ\\n¸\\n“\\n´\\nβp1´ σq1{d ´ γ\\n¯\\nwhich is nonnegative for an appropriate choice of β depending on σ and γ.\\nReferences\\n[1] E. Aarts and J. K. Lenstra, editors. Local Search in Combinatorial Optimization. John Wiley\\n& Sons, Inc., New York, NY, USA, 1st edition, 1997.\\n[2] A. A. Ageev. An approximation scheme for the uncapacitated facility location problem on\\nplanar graphs. In Proceedings of the 12th International Baikal Workshop, pages 9–13, 2001.\\n[3] N. Alon, P. D. Seymour, and R. Thomas. A separator theorem for graphs with an excluded\\nminor and its applications. In Proceedings of the 22nd Annual ACM Symposium on Theory of\\nComputing, May 13-17, 1990, Baltimore, Maryland, USA, pages 293–299, 1990.\\n[4] S. Arora, P. Raghavan, and S. Rao. Approximation schemes for Euclidean k -medians and\\nrelated problems. In Proceedings of the Thirtieth Annual ACM Symposium on the Theory of\\nComputing, Dallas, Texas, USA, May 23-26, 1998, pages 106–113, 1998.\\n[5] D. Arthur, B. Manthey, and H. Ro¨glin. Smoothed analysis of the k-means method. J. ACM,\\n58(5):19, 2011.\\n[6] D. Arthur and S. Vassilvitskii. Worst-case and smoothed analysis of the ICP algorithm, with\\nan application to the k-means method. SIAM J. Comput., 39(2):766–782, 2009.\\n[7] V. Arya, N. Garg, R. Khandekar, A. Meyerson, K. Munagala, and V. Pandit. Local search\\nheuristics for k-median and facility location problems. SIAM J. Comput., 33(3):544–562, 2004.\\n[8] P. Awasthi, A. Blum, and O. Sheffet. Stability yields a PTAS for k-median and k-means\\nclustering. In 51th Annual IEEE Symposium on Foundations of Computer Science, FOCS\\n2010, October 23-26, 2010, Las Vegas, Nevada, USA, pages 309–318, 2010.\\n[9] P. Awasthi, M. Charikar, R. Krishnaswamy, and A. K. Sinop. The hardness of approximation\\nof Euclidean k-means. In 31st International Symposium on Computational Geometry, SoCG\\n2015, June 22-25, 2015, Eindhoven, The Netherlands, pages 754–767, 2015.\\n20\\n[10] P. Awasthi and O. Sheffet. Improved spectral-norm bounds for clustering. In Approxima-\\ntion, Randomization, and Combinatorial Optimization. Algorithms and Techniques - 15th In-\\nternational Workshop, APPROX 2012, and 16th International Workshop, RANDOM 2012,\\nCambridge, MA, USA, August 15-17, 2012. Proceedings, pages 37–49, 2012.\\n[11] M. Ba˘doiu, S. Har-Peled, and P. Indyk. Approximate clustering via core-sets. In STOC, pages\\n250–257, 2002.\\n[12] B. Baker. Approximation algorithms for NP-complete problems on planar graphs. J. of the\\nACM, 41(1):153–180, 1994.\\n[13] M. Balcan, A. Blum, and A. Gupta. Approximate clustering without the approximation. In\\nProceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA\\n2009, New York, NY, USA, January 4-6, 2009, pages 1068–1077, 2009.\\n[14] M. Balcan and Y. Liang. Clustering under perturbation resilience. SIAM J. Comput.,\\n45(1):102–155, 2016.\\n[15] S. Bandyapadhyay and K. R. Varadarajan. On variants of k-means clustering. CoRR,\\nabs/1512.02985, 2015.\\n[16] M. Bateni, A. Bhaskara, S. Lattanzi, and V. S. Mirrokni. Distributed balanced clustering\\nvia mapping coresets. In Advances in Neural Information Processing Systems 27: Annual\\nConference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal,\\nQuebec, Canada, pages 2591–2599, 2014.\\n[17] V. V. S. P. Bhattiprolu and S. Har-Peled. Separating a voronoi diagram. CoRR, abs/1401.0174,\\n2014.\\n[18] Y. Bilu and N. Linial. Are stable instances easy? Combinatorics, Probability & Computing,\\n21(5):643–660, 2012.\\n[19] G. E. Blelloch and K. Tangwongsan. Parallel approximation algorithms for facility-location\\nproblems. In SPAA 2010: Proceedings of the 22nd Annual ACM Symposium on Parallelism\\nin Algorithms and Architectures, Thira, Santorini, Greece, June 13-15, 2010, pages 315–324,\\n2010.\\n[20] T. M. Chan and S. Har-Peled. Approximation algorithms for maximum independent set of\\npseudo-disks. Discrete & Computational Geometry, 48(2):373–392, 2012.\\n[21] M. Charikar and S. Guha. Improved combinatorial algorithms for facility location problems.\\nSIAM J. Comput., 34(4):803–824, 2005.\\n[22] V. Cohen-Addad and C. Mathieu. Effectiveness of local search for geometric optimization. In\\n31st International Symposium on Computational Geometry, SoCG 2015, June 22-25, 2015,\\nEindhoven, The Netherlands, pages 329–343, 2015.\\n[23] D. Feldman and M. Langberg. A unified framework for approximating and clustering data.\\nIn Proceedings of the 43rd ACM Symposium on Theory of Computing, STOC 2011, San Jose,\\nCA, USA, 6-8 June 2011, pages 569–578, 2011.\\n[24] D. Feldman, M. Monemizadeh, and C. Sohler. A PTAS for k-means\\nclustering based on weak coresets. In SoCG, pages 11–18, 2007.\\n21\\n[25] G. N. Frederickson. Fast algorithms for shortest paths in planar graphs, with applications.\\nSIAM J. Comput., 16(6):1004–1022, 1987.\\n[26] Z. Friggstad, M. Rezapour, and M. R. Salavatipour. Local search yields a ptas for k-means in\\ndoubling metrics. CoRR, 2016.\\n[27] S. Guha and S. Khuller. Greedy strikes back: Improved facility location algorithms. J.\\nAlgorithms, 31(1):228–248, 1999.\\n[28] S. Guha, A. Meyerson, N. Mishra, R. Motwani, and L. O’Callaghan. Clustering data streams:\\nTheory and practice. IEEE Trans. Knowl. Data Eng., 15(3):515–528, 2003.\\n[29] V. Guruswami and P. Indyk. Embeddings and non-approximability of geometric problems. In\\nProceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, Jan-\\nuary 12-14, 2003, Baltimore, Maryland, USA., pages 537–538, 2003.\\n[30] S. Har-Peled and A. Kushal. Smaller coresets for k-median and k-means clustering. Discrete\\n& Computational Geometry, 37(1):3–19, 2007.\\n[31] S. Har-Peled and S. Mazumdar. On coresets for k-means and k-median clustering. In Proceed-\\nings of the 36th Annual ACM Symposium on Theory of Computing, Chicago, IL, USA, June\\n13-16, 2004, pages 291–300, 2004.\\n[32] S. Har-Peled and K. Quanrud. Approximation algorithms for polynomial-expansion and low-\\ndensity graphs. In Algorithms - ESA 2015 - 23rd Annual European Symposium, Patras, Greece,\\nSeptember 14-16, 2015, Proceedings, pages 717–728, 2015.\\n[33] D. S. Hochbaum. Heuristics for the fixed cost median problem. Math. Program., 22(1):148–162,\\n1982.\\n[34] M. Inaba, N. Katoh, and H. Imai. Applications of weighted voronoi diagrams and randomiza-\\ntion to variance-based k -clustering (extended abstract). In Proceedings of the Tenth Annual\\nSymposium on Computational Geometry, Stony Brook, New York, USA, June 6-8, 1994, pages\\n332–339, 1994.\\n[35] K. Jain, M. Mahdian, and A. Saberi. A new greedy approach for facility location problems.\\nIn Proceedings on 34th Annual ACM Symposium on Theory of Computing, May 19-21, 2002,\\nMontre´al, Que´bec, Canada, pages 731–740, 2002.\\n[36] K. Jain and V. Vazirani. Approximation algorithms for metric facility location and k -median\\nproblems using the primal-dual schema and Lagrangian relaxation. J. ACM, 48(2):274–296,\\n2001.\\n[37] T. Kanungo, D. Mount, N. Netanyahu, C. Piatko, R. Silverman, and A. Wu. A local search\\napproximation algorithm for k-means clustering. Comput. Geom., 28(2-3):89–112, 2004.\\n[38] S. G. Kolliopoulos and S. Rao. A nearly linear-time approximation scheme for the euclidean\\nk-median problem. SIAM J. Comput., 37(3):757–782, June 2007.\\n[39] M. R. Korupolu, C. G. Plaxton, and R. Rajaraman. Analysis of a local search heuristic for\\nfacility location problems. J. Algorithms, 37(1):146–188, 2000.\\n22\\n[40] A. Kumar and R. Kannan. Clustering with spectral norm and the k-means algorithm. In 51th\\nAnnual IEEE Symposium on Foundations of Computer Science, FOCS 2010, October 23-26,\\n2010, Las Vegas, Nevada, USA, pages 299–308, 2010.\\n[41] A. Kumar, Y. Sabharwal, and S. Sen. A simple linear time (1 + epsiv;)-approximation algo-\\nrithm for k-means clustering in any dimensions. In Foundations of Computer Science, 2004.\\nProceedings. 45th Annual IEEE Symposium on, pages 454–462, Oct 2004.\\n[42] A. Kumar, Y. Sabharwal, and S. Sen. Linear-time approximation schemes for clustering prob-\\nlems in any dimensions. J. ACM, 57(2), 2010.\\n[43] S. Li. A 1.488 approximation algorithm for the uncapacitated facility location problem. In-\\nformation and Computation, 222:45–58, 2013.\\n[44] S. Li and O. Svensson. Approximating k-median via pseudo-approximation. In Symposium\\non Theory of Computing Conference, STOC’13, Palo Alto, CA, USA, June 1-4, 2013, pages\\n901–910, 2013.\\n[45] R. Ostrovsky, Y. Rabani, L. J. Schulman, and C. Swamy. The effectiveness of Lloyd-type\\nmethods for the k-means problem. J. ACM, 59(6):28, 2012.\\n[46] D. B. Shmoys, E´. Tardos, and K. Aardal. Approximation algorithms for facility location\\nproblems. In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing,\\npages 265–274. ACM, 1997.\\n23\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd21'), 'authors': 'Eberly, Wayne, Giesbrecht, Mark, Giorgi, Pascal, Storjohann, Arne, Villard, Gilles', 'year': '2012', 'title': 'Solving Sparse Integer Linear Systems', 'full_text': 'ar\\nX\\niv\\n:c\\ns/0\\n60\\n30\\n82\\nv1\\n  [\\ncs\\n.SC\\n]  \\n21\\n M\\nar \\n20\\n06\\nSolving Sparse Integer Linear Systems\\nWayne Eberly,\\nDepartment of Computer Science, University of Calgary\\nhttp://pages.cpsc.ucalgary.ca/˜eberly\\nMark Giesbrecht, Pascal Giorgi∗, Arne Storjohann,\\nDavid R. Cheriton School of Computer Science, University of Waterloo\\nhttp://www.uwaterloo.ca/˜{mwg,pgiorgi,astorjoh}\\nGilles Villard\\nCNRS, LIP, ´Ecole Normale Supe´rieure de Lyon\\nhttp://perso.ens-lyon.fr/gilles.villard\\nAbstract\\nWe propose a new algorithm to solve sparse linear systems of equa-\\ntions over the integers. This algorithm is based on a p-adic lifting tech-\\nnique combined with the use of block matrices with structured blocks. It\\nachieves a sub-cubic complexity in terms of machine operations subject to\\na conjecture on the effectiveness of certain sparse projections. A LinBox -\\nbased implementation of this algorithm is demonstrated, and emphasizes\\nthe practical benefits of this new method over the previous state of the\\nart.\\n1 Introduction\\nA fundamental problem of linear algebra is to compute the unique solution of\\na non-singular system of linear equations. Aside from its importance in and of\\nitself, it is key component in many recent proposed algorithms for other prob-\\nlems involving exact linear systems. Among those algorithms are Diophantine\\nsystem solving [10, 19, 20], Smith form computation [8, 21], and null-space and\\nkernel computation [3]. In its basic form, the problem we consider is then to\\ncompute the unique rational vector A−1b ∈ Qn×1 for a given non-singular ma-\\ntrix A ∈ Zn×n and right hand side b ∈ Zn×1. In this paper we give new and\\neffective techniques for when A is a sparse integer matrix, which have sub-cubic\\ncomplexity on sparse matrices.\\n∗Author is currently affiliated to LP2A laboratory, University of Perpignan\\n1\\nA classical and successful approach to solving this problem for dense integer\\nmatrices A was introduced by Dixon in 1982 [5], following polynomial case\\nstudies from [18]. His proposed technique is to compute, iteratively, a sufficiently\\naccurate p-adic approximation A−1b mod pk of the solution. The prime p is\\nchosen such that det(A) 6≡ 0 mod p (see, e.g., [22] for details on the choice of\\np). Then, using radix conversion (see e.g. [9, §12]) combined with continued\\nfraction theory [13, §10], one can easily reconstruct the rational solution A−1b\\nfrom A−1b mod pk (see [25] for details).\\nThe principal feature of Dixon’s technique is the pre-computation of the\\nmatrix A−1 mod p which leads to a decreased cost of each lifting step. This\\nleads to an algorithm with a complexity of O (˜n3 log(‖A‖+ ‖b‖)) bit operations\\n[5]. Here and in the rest of this paper ‖ . . . ‖ denotes the maximum entry in\\nabsolute value and the O˜ notation indicates some possibly omitting logarithmic\\nfactor in the variables.\\nFor a given non-singular matrix A ∈ Zn×n, a right hand side b ∈ Zn×1, and\\na suitable integer p, Dixon’s scheme is the following:\\n• compute B = A−1 mod p;\\n• compute ℓ p-adic digits of the approximation iteratively by multiplying B\\ntimes the right hand side, which is updated according to each new digit;\\n• use radix conversion and rational number reconstruction to recover the\\nsolution.\\nThe number ℓ of lifting steps required to find the exact rational solution to\\nthe system is O (˜n log(‖A‖ + ‖b‖)), and one can easily obtain the announced\\ncomplexity (each lifting steps requires a quadratic number of bit operations in\\nthe dimension of A; see [5] for more details).\\nIn this paper we study the case when A is a sparse integer matrix, for\\nexample, when only O (˜n) entries are non-zero. The salient feature of such\\na matrix A is that applying A, or its transpose, to a dense vector c ∈ Zn×1\\nrequires only O (˜n log(‖A‖+ ‖c‖)) bit operations.\\nFollowing techniques proposed by Wiedemann in [26], one can compute a\\nsolution of a sparse linear system over a finite field in O (˜n2) field operations,\\nwith only O (˜n) memory. Kaltofen & Saunders [16] studied the use of Wiede-\\nmann’s approach, combined with p-adic approximation, for sparse integer linear\\nsystem. Nevertheless, this combination doesn’t help to improve the bit com-\\nplexity compared to Dixon’s algorithm: it still requires O (˜n3) operations in the\\nworst case. One of the main reasons is that Wiedemann’s technique requires\\nthe computation, for each right hand side, of a new Krylov subspace, which re-\\nquires O(n) matrix-vector products by A mod p. This implies the requirement\\nof Θ(n2) operations modulo p for each lifting step, even for a sparse matrix\\n(and Θ(n(log ‖A‖+ ‖b‖)) such lifting steps are necessary in general). The only\\nadvantage then of using Wiedemann’s technique is memory management: only\\nO(n) additional memory is necessary, as compared to the O(n2) space needed\\nto store matrix inverse modulo p explicitly, which may well be dense even for\\nsparse A.\\nThe main contribution of this current paper is to provide a new Krylov-like\\n2\\npre-computation for the p-adic algorithm with a sparse matrix which allows us\\nto improve the bit complexity of linear system solving. The main idea is to use\\nblock-Krylov method combined with special block projections to minimize the\\ncost of each lifting step. The Block Wiedemann algorithm [4, 24, 14] would be\\na natural candidate to achieve this. However, the Block Wiedemann method\\nis not obviously suited to being incorporated into a p-adic scheme. Unlike the\\nscalar Wiedemann algorithm, wherein the minimal polynomial can be used for\\nevery right-hand side, the Block Wiedemann algorithm needs to use different\\nlinear combinations for each right-hand side. In particular, this is due to the\\nspecial structure of linear combinations coming from a column of a minimal\\nmatrix generating polynomial (see [24, 23]) and then be totally dependent on\\nthe right hand side.\\nOur new scheme reduces the cost of each lifting step, on a sparse matrix as\\nabove, to O (˜n1.5) bit operations. This means the cost of the entire solver is\\nO (˜n2.5(log(‖A‖+ ‖b‖)) bit operations. The algorithm makes use of the notion\\nof an efficient sparse projection, for which we currently only offer a construc-\\ntion which is conjectured to work in all cases. However, we do provide some\\ntheoretical evidence to support its applicability, and note its effectiveness in\\npractice.\\nMost importantly, the new algorithm is shown to offer significant practical\\nimprovement on sparse integer matrices. The algorithm is implemented in the\\nLinBox library [6], a generic C++ library for exact linear algebra. We com-\\npare it against the best known solvers for integer linear equations, in particular\\nagainst the Dixon lifting scheme and Chinese remaindering. We show that in\\npractice it runs many times faster than previous schemes on matrices of size\\ngreater than 2500× 2500 with suffiently high sparsity. This also demonstrates\\nthe effectiveness in practice of so-called “asymptotically fast” matrix-polynomial\\ntechniques, which employ fast matrix/polynomial arithmetic. We provide a de-\\ntailed discussion of the implementation, and isolate the performance benefits\\nand bottlenecks. A comparison with Maple dense solver emphasizes the high\\nefficiency of the LinBox library and the needs of well-designed sparse solvers as\\nwell.\\n2 Block projections\\nThe basis for Krylov-type linear algebra algorithms is the notion of a projec-\\ntion. In Wiedemann’s algorithm, for example, we solve the ancillary problem of\\nfinding the minimal polynomial of a matrix A ∈ Fn×n over a field F by choosing\\nrandom u ∈ F1×n and v ∈ Fn×1 and computing the minimal polynomial of the\\nsequence uAiv for i = 0..2n− 1 (which is both easy to compute and with high\\nprobability equals the minimal polynomial of A). As noted in the introduction,\\nour scheme will ultimately be different, a hybrid Krylov and lifting scheme, but\\nwill still rely on the notion of a structured block projection.\\nFor the remainder of the paper, we adopt the following notation:\\n• A ∈ Fn×n be a non-singular matrix,\\n3\\n• s be a divisor of n, the blocking factor, and\\n• m := n/s.\\nUltimately F will be Q and we will have A ∈ Zn×n, but for now we work in the\\ncontext of a more general field F.\\nFor a block v ∈ Fn×s and 0 ≤ t ≤ m, define\\nK(A, v) := [ v Av · · · Am−1v ] ∈ Fn×n.\\nWe call a triple (R, u, v) ∈ Fn×n × Fs×n × Fn×s an efficient block projection\\nif and only if\\n1. K(AR, v) and K((AR)T , uT ) are non-singular;\\n2. R can be applied to a vector with O (˜n) operations in F;\\n3. we can compute vx, uTx, yv and yuT for any x ∈ Fs×1 and y ∈ F1×n,\\nwith O (˜n) operations in F.\\nIn practice we might hope that R, u and v in an efficient block projection\\nare extremely simple, for example R is a diagonal matrix and u and v have only\\nn non-zero elements.\\nConjecture 2.1. For any non-singular A ∈ Fn×n and s |n, there exists an\\nefficient block projection (R, u, v) ∈ Fn×n×Fs×n×Fn×s, and it can be constructed\\nquickly.\\n2.1 Constructing efficient block projections\\nIn what follows we present an efficient sparse projection which we conjecture\\nto be effective for all matrices. We also present some supporting evidence (if\\nnot proof) for its theoretical effectiveness. As we shall see in Section 4, the\\nprojection performs extremely well in practice.\\nWe focus only on R and v, since its existence should imply the existence of\\na u of similar structure.\\nFor convenience, assume for now that all elements in v and R are alge-\\nbraically independent indeterminates, modulo some imposed structure. This is\\nsufficient, since the existence of an efficient sparse projection with indeterminate\\nentries would imply that a specialization to an effective sparse projection over\\nZp is guaranteed to work with high probability, for sufficiently large p. We also\\nconsider some different possibilities for choosing R and v.\\n2.1.1 Dense Projections\\nThe “usual” scheme for block matrix algorithms is to choose R diagonal, and v\\ndense. The argument to show this works has several steps. First, AR will have\\ndistinct eigenvalues and thus will be non-derogatory (i.e., its minimal polynomial\\nequals its characteristic polynomial). See [2], Lemma 4.1. Second, for any\\nnon-derogatory matrix B and dense v we have K(B, v) non-singular (see [15]).\\nHowever, a dense v is not an efficient block projection since condition (2) is not\\nsatisfied.\\n4\\n2.1.2 Structured Projections\\nThe following projection scheme is the one we use in practice. Its effectiveness\\nin implementation is demonstrated in Section 4.\\nChoose R diagonal as before. Choose\\nv =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n∗\\n∗\\n. . .\\n∗\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n∈ kn×s (1)\\nwith each ∗ of dimension m × 1. The intuition behind the structure of v is\\ntwofold. First, if s = 1 then v is a dense column vector, and we know K(AR, v)\\nis non-singular in this case. Second, since the case s = 1 requires only n nonzero\\nelements in the “block”, it seems that n nonzero elements should suffice in the\\ncase s > 1 also. Third, if E is a diagonal matrix with distinct eigenvalues then,\\nup to a permutation of the columns, K(E, v) is a block Vandermonde matrix,\\neach m×m block defined via m distinct roots, thus non-singular. In the general\\ncase with s > 1 we ask:\\nQuestion 2.2. For R diagonal and v as in (1), is K(AR, v) necessarily non-\\nsingular?\\nOur work thus far has not led to a resolution of the question. However,\\nby focusing on the case s = 2 we have answered the following similar question\\nnegatively: If A is nonsingular with distinct eigenvalues and v is as in (1), is\\nK(A, v) necessarily nonsingular?\\nLemma 2.3. If m = 2 there exists a nonsingular A with distinct eigenvalues\\nsuch that for v as in (1) the matrix K(A, v) is singular.\\nProof. We give a counterexample with n = 4. Let\\nE =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 0 0 0\\n0 2 0 0\\n0 0 3 0\\n0 0 0 4\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb and P =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1 0 0 0\\n0 1 1/4 0\\n0 1 1 0\\n0 0 0 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb .\\nDefine\\nA = 3P−1EP =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n3 0 0 0\\n0 5 −1 0\\n0 4 10 0\\n0 0 0 12\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb .\\n5\\nFor the generic block\\nv =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8f0\\na1\\na2\\nb1\\nb2\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fb\\nthe matrix K(A, v) is singular. By embedding A into a larger block diagonal\\nmatrix we can construct a similar counterexample for any n and m = 2.\\nThus, if Question 2.2 has an affirmative answer, then proving it will neces-\\nsitate considering the effect of the diagonal preconditioner R above and beyond\\nthe fact that “AR has distinct eigenvalues”. For example, are the eigenvalues\\nof AR algebraically independent, using the fact that entries in R are? This may\\nalready be sufficient.\\n2.1.3 A Positive Result for the Case s = 2\\nFor s = 2 we can prove the effectiveness of our efficient sparse projection scheme.\\nSuppose that A ∈ Fn×n where n is even and A is diagonalizable with distinct\\neigenvalues in an extension of F. Then A = X−1DX ∈ Fn×n for some diagonal\\nmatrix D with distinct diagonal entries (in this extension). Note that the rows\\nof X can be permuted (replacing X with PX for some permutation P ),\\nA = ((PX)−1(P−1DP )(PX)),\\nand P−1DP is also a diagonal matrix with distinct diagonal entries. Conse-\\nquently we may assume without loss of generality that the top left (n/2)×(n/2)\\nsubmatrix X1,1 of X is nonsingular. Suppose that\\nX =\\n[\\nX1,1 X1,2\\nX2,1 X2,2\\n]\\nand consider the decomposition\\nA = Z−1ÂZ, (2)\\nwhere\\nZ =\\n[\\nX−11,1 0\\n0 X−11,1\\n]\\nX =\\n[\\nI Z1,2\\nZ2,1 Z2,2\\n]\\nfor n/2× n/2 matrices Z1,2, Z2,1, and Z2,2, and where\\nÂ =\\n[\\nX−11,1 0\\n0 X−11,1\\n]\\nD\\n[\\nX1,1 0\\n0 X1,1\\n]\\n,\\nso that\\nÂ =\\n[\\nA1 0\\n0 A2\\n]\\n,\\n6\\nfor matrices A1 and A2. The matrices A1 and A2 are each diagonalizable over\\nan extension of F, since A is, and the eigenvalues of these matrices are also\\ndistinct.\\nNotice that, for vectors a, b with dimension n/2, and for any nonnegative\\ninteger i,\\nAi\\n[\\na\\n0\\n]\\n= Z−1Âi\\n[\\na\\nZ2,1a\\n]\\nand Ai\\n[\\n0\\nb\\n]\\n= Z−1Âi\\n[\\nZ1,2b\\nZ2,2b\\n]\\n.\\nThus, if\\nx =\\n[\\na\\nZ2,1a\\n]\\nand y =\\n[\\nZ1,2b\\nZ2,2b\\n]\\nthen the matrix with columns\\na,Aa,A2a, . . . , An/2−1a, b, Ab,A2b, . . . , An−2−1b\\nis nonsingular if and only if the matrix with columns\\nx, Âx, Â2x, . . . , Ân/2−1x, y, Ây, Â2y, . . . , Ân/2−1y\\nis nonsingular. The latter condition fails if and only if there exist polynomials f\\nand g, each with degree less than n/2, such that at least one of these polynomials\\nis nonzero and\\nf(Â)x+ g(Â)y = 0. (3)\\nTo proceed, we should therefore determine a condition on A ensuring that no\\nsuch polynomials f and g exist for some choice of x and y (that is, for some\\nchoice of a and b).\\nA suitable condition on A is easily described: We will require that the top\\nright submatrix Z1,2 of Z is nonsingular.\\nNow suppose that the entries of the vector b are uniformly and randomly\\nchosen from some (sufficiently large) subset of F, and suppose that a = −Z1,2b.\\nNotice that at least one of f and g is nonzero if and only if at least one of f\\nand g − f is nonzero. Furthermore,\\nf(Â)(x) + g(Â)(y) = f(Â)(x + y) + (g − f)(Â)(y).\\nIt follows by the choice of a that\\nx+ y =\\n[\\n0\\n(Z2,2 − Z2,1Z1,2)b\\n]\\n.\\nSince Â is block diagonal, the top n/2 entries of f(Â)(x+y) are nonzero as well\\nfor every polynomial f . Consequently, failure condition (3) can only be satisfied\\nif the top n/2 entries of the vector (g − f)(Â)(y) are also all zero.\\nRecall that g − f has degree less than n/2 and that the top left submatrix\\nof the block diagonal matrix Â is diagonalizable with n/2 distinct eigenvalues.\\nAssuming, as noted above, that Z1,2 is nonsingular (and recalling that the top\\n7\\nhalf of the vector y is Z1,2b), the Schwartz-Zippel lemma is easily used to show\\nthat if b is randomly chosen as described then, with high probability, the failure\\ncondition can only be satisfied if g − f = 0. That is, it can only be satisfied if\\nf = g.\\nObserve next that, in this case,\\nf(Â)(x) + g(Â)(y) = f(Â)(x+ y),\\nand recall that the bottom half of the vector x+y is the vector (Z2,2−Z2,1Z1,2)b.\\nThe matrix Z2,2 − Z2,1Z1,2 is clearly nonsingular (it is a Schur complement\\nformed from Z) so, once again, the Schwartz-Zippel lemma can be used to show\\nthat if b is randomly chosen as described above then f(Â)(x + y) = 0 if and\\nonly if f = 0 as well.\\nThus if Z1,2 is nonsingular and a and b are chosen as described above then,\\nwith high probability, equation (3) is satisfied only if f = g = 0. There must\\ntherefore exist a choice of a and b providing an efficient block projection — once\\nagain, supposing that Z1,2 is nonsingular.\\nIt remains only to describe a simple and efficient randomization of A that\\nachieves this condition with high probability: Let us replace A with the matrix\\nA˜ =\\n[\\nI tI\\n0 I\\n]\\n−1\\nA\\n[\\nI tI\\n0 I\\n]\\n=\\n[\\nI −tI\\n0 I\\n]\\nA\\n[\\nI tI\\n0 I\\n]\\n,\\nwhere t is chosen uniformly from a sufficiently large subset of F. This has the\\neffect of replacing Z with the matrix\\nZ\\n[\\nI tI\\n0 I\\n]\\n=\\n[\\nI Z1,2 + tI\\nZ2,1 Z2,2 + tZ2,1\\n]\\n(see, again, (2)), effectively replacing Z1,2 with Z1,2 + tI. There are clearly at\\nmost n/2 choices of t for which the latter matrix is singular.\\nFinally, note that if v is a vector and i ≥ 0 then\\nA˜iv =\\n[\\nI −tI\\n0 I\\n]\\nAi\\n[\\nI tI\\n0 I\\n]\\nv.\\nIt follows by this and similar observations that this randomization can be applied\\nwithout increasing the asymptotic cost of the algorithm described in this paper.\\nQuestion: Can the above randomization and proof be generalized to a similar\\nresult for larger s?\\nOther sparse block projections\\nOther possible projections are summarized as follows.\\n• Iterative Choice Projection. Instead of choosing v all at once, choose\\nthe columns of v = [v1|v2| · · · |vs] in succession. For example, suppose up\\nto preconditioning we can assume we are working with a B ∈ Fn×n that\\n8\\nis simple as well as has the property that the characteristic polynomial\\nis irreducible. Then we can choose v1 to be the first column of In to\\nachieve K(B, v1) ∈ Fn×m of rank m. Next choose v2 to have two nonzero\\nentries, locations chosen randomly until [K(B, v1)|K(B, v2)] ∈ Fn×2m has\\nrank 2m, etc. This gives a v with m(m+ 2)/2 nonzero entries.\\nThe point of choosing v column by column is that, while choosing all of v\\nsparse may have a very small probability of success, the success rate for\\nchoosing vi when v1, v2, . . . , vi−1 are already chosen may be high enought\\n(e.g., maybe only expected O(log n)) choices for vi before success).\\n• Toeplitz projections. Choose R and/or v to have a Toeplitz structure.\\n• Vandermonde projections. Choose v to have a Vandermonde or a\\nVandermonde-like structure.\\n3 Non-singular sparse solver\\nIn this section we show how to employ a block-Krylov type method combined\\nwith the (conjectured) efficient block projections of Section 2 to improve the\\ncomplexity of evaluating the inverse modulo p of a sparse matrix. Applying\\nDixon’s p-adic scheme with such an inverse yields an algorithm with better\\ncomplexity than previous methods for sparse matrices, i.e., those with a fast\\nmatrix-vector product. In particular, we express the cost of our algorithm in\\nterms of the number of applications of the input matrix to a vector, plus the\\nnumber of auxiliary operations.\\nMore precisely, given A ∈ Zn×n and v ∈ Zn×1, let µ(n) be the number of op-\\nerations in Z to compute Av or vTA. Then, assuming Conjecture 2.1, our algo-\\nrithm requires\\nO (˜n1.5(log(‖A‖ + ‖b‖)) matrix-vector products w 7→ Aw on vectors w ∈ Zn×1\\nwith ‖w‖ = O(1), plus O (˜n2.5(log(‖A‖+ ‖b‖)) additional bit operations.\\nSummarizing this for practical purposes, in the common case of a matrix A ∈\\nZn×n with O (˜n) constant-sized non-zero entries, and b ∈ Zn×1 with constant-\\nsized entries, we can compute A−1b with O (˜n2.5) bit operations.\\nWe achieve this by first introducing a structured inverse of the matrix Ap =\\nA mod p which links the problem to block-Hankel matrix theory. We will assume\\nthat we have an efficient block projection (R, u, v) ∈ Zn×np × Zs×np × Zn×sp for\\nAp, and let B = AR ∈ Zn×np . We thus assume we can evaluate Bw and wTB,\\nfor any w ∈ Zn×1p , with O (˜µ(n)) operations in Zp. The proof of the following\\nlemma is left to the reader.\\nLemma 3.1. Let B ∈ Zn×np be non-singular, where n = ms for m, s ∈ Z>0.\\nLet u ∈ Zs×np and v ∈ Zn×sp be efficient block projections such that V =\\n[v|Bv| · · · |Bm−1v] ∈ Zn×np and UT = [uT |BTuT | · · · |(BT )m−1uT ] ∈ Zn×np are\\nnon-singular. The matrix H = UBV ∈ Zn×np is then a block-Hankel matrix,\\nand the inverse for B can be written as B−1 = V H−1U .\\n9\\nIn fact\\nH =\\n\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nα1 α2 · · · αm\\nα2 α3 · · · αm+1\\n...\\nαm αm · · · α2m−1\\n\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f8 ∈ Zn×np , (4)\\nwith αi = uB\\niv ∈ Zs×s for i = 1 . . . 2m − 1. H can thus be computed with\\n2m− 1 applications of B to a (block) vector plus 2m− 1 pre-multiplications by\\nu, for a total cost of 2nµ(n) +O (˜n2) operations in Zp. For a word-sized prime\\np, we can find H with O (˜nµ(n)) bit operations (where, by “word-sized”, we\\nmean having a constant number of bits, typically 32 or 64, depending upon the\\nregister size of the target machine).\\nWe will need to apply H−1 to a number of vectors at each lifting step and\\nso require that this be done efficiently. We will do this by fist representing H−1\\nusing the off-diagonal inverse formula of [17]:\\nH−1 =\\n( αm−1 ··· α0\\n... . .\\n.\\nα0\\n)( β∗\\nm−1\\n··· β∗\\n0\\n. . .\\n...\\nβm−1\\n)\\n−\\n\\uf8eb\\uf8ed βm−2 ··· β0 0... . .. . ..\\nβ0 . .\\n.\\n0\\n\\uf8f6\\uf8f8( α∗m ··· α1. . . ···\\nα∗\\nm\\n)\\nwhere αi, α\\n∗\\ni , βi, β\\n∗\\ni ∈ Zs×sp .\\nThis representation can be computed using the Sigma Basis algorithm of\\nBeckermann-Labahn [17]. We use the version given in [11] which ensures the\\ndesired complexity in all cases. This requires O (˜s3m) operations in Zp (and\\nwill only be done once during the algorithm, as pre-computation to the lifting\\nsteps).\\nThe Toeplitz/Hankel forms of the components in this formula allow to eval-\\nuate H−1w for any w ∈ Zn×1p with O (˜s2m) or O (˜ns) operations in Zp using\\nan FFT-based polynomial multiplication (see [1]). An alternative to computing\\nthe inversion formula would be to use the generalization of the Levinson-Durbin\\nalgorithm in [14].\\nCorollary 3.2. Assume that we have pre-computed H−1 ∈ Zn×np for a word-\\nsized prime p. Then, for any v ∈ Zn×1p , we can compute B−1v mod p with\\n2(m− 1)µ(n) +O (˜n(m+ s)) operations in Zp.\\nProof. By Lemma 3.1 we can express the application of B−1 to a vector by an\\napplication of U , followed by an application of H−1 followed by an application\\nof V .\\nTo apply U to a vector w ∈ Zn×1p , we note that\\n(Uw)T = [(uw)T , (uBw)T , . . . , (uBm−1)Tw)t]T .\\nWe can find this iteratively, for i = 0, . . . ,m−1, by computing bi = Biw = Bbi−1\\n(assume b0 = w) and uB\\niw = ubi, for i = 0..m− 1 in sequence. This requires\\n(m− 1)µ(n) +O (˜mn) operations in Zp.\\n10\\nTo apply V to a vector y ∈ Zn×1p , write y = [y0|y1| · · · |ym−1]T , where yi ∈ Zsp.\\nThen\\nV y = vy0 +Bvy1 +B\\n2vy2 + · · ·+Bm−1vym−1\\n= vx0 +B (vx1 +B (vx1 + · · · ((vxm−2 +Bvxm−1) · · · )))\\nwhich can be accomplished with m− 1 applications of B and m applications of\\nthe projection v. This requires (m− 1)µ(n) +O (˜mn) operations in Zp.\\nP-adic scheme\\nWe employ the inverse computation described above in the p-adic lifting algo-\\nrithm of Dixon [5]. We briefly describe the method here and demonstrate its\\ncomplexity in our setting.\\nInput: A ∈ Zn×n non-singular, b ∈ Zn×1;\\nOutput: A−1b ∈ Qn×1\\n(1) Choose a prime p such that detA 6≡ 0 mod p;\\n(2) Determine an efficient block projection for A:\\nR, u, v ∈ Zn×n × Zs×np × Zn×sp ; Let B = AR;\\n(3) Compute αi = uB\\niv for i = 1 . . . 2m − 1 and define H as in (4). Recall\\nthat B−1 = V H−1U ;\\n(4) Compute the inverse formula of H−1 (see above);\\n(5) Let ℓ := n\\n2\\n· ⌈logp(n‖A‖2) + logp((n− 1)‖A‖2 + ‖b‖2)⌉);\\nb0 := b;\\n(6) For i from 0 to ℓ do\\n(7) xi := B\\n−1bi mod p;\\n(8) bi+1 := p\\n−1(bi −Bxi)\\n(9) Reconstruct x ∈ Qn×1 from xℓ using rational reconstruction.\\nTheorem 3.3. The above p-adic scheme solves the linear system A−1b with\\nO (˜n1.5(log(‖A‖ + ‖b‖)) matrix-vector products by A mod p (for a machines-\\nword sized prime p) plus O (˜n2.5(log(‖A‖+ ‖b‖)) additional bit-operations.\\nProof. The total cost of the algorithm is O (˜nµ(n)+n2+n log(‖A‖+‖b‖)(mµ+\\nn(m + s)). For the optimal choice of s =\\n√\\nn and m = n/s, this is easily seen\\nto equal the stated cost. The rational reconstruction in the last step is easily\\naccomplished using radix conversion (see, e.g., [9]) combined with continued\\nfraction theory, in a cost which is dominated by the other operations (see [25]\\nfor details).\\n11\\n4 Efficient implementation\\nAn implementation of our algorithm has been done in the LinBox library [6].\\nThis is a generic C++ library which offers both high performance and the\\nflexibility to use highly tuned libraries for critical components. The use of\\nhybrid dense linear algebra routines [7], based on fast numerical routine such\\nas BLAS, is one of the successes of the library. Introducing blocks to solve\\ninteger sparse linear systems is then an advantage since it allows us to use such\\nfast dense routines. One can see in Section 4.2 that this becomes necessary to\\nachieve high performance, even for sparse matrices.\\n4.1 Optimizations\\nIn order to achieve the announced complexity we need to use asymptotically fast\\nalgorithms, in particular to deal with polynomial arithmetic. One of the main\\nconcerns is then the computation of the inverse of the block-Hankel matrix and\\nthe matrix-vector products with the block-Hankel/Toeplitz matrix.\\nConsider the block-Hankel matrix H ∈ Zn×np defined by 2m − 1 blocks of\\ndimension s denoted αi in equation (4). Let us denote the matrix power series\\nH(z) = α1 + α2z + . . .+ α2m−1z\\n2m−2.\\nOne can compute the off-diagonal inverse formula of H using [17, theorem 3.1]\\nwith the computation of\\n• two left sigma bases of [H(z)t | I]T of degrees 2m− 2 and 2m, and\\n• two right sigma bases of [H(z) | I] of degrees 2m− 2 and 2m.\\nThis computation can be done with O (˜s3m) field operation with the fast\\nalgorithm PM-Basis of [11]. However, the use of a slower algorithm such as\\nM-Basis of [11] will give a complexity of O(s3m2) or O(n2s) field operations. In\\ntheory, the latter is not a problem since the optimal s is equal to\\n√\\nn, and thus\\ngives a complexity of O(n2.5) field operations, which still yields the announced\\ncomplexity.\\nIn practice, we developed implementations for both algorithms (M-Basis\\nand PM-Basis), using the efficient dense linear algebra of [7] and an FFT-based\\npolynomial matrix multiplication. Nevertheless, due to the special structure\\nof the series to approximate, the use of a third implementation based on a\\nmodified version of M-Basis, where only half of the first columns (or rows) of\\nthe basis are computed, allows us to achieve the best performance. Note that\\nthe approximation degrees remain small (less than 1 000).\\nAnother important point in our algorithm is the application of the off di-\\nagonal inverse to a vector x ∈ Zn×1p . This computation reduces to polynomial\\nmatrix-vector product; x is cut into chunks of size s. Contrary to the block-\\nHankel matrix inverse computation, we really need to use fast polynomial arith-\\nmetic to achieve our complexity. However, we can avoid the use of FFT-based\\narithmetic since the evaluation of H−1, which is the dominant cost, can be done\\n12\\nonly once at the beginning of the lifting. Let t = O(m) be the number of evalu-\\nation points. One can evaluate H−1 at t points using Horner’s rules with O(n2)\\nfield operations.\\nHence, applyingH−1 in each lifting step reduces to the evaluation of a vector\\ny ∈ Zp[x]s×1 of degree m at t points, to computing t matrix-vector product of\\ndimension s, and to interpolating the result. The cost for each application of\\nH−1 is then O(m2s+ms2) field operations, giving O(n1.5) field operations for\\nthe optimal choice of s = m =\\n√\\nn. This cost is deduced easily from Horner’s\\nevaluation and Lagrange’s interpolation.\\nTo achieve better performances in practice, we use a Vandermonde matrix\\nand its inverse to perform the evaluation/interpolation steps. This allows us to\\nmaintain the announced complexity, and to benefit from the fast dense linear\\nalgebra routine of LinBox library.\\n4.2 Timings\\nWe now compare the performance of our new algorithm against the best known\\nsolvers. As noted earlier, the previously best known complexity for algorithms\\nsolving integer linear systems is O (˜n3 log(||A||+ ||b||)) bit operations, indepen-\\ndent of their sparsity. This can be achieved with several algorithms: Wiede-\\nmann’s technique combined with the Chinese remainder algorithm [26], Wiede-\\nmann’s technique combined with p-adic lifting [16], or Dixon’s algorithm [5]. All\\nof these algorithms are implemented within the LinBox library and we ensure\\nthey benefits from the optimized code and libraries to the greatest extent possi-\\nble. In our comparison, we refer to these algorithms by respectively: CRA-Wied,\\nP-adic-Wied and Dixon. In order to give a timing reference, we also compare\\nagainst the dense Maple solver. Note that algorithm used by Maple 10 has a\\nquartic complexity in matrix dimension.\\nIn the following, matrices are chosen randomly sparse, with fixed or variable\\nsparsity, and some non-zero diagonal elements are added in order to ensure the\\nnon-singularity.\\n400 900 1600 2500 3600\\nMaple 64.7s 849s 11098s − −\\nCRA-Wied 14.8s 168s 1017s 3857s 11452s\\nP-adic-Wied 10.2s 113s 693s 2629s 8034s\\nDixon 0.9s 10s 42s 178s 429s\\nOur algo. 2.4s 15s 61s 175s 426s\\nTable 1: Solving sparse integer linear system (10 non-zero elts per row) on a\\nItanium2, 1.3GHz\\nFirst, one can see from Table 1 that even if most of the algorithms have\\n13\\nthe same complexity, their performance varies widely. The P-adic-Wied imple-\\nmentation is a bit faster than CRA-Wied since the matrix reduction modulo\\na prime number and the minimal polynomial computation is done only once,\\ncontrary to the O (˜n) times needed by CRA. Another important feature of this\\ntable is to show the efficiency of dense LinBox ’s routines compared to sparse\\nroutines. One can notice the improvement by a factor 10 to 20 with Dixon. An\\nimportant point to note is that O(n) sparse matrix-vector products is not as fast\\nin practice as one dense matrix-vector product. Our new algorithm completely\\nbenefits from this remark and allows it to achieve similar performances to Dixon\\non smaller matrices, and to outperform it for larger matrices.\\nIn order to emphasize the asymptotic benefit of our new algorithm, we now\\ncompare it on larger matrices with different levels of sparsity. In Figure 1, we\\nstudy the behaviour of our algorithm compared to that of Dixon with fixed\\nsparsity (10 and 30 non-zero elements per rows). The goal is to conserve a fixed\\nexponent in the complexity of our algorithm.\\n 0\\n 10000\\n 20000\\n 30000\\n 40000\\n 50000\\n 60000\\n 6000  8000  10000  12000  14000\\nTi\\nm\\nin\\ng \\nin\\n s\\nec\\non\\nds\\nMatrix dimension\\nTimings comparison  Dixon Vs Our algo. for integer linear system solving\\nOur algo. − 10 non−zero elts/row\\nDixon  − 10 non−zero elts/row\\nOur algo. − 30 non−zero elts/row\\nDixon  − 30 non−zero elts/row\\nFigure 1: Comparing our algo. with Dixon’s algorithm (fixed sparsity) on a\\nItanium2, 1.3GHz\\nWith 10 non-zero element per row, our algorithm is always faster than\\nDixon’s and the gain tends to increase with matrix dimension. Its not exactly\\nthe same behaviour when matrices have 30 non-zero element per row. For small\\nmatrices, Dixon still outperforms our algorithm. The crossover appears only\\nafter dimension 10 000. This phenomenon is explained by the fact that sparse\\nmatrix operations remain too costly compared to dense ones until matrix di-\\nmensions become sufficiently large that the overall asymptotic complexity plays\\na more important role.\\n14\\nThis explanation is verified in Figure 2 where different sparsity percentages\\nare used. The sparser the matrices are, the earlier the crossover appears. For\\ninstance, with a sparsity of 0.07%, our algorithm becomes more efficient than\\nDixon’s for matrices dimension greater than 1600, while this is only true for\\ndimension greater than 2500 with a sparsity of 1%. Another phenomenon when\\nexamining matrices of a fixed percentage density is emphasized by the Figure\\n2. This is because Dixon’s algorithm again becomes the most efficient, in this\\ncase, when the matrices become large. This is explained by the variable sparsity\\nwhich leads to a variable complexity. For a given sparsity, the larger the matrix\\ndimensions the more non-zero entries per row, and the more costly our algorithm\\nis. As an example, with 1% of non zero element, the complexity is doubled from\\nmatrix dimension n = 3 000 to n = 6 000. As a consequence, the performances\\nof our algorithm drop with matrix dimension in this particular case.\\n 0.6\\n 0.8\\n 1\\n 1.2\\n 1.4\\n 1.6\\n 1.8\\n 2\\n 2.2\\n 2.4\\n 2.6\\n 1000  2000  3000  4000  5000  6000\\nR\\nat\\nio\\ns\\nMatrix dimension\\nTimings ratio of  our algo./Dixon for integer linear system solving\\nsparsity=0.07%\\nsparsity=0.30%\\nsparsity=1.00%\\ncrossover line\\nFigure 2: Gain of our algo. from Dixon’s algorithm (variable sparsity) on a\\nItanium2, 1.3GHz\\n4.3 The practical effect of different blocking factors\\nIn order to achieve even better performance, one can try to use different block\\ndimensions rather than the theoretical optimal\\n√\\nn. The Table 2 studies exper-\\nimental blocking factors for matrices of dimension n = 10 000 and n = 20 000\\nwith a fixed sparsity of 10 non-zero elements per rows.\\nOne notices that the best experimental blocking factors are far from the\\noptimal theoretical ones (e.g., the best blocking factor is 400 when n = 10 000\\nwhereas theoretically it is 100). This behaviour is not surprising since the larger\\n15\\nn= 10 000\\nblock size 80 125 200 400 500\\ntiming 7213s 5264s 4059s 3833s 4332s\\nn= 20 000\\nblock size 125 160 200 500 800\\ntiming 44720s 35967s 30854s 28502s 37318s\\nTable 2: Blocking factor impact (sparsity= 10 elts per row) on a Itanium2,\\n1.3GHz\\nthe blocking factor is, the fewer sparse matrix operations and the more dense\\nmatrix operations are performed. As we already noted earlier, operations are\\nperformed more efficiently when they are dense rather than sparse (the cache\\neffect is of great importance in practice). However, as shown in Table 2, if the\\nblock dimensions become too large, the overall complexity of the algorithm in-\\ncreases and then becomes too important compared to Dixon’s. A function which\\nshould give a good approximation of the best practical blocking factor would\\nbe based on the practical efficiency of sparse matrix-vector product and dense\\nmatrix operations. Minimizing the complexity according to this efficiency would\\nlead to a good candidate blocking factor. This could be done automatically at\\nthe beginning of the lifting by checking efficiency of sparse matrix-vector and\\ndense operation for the given matrix.\\nConcluding remarks\\nWe give a new approach to solving sparse linear algebra problems over the inte-\\ngers by using sparse or structured block projections. The algorithm we exhibit\\nworks well in practice. We demonstrate it on a collection of very large ma-\\ntrices and compare it against other state-of-the art algorithms. Its theoretical\\ncomplexity is sub-cubic in terms of bit complexity, though it rests still on a\\nconjecture which is not proven in the general case. We offer a rigorous treat-\\nment for a small blocking factor (2) and provide some support for the general\\nconstruction.\\nThe use of a block-Krylov-like algorithm allows us to link the problem of\\nsolving sparse integer linear systems to polynomial linear algebra, where we can\\nbenefit from both theoretical advances in this field and from the efficiency of\\ndense linear algebra libraries. In particular, our experiments point out a general\\nefficiency issue of sparse linear algebra: in practice, are (many) sparse operations\\nas fast as (correspondingly fewer) dense operations? We have tried to show\\nin this paper a negative answer to this question. Therefore, our approach to\\nproviding efficient implementations for sparse linear algebra problems has been\\nto reduce most of the operations to dense linear algebra on a smaller scale. This\\n16\\nwork demonstrates an initial success for this approach (for integer matrices),\\nand it certainly emphasizes the importance of well-designed (both theoretically\\nand practically) sparse, symbolic linear algebra algorithms.\\nAcknowledgment\\nWe would like to thank George Labahn for his comments and assistance on the\\nHankel matrix inversion algorithms.\\nReferences\\n[1] D. Cantor and E. Kaltofen. Fast multiplication of polynomials over arbi-\\ntrary algebras. Acta Informatica, 28:693–701, 1991.\\n[2] L. Chen, W. Eberly, E. Kaltofen, B. D. Saunders, W. J. Turner, and G. Vil-\\nlard. Efficient matrix preconditioners for black box linear algebra. Linear\\nAlgebra and its Applications, 343–344:119–146, 2002.\\n[3] Z. Chen and A. Storjohann. A blas based c library for exact linear algebra\\non integer matrices. In ISSAC ’05: Proceedings of the 2005 international\\nsymposium on Symbolic and algebraic computation, pages 92–99, New York,\\nNY, USA, 2005. ACM Press.\\n[4] D. Coppersmith. Solving homogeneous linear equations over GF[2] via\\nblock Wiedemann algorithm. Mathematics of Computation, 62(205):333–\\n350, Jan. 1994.\\n[5] J. D. Dixon. Exact solution of linear equations using p-adic expansions.\\nNumerische Mathematik, 40:137–141, 1982.\\n[6] J.-G. Dumas, T. Gautier, M. Giesbrecht, P. Giorgi, B. Hovinen,\\nE. Kaltofen, B. D. Saunders, W. J. Turner, and G. Villard. LinBox: A\\ngeneric library for exact linear algebra. In A. M. Cohen, X.-S. Gao, and\\nN. Takayama, editors, Proceedings of the 2002 International Congress of\\nMathematical Software, Beijing, China, pages 40–50. World Scientific, Aug.\\n2002.\\n[7] J.-G. Dumas, P. Giorgi, and C. Pernet. FFPACK: Finite field linear algebra\\npackage. In Gutierrez [12], pages 63–74.\\n[8] W. Eberly, M. Giesbrecht, and G. Villard. On computing the determinant\\nand Smith form of an integer matrix. In Proceedings of the 41st Annual\\nSymposium on Foundations of Computer Science, page 675. IEEE Com-\\nputer Society, 2000.\\n[9] J. von zur Gathen and J. Gerhard. Modern Computer Algebra. Cambridge\\nUniversity Press, New York, USA, 1999.\\n17\\n[10] M. Giesbrecht. Efficient parallel solution of sparse systems of linear dio-\\nphantine equations. In Parallel Symbolic Computation (PASCO’97), pages\\n1–10, Maui, Hawaii, July 1997.\\n[11] P. Giorgi, C.-P. Jeannerod, and G. Villard. On the complexity of poly-\\nnomial matrix computations. In R. Sendra, editor, Proceedings of the\\n2003 International Symposium on Symbolic and Algebraic Computation,\\nPhiladelphia, Pennsylvania, USA, pages 135–142. ACM Press, New York,\\nAug. 2003.\\n[12] J. Gutierrez, editor. ISSAC’2004. Proceedings of the 2004 International\\nSymposium on Symbolic and Algebraic Computation, Santander, Spain.\\nACM Press, New York, July 2004.\\n[13] G. H. Hardy and E. M. Wright. An Introduction to the Theory of Numbers.\\nOxford University Press, fifth edition, 1979.\\n[14] E. Kaltofen. Analysis of Coppersmith’s block Wiedemann algorithm for\\nthe parallel solution of sparse linear systems. Mathematics of Computation,\\n64(210):777–806, Apr. 1995.\\n[15] E. Kaltofen. Analysis of Coppersmith’s block Wiedemann algorithm for\\nthe parallel solution of sparse linear systems. Mathematics of Computation,\\n64(210):777–806, 1995.\\n[16] E. Kaltofen and B. D. Saunders. On Wiedemann’s method of solving\\nsparse linear systems. In Applied Algebra, Algebraic Algorithms and Error–\\nCorrecting Codes (AAECC ’91), volume 539 of LNCS, pages 29–38, Oct.\\n1991.\\n[17] G. Labahn, D. K. Chio, and S. Cabay. The inverses of block hankel and\\nblock toeplitz matrices. SIAM J. Comput., 19(1):98–123, 1990.\\n[18] R. T. Moenck and J. H. Carter. Approximate algorithms to derive exact\\nsolutions to systems of linear equations. In Proc. EUROSAM’79, volume\\n72 of Lecture Notes in Computer Science, pages 65–72, Berlin-Heidelberg-\\nNew York, 1979. Springer-Verlag.\\n[19] T. Mulders and A. Storjohann. Diophantine linear system solving. In\\nInternational Symposium on Symbolic and Algebraic Computation (ISSAC\\n99), pages 181–188, Vancouver, BC, Canada, July 1999.\\n[20] T. Mulders and A. Storjohann. Certified dense linear system solving. Jour-\\nnal of Symbolic Computation, 37(4):485–510, 2004.\\n[21] B. D. Saunders and Z. Wan. Smith normal form of dense integer matrices,\\nfast algorithms into practice. In Gutierrez [12].\\n[22] A. Storjohann. The shifted number system for fast linear algebra on integer\\nmatrices. Journal of Complexity, 21(4):609–650, 2005.\\n18\\n[23] W. J. Turner. Black Box Linear Algebra with Linbox Library. PhD thesis,\\nNorth Carolina State University, May 2002.\\n[24] G. Villard. A study of Coppersmith’s block Wiedemann algorithm using\\nmatrix polynomials. Technical Report 975–IM, LMC/IMAG, Apr. 1997.\\n[25] P. S. Wang. A p-adic algorithm for univariate partial fractions. In Proceed-\\nings of the fourth ACM symposium on Symbolic and algebraic computation,\\npages 212–217. ACM Press, 1981.\\n[26] D. H. Wiedemann. Solving sparse linear equations over finite fields. IEEE\\nTransactions on Information Theory, 32(1):54–62, Jan. 1986.\\n19\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd22'), 'authors': 'Studený, Jan, Uznański, Przemysław', 'year': '2019', 'title': 'Approximating Approximate Pattern Matching', 'full_text': 'ar\\nX\\niv\\n:1\\n81\\n0.\\n01\\n67\\n6v\\n3 \\n [c\\ns.D\\nS]\\n  2\\n3 J\\nul \\n20\\n19\\nApproximating Approximate Pattern Matching\\nJan Studený1 and Przemysław Uznański2\\n1ETH Zürich, Switzerland\\n2University of Wrocław, Poland\\nAbstract\\nGiven a text T of length n and a pattern P of length m, the approximate pattern matching\\nproblem asks for computation of a particular distance function between P and everym-substring\\nof T . We consider a (1±ε) multiplicative approximation variant of this problem, for ℓp distance\\nfunction. In this paper, we describe two (1 + ε)-approximate algorithms with a runtime of\\nO˜(n\\nε\\n) for all (constant) non-negative values of p. For constant p ≥ 1 we show a deterministic\\n(1 + ε)-approximation algorithm. Previously, such run time was known only for the case of ℓ1\\ndistance, by Gawrychowski and Uznański [ICALP 2018] and only with a randomized algorithm.\\nFor constant 0 ≤ p ≤ 1 we show a randomized algorithm for the ℓp, thereby providing a smooth\\ntradeoff between algorithms of Kopelowitz and Porat [FOCS 2015, SOSA 2018] for Hamming\\ndistance (case of p = 0) and of Gawrychowski and Uznański for ℓ1 distance.\\n1 Introduction\\nPattern matching is one of the core problems in text processing algorithms. Given a text T of length\\nn and a pattern P of length m, m ≤ n, both over an alphabet Σ, one searches for occurrences of\\nP in T as a substring. A generalization of a pattern matching is to find substrings of T that are\\nsimilar to P , where we consider a particular string distance and ask for all m-substrings of T where\\nthe distance to P does not exceed a given threshold, or simply report the distance from P to every\\nm-substring of T . Typical distance functions considered are Hamming distance, ℓ1 distance, or in\\ngeneral ℓp distances for some constant p, assuming input is over a numerical, e.g. integer, alphabet.\\nFor reporting all Hamming distances, Abrahamson [Abr87] described an algorithm with the com-\\nplexity of O(n√m logm). Using a similar approach, the same complexity was obtained in [Lip03]\\nand later in conference works [ALPU05, CCI05] for reporting all ℓ1 distances. It is a major open\\nproblem whether near-linear time algorithm, or even O(n3/2−ε) time algorithm, is possible for such\\nproblems. A conditional lower bound [Cli09] was shown, via a reduction from matrix multiplication.\\nThis means that existence of combinatorial algorithm with runtime O(n3/2−ε) solving the problem\\nfor Hamming distances implies combinatorial algorithms for boolean matrix multiplication with\\nO(n3−δ) runtime, which existence is unlikely. If one is uncomfortable with poorly defined notion\\nof combinatorial algorithms, one can apply the reduction to obtain a lowerbound of Ω(nω/2) for\\nHamming distances pattern matching, where 2 ≤ ω < 2.373 is a matrix multiplication exponent.1\\n1Although the issue is that we do not even know whether ω > 2 or not.\\nLater, the complexity of pattern matching under Hamming distance and under ℓ1 distance was\\nproven to be identical (up to polylogarithmic terms) [LUW19, LP08].\\nThe mentioned hardness results serve as a motivation for considering relaxation of the prob-\\nlems, with (1+ε) multiplicative approximation being the obvious candidate. For Hamming distance,\\nKarloff [Kar93] was the first to propose an efficient approximation algorithm with a run time of\\nO( nε2 log3m). The 1ε2 dependency was believed to be inherent, as is the case for e.g. space com-\\nplexity of sketching of Hamming distance, cf. [Woo04, JKS08, CR12]. However, for approximate\\npattern matching that was refuted by Kopelowitz and Porat [KP15, KP18], by providing ran-\\ndomized algorithms with complexity O(nε log n logm log 1ε log |Σ|) and O(nε log n logm) respectively.\\nMoving to ℓ1 distance, Lipsky and Porat [LP11] gave a deterministic algorithm with a run time\\nof O( n\\nε2\\nlogm logU), while later Gawrychowski and Uznański [GU18] have improved the complex-\\nity to a (randomized) O(nε log2 n logm logU), where U is the maximal integer value on the input.\\nAdditionally, we refer the reader to the line of work on other relaxations on exact the distance\\nreporting [ALP04, CFP+16, GU18, ALPU05].\\nA folklore result (c.f. [LP11]) states that the randomized algorithm with a run time of O˜( nε2 ) is\\nin fact possible for any ℓp distance, 0 < p ≤ 2, with use of p-stable distributions and convolution.2\\nSuch distributions exist only when p ≤ 2, which puts a limit on this approach. See [Nol03] for wider\\ndiscussion on p-stable distributions. Porat and Efremenko [PE08] have shown how to approximate\\ngeneral distance functions between pattern and text in time O( nε2 log2m log3 |Σ| logBd), where Bd\\nis upperbound on distance between two characters in Σ. Their solution does not immediately\\ntranslates to ℓp distances, since it allows only for score functions of form\\n∑\\nj d(ti+j , pj) where d\\nis arbitrary metric over Σ. Authors state that their techniques generalize to computation of ℓ2\\ndistances, but the dependency ε−2 in their approach is unavoidable. [LP11] observe that ℓ2 pattern\\nmatching can be in fact computed in O(n logm) time, by reducing it to a single convolution\\ncomputation. This case and analogously case of p = 4, 6, . . . are the only ones where fast and exact\\nalgorithm is known.\\nWe want to point that for ℓ∞ pattern matching there is an approximation algorithm of complex-\\nity O(nε logm logU) by Lipsky and Porat [LP11]. Moving past pattern matching, we want to point\\nthat in a closely related problem of computing (min,+)-convolution there exists O(nε log nε logU)\\ntime algorithm computing (1 + ε) approximation, cf. Mucha et al. [MWW19].\\nTwo questions follow naturally. First, is there a O˜( npoly(ε)) algorithm for ℓp norms pattern\\nmatching when p > 2? Second, is there anything special to p = 0 and p = 1 cases that allows for\\nfaster algorithms, or can we extend their complexities to other ℓp norms? To motivate further those\\nquestions, observe that in the regime of maintaining ℓp sketches in the turnstile streaming model\\n(sequence of updates to vector coordinates), one needs small space of Θ(log n) bits when p ≤ 2 (cf.\\n[KNW10]), while when p > 2 one needs large space of Θ(n1−2/p log n) bits (cf. [Gan15, LW13])\\nmeaning there is a sharp transition in problem complexity at p = 2. Similar phenomenon of\\ntransition at p = 2 is observed for p-stable distributions, and one could expect such transition to\\nhappen in the pattern matching regime as well.\\nIn this work we show that for any constant p ≥ 0 there is an algorithm of complexity O˜(nε ),\\nreplicating the phenomenon of linear dependency on ε−1 from Hamming distance and ℓ1 distance\\nto all ℓp norms. Additionally this provides evidence that no transition at p = 2 happens, and so\\nfar to our understanding cases of p > 2 and p < 2 are of similar hardness.\\n2We use O˜ notation to hide factors polylogarithmic in n, m, |Σ|, U and ε−1.\\n2\\n1.1 Definitions and preliminaries.\\nModel. In most general setting, our inputs are strings taken from arbitrary alphabet Σ. We\\nuse this notation only when structure of alphabet is irrelevant for the problem (e.g. Hamming\\ndistances). However, when considering ℓp distances we focus our attention over an integer alphabet\\n[U ]\\ndef\\n= {0, 1, ..., U − 1} for some U . One can usually assume that U = poly(n), and then logU term\\ncan be safely hidden in the O˜ notation, however we provide the dependency explicitly in Theorem\\nstatements. Even without such assumption, we can assume standard word RAM model, in which\\narithmetic operations on words of size logU take constant time. Otherwise the complexities have\\nan additional logU factor. We also denote u = logU . While we restrict input integer values, we\\nallow intermediate computation and output to consist of floating point numbers having u bits of\\nprecision.\\nDistance between strings. Let X = x1x2 . . . xn and Y = y1y2 . . . yn be two strings. For any\\np > 0, we define their ℓp distance as\\nℓp(X,Y ) =\\n(∑\\ni\\n|xi − yi|p\\n)1/p\\n.\\nParticularly, ℓ1 distance is known as Manhattan distance, and ℓ2 distance is known as Euclidean\\ndistance. Observe that the p-th power of ℓp distance has particularly simpler form of ℓp(X,Y )\\np =∑\\ni |xi − yi|p.\\nThe Hamming distance between two strings is defined as\\nHam(X,Y ) = |{i : xi 6= yi}|.\\nAdopting the convention that 00 = 0 and x0 = 1 for x 6= 0, we observe that (ℓp)p approaches\\nHamming distance as p → 0. Thus Hamming distance is usually denoted as ℓ0 (although (ℓ0)0 is\\nmore precise notation).\\nText-to-pattern distance. For text T = t1t2 . . . tn and pattern P = p1p2 . . . pm, the text-\\nto-pattern distance is defined as an array S such that, for every i, S[i] = d(T [i + 1 .. i + m], P )\\nfor particular distance function d. Thus, for ℓp distance S[i] =\\n(∑m\\nj=1 |ti+j − pj|p\\n)1/p\\n, while for\\nHamming distance S[i] = |{j ∈ {1, . . . ,m} : ti+j 6= pj}|. Then (1 + ε)-approximate distance is\\ndefined as an array Sε such that, for every i, (1− ε) · S[i] ≤ Sε[i] ≤ (1 + ε) · S[i].\\nRounding and arithmetic operations. For any value x, we denote by x(i) = ⌊x/2i⌋ ·2i the\\nvalue with i y bits rounded. However, with a little stretch of notation, we do not limit value of i\\nto be positive. We denote by ‖r‖c the norm modulo c, that is ‖r‖c = min(r mod c, c − (r mod c)).\\n1.2 Our results.\\nIn this paper we answer favorably both questions by providing relevant algorithms. First, we show\\nhow to extend the deterministic ℓ1 distances algorithm into ℓp distances, when p ≥ 1.\\nTheorem 1.1. For any p ≥ 1 there is a deterministic algorithm computing (1 + ε) approximation\\nto pattern matching under ℓp distances in time O(nε logm logU) (assuming ε ≤ 1/p).\\n3\\nWe then move to the case of ℓp distances when p < 1. We show that it is possible to construct\\na randomized algorithm with the desired complexity.\\nTheorem 1.2. For 0 < p < 1, there is a randomized algorithm computing (1 + ε) approximation\\nto pattern matching under ℓp distances in time O(p−1ε−1n logm log2 U log n). The algorithm is\\ncorrect with high probability.3\\nFinally, combining with existing ℓ0 algorithm from [KP18] we obtain as a corollary that for\\nconstant p ≥ 0 approximation of pattern matching under ℓp distances can be computed in O˜(nε )\\ntime.\\n2 Approximation of ℓp distances\\nWe start by showing how convolution finds its use in counting versions of pattern matching, either\\nexact or approximation algorithms. Consider the case of pattern matching under ℓ2 distances.\\nObserve that we are looking for S such that S[i]2 =\\n∑\\nj−k=i(tj−pk)2 =\\n∑\\nj t\\n2\\nj+\\n∑\\nk p\\n2\\nk−2\\n∑\\nj−k=i tjpk.\\nThe last term is just a convolution of vectors in disguise and is equivalent to computing convolution\\nof T and reverse ordered P . Such approach can be applied to solving exact pattern matching via\\nconvolution (observing that ℓ2 distance is 0 iff there is an exact match).\\nWe follow with a technique for computing exact text-to-pattern distance, for arbitrary distance\\nfunctions, introduced by [LP11], which is a generalization of a technique used in [FP74]. We provide\\na short proof for completeness.\\nTheorem 2.1 ([LP11]). Text-to-pattern distance where strings are over arbitrary alphabet Σ can\\nbe computed exactly in time O(|Σ| · n logm).\\nProof. For every letter c ∈ Σ, construct a new text T c by setting T c[i] = 1 if ti = c and T c[i] =\\n0 otherwise. A new pattern P c is constructed by setting P c[i] = d(c, pi). Since d(ti+j , pj) =∑\\nc∈Σ T\\nc[i+ j] · P c[j], it is enough to invoke |Σ| times convolution.\\nTheorem 2.1 allows us to compute text-to-pattern distance exactly, but the time complexity\\nO(|Σ|n logm) is prohibitive for large alphabets (when |Σ| = poly(n)). However, it is enough to\\nreduce the size of alphabet used in the problem (at the cost of reduced precision) to reach desired\\ntime complexity. While this might be hard, we proceed as follows: we decompose our weight\\nfunction into a sum of components, each of which is approximated by a corresponding function on\\na reduced alphabet.\\nWe say that a function d is effectively over smaller alphabet Σ′ if it is represented as d(x, y) =\\nd′(ι1(x), ι2(y)) for some ι1, ι2 : Σ → Σ′ and d′. It follows from Theorem 2.1 that text-to-pattern\\nunder distance d can be computed in time O˜(|Σ′|n) (ignoring the cost of computing ι1 and ι2).\\nDecomposition. Let D(x, y) = |x−y|p be a function corresponding to (ℓp)p distance, that is\\nℓp(X,Y )\\np =\\n∑\\niD(xi, yi). Our goal is to decompose D(x, y) =\\n∑\\ni αi(x, y) into small (polylogarith-\\nmic) number of functions, such that each αi(x, y) is approximated by βi(x, y) that is effectively over\\nalphabet of O(1ε ) size (up to polylogarithmic factors). Now we can use Theorem 2.1 to compute con-\\ntribution of each βi. We then have that G(x, y) =\\n∑\\ni βi(x, y) approximates F , and text-to-pattern\\n3Probability at least 1− 1/nc for arbitrarily large constant c.\\n4\\ndistance under G can be computed in the desired O˜(nε ) time. We present such decomposition,\\nuseful immediately in case of p ≥ 1 and as we see in section 2.2 with a little bit of effort as well in\\ncase when 0 < p ≤ 1.\\nUseful estimations. We use following estimations in our proofs. For p ≥ 1\\n(1− ε)p ≥ 1− pε, for 0 ≤ ε ≤ 1, (1)\\n(1 + ε)p ≥ 1 + pε, for 0 ≤ ε, (2)\\n(1− ε)p ≤ 1− pε(1− 1/e), for 0 ≤ ε ≤ 1/p, (3)\\nap − (a− b)p ≤ pap−1b, for a ≥ b ≥ 0. (follows from 2) (4)\\nFor 0 ≤ p ≤ 1\\n(1− ε)p ≤ 1− pε, for 0 ≤ ε ≤ 1, (5)\\n(1− ε)p ≥ 1− 2pε ln 2, for 0 ≤ ε ≤ 1/2, (6)\\n(1 + ε)p ≥ 1 + pε ln 2, for 0 ≤ ε ≤ 1, (7)\\nap − (a− b)p ≤ 2pap−1b ln 2, for a ≥ 2b ≥ 0. (follows from 6) (8)\\n2.1 Algorithm for p ≥ 1\\nIn this section we prove Theorem 1.1. We start by constructing a family of functions Fi, which are\\nbetter refinements of F as i decreases.\\nFirst step: Let us denote\\nFi(x, y) =\\n(\\nmax(0, |x − y| − 2i)\\n)p\\nand fi = Fi − Fi+1.\\nObserve that Fu = 0 (for 0 ≤ x, y ≤ U). Moreover, there is a telescoping sum Fi =\\nu∑\\nj=i\\nfj. To better\\nsee the the telescopic sum, consider case p = 1. We then represent F−u(x, y) =\\n∑u\\ni=−u fi(x, y) =\\n(−2−u + 2−u+1) + (−2−u+1 + 2−u+2) + . . . + (−2t−1 + 2t) + (|x − y| − 2t) + 0 + . . . + 0. Such\\ndecomposition (for p = 1) was first considered, to our knowledge, in [LP11].\\nSecond step: Instead of using x and y for evaluation of Fi, we evaluate Fi using x and y with\\nall bits younger than i-th one set to zero. Formally, define x(i) = ⌊x/2i⌋ · 2i, y(i) = ⌊y/2i⌋ · 2i. Now\\nwe denote\\nGi(x, y) = Fi(x\\n(i), y(i))\\nSimilarly as for fi, define gi = Gi − Gi+1. Using the same reasoning, we have Gu = 0. For\\nintegers i ≤ 0 the functions Fi and Gi are the same (as we are not rounding) and therefore\\nF−u = G−u =\\nu∑\\ni=−u\\ngi. Intuitively, gi captures contribution of i-th bit of input to the output value\\n(assuming all older bits are set and known, and all younger bits are unknown).\\n5\\nThird step: Let η be a value to be fixed later, depending on ε and p. Assume w.l.o.g. that\\nη is such that 1/η is an integer. We now define ĝi as a refinement of gi, by replacing |x(i) − y(i)|\\nwith ‖x(i) − y(i)‖Bi and |x(i+1) − y(i+1)| with ‖x(i+1) − y(i+1)‖Bi , where Bi = 2i/η, that is doing all\\nthe computation modulo Bi. To be precise, define\\n−→\\nG i(x, y) =\\n(\\nmax(0, ‖x(i) − y(i)‖Bi − 2i)\\n)p\\n←−\\nG i+1(x, y) =\\n(\\nmax(0, ‖x(i+1) − y(i+1)‖Bi − (2i+1)\\n)p\\nand then ĝi =\\n−→\\nG i −←−G i+1. Additionally, we denote for short Ĝi =\\nu∑\\nj=i\\nĝj .\\nIntuitively, ĝi approximates gi in the scenario of limited knowledge – it estimates contribution\\nof i-th bit of input to the output, assuming knowledge of bits i+ 1 to i+ log η−1 of input. We are\\nnow ready to provide an approximation algorithm to (ℓp)\\np text-to-pattern distances.\\nAlgorithm 2.2.\\nInput:\\n• T is the text,\\n• P is the pattern,\\n• η controls the precision of the approximation.\\nSteps:\\n1. For each i ∈ {−u, . . . , u} compute array Si being the text-to-pattern distance between T and\\nP using ĝi distance function (parametrized by η) using Theorem 2.1.\\n2. Output array Sε[i] =\\n(\\nu∑\\nj=−u\\nSj [i]\\n)1/p\\n.\\nTo get the (1 + ε) approximation we run the Algorithm 2.2 with η = ε128 .\\nNow, we need to show the running time and correctness of the result. Firstly, to prove the\\ncorrectness, we divide summands ĝi into three groups and reason about them separately. As\\ncomputing F−u, G−u(by summing fi’s and gi’s respectively) yields (1 + ε) multiplicative error,\\nwe will show that the difference between computing gi and ĝi brings only an additional (1 + ε)\\nmultiplicative error.\\nLemma 2.3. For i such that |x− y| ≤ 2i both gi(x, y) = 0 and ĝi(x, y) = 0.\\nProof. As both gi, ĝi are symmetric functions, we can w.l.o.g. assume x ≥ y. ∀j ≥ i:\\n∣∣∣x(j) − y(j)∣∣∣ = 2j (⌊ x\\n2j\\n⌋\\n−\\n⌊\\ny\\n2j\\n⌋)\\n≤ 2j\\n(⌊\\nx\\n2j\\n⌋\\n−\\n⌊\\nx− 2i\\n2j\\n⌋)\\n≤ 2j .\\nTherefore Gj = 0 from which gi(x, y) = 0 follows. And because ‖x(j) − y(j)‖Bj ≤ |x(j) − y(j)|\\nwe have ĝi(x, y) = 0 as well.\\n6\\nLemma 2.4. For i such that |x− y| > 2i ≥ 4η|x − y| we have gi(x, y) = ĝi(x, y).\\nProof. For gi(x, y) = ĝi(x, y) to hold, it is enough to show that both norms | · | and ‖ · ‖Bi are the\\nsame for x(i) − y(i) and x(i+1)− y(i+1). This happens if the absolute values of the respective inputs\\nare smaller than Bi/2. Let us bound both |x(i) − y(i)| and |x(i+1) − y(i+1)|:\\nmax(|x(i) − y(i)|, |x(i+1) − y(i+1)|) ≤ |x− y|+ 2i+1 ≤ 2i+1(1 + 1\\n8η\\n).\\nWe can w.l.o.g. assume η ≤ 1/8 in order to make 18η a dominant term in the parentheses and\\nreach:\\nmax(|x(i) − y(i)|, |x(i+1) − y(i+1)|) ≤ 2i+1(1 + 1\\n8η\\n) ≤ 2\\ni\\n2η\\n=\\nBi\\n2\\n.\\nTherefore ‖x(i) − y(i)‖Bi = |x(i) − y(i)| as well as ‖x(i+1) − y(i+1)‖Bi = |x(i+1) − y(i+1)| which\\ncompletes the proof.\\nLemma 2.5. If p ≥ 1 then for i such that 4η|x− y| > 2i we have |gi(x, y)| ≤ 2p2i · |x− y|p−1.\\nProof. For the sake of the proof, we will w.l.o.g. assume η ≤ 1/8. Denote A = |x(i) − y(i)|,\\nB = |x(i+1) − y(i+1)|, A′ = max(0, A − 2i) and B′ = max(0, B − 2i+1). Observe that |x− y| − 2i ≤\\nA ≤ |x− y|+ 2i thus |x− y| − 2 · 2i ≤ A′ ≤ |x− y|, and similarly |x− y| − 2 · 2i+1 ≤ B′ ≤ |x− y|\\nso |A′ −B′| ≤ 2 · 2i. Assume w.l.o.g. that A′ ≥ B′. We bound\\n|gi(x, y)| = (A′)p − (A′ − (A′ −B′))p\\n≤ p(A′ −B′)(A′)p−1 (by (4))\\n≤ 2p2i · |x− y|p−1\\nLemma 2.6. If p ≥ 1 then for i such that 4η|x− y| > 2i we have |ĝi(x, y)| ≤ 2p2i · |x− y|p−1.\\nProof. Follows by the same proof strategy as in proof of Lemma 2.5, replacing | · | with ‖ · ‖Bi .\\nTheorem 2.7. Ĝ−u =\\n∑\\ni≥−u\\nĝi approximates F−u up to an additive 32 · p · η · |x− y|p term.\\nProof. We bound the difference between two terms:\\n|F−u(x, y)−\\nu∑\\ni=−u\\nĝi(x, y)| ≤\\nlog2(4η|x−y|)∑\\ni=−u\\n(|ĝi(x, y)| + |gi(x, y)|)\\n≤ 2 ·\\n\\uf8eb\\uf8edlog2(4η|x−y|)∑\\ni=−∞\\n2i\\n\\uf8f6\\uf8f8 · 2 · p · |x− y|p−1\\n≤ 32 · η|x− y| · p · |x− y|p−1\\nwhere the bound follows from Lemma 2.3, 2.4, 2.5 and 2.6.\\nWe now show that F−u is a close approximation of D (recall D(x, y) = |x− y|p).\\nLemma 2.8. For integers x, y there is D(x, y) · (1− (2 ln 2)p/U) ≤ F−u(x, y) ≤ D(x, y).\\n7\\nProof. For x = y the lemma trivially holds, so for the rest of the proof we will assume x 6= y. As\\nx, y are integers only, their smallest non-zero distance is 1. As −u < 0 the |x − y| − 2−u > 0 and\\nwe bound |x− y| · (1− 1/U) ≤ max(0, |x − y| − 2−u) ≤ |x− y|. By (1) (when p ≥ 1) or (6) (when\\np ≤ 1) the claim follows.\\nBy combining Theorem 2.7 with the Lemma 2.8 above we conclude that additive error of\\nAlgorithm 2.2 at each position is (32p · η+ pU ) · |x− y|p = p(ε/4+ 1/U) · |x− y|p ≤ pε|x− y|p (since\\nw.l.o.g. ε ≥ 4/U), thus the relative error is (1 + pε/2).\\nObserve that each ĝi is effectively a function over the alphabet of size Bi/2\\ni = 1/η. Thus, the\\ncomplexity of computing text-to-pattern distance using ĝi distance is O(η−1n logm), and iterating\\nover at most 2u summands makes the total time O(ε−1n logm logU).\\nFinally, since p ≥ 1 and w.l.o.g. ε ≤ 1/p, by (2) and (3) (1+pε/2) approximation of ℓpp distances\\nis enough to guarantee (1 + ε) approximation of ℓp distances.\\n2.2 Algorithm for 0 < p ≤ 1\\nIn this section we prove Theorem 1.2. We note that the algorithm presented in the previous section\\ndoes not work, since in the proof of Lemma 2.5 and 2.6 we used the convexity of function |t|p, which\\nis no longer the case when p < 1.\\nHowever, we observe that Lemma 2.3 and 2.4 hold even when 0 < p ≤ 1. To combat the\\nsituation where adversarial input makes the estimates in Lemma 2.5 and 2.6 to grow too large, we\\nuse a very weak version of hashing. Specifically, we pick at random a linear function σ(t) = r · t,\\nwhere r ∈ [1, 9) is a random independent variable. Such function applied to the input makes its bit\\nsequences appear more ”random” while preserving the inner structure of the problem.\\nConsider a following approach:\\nAlgorithm 2.9.\\n1. Fix η = ε·p15555 logU ln 2 .\\n2. Pick r ∈ [1, 9) uniformly at random.\\n3. Compute T ′ = r · T and P ′ = r · P .\\n4. Use Algorithm 2.2 to compute S′, text-to-pattern distance between T ′ and P ′ using Ĝ−u dis-\\ntance function.\\n5. Output S′′ = S′ · r−1.\\nNow we analyze the expected error made by estimation from Algorithm 2.9. We denote the\\nexpected additive error of estimation of (ℓp)\\np distances as\\nerr(x, y)\\ndef\\n= Er∈[1,9)\\n[\\n[\\n(\\n1\\nr\\n)p ∣∣∣Ĝ−u(rx, ry)− |rx− ry|p∣∣∣ ].\\nTheorem 2.10. The procedure of Algorithm 2.9 has the expected additive error err(x, y) ≤\\nεp\\n3 ln 2 |x− y|p.\\n8\\nProof. Assume that x 6= y, as otherwise the bound trivially follows. We bound the absolute error\\nas follow, denoting k = log(8η|x − y|)).\\nerr(x, y) ≤ Er∈[1,9)\\n[ (1\\nr\\n)p ∣∣∣Ĝ−u(rx, ry)− F−u(rx, ry)∣∣∣ ]\\n+ Er∈[1,9)\\n[\\n|F−u(rx, ry)−D(rx, ry)|\\n]\\n≤ Er∈[1,9)\\n[ ∣∣∣∣∣∣\\nu∑\\ni=−u\\n(ĝi(rx, ry)− gi(rx, ry))\\n∣∣∣∣∣∣\\n]\\n+ Er∈[1,9)\\n[(1\\nr\\n)p\\n2(ln 2)\\np\\nU\\nD(rx, ry)\\n]\\n((1/r)p ≤ 1)\\n≤\\nk∑\\ni=−u\\nEr∈[1,9)\\n[\\n|ĝi(rx, ry)|\\n]\\n+ Er∈[1,9)\\n[ ∣∣∣∣∣∣\\nk∑\\ni=−u\\ngi(rx, ry)\\n∣∣∣∣∣∣\\n]\\n+ 2(ln 2)\\np\\nU\\n|x− y|p (Lemma 2.3, 2.4)\\nNow, we bound the first two summands separately in following lemmas.\\nLemma 2.11. |∑ki=−u gi(rx, ry)| is upper bounded by 32(ln 2)η|x − y|p.\\nProof. Since w.l.o.g. η ≤ 1/32 thus 2k+1 ≤ 1/2 · r|x− y|):\\n∣∣∣∣∣∣\\nk∑\\ni=−u\\ngi(rx, ry)\\n∣∣∣∣∣∣ ≤\\n∣∣∣∣∣∣\\nk∑\\ni=−∞\\ngi(rx, ry)\\n∣∣∣∣∣∣\\n≤ |Gk+1(rx, ry)−D(rx, ry)|\\n≤ ((r|x− y|)p − (r|x− y| − 2k+1)p)\\n≤ rp|x− y|p · 2p(ln 2) 2\\nk+1\\nr|x− y| (by (8))\\n≤ 32(ln 2)η|x − y|p. (rp−1 ≤ 1)\\nLemma 2.12. For i ≤ k = log(8η|x− y|) we have Er∈[1,9)\\n[\\n|ĝi(rx, ry)|\\n]\\n≤ (1152 +192(ln 2)))η|x−\\ny|p.\\nProof. First, we define symbols A,B,A′, B′ to be parts of the ĝi.\\nA = ‖(rx)(i) − (ry)(i)‖Bi\\nB = ‖(rx)(i+1) − (ry)(i+1)‖Bi\\nA′ = max(0, A − 2i)\\nB′ = max(0, B − 2i+1)\\n9\\nRepeating reasoning from proof of Lemma 2.5, we get\\n|A′ −B′| ≤ 2 · 2i\\n‖rx− ry‖Bi − 2 · 2i ≤ A′ ≤ ‖rx− ry‖Bi (9)\\n‖rx− ry‖Bi − 2 · 2i+1 ≤ B′ ≤ ‖rx− ry‖Bi (10)\\nWe also bound Bi = 2\\ni/η ≤ 2k/η = 8|x− y|. Now let’s bound the |ĝi(rx, ry)|. A simple bound that\\ncomes from the definition of ĝi gives us:\\n|ĝi(rx, ry)| = |A′p −B′p| ≤ max(A′p, B′p) ≤ ‖rx− ry‖pBi . (Use of 9,10) (11)\\nUnfortunately, this bound is not tight enough for larger values of ‖rx− ry‖Bi , so for ‖rx− ry‖Bi ≥\\n6 · 2i, we prove stronger bound:\\n|ĝi(rx, ry)| = |(A′)p − (B′)p|\\n= max(A′, B′)p −min(A′, B′)p\\n= max(A′, B′)p − (max(A′, B′)− |A′ −B′|)p\\n= max(A′, B′)p\\n(\\n1−\\n(\\n1− |A\\n′ −B′|\\nmax(A′, B′)\\n)p)\\n≤ ‖rx− ry‖pBi · (1− (1−\\n2 · 2i\\n‖rx− ry‖Bi − 2 · 2i\\n)p)\\n≤ ‖rx− ry‖pBi · (1− (1−\\n3 · 2i\\n‖rx− ry‖Bi\\n)p)\\n≤ 6p(ln 2)‖rx− ry‖p−1Bi · 2i (‖rx− ry‖Bi ≥ 6 · 2i, by (6)) .\\nThe norm function ‖x‖Bi = min(x mod Bi, Bi− (x mod Bi)) is in fact a triangle wave function\\nvarying between 0 and Bi/2 with periodicity of Bi. So if the input is a random variable that\\nfollows uniform distribution at interval that is larger than its period (in our case Bi), the output\\nhas piece-wise uniform distribution, and its probability density function can be bounded by two\\ntimes the probability density function of the uniform distribution for the whole domain. Formally,\\nif X = U(a, b) with b− a ≥ Bi then for Y = ‖X‖Bi its probability density function fY (y) is:\\nfY (y) ≤ 2\\nBi/2\\nfor 0 ≤ y ≤ Bi/2 (12)\\nAs the input in the expression ‖rx−ry‖Bi to the norm function is uniformly distributed between\\na = |x − y| and b = 9|x − y| and Bi ≤ 8|x − y|, we can use 12 to bound the probability density\\nfunction of the Z = ‖rx− ry‖Bi by fZ(y) ≤ 2Bi/2 .\\nNow when we have the approximate probability density function (namely its upper bound) we\\ncan condition on the value of ‖rx− ry‖Bi to be able to use the bounds for small and large values\\nof ‖rx− ry‖Bi .\\nEr∈[1,9)\\n[\\n|ĝi(rx, ry)|\\n]\\n=\\n= Er∈[1,9)\\n[\\n|ĝi(rx, ry)|\\n∣∣∣ ‖rx− ry‖Bi ≤ 6ηBi] Pr\\nr∈[1,9)\\n[\\n‖rx− ry‖Bi ≤ 6ηBi\\n]\\n+\\n+ Er∈[1,9)\\n[\\n|ĝi(rx, ry)|\\n∣∣∣ ‖rx− ry‖Bi > 6ηBi] Pr\\nr∈[1,9)\\n[\\n‖rx− ry‖Bi > 6ηBi\\n]\\n10\\nWe bound those two summands separately. Now, bound on the first part:\\nEr∈[1,9)\\n[\\n|ĝi(rx, ry)|\\n∣∣∣ ‖rx− ry‖Bi ≤ 6ηBi] Pr\\nr∈[1,9)\\n[\\n‖rx− ry‖Bi ≤ 6ηBi\\n]\\n≤\\n≤ Er∈[1,9)\\n[\\n|ĝi(rx, ry)|\\n∣∣∣ ‖rx− ry‖Bi ≤ 6ηBi]24η\\n≤ Er∈[1,9)\\n[\\n‖rx− ry‖pBi\\n∣∣∣ ‖rx− ry‖Bi ≤ 6ηBi]24η (by 11)\\n≤ (6ηBi)p24η\\n≤ 24η(6 · 2i)p\\n≤ 24 · 6η(8η|x − y|)p\\n≤ 1152η|x − y|p\\nAnd on the second part:\\nEr∈[1,9)\\n[\\n|ĝi(rx, ry)|\\n∣∣∣‖rx− ry‖Bi > 6ηBi] Pr\\nr∈[1,9)\\n[\\n‖rx− ry‖Bi > 6ηBi\\n]\\n≤\\n≤ Er∈[1,9)\\n[\\n6p(ln 2)‖rx− ry‖p−1Bi · 2i\\n∣∣∣‖rx− ry‖Bi > 6ηBi]·\\n· Pr\\nr∈[1,9)\\n[\\n‖rx− ry‖Bi > 6ηBi\\n]\\n≤\\n∫ 9\\n1\\n6p(ln 2)‖rx− ry‖p−1Bi · 2i\\n1\\n8\\n· 1[‖rx− ry‖Bi > 6ηBi] dr (1/8 is the\\ndensity of r.v. r,\\n1[·] is the indicator\\nfunction)\\n≤ 6p(ln 2) · 2i\\n∫ Bi/2\\n0\\nzp−1\\n2\\nBi/2\\n1[z > 6ηBi] dz (changed to r.v.\\nz = ‖rx− ry‖Bi)\\n≤ p(ln 2)24\\nBi\\n· 2i\\n∫ Bi/2\\n0\\nzp−1dz\\n≤ (ln 2)24\\nBi\\n· 2i\\n(\\nBi\\n2\\n)p\\n≤ 24(ln 2)Bp−1i · 2i\\n≤ 24(ln 2)2ipη−p+1\\n≤ 24(ln 2)(8η|x − y|)pη−p+1\\n≤ 192(ln 2)η|x− y|p\\nSo finally, we reach:\\nEr∈[1,9)\\n[\\n|ĝi(rx, ry)|\\n]\\n=\\n= Er∈[1,9)\\n[\\n|ĝi(rx, ry)|\\n∣∣∣‖rx− ry‖Bi ≤ 6ηBi] Pr\\nr∈[1,9)\\n[\\n‖rx− ry‖Bi ≤ 6ηBi\\n]\\n+\\n+ Er∈[1,9)\\n[\\n|ĝi(rx, ry)|\\n∣∣∣‖rx− ry‖Bi > 6ηBi] Pr\\nr∈[1,9)\\n[\\n‖rx− ry‖Bi > 6ηBi\\n]\\n≤ 1152η|x − y|p + 192(ln 2)η|x− y|p\\n≤ (1152 + 192(ln 2)))η|x − y|p\\n11\\nBy combining bounds from Lemma 2.11, and Lemma 2.12 we get:\\nerr(x, y) ≤\\nk∑\\ni=−u\\nEr∈[1,9)\\n[\\n|ĝi(rx, ry)|\\n]\\n+ Er∈[1,9)\\n[ ∣∣∣∣∣∣\\nk∑\\ni=−u\\ngi(rx, ry)\\n∣∣∣∣∣∣\\n]\\n+ 2(ln 2)\\np\\nU\\n|x− y|p\\n≤ 2(ln 2) p\\nU\\n|x− y|p + (32(ln 2)η|x − y|p +\\nu∑\\ni=−u\\n(1152 + 192(ln 2)))η|x − y|p\\n≤ 2(ln 2) p\\nU\\n|x− y|p + (32(ln 2)η|x − y|p + 2 logU(1152 + 192(ln 2)))η|x − y|p\\n≤ 2(ln 2) p\\nU\\n|x− y|p + (32(ln 2) + 2 logU(1152 + 192(ln 2))))η|x − y|p\\n≤ 2(ln 2) p\\nU\\n|x− y|p + 2593 logUη|x− y|p\\n≤ εp\\n6 ln 2\\n|x− y|p + εp\\n6 ln 2\\n|x− y|p w.l.o.g. ε ≥ 12(ln 2)\\n2\\nU\\n≤ εp\\n3 ln 2\\n|x− y|p\\nTo finish the proof of Theorem 1.2 we observe, that for any position i of output, Algorithm 2.9\\noutputs S′′[i] such that Er[|(S′′[i])p − (S[i])p|] ≤ pε3 ln 2 · (S[i])p. By Markov’s inequality it means\\nthat with probability 2/3 the relative error of (ℓp)\\np approximation is at most pln 2 · ε. Thus, by\\n(5) and (7) relative error of ℓp approximation is ε with probability at least 2/3. Now a standard\\namplification procedure follows: invoke Algorithm 2.9 independently t times and take the median\\nvalue from S′′(1)[i], . . . S\\n′′\\n(t)[i] as the final estimate Sε[i]. Taking t = Θ(log n) to be large enough\\nmakes the final estimate good with high probability, and by the union bound whole Sε is a good\\nestimate of S. The complexity of the whole procedure is thus O(log n · logU · η−1 · n logm) =\\nO(p−1ε−1n logm log2 U log n).\\n3 Hamming distances\\nAs a final note we comment on a particularly simple form that Algorithm 2.9 takes for Hamming\\ndistances (limit case of p = 0).\\nĝi(x, y) =\\n{\\n1 if ‖x(i) − y(i)‖Bi = 1\\n0 otherwise,\\nwith Algorithm being simply: pick at random r ∈ [1, 9], apply it multiplicatively to the input,\\ncompute text-to-pattern distance using\\n∑\\ni ĝi function.\\nTaking a limit of p→ 0 in proof of Theorem 1.2, we reach that bound from Lemma 2.12 becomes\\nEr∈[1,9)\\n[\\n|ĝi(rx, ry)|\\n]\\n≤ 24η\\nand since all other terms in error estimate have multiplicative term p in front, we reach\\nerr(x, y) ≤ 2 logU · Er∈[1,9)\\n[\\n|ĝi(rx, ry)|\\n]\\n≤ 48η logU.\\nWe thus observe that expected relative error in estimation of Hamming distance is: E[S′′[i]−S[i]] ≤\\n48η logU · S[i]. With probability at least 2/3 the relative error is at most 144η logU . Setting\\n12\\nη = ε144 logU and repeating the randomized procedure Θ(logn) with taking median for concentration\\ncompletes the algorithm. The total runtime is, by a standard trick of reducing alphabet size to\\n2m, O(nε log2m log n), and while it compares unfavorably to algorithm from [KP18] (in terms of\\nruntime), it gives another insight on why O˜(n/ε) time algorithm is possible for Hamming distance\\nversion of pattern matching.\\nReferences\\n[Abr87] Karl R. Abrahamson. Generalized string matching. SIAM J. Comput., 16(6):1039–1051,\\n1987.\\n[ALP04] Amihood Amir, Moshe Lewenstein, and Ely Porat. Faster algorithms for string matching\\nwith k mismatches. J. Algorithms, 50(2):257–275, 2004.\\n[ALPU05] Amihood Amir, Ohad Lipsky, Ely Porat, and Julia Umanski. Approximate matching\\nin the L1 metric. In CPM, pages 91–103, 2005.\\n[CCI05] Peter Clifford, Raphaël Clifford, and Costas S. Iliopoulos. Faster algorithms for δ,γ-\\nmatching and related problems. In CPM, pages 68–78, 2005.\\n[CFP+16] Raphaël Clifford, Allyx Fontaine, Ely Porat, Benjamin Sach, and Tatiana A.\\nStarikovskaya. The k-mismatch problem revisited. In SODA, pages 2039–2052. SIAM,\\n2016.\\n[Cli09] Raphaël Clifford. Matrix multiplication and pattern matching under Hamming norm.\\nhttp://www.cs.bris.ac.uk/Research/Algorithms/events/BAD09/BAD09/Talks/BAD09-Hammingnotes.pdf,\\n2009. Retrieved March 2017.\\n[CR12] Amit Chakrabarti and Oded Regev. An optimal lower bound on the communication\\ncomplexity of gap-hamming-distance. SIAM J. Comput., 41(5):1299–1317, 2012.\\n[FP74] M. J. Fischer and M. S. Paterson. String-matching and other products. Technical report,\\n1974.\\n[Gan15] Sumit Ganguly. Taylor polynomial estimator for estimating frequency moments. In\\nICALP, pages 542–553, 2015.\\n[GU18] Paweł Gawrychowski and Przemysław Uznański. Towards unified approximate pattern\\nmatching for hamming and L1 distance. In ICALP, pages 62:1–62:13, 2018.\\n[JKS08] T. S. Jayram, Ravi Kumar, and D. Sivakumar. The one-way communication complexity\\nof hamming distance. Theory of Computing, 4(1):129–135, 2008.\\n[Kar93] Howard J. Karloff. Fast algorithms for approximately counting mismatches. Inf. Process.\\nLett., 48(2):53–60, 1993.\\n[KNW10] Daniel M. Kane, Jelani Nelson, and David P. Woodruff. On the exact space complexity\\nof sketching and streaming small norms. In SODA, pages 1161–1178, 2010.\\n13\\n[KP15] Tsvi Kopelowitz and Ely Porat. Breaking the variance: Approximating the hamming\\ndistance in 1/ǫ time per alignment. In FOCS, pages 601–613, 2015.\\n[KP18] Tsvi Kopelowitz and Ely Porat. A simple algorithm for approximating the text-to-\\npattern hamming distance. In SOSA, pages 10:1–10:5, 2018.\\n[Lip03] Ohad Lipsky. Eficient distance computations. Master’s thesis, 2003.\\n[LP08] Ohad Lipsky and Ely Porat. L1 pattern matching lower bound. Inf. Process. Lett.,\\n105(4):141–143, 2008.\\n[LP11] Ohad Lipsky and Ely Porat. Approximate pattern matching with the L1, L2 and L∞\\nmetrics. Algorithmica, 60(2):335–348, 2011.\\n[LUW19] Karim Labib, Przemysław Uznański, and Daniel Wolleb-Graf. Hamming distance com-\\npleteness. In CPM, pages 14:1–14:17, 2019.\\n[LW13] Yi Li and David P. Woodruff. A tight lower bound for high frequency moment estimation\\nwith small error. In APPROX-RANDOM, pages 623–638, 2013.\\n[MWW19] Marcin Mucha, Karol Węgrzycki, and Michał Włodarczyk. A subquadratic approxima-\\ntion scheme for partition. In SODA, pages 70–88, 2019.\\n[Nol03] John Nolan. Stable distributions: models for heavy-tailed data. Birkhauser New York,\\n2003.\\n[PE08] Ely Porat and Klim Efremenko. Approximating general metric distances between a\\npattern and a text. In SODA, pages 419–427, 2008.\\n[Woo04] David P. Woodruff. Optimal space lower bounds for all frequency moments. In SODA,\\npages 167–175, 2004.\\n14\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd23'), 'authors': 'Eppstein, David, Har-Peled, Sariel, Sidiropoulos, Anastasios', 'year': '2015', 'title': 'Approximate Greedy Clustering and Distance Selection for Graph Metrics', 'full_text': 'Approximate Greedy Clustering and Distance Selection for\\nGraph Metrics\\nDavid Eppstein∗ Sariel Har-Peled† Anastasios Sidiropoulos‡\\nJuly 7, 2015\\nAbstract\\nIn this paper, we consider two important problems defined on finite metric spaces, and provide\\nefficient new algorithms and approximation schemes for these problems on inputs given as graph\\nshortest path metrics or high-dimensional Euclidean metrics. The first of these problems is the\\ngreedy permutation (or farthest-first traversal) of a finite metric space: a permutation of the\\npoints of the space in which each point is as far as possible from all previous points. We describe\\nrandomized algorithms to find (1 + ε)-approximate greedy permutations of any graph with n vertices\\nand m edges in expected time O\\n(\\nε−1(m + n) log n log(n/ε)\\n)\\n, and to find (1 + ε)-approximate greedy\\npermutations of points in high-dimensional Euclidean spaces in expected time O(ε−2n1+1/(1+ε)2+o(1)).\\nAdditionally we describe a deterministic algorithm to find exact greedy permutations of any graph\\nwith n vertices and treewidth O(1) in worst-case time O(n3/2 logO(1) n). The second of the two\\nproblems we consider is distance selection: given k ∈ q(n2)y, we are interested in computing the\\nkth smallest distance in the given metric space. We show that for planar graph metrics one can\\napproximate this distance, up to a constant factor, in near linear time.\\n1. Introduction\\nIn this paper we are interested in several important algorithmic problems on finite metric spaces, including\\nthe construction of greedy permutations, the problem of selecting the kth distance among all pairs of\\npoints in the space, and the problem of counting the number of points in a metric ball. These problems\\nhave known polynomial time algorithms (for instance, the kth distance may be found by applying a\\nselection algorithm to the coefficients of the distance matrix); however, we are interested in algorithms\\nthat scale well to large data sets, so we seek algorithms that take subquadratic time (substantially\\nsmaller than the time to list all distances). To achieve this, we require the metric space to be defined\\nimplicitly, for instance as the distances in a sparse weighted graph or as the distances among points in a\\nEuclidean metric space. Despite the increased difficulty of working with implicit metrics, we show that\\nthe problems we study can be solved efficiently.\\n∗Computer Science Dept., Univ. of California, Irvine; eppstein@uci.edu; http://www.ics.uci.edu/~eppstein. Work\\nsupported in part by the National Science Foundation under grants 0830403 and 1217322, and by the Office of Naval\\nResearch under MURI grant N00014-08-1-1015.\\n†Department of Computer Science; University of Illinois; 201 N. Goodwin Avenue; Urbana, IL, 61801, USA;\\nsariel@illinois.edu; http://sarielhp.org. Work on this paper was partially supported by a NSF AF awards\\nCCF-0915984 and CCF-1217462.\\n‡Dept. of Computer Science and Engineering and Dept. of Mathematics, The Ohio State University, Columbus, OH 43210;\\nsidiropoulos.1@osu.edu; http://sidiropoulos.org. Supported in part by David and Lucille Packard Fellowship, NSF\\nAF award CCF-0915984, and NSF grants CCF-0915519 and CCF-1423230.\\n1\\nar\\nX\\niv\\n:1\\n50\\n7.\\n01\\n55\\n5v\\n1 \\n [c\\ns.C\\nG]\\n  6\\n Ju\\nl 2\\n01\\n5\\nFigure 1: Left: The first five points of a greedy permutation. Each point is as far as possible from all\\npreviously chosen points. Right: The six red points at the centers of the disks form an r-net of the set V\\nof red and white points, where r is the radius of the large yellow disks. The darker disks of radius r/2\\nare disjoint from each other, and the disks of radius r together cover all of V .\\n1.1. Greedy permutation\\nIn the first sections of this paper we are interested in an ordering problem on metric spaces: the\\nconstruction of greedy permutations. We solve this problem exactly and approximately, for the shortest\\npath metrics of sparse weighted graphs and for high-dimensional Euclidean spaces.\\nA permutation Π = 〈pi1, pi2, . . .〉 of the vertices of a metric space (V, d) is a greedy permutation\\n(also called a farthest-first traversal or farthest point sampling) if each vertex pii is the farthest in V from\\nthe set Πi−1 = {pi1, . . . , pii−1} of preceding vertices (Figure 1, left). Greedy permutations were introduced\\nby Rosenkrantz et al. [RSL77] for the “farthest insertion” traveling salesman heuristic, and used by\\nGonzalez [Gon85] to 2-approximate the k-center. Different prefixes of the greedy permutation provide\\ndifferent multi-resolution clusterings of the input point set; see Figure 2p3.\\nGreedy permutations are closely related to another concept for finite metric spaces, r-nets. An r-net\\nfor the metric space (V, d) and the numerical parameter r is a subset N of the points of V such that\\nno two of the points of N are within distance r of each other, and such that every point of V is within\\ndistance r of a point of N . Equivalently, the closed r/2-balls centered at the points of N are disjoint,\\nand the closed r-balls around the same points cover all of V (Figure 1, right). Each prefix of a greedy\\npermutation is an r-net, for r equal to the minimum distance between points in the prefix, and for every\\nr an r-net may be obtained as a prefix of a greedy permutation1.\\nGreedy permutations may be computed for metric spaces in O(n2) time, and for graphs in the same\\ntime as all pairs shortest paths, by a naive algorithm (Section 2) that maintains the distances of all points\\nfrom the selected points. The only previous improvement on the naive algorithm, by Har-Peled and\\nMendel [HM06] defines a concept of approximation for greedy permutations that we will also use. They\\nshowed that (1 + ε)-greedy permutations can be computed in O(n log n) time in metric spaces with\\nconstant doubling dimension; these are permutations Π = 〈pi1, pi2, . . .〉 for which there exists a sequence\\nof numbers r1 ≥ r2 ≥ · · · such that\\n(A) the maximum distance of a point of V from Πi is in the range\\n[\\nri, (1 + ε)ri\\n]\\n, and\\n(B) the distance between every two points u, v ∈ Πi is at least ri.\\nIn this paper we give approximation schemes for metric spaces defined by sparse graphs, and high-\\ndimensional Euclidean spaces, neither of which have constant doubling dimension. Greedy permutations\\nfor graph distances were previously mentioned by Gu et al. [GJG11], in connection with an application\\n1The notion of nets is closely related to congruent disc packing, which was studied in the end of the 19th century by\\nThue (see [PA95, Chapter 3]). It is however natural to assume that the concept is much older, as it is related to numerical\\nintegration and discrepancy [Mat99].\\n2\\n(A) (B) (C) (D)\\n(E) (F) (G)\\nFigure 2: (A) A point set. (B)–(G) The different representations of this point set as a union of balls, as\\nprovided by different prefixes of the greedy permutation. As this demonstrates, one can think about a\\nprefix of the greedy permutation as a partial representation of the point set that keeps improving as the\\nprefix used gets longer.\\nin molecular dynamics, but rejected by them because the naive algorithm was too slow.\\nOne reason for interest in greedy or (1 + ε)-greedy permutations is that, in a single structure, they\\napproximate an optimal clustering for all possible resolutions. Specifically, the prefix of the first k vertices\\nin such a permutation, provides, for any k, a 2(1 + ε)-approximation to the optimal k-center clustering\\nof G. (See Lemma 2.1 for an easy proof of this.) The k-center problem may be 2-approximated in\\nO(kn) time by computing the first k vertices of a greedy permutation, and is NP-hard to approximate\\nto a ratio better than 2 [Gon85].2 For points in Euclidean spaces of bounded dimension a linear-time\\n2-approximation is known [Har04, HR13]. Thorup [Tho05] provided a fast k-center approximation for\\ngraph shortest path metrics with a single choice of k. The k-center problem may be solved exactly on\\ntrees and cactus graphs in O(n log n) time [FJ83]. Voevodski et al. [VBR+10] use an algorithm closely\\nrelated to greedy permutation to approximate a different clustering problem, k-medians.\\nIntuitively, every prefix of a greedy permutation is as informative as possible about the whole\\nset, so greedy permutations form a natural ordering in which to stream large data sets. Because\\nof these properties, greedy permutations have many additional applications, including color quanti-\\nzation [Xia97], progressive image sampling [ELPZ97], selecting landmarks of probabilistic roadmaps\\nfor motion planning [MAB98], point cloud simplification [MD03], halftone mask generation [SMR04],\\nhierarchical clustering [DL05], detecting isometries between surface meshes [LF09], novelty detection\\nand time management for autonomous robot exploration [GGD12], industrial fault detection [AYE12],\\nand range queries seeking diverse sets of points in query regions [AAYI+13].\\n2Another 2-approximation for the k-center by Hochbaum and Shmoys [HS85] is often erroneously credited as being the\\norigin of the farthest-first traversal method, but actually uses a different algorithm.\\n3\\n1.2. Distance selection and approximate range counting\\nThe distance selection problem, in computational geometry, has as input a set of points in Rd; the output\\nis the kth smallest distance defined by a pair of points of P . It is believed that such exact distance\\nselection requires Ω\\n(\\nn4/3\\n)\\ntime in the worst case [Eri95], even in the plane (in higher dimensions the bound\\ndeteriorates). Recently, Har-Peled and Raichel [HR13] provided an algorithm that (1 + ε)-approximates\\nthis distance in O(n/εd) time.\\nWe are interested in solving the problem for the finite metric case. Specifically, consider a shortest\\npath metric defined over a graph G with n vertices and m edges. Given k ∈ q(n\\n2\\n)y\\n=\\n{\\n1, . . . ,\\n(\\nn\\n2\\n)}\\n, we\\nwould like to compute the kth smallest distance in this shortest path metric. This problem was studied\\nfor trees [MTZC81], where Frederickson and Johnson [FJ83] provided a beautiful algorithm that works\\nby using tree separators, and selection in sorted matrices [FJ84].\\nThe “dual” problem to distance selection is distance counting. Here, given a distance r, the task is to\\ncount the number of pairs of points of P that are of distance ≤ r. While the problems are essentially\\nequivalent in the exact case, approximate distance counting seems to be significantly easier than selection.\\nThroughout this paper, G = (V,E) will denote an undirected graph with n vertices and m edges,\\nwith non-negative edge weights that obey the triangle inequality. The shortest path distances in G\\ninduce a metric d. Specifically, for any u, v ∈ V , let dG(u, v) denotes the shortest path between u and v\\nin G. Given a graph G = (V,E), and a query distance r, the set of r-short pairs is\\nP≤r =\\n{\\n{u, v} ⊆ V\\n∣∣∣ u 6= v and dG(u, v) ≤ r} . (1)\\nIn the distance counting problem, the task is to compute (or approximate) |P≤r|. In the distance\\nselection problem, given k ∈ q(n\\n2\\n)y\\n, the task is to compute (or approximate) the smallest r such that\\n|P≤r| ≥ k.\\nDistance counting is easy to approximate using known techniques, since this problem is malleable\\nto random sampling, see [Coh14] and references therein. However, approximate distance selection is\\nsignificantly harder, as random sampling can not be used in this case – indeed, trying to use approximate\\ndistance counting (or sketches approach as in [Coh14]), may result in an arbitrarily bad approximation to\\nthe kth distance, if the (k − 1)th and (k + 1)th distance are significantly smaller and larger, respectively,\\nthan the kth distance (or similar sparse scenarios).\\n1.3. New results\\nGreedy permutation for sparse graphs. In Section 2, we show that an (1 + ε)-greedy permutation\\ncan be found for graphs with n vertices and m edges in time O(ε−1m log n log(n/ε)) = O˜(m)3.\\nApproximate greedy permutation for high dimensional Euclidean space. In Section 3, we\\nshow that an approximate greedy permutation can be computed for a set of points in high-dimensional\\nEuclidean space. The algorithm runs in subquadratic time and has polynomial dependency on the\\ndimension. Our approximations are based on finding r-nets (or in the Euclidean case approximate r-nets)\\nfor a geometric sequence of values of r\\nIn an earlier paper [HIS13], the authors showed that for high dimensional point sets one can get\\na sparse spanner. Applying the above algorithm for sparse graphs to this spanner yields a greedy\\npermutation, but with significantly weaker bounds, as the stretch in the constructed spanner is at lease 2.\\n3The O˜ notation hides logarithmic factors in n and polynomial terms in 1/ε. We assume throughout the paper that\\nn = O(m).\\n4\\nExact greedy permutation for bounded tree-width. In Section 4, we show how to find an exact\\ngreedy permutation for graphs of bounded treewidth, in time O˜\\n(\\nn3/2\\n)\\n, by partitioning the input graph\\ninto small subgraphs separated from the rest of the graph by O(1) vertices, and by using an orthogonal\\nrange searching data structure in each subgraph to find the farthest vertex from the already-selected\\nvertices.\\nDistance selection in planar graphs. In Section 5, we show how to O(1)-approximate the kth\\ndistance in a planar graph G. Specifically, given k, the algorithm computes in near linear time, a number\\nα, such that the kth shortest distance in G is at least α, and at most O(α). This algorithm uses a planar\\nseparator and distance oracles in an interesting way to count distances.\\n2. Approximate greedy permutation on a sparse graph\\nWe are interested in approximating the greedy permutation for a graph G. Among other motivations,\\nthis provides a good approximation for k-center clustering:\\nLemma 2.1. If Π is a (1 + ε)-greedy permutation of M = (V, d), then, for all k, Πk provides a\\n2(1 + ε)-approximation to the optimal k-center clustering and minimax diameter k-clustering of M.\\nProof: By Property (A) of such a permutation (see Section 1.1), all points of V can be covered by balls of\\nradius (1 + ε)rk centered at pi1, . . . , pik; these balls have diameter ≤ 2(1 + ε)rk. Let S = Πk ∪ {v}, where\\nv is the farthest point in V from Πk. By the definition of ri and by Property (B) of these permutations,\\nevery two points in S have distance at least ri, so no k clusters of radius smaller than ri/2 or diameter\\nsmaller than ri can cover the k + 1 points in S.\\nA naive algorithm for computing the greedy permutation maintains for each vertex v its distance `v\\nto the set of centers picked so far, and uses these distances as priorities in a max-heap, which it uses\\nto select each successive center, using Dijkstra’s algorithm to update the distances after each center is\\npicked. There are n instantiations of Dijkstra’s algorithm, taking time O(n(m+ n log n)).\\nTo improve performance, we may avoid adding a vertex v to the min-heap used within Dijkstra’s\\nalgorithm unless its tentative distance is smaller than `v, preventing the expansion of vertices for which\\nthe distance from vi is no smaller than `v. This idea does not immediately improve the worst-case\\nrunning time of the algorithm but will be important in our approximation algorithm.\\n2.1. Computing an r-net in a sparse graph\\nWe compute an r-net in a sparse graph using a variant of Dijkstra’s algorithm with the sequence of\\nstarting vertices chosen in a random permutation. A similar idea was used by Mendel and Schwob\\n[MS09] for a different problem; however, using this method for our problem involves a more complicated\\nanalysis.\\nLet G = (V,E) be a weighted graph with n vertices and m edges, let r > 0, and let pii be the ith\\nvertex in a random permutation of V . For each vertex v we initialize δ(v) to +∞. In the ith iteration,\\nwe test whether δ(pii) ≥ r, and if so we do the following steps:\\n1. Add pii to the resulting net N .\\n2. Set δ(pii) to zero.\\n3. Perform Dijkstra’s algorithm starting from pii, modified as in Section 2to avoid adding a vertex u\\nto the priority queue unless its tentative distance is smaller than the current value of δ(u). When\\nsuch a vertex u is expanded, we set δ(u) to be its computed distance from pii, and relax the edges\\nadjacent to u in the graph.\\n5\\nThe difference from the algorithm of Mendel and Schwob is that their algorithm initiates an instance of\\nDijkstra’s algorithm starting from every vertex pii, whereas we do so only when δ(pii) ≥ r.\\nLemma 2.2. The set N is an r-net in G.\\nProof: By the end of the algorithm, each v ∈ V has δ(v) < r, for δ(v) is monotonically decreasing, and if\\nit were larger than r when v was visited then v would have been added to the net.\\nAn induction shows that if ` = δ(v), for some vertex v, then the distance of v to the set N is at most\\n`. Indeed, for the sake of contradiction, let j be the (end of) the first iteration where this claim is false.\\nIt must be that pij ∈ N , and it is the nearest vertex in N to v. But then, consider the shortest path\\nbetween pij and v. The modified Dijkstra must have visited all the vertices on this path, thus computing\\nδ(v) correctly at this iteration, which is a contradiction.\\nFinally, observe that every two points in N have distance ≥ r. Indeed, when the algorithm handles\\nvertex v ∈ N , its distance from all the vertices currently in N is ≥ r, implying the claim.\\nLemma 2.3. Consider an execution of the algorithm, and any vertex v ∈ V . The expected number of\\ntimes the algorithm updates the value of δ(v) during its execution is O(log n), and more strongly the\\nnumber of updates is O(log n) with high probability.\\nProof: For simplicity of exposition, assume all distances in G are distinct. Let Si be the set of all the\\nvertices x ∈ V , such that the following two properties both hold:\\n(A) d(x, v) < d(v,Πi), where Πi = {pi1, . . . , pii}.\\n(B) If pii+1 = x then δ(v) would change in the (i+ 1)th iteration.\\nLet si = |Si|. Observe that S1 ⊇ S2 ⊇ · · · ⊇ Sn, and |Sn| = 0.\\nIn particular, let Ei+1 be the event that δ(v) changed in iteration (i+ 1) – we will refer to such an\\niteration as being active . If iteration (i+ 1) is active then one of the points of Si is pii+1. However, pii+1\\nhas a uniform distribution over the vertices of Si, and in particular, if Ei+1 happens then si+1 ≤ si/2,\\nwith probability at least half, and we will refer to such an iteration as being lucky . (It is possible that\\nsi+1 < si even if Ei+1 does not happen, but this is only to our benefit.) After O(log n) lucky iterations\\nthe set Si is empty, and we are done. Clearly, if both the ith and jth iteration are active, the events\\nthat they are each lucky are independent of each other. By the Chernoff inequality, after c log n active\\niterations, at least dlog2 ne iterations were lucky with high probability, implying the claim. Here c is a\\nsufficiently large constant.\\nInterestingly, in the above proof, all we used was the monotonicity of the sets S1, . . . , Sn, and the\\nfact that if δ(v) changes in an iteration then the size of the set Si shrinks by a constant factor with good\\nprobability in this iteration. This implies that there is some flexibility in deciding whether or not to\\ninitiate Dijkstra’s algorithm from each vertex of the permutation, without damaging the number of times\\nof the values of δ(v) are updated. We will use this flexibility later on.\\nLemma 2.4. Given a graph G = (V,E), with n vertices and m edges, the above algorithm computes an\\nr-net of G in O((n+m) log n) expected time.\\nProof: By Lemma 2.3, the two δ values associated with the endpoints of an edge get updated O(log n)\\ntimes, in expectation, during the algorithm’s execution. As such, a single edge creates O(log n) decrease-\\nkey operations in the heap maintained by the algorithm. Each such operation takes constant time if we\\nuse Fibonacci heaps to implement the algorithm.\\n6\\n2.2. An approximation whose time depends on the spread\\nGiven a finite metric space (V, d) defined over a set V , its spread is the ratio between the maximum\\nand minimum distance in the metric; formally,\\nspread(d) = max\\nu,v∈V,u6=v\\ndG(u, v)/ min\\nu,v∈V,u6=v\\ndG(u, v).\\nLet graph G = (V,E) and ε > 0 be given. Assume for now that the minimum edge length is 1, and\\nthat the diameter of G is at most ∆. Set ri = ∆/(1 + ε)\\ni−1, for i = 1, . . . ,M =\\n⌈\\nlog1+ε ∆\\n⌉\\n. We compute\\na sequence of nets in a sequence of iterations. In the first iteration, compute an r1-net N1 of G, using\\nLemma 2.4. In the beginning of the ith iteration, for i > 1, let Si = ∪j<iNi. Using Dijkstra, mark\\nas used all vertices within distance ri of Si. Compute an ri-net Ni in G, modifying the algorithm of\\nLemma 2.4 by disallowing used vertices from being considered as net points.\\nAfter completing these computations, combine the vertices into a single permutation in which the\\nvertices of Ni form the ith contiguous block. Within this block, the ordering of the vertices of Ni is\\narbitrary. The following is easy to verify, and we omit the easy proof.\\nLemma 2.5. Let N = N1 ∪ · · · ∪ Nt, for an arbitrary t. Then the distance of every vertex v ∈ G from\\nN is at most rt, and the distance between any pair of vertices of the net is at least rt.\\nThat is, the net computed by the tth iteration is a “perfect” rt net. In between such blocks, it might\\nbe less then perfect. Formally, we have the following (again, we omit the relatively easy proof).\\nLemma 2.6. Let pi be the permutation computed by the above algorithm, and consider the ith vertex pii\\nin this permutation. Assume that pii ∈ Nt. Then we have the following guarantees:\\n(A) The distance between any two vertices of Πi = {pi1, . . . , pii} is at least rt.\\n(B) The distance of any vertex of v ∈ V from Πi, is at most rt−1 = (1 + ε)rt.\\nLemma 2.7. Let G = (V,E) be a graph, let ε > 0, and let Φ be the spread of G. Then, one can compute\\na (1 + ε)-greedy permutation in O(ε−1(n+m) log n log Φ) expected time.\\nProof: A 2-approximation to the diameter of G can be found by running Dijkstra’s algorithm from an\\narbitrary starting vertex. The minimum distance in G is achieved by an edge (since the edge weights\\nare positive), so it can be computed in linear time. After scaling, we can use the above algorithm, with\\nM = O\\n(\\nlog1+ε Φ\\n)\\n= O(ε−1 log Φ) iterations, each using a modified version of the algorithm of Section 2.1.\\nIt is straightforward to modify the analysis to show that the each such iteration takes O((n+m) log n)\\ntime in expectation.\\n2.3. Eliminating the dependence on the spread\\nAn arbitrary graph G may not have small enough spread to apply the previous algorithm directly. In\\nthis case, following by-now standard methods for eliminating the dependence on spread (see Section 4\\nof [MS09]), we simulate the algorithm more efficiently, using a value of ε smaller by a constant factor to\\nmake up for some additional approximation in our simulation.\\nConsider an iteration of the above algorithm for distance ri. Edges longer than nri can be ignored or\\n(conceptually) deleted, as they cannot be used by the r-net algorithm of Section 2.1. Similarly, edges of\\nlength O(εri/n\\n2) can be collapsed and treated as having length zero. Thus, an edge e of length ` is active\\nwhen ri is in the interval\\n[\\ncε`\\nn2\\n, `n\\n]\\n, which happens for O\\n(\\nlog1+ε(n\\n3/ε)\\n)\\n= O(ε−1 log(n/ε)) iterations. Let\\nmi be the number of active edges in the ith iteration, and let Gi be the resulting graph, in which all\\n7\\nthe edges of lengths ≤ εri/n2 are contracted (the resulting super vertex is identified with one of the\\noriginal vertices), and all the edges of length > rin are removed. Any singleton vertex in this graph is\\nnot relevant for computing the permutation in this resolution, and it can be ignored. The running time\\nof Lemma 2.4 on Gi is O(mi logmi).\\nWhen the algorithm moves to the next iteration, it needs to introduce into Gi all the new edges that\\nbecome active. Using a careful implementation, this can be done in O(1) amortized time, for any newly\\nintroduced edge. Similarly, edges that become inactive should be deleted. Of course, if there are no\\nactive edges, the algorithm can skip directly to the next resolution. This can be easily done, by putting\\nthe edges into a heap, sorted by their length, and adding the edges and removing them as the algorithm\\nprogresses down the resolutions.\\nThe overall expected running time of this algorithm is O(\\n∑\\nimi logmi +m logm). However, since ev-\\nery edge is active inO(ε−1 log(n/ε)) iterations, we get that the expected running time isO(ε−1m log n log(n/ε)).\\nWe thus get the following.\\nTheorem 2.8. Given a non-negatively weighted graph G = (V,E), with n vertices and m edges, and\\na parameter ε > 0, one can compute a (1 + ε)-approximate greedy permutation for G in expected time\\nO(ε−1m log n log(n/ε)).\\n2.4. k-center clustering for bounded spread with integer weights\\nOur greedy permutation algorithm for sparse graphs leads to a fast (2 + \\x0f)-approximation to the k-center\\nproblem for graph metrics. In this case, it is possible to eliminate the dependence on \\x0f, giving a\\n2-approximation (best possible as achieving a smaller approximation ratio is NP-hard).\\nTheorem 2.9. Let G be a graph with n vertices and m edges, with positive integer weights on the edges,\\nand spread Φ. Given k, one can compute a 2-approximation to the optimal k-center clustering of G, in\\nO(m log n log Φ) time both in expectation and with high probability.\\nProof: Using Dijkstra’s algorithm starting from an arbitrary vertex in G, compute, in O(m+ n log n)\\ntime, a number ∆, such that diam(G) ≤ ∆ ≤ 2diam(G). We next perform a binary search for the radius\\nropt of the optimal k-center clustering of G, in the range 1, . . . ,∆.\\nGiven a candidate radius x, let r = 2x and compute an r-net of G using the algorithm of Lemma 2.4.\\nIt is easy to verify that if x ≥ ropt, then the resulting net N has at most k vertices in it. Indeed, consider\\nall the vertices in G assigned to a single cluster C in the optimal k-center clustering, and observe that\\ndiam(C) ≤ 2ropt ≤ r. Therefore, at most one vertex of C may belong to any r-net, so every r-net for\\nthis value of r has at most k vertices. On the other hand, if x is too small then the resulting r-net has\\nmore then k vertices.\\nThus, using the r-net procedure of Lemma 2.4 as a decider in a binary search yields the desired\\napproximation algorithm.\\n3. Approximate greedy permutation on Euclidean metrics\\n3.1. Approximate nets\\nLemma 3.1 (Johnson–Lindenstrauss lemma [JL84]). For any ε > 0, every set of n points in\\nEuclidean space admits an embedding into (RO(logn/ε2), ‖ · ‖2), with distortion 1 + ε.\\nDefinition 3.2. Let H be a family of hash functions mapping Rd to some universe U . We say that H is\\n(δ, cδ, p1, p2)-sensitive if for any x, y ∈ Rd it satisfies the following properties:\\n8\\n(A) If ‖x− y‖2 ≤ δ then Prh∈H [h(x) = h(y)] ≥ p1.\\n(B) If ‖x− y‖2 ≥ cδ then Prh∈H [h(x) = h(y)] ≤ p2.\\nLemma 3.3 (Andoni & Indyk [AI06]). For any δ > 0, dimension d > 0, and c > 1, there exists a(\\nδ, cδ, 1/n1/c\\n2+o(1), 1/n\\n)\\n-sensitive family of hash functions for Rd, where every function can be evaluated\\nin time O\\n(\\nd · no(1)).\\nWe extend the notion of nets to c-approximate r-nets, for c ≥ 1, r > 0, and metric space (V, d).\\nThese are subsets N of the points of V such that no two of the points of N are within distance r of each\\nother, and such that every point of V is within distance c · r of a point of N .\\nTheorem 3.4. Let d > 0, r > 0, ε > 0. Given a set X of n points in Rd, one can compute in expected\\nrunning time O\\n(\\nε−2n1+1/(1+ε)\\n2+o(1)\\n)\\na set N ⊆ X such that N is a (1 + ε)-approximate r-net for the\\nEuclidean metric on X with high probability.\\nProof: By Lemma 3.1 we may assume that d = O(log n/ε2), since otherwise we can embed X into\\nEuclidean space of dimension O(log n/ε2), with distortion 1 +O(ε). Any (1 +O(ε))-approximate r-net\\nfor this new point set is a (1 +O(ε))-approximate r-net for X.\\nLet c = 1 + ε. Let H be the (r, cr, p1, p2)-sensitive family of hash functions given by Lemma 3.3,\\nwith p1 = 1/n\\n1/c2+o(1), p2 = 1/n. We sample k = O((1/p1) · log n) = O\\n(\\nn1/c\\n2+o(1)\\n)\\nhash functions\\nh1, . . . , hk ∈ H. For every i ∈ {1, . . . , k}, and for every x ∈ X, we evaluate hi(x).\\nWe construct a set N ⊆ X which is initially empty, and it will be the desired net at the end of the\\nalgorithm. Initially, we consider all points in X as being unmarked. We pick an arbitrary ordering\\nx1, . . . , xn of the points in X, and we iterate over all points in this order. When the iteration reaches point\\nxi, if it is already marked, we skip it, and continue with the next point. Otherwise, we add xi to N , and\\nwe proceed as follows. Let Mi be the set of all currently marked points. Let Si =\\n⋃k\\nj=1 h\\n−1\\nj (xi)\\\\Mi be the\\nset of unmarked points that are hashed to the same value in at least one of the sampled hash functions\\nh1, . . . , hk. We mark all points y ∈ Si, such that ‖xi − y‖2 ≤ c · r. This completes the construction of\\nthe desired set N .\\nWe next argue that N is indeed a c-approximate r-net. Every point y ∈ X \\\\ N must have been\\nmarked when considering some earlier point xi ∈ N , implying that ‖xi− y‖2 ≤ c · r. Thus, every non-net\\npoint is covered by a net point. On the other hand, consider a pair of points xi, xj (i < j) for which\\n‖xi, xj‖2 ≤ r. Then with high probability, there exists t′ ∈ {1, . . . , k}, with ht′(xi) = ht′(xj). If so, xj\\nwill be marked when we consider xi, preventing it from belonging to N . Therefore, with high probability,\\nwe have that for any x, x′ ∈ N , ‖x− x′‖2 > r. This establishes that N is indeed a c-approximate r-net.\\nIt remains to bound the running time. We perform n·k hash function evaluations in time O(d · no(1)) =\\nO\\n(\\nε−2no(1)\\n)\\neach, totaling time O\\n(\\nε−2n1+1/c\\n2+o(1)\\n)\\n. The remaining time is dominated by O(\\n∑n\\ni=1 |Si|).\\nEach point is marked at most once, so O(\\n∑n\\ni=1 |Si|) = O(n+ L), where L is the total number of false\\npositives, i.e. the number of triples x, y, t with x, y ∈ X, ‖x−y‖2 > c ·r, t ∈ {1, . . . , k}, and ht(x) = ht(y).\\nFor any x, y ∈ X, with ‖x − y‖2 > c · r, and for any t ∈ {1, . . . , k}, we have Pr[ht(x) = ht(y)] ≤ 1/n.\\nSince there are O(n2) pairs of points, we conclude that the expected number of false positives is\\nE[L] ≤ O\\n(∑\\nx 6=y∈X\\n∑k\\nt=1 1/n\\n)\\n= O\\n(\\nn1+1/c\\n2+o(1)\\n)\\n. We conclude that the total expected running time of\\nthe algorithm is O\\n(\\nε−2n1+1/c\\n2+o(1)\\n)\\n, as required.\\n9\\n3.2. An approximation whose time depends on the spread\\nLemma 3.5. Let d ≥ 1, let X be a set of n points in Rd, let ε > 0, and let Φ be the spread of the\\nEuclidean metric on X. Then, one can compute in O\\n(\\nε−2n1+1/(1+ε)\\n2+o(1) log Φ\\n)\\nexpected time a sequence\\nthat is a (1 + ε)-greedy permutation for the Euclidean metric on X, with high probability.\\nProof: We use the algorithm from Lemma 2.7. A 2-approximate diameter can easily be computed in\\nlinear time, by choosing one point arbitrarily and finding a second point as far from it as possible. The\\nonly new needed observation is that it is sufficient for the algorithm to compute (1 +O(ε))-approximate\\nr-nets, using Theorem 3.4, in place of r-nets. As in Lemma 2.7, the approximate r-net algorithm needs to\\nbe modified to mark near-neighbors of previously selected points as used, so that they are not selected as\\npart of the net; this step does not increase the total running time for the approximate r-net construction.\\nThe rest of the analysis remains the same.\\n3.3. Approximating all-pairs min-max paths\\nAs a tool for eliminating the dependence on the spread in our approximate greedy permutation algorithm,\\nwe will use an approximation to the minimum spanning tree. However, we do not wish to approximate\\nthe total edge length of the tree, as has been claimed by Andoni and Indyk [AI06]; rather, we wish to\\napproximate a different property of the minimum spanning tree, the fact that for every two vertices it\\nprovides a path that minimizes the maximum edge length among all paths connecting the same two\\nvertices.\\nLemma 3.6 ([AI06]). Given n points in a Euclidean space of dimension d, and given a parameter\\nc > 1, we may preprocess the points in time and space O\\n(\\ndn+ n1+1/c\\n2+o(1)\\n)\\ninto a data structure that\\nmay be used to answer c-approximate nearest neighbor queries in query time O\\n(\\ndn1/c\\n2+o(1)\\n)\\nwith high\\nprobability of correctness.\\nLemma 3.7. Given n points in a Euclidean space of dimension d, and given a parameter ε > 0, we may\\nin expected time O\\n(\\nn1+1/c\\n2+o(1)\\n)\\nfind a spanning tree T of the points such that, for every two points u\\nand v, the maximum edge length of the path in T from u to v is at most (1 + ε) times the maximum edge\\nlength of the path in the minimum spanning tree from u to v.\\nProof: As before, we may assume without loss of generality that d = O(log n/ε2). We build T by an\\napproximate version of Bor˚uvka’s algorithm, in which we maintain a forest (initially having a separate\\none-node tree per point) and then in a sequence of O(log n) stages add to the forest the edge connecting\\neach tree with its (approximate) nearest neighbor.\\nIn each stage, we assign each tree of the forest an O(log n)-bit identifier. For each pair (i, b) where i is\\none of the O(log n) bit positions of these identifiers and b is zero or one, we build a (1 + ε)-approximate\\nnearest neighbor data structure for the points whose tree has b in the ith bit of its identifier, for a total of\\nO(log n) structures. Then, for each point p of the input set, we use these data structures to find O(log n)\\ncandidate neighbors of p, one for each of the O(log n) structures that do not contain p. This gives us a\\nset of O(n log n) candidate edges, among which we select for each tree of the current forest the shortest\\nedge that has one endpoint in that tree. As in Bor˚uvka’s algorithm, with an appropriate tie-breaking\\nrule, adding the selected edges to the forest does not produce any cycles, and reduces the number of\\ntrees in the forest by at least a factor of two, so after O(log n) stages we will have a single tree, which we\\nreturn as our result.\\n10\\nTo show that this tree has the desired approximation property, consider a complete graph Kn on\\nthe input points, in which the weight of an edge that does not belong to the output tree is the distance\\nbetween the points. However, in this graph, we set the weight of an edge e that does belong to the tree\\nby letting T1 and T2 be the two trees containing the endpoints of e in the last stage of the algorithm\\nfor which these endpoints belonged to different trees, and setting the weight of e to be the minimum\\ndistance between a pair of vertices one of which belongs to T1 or T2 and the other of which does not\\nbelong to the same tree. Then, by the correctness of the approximate nearest neighbor data structure,\\nthe weight of e is at most equal to the distance between its endpoints and at least equal to that distance\\ndivided by 1 + ε. However (with an appropriate tie-breaking rule) the algorithm we followed to construct\\nour tree T is exactly the usual version of Bor˚uvka’s algorithm as applied to the weighted graph Kn.\\nTherefore, for every u and v, the path from u to v in T exactly minimizes the maximum edge length\\namong all paths in Kn, and thus has the desired approximation for the original distances.\\n3.4. Eliminating the dependence on the spread\\nAs in Section 2.3, we will eliminate the log Φ term in the running time for our approximate greedy\\npermutation algorithm by, in effect, contracting and uncontracting edges of a graph, the approximate\\nminimum spanning tree of Section 3.3.\\nTheorem 3.8. Let d ≥ 1, let X be a set of n points in Rd, and let ε > 0, Then, one can compute in\\nO\\n(\\nε−2n1+1/(1+ε)\\n2+o(1)\\n)\\nexpected time a sequence that is a (1 + ε)-greedy permutation for the Euclidean\\nmetric on X, with high probability.\\nProof: We maintain a partition of the input into subproblems, defined by subtrees of the spanning tree\\nT computed by Lemma 3.7. Initially, there is one subproblem, defined by the whole tree, but it does not\\ninclude all the input points. Rather, as the algorithm progresses, certain points within each subproblem’s\\nsubtree will be active, depending on the current value r for which we are computing approximate r-nets.\\nWe delete edge e from tree T , splitting its subproblem into two smaller sub-subproblems, whenever r\\nbecomes smaller than the length of e divided by 1 +O(ε). After this point, the points on one side of e are\\ntoo far away to affect the choices made on the other side of e. We will include the endpoints of an edge e\\ninto the active points of the subproblem containing e whenever the current value of r becomes smaller\\nthan cn/ε times the length of e (for an appropriate constant c < 1). Until that time there will always be\\nanother active point within distance cεr of e, so omitting the endpoints of e will not significantly affect\\nthe approximation quality of the greedy permutation we construct.\\nAt each stage of the algorithm, when we construct approximate r-nets for some particular value\\nof r, we do so separately in each subproblem that has two or more active points. (In a subproblem\\nwith only one active point, that point must have been included in the approximate greedy permutation\\nalready, and so cannot be chosen for the new r-net. However, the points that have been included in the\\npermutation must remain active in order to prevent other points within distance r of them from being\\nadded to the net.) Each edge e contributes to the size of a subproblem only for a logarithmic number of\\ndifferent values of r, so the total time is as stated.\\n4. Exact greedy permutation for bounded treewidth\\nFor restricted classes of graphs, we can compute a greedy permutation exactly, more quickly than the naive\\nalgorithm of Section 2. Our algorithms follow Cabello and Knauer [CK09] in applying orthogonal range\\nsearching data structures to the vectors of distances from small sets of separator vertices. Specifically,\\nwe need the following result.\\n11\\nLemma 4.1 ([GBT84]). Let S be a set of n points in Rk, for a constant k > 1. Then we may process\\nS in time and space O\\n(\\nn logk−1 n\\n)\\nso that the `∞-nearest neighbor in S of any query point q may be\\nfound in query time O\\n(\\nlogk−1 n\\n)\\n.\\n4.1. Tree decomposition and restricted partitions\\nA tree decomposition of a graph G is a tree D whose nodes are associated with sets of vertices of G\\ncalled bags, satisfying the following two properties:\\n(A) For each vertex v of G, the bags containing v induce a connected subtree of D.\\n(B) For each edge uv of G, there exists a bag in D containing both u and v.\\nThe width of a tree decomposition is one less than the maximum size of a bag, and the treewidth of a\\ngraph is the minimum width of any of its tree decompositions.\\nFollowing Frederickson [Fre97], who defined a similar concept on trees, we define a restricted order-\\nk partition of a graph G of treewidth w to be a partition of the edges of G into O(n/k) subgraphs Si\\nsuch that, for all i, the subgraph Si has at most k edges, and such that, for each Si, at most 2w + 2\\nvertices are endpoints of edges both in and outside Si.\\nLemma 4.2. Fix w = O(1). Then for every n-vertex graph G of treewidth ≤ w and every k ≥ (w\\n2\\n)\\n, a\\nrestricted order-k partition of G may be constructed in time O(n) from a tree decomposition of G.\\nProof: Let D be a tree decomposition of D, of width w. Without loss of generality (by choosing a root\\narbitrarily and splitting some nodes of D into multiple nodes having equal bags) we may assume that D\\nis a rooted binary tree. Associate each edge of G with a unique node of D having the two endpoints of\\nthe edge in its bag, choosing arbitrarily if there is more than one node.\\nFollowing Frederickson [Fre97], find a partition of the nodes of D into subsets Di, having the following\\nproperties:\\n(A) Each subset Di induces a connected subtree of D.\\n(B) Each subset Di is associated with at most k edges of G.\\n(C) For each i, if Di contains more than one node, then at most two edges of D have one endpoint\\nin Di and the other endpoint in a different subset.\\n(D) No two adjacent subsets Di can be merged while preserving properties (A), (B), and (C).\\nThis partition may be found in linear time by a greedy bottom-up traversal of D. For each subset Di, we\\nform a subgraph Si of G consisting of the edges associated with nodes in Di. Property (B) implies that\\neach subgraph Si has at most k edges, and property (C) implies that there are at most 2w + 2 vertices\\nthat are endpoints of edges both in Si and in other subgraphs (namely the vertices in the bags of the\\ntwo nodes of Di that are endpoints of edges connecting Di to other sets).\\nIt remains to show that there are O(n/k) subsets. Contracting each subset Di in D to a single node\\nproduces a binary tree T in which each edge with one degree-one endpoint or two degree-two endpoints\\nconnects subgraphs that together have more than k edges of G, for otherwise these two subgraphs would\\nhave been merged. It follows that T has O(n/k) such edges, and therefore that it has only O(n/k) nodes.\\nThus, we have formed O(n/k) subsets Si, as desired.\\nGiven a restricted partition of G, we define an interior vertex of a subgraph Si to be a vertex of G\\nall of whose incident edges belong to Si, and we define a boundary vertex of Si to be a vertex that is\\nincident to an edge in Si but is not an interior vertex of Si.\\n12\\n4.2. The algorithm\\nSuppose graph G has treewidth w = O(1). We may find a greedy permutation of G by performing the\\nfollowing steps.\\n1. Find a tree-decomposition of G of width w [Bod96].\\n2. Apply Lemma 4.2 to find a restricted order-\\n√\\nn partition of G.\\n3. Construct a weighted graph H whose vertices are the boundary vertices of the restricted partition,\\nwhere two vertices u and v are connected by an edge if they belong to the same subgraph Si of the\\npartition, and where the weight of this edge is the length of the shortest path in Si from u to v (or\\na sufficiently large dummy value Z, greater than n times the heaviest edge of G, if no such path\\nexists).\\n4. For each vertex v of H, initialize a value d(v) to Z. As the algorithm progresses, d(v) will represent\\nthe length of the shortest path to v from a vertex already belonging to the greedy permutation.\\n5. For each subgraph Si of the restricted partition, having k boundary vertices, construct a (k + 1)-\\ndimensional `∞-nearest neighbor data structure (Lemma 4.1) whose points correspond to interior\\nvertices of Si. The first coordinate of the point for v is the length of the shortest path within Si to\\nv from an interior vertex of Si that belongs to the greedy permutation; initially, and until such a\\npath exists, it is Z. The remaining coordinates give the lengths of the shortest paths in Si from\\neach boundary vertex to v, or Z if no such path exists.\\n6. Repeat n times:\\n(a) For each subgraph Si of the restricted partition, find the farthest interior vertex of Si from\\nthe already-selected vertices, and its distance from the vertices selected so far. This may be\\ndone by querying the nearest neighbor of a point q whose first coordinate is 2Z and whose ith\\ncoordinate is 2Z − d(vi), where vi is the ith boundary vertex of Si.\\n(b) Among the O(\\n√\\nn) vertices found in the previous step, and the O(\\n√\\nn) boundary vertices whose\\ndistance d(v) from the greedy permutation was already known, select a vertex v whose distance\\nfrom the greedy permutation is maximum, and add it to the permutation.\\n(c) If v is an interior vertex of a subgraph Si of the restricted partition, use Dijkstra’s algorithm\\nto compute the shortest path within Si from it to the other interior vertices, and rebuild the\\nnearest neighbor data structure associated with Si after updating the first coordinates of each\\nof its points.\\n(d) Use Dijkstra’s algorithm on H (if v is a boundary vertex) or on H + Si (if it is interior to Si),\\nstarting from v, to update the values d(u) for each boundary vertex u.\\nThe time analysis of this algorithm is straightforward and gives us the following result.\\nTheorem 4.3. Let G be an n-vertex graph of treewidth w = O(1) with non-negative edge weights. Then\\nin time O\\n(\\nn3/2 log2w+2 n\\n)\\nwe may construct an exact greedy permutation for G.\\n5. Counting distances in planar graphs\\nIn this section we give a near-linear time bicriterion approximation algorithm for counting pairs of vertices\\nin a planar graph with a given pairwise distance r > 0. The result is approximate in the following sense.\\nIf we let c and c′ be the number of pairs with distance at most r and at most (3 + ε)r respectively, for\\nsome ε > 0, then we output a number α ∈ [c, c′].\\nThe following is due to Thorup [Tho04] (see also [KST13]).\\nTheorem 5.1 (Thorup [Tho04]). For any n-vertex undirected planar graph with non-negative edge\\nlengths, and for any ε > 0, there exists a (1 + ε)-approximate distance oracle with query time O(ε−1),\\nspace O(ε−1n log n), and preprocessing time O(ε−2n log3 n).\\n13\\nThe basic idea is now to recursively decompose G using planar separators. Fortunately, one can do it\\nin such a way, that when looking on a patch P , with m vertices, formed by this recursive decomposition,\\nthe distances between the boundary vertices of P (in the original graph) are known. The details of\\nhow to compute this decomposition is described by Fakcharoenphol and Rao [FR06], and we recall their\\nresult.\\nLet G = (V,E) be a graph. A patch of a graph is a subset C ⊆ V , such that the induced subgraph\\nG[C] is connected. A vertex v ∈ C is a boundary vertex if there exists a vertex u ∈ V \\\\ C such that\\nuv ∈ E. A hierarchical decomposition H of G is a set of subsets of the vertices of G, that can be\\ndescribed via a binary tree T , having patches of G associated with each node of it. The root r of tree is\\nassociated with the whole graph; that is, C(r) = V . Every node of u ∈ T has two children v1, v2, such\\nthat C(v1) ∪ C(v2) = C(u), and |C(v1)| , |C(v2)| ≤ (2/3) |C(u)|. A leaf of this tree is associated with a\\nsingle vertex of G. Finally, we require that for any patch C in this decomposition, the set of its boundary\\nvertices ∂C, has at most O\\n(√|C| ) vertices.\\nFor every C ∈ H, the (inner) distance graph of C, denoted by DC is a clique over ∂C, with\\nu, v ∈ ∂C assigned length dG[C](u, v), i.e. equal to the shortest distance of all paths between u and\\nv, that are contained entirely inside G[C]. The dense distance graph associated with H is the graph\\nG′ =\\n⋃\\nC∈C DC .\\nTheorem 5.2 (Fakcharoenphol and Rao [FR06]). Let G be an n-vertex undirected planar graph\\nwith non-negative edge lengths. Then, one can compute, in O(n log3 n) time, a hierarchical decomposition\\nH of G, and all the inner and outer distance graphs associated with its patches (i.e., DC for all C ∈ H).\\nWe are now ready to prove the main result of this section.\\nTheorem 5.3. Let G be a given n-vertex undirected planar graph with non-negative edge lengths, let\\nr > 0, and ε > 0. Let c = |P≤r| and c′ =\\n∣∣P≤(3+ε)r∣∣, see Eq. (1)p4. Then, one can compute, in\\nO(ε−2n log3 n) time, an integer α, such that c ≤ α ≤ c′.\\nProof: We compute a hierarchical decomposition of H, and for every C ∈ H, the distance graph DC , in\\ntotal time O(n log3 n), using Theorem 5.2. We consider all patches C ∈ H. If |C| = 1 then we let αC = 0.\\nOtherwise, our purpose here is to count the number of pairs of vertices u, v ∈ C, such that dG(u, v) ≤ r,\\nand u and v belong to different children of C.\\nSo, let C1, C2 be the two patches that are the children of C in H. Let B1, B2 be the set of border\\nvertices of C1, C2, respectively, and let B = B1 ∪ B2. Let G1 = G[C1] ∪ DC2 , and G2 = G[C2] ∪ DC1 .\\nThat is, G1 is the union of the subgraph of G induced on the cluster C1, and the distance graph inside\\nC2, and the distance graph outside C1 ∪ C2. Note that (i) V (G1) = C1, (ii) for any u, v ∈ V (G1), we\\nhave dG1(u, v) = dG(u, v), and (iii) |E(G1)| = O(|C1|) +O\\n(|∂C2|2)+O(|∂C|2) = O(|C1|). Therefore, for\\nany i ∈ {1, 2}, by running Dijkstra’s algorithm on Gi starting from B, we can compute the set of vertices\\nUi = {v ∈ Ci | dG(v,B) ≤ r} ,\\nin time O(|Ci| log n). Moreover, for any i ∈ {1, 2}, for any v ∈ Ci, we can compute (within the same time\\nbounds) its closest vertex in B; specifically, a vertex Γi(v) ∈ B, with dG(v,Γi(v)) = dG(v,B). Let B′ be\\nthe set of all vertices that are border vertices in some ancestor of C in H. For any u ∈ B, i ∈ {1, 2}, let\\nUi(u) =\\n{\\nv ∈ Ci\\n∣∣ Γi(v) = u} \\\\B′.\\nUsing Theorem 5.1 we can find in time O(ε−1|B|2) all pairs of vertices u, v ∈ B, with dG(u, v) ≤ (1 + ε)r.\\nThat is, we compute the set of border vertex pairs\\nT =\\n{\\n(u, v) ∈ B ×B ∣∣ dG(u, v) ≤ (1 + ε)r} .\\n14\\nWe now explicitly count all the pairs of vertices that are in distance at most r from a pair of vertices on\\nthe boundary, such that these boundary vertices in turn are in distance at most (1 + ε)r from each other.\\nThat is, we set\\nαC =\\n∑\\n{u,v}∈T\\n(\\n|U1(u)| · |U2(v)|+ |U2(u)| · |U1(v)|\\n)\\n.\\nFinally, we return the value α =\\n∑\\nC∈H αC. Every ordered pair (u, v) ∈ V (G)× V (G) is counted exactly\\nonce in the above summation. Moreover, every pair with dG(u, v) ≤ r contributes 1, while every pair\\nwith dG(u, v) > (1 + ε)3r contributes 0, implying that c ≤ α ≤ c′, as required.\\nIt remains to bound the running time. Constructing H takes time O(n log3 n). The hierarchical\\ndecomposition has O(log n) levels. For every level, we spend a total of O(n log n) time running Dijkstra’s\\nalgorithm. We also spend a total time of O(ε−1n) computing the sets T . Finally, we spend O(ε−2n log3 n)\\ntime at the preprocessing step of the distance oracle from Theorem 5.1. Therefore, the total running\\ntime is O(ε−2n log3 n), which concludes the proof.\\n6. Conclusions\\nWe have found efficient approximation algorithms for greedy permutations in graphs and in high-\\ndimensional Euclidean spaces, and fast exact algorithms for graphs of bounded treewidth. This implies\\n(2 + ε)-approximate k-center clustering of graph metrics in Oε(m log\\n2 n) time (ignoring the dependency\\non ε), for all values of k simultaneously. For a single value of k, and for graphs whose weights are\\npositive integers, our technique can be used to construct a 2-approximation to the k-center clustering\\nin O(m log n log spread(G)) expected time (Theorem 2.9). This compares favorably with a significantly\\nmore complicated algorithm of Thorup [Tho05] that has running time worse by at least a factor of\\nO(log3 n).\\nWe leave open for future research finding other graphs in which we may construct exact greedy\\npermutations more quickly than the naive algorithm. Another direction for research concerns hyperbolic\\nspaces of bounded dimension. Krauthgamer and Lee [KL06] claim without details that for all ε > 0, sets\\nof n points in hyperbolic spaces of bounded dimension have (1 + ε)-Steiner spanners with O(n) vertices\\nand edges. Applying our graph algorithm to these spanners (modified to avoid including Steiner points\\nin its r-nets) would give a near-linear time approximate greedy permutation for those spaces as well, but\\nperhaps a more direct and more efficient algorithm is possible.\\nReferences\\n[AAYI+13] S. Abbar, S. Amer-Yahia, P. Indyk, S. Mahabadi, and K. R. Varadarajan. Diverse near\\nneighbor problem. In Proc. 29th Annu. Sympos. Comput. Geom. (SoCG), pages 207–214,\\n2013.\\n[AI06] A. Andoni and P. Indyk. Near-optimal hashing algorithms for approximate nearest neighbor\\nin high dimensions. In Proc. 47th Annu. IEEE Sympos. Found. Comput. Sci. (FOCS), pages\\n459–468, 2006.\\n[AYE12] U. Altinisik, M. Yildirim, and K. Erkan. Isolating non-predefined sensor faults by using\\nfarthest first traversal algorithm. Ind. Eng. Chem. Res., 51(32):10641–10648, 2012.\\n[Bod96] H. L. Bodlaender. A linear-time algorithm for finding tree-decompositions of small treewidth.\\nSIAM Journal on Computing, 25(6):1305–1317, 1996.\\n15\\n[CK09] S. Cabello and C. Knauer. Algorithms for graphs of bounded treewidth via orthogonal range\\nsearching. Comput. Geom. Theory Appl., 42(9):815–824, 2009.\\n[Coh14] E. Cohen. All-distances sketches, revisited: HIP estimators for massive graphs analysis. In\\nProc. 33rd ACM Sympos. Principles Database Syst. (PODS), pages 88–99, 2014.\\n[DL05] S. Dasgupta and P. M. Long. Performance guarantees for hierarchical clustering. J. Comput.\\nSystem Sci., 70(4):555–569, 2005.\\n[ELPZ97] Y. Eldar, M. Lindenbaum, M. Porat, and Y. Y. Zeevi. The farthest point strategy for\\nprogressive image sampling. IEEE Trans. Image Proc., 6(9):1305–1315, 1997.\\n[Eri95] J. Erickson. On the relative complexities of some geometric problems. In Proc. 7th Canad.\\nConf. Comput. Geom. (CCCG), pages 85–90, 1995.\\n[FJ83] G. N. Frederickson and D. B. Johnson. Finding k-th paths and p-centers by generating and\\nsearching good data structures. J. Algorithms, 4(1):61–80, 1983.\\n[FJ84] G. N. Frederickson and D. B. Johnson. Generalized selection and ranking: Sorted matrices.\\nSIAM Journal on Computing, 13:14–30, 1984.\\n[FR06] J. Fakcharoenphol and S. Rao. Xxxplanar graphs, negative weight edges, shortest paths, and\\nnear linear time. J. Comput. Sys. Sci., 72(5):868–889, 2006.\\n[Fre97] G. N. Frederickson. Ambivalent data structures for dynamic 2-edge-connectivity and k\\nsmallest spanning trees. SIAM Journal on Computing, 26(2):484–538, 1997.\\n[GBT84] H. N. Gabow, J. L. Bentley, and R. E. Tarjan. Scaling and related techniques for geometry\\nproblems. In Proc. 16th Annu. ACM Sympos. Theory Comput. (STOC), pages 135–143,\\n1984.\\n[GGD12] Y. Girdhar, P. Gigue`re, and G. Dudek. Autonomous adaptive underwater exploration using\\nonline topic modelling. In Proc. Int. Symp. Experimental Robotics, 2012.\\n[GJG11] C. Gu, X. Jiang, and L. Guibas. Kinetically-aware conformational distances in molecular\\ndynamics. In Proc. 23rd Canad. Conf. Comput. Geom. (CCCG), pages 217–222, 2011.\\n[Gon85] T. F. Gonzalez. Clustering to minimize the maximum intercluster distance. Theoret. Comput.\\nSci., 38(2-3):293–306, 1985.\\n[Har04] S. Har-Peled. Clustering motion. Discrete Comput. Geom., 31(4):545–565, 2004.\\n[HIS13] S. Har-Peled, P. Indyk, and A. Sidiropoulos. Euclidean spanners in high dimensions. In Proc.\\n24rd ACM-SIAM Sympos. Discrete Algs. (SODA), pages 804–809, 2013.\\n[HM06] S. Har-Peled and M. Mendel. Fast construction of nets in low-dimensional metrics, and their\\napplications. SIAM Journal on Computing, 35(5):1148–1184, 2006.\\n[HR13] S. Har-Peled and B. Raichel. Net and prune: A linear time algorithm for Euclidean distance\\nproblems. In Proc. 45th Annu. ACM Sympos. Theory Comput. (STOC), pages 605–614,\\n2013.\\n16\\n[HS85] D. S. Hochbaum and D. B. Shmoys. A best possible heuristic for the k-center problem. Math.\\nOper. Res., 10(2):180–184, 1985.\\n[JL84] W. B. Johnson and J. Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space.\\nIn Conf. Modern Analysis and Probability, volume 26 of Contemporary Mathematics, pages\\n189–206. Amer. Math. Soc., 1984.\\n[KL06] R. Krauthgamer and J. R. Lee. Algorithms on negatively curved spaces. In Proc. 47th Annu.\\nIEEE Sympos. Found. Comput. Sci. (FOCS), pages 119–132, 2006.\\n[KST13] K. Kawarabayashi, C. Sommer, and M. Thorup. More compact oracles for approximate\\ndistances in undirected planar graphs. In Proc. 24rd ACM-SIAM Sympos. Discrete Algs.\\n(SODA), pages 550–563, 2013.\\n[LF09] Y. Lipman and T. Funkhouser. Mo¨bius voting for surface correspondence. In Proc. ACM\\nSIGGRAPH, pages 72:1–72:12, 2009.\\n[MAB98] E. Mazer, J. M. Ahuactzin, and P. Bessiere. The Ariadne’s clew algorithm. J. Art. Intell.\\nRes., 9:295–316, 1998.\\n[Mat99] J. Matousˇek. Geometric Discrepancy. Springer, 1999.\\n[MD03] C. Moenning and N. A. Dodgson. A new point cloud simplification algorithm. In 3rd IASTED\\nInt. Conf. Visualization, Imaging, and Image Processing, 2003.\\n[MS09] M. Mendel and C. Schwob. Fast C-K-R partitions of sparse graphs. Chicago J. Theor.\\nComput. Sci., 2, 2009.\\n[MTZC81] N. Megiddo, A. Tamir, E. Zemel, and R. Chandrasekaran. An o(nlog2n) algorithm for\\nthe k-th longest path in a tree with applications to location problems. SIAM Journal on\\nComputing, 10(2):328–337, 1981.\\n[PA95] J. Pach and P. K. Agarwal. Combinatorial Geometry. John Wiley & Sons, 1995.\\n[RSL77] D. J. Rosenkrantz, R. E. Stearns, and P. M. Lewis, II. An analysis of several heuristics for\\nthe traveling salesman problem. SIAM Journal on Computing, 6(3):563–581, 1977.\\n[SMR04] R. Shahidi, C. Moloney, and G. Ramponi. Design of farthest-point masks for image halftoning.\\nEURASIP J. Applied Signal Proc., 12:1886–1898, 2004.\\n[Tho04] M. Thorup. Compact oracles for reachability and approximate distances in planar digraphs.\\nJ. Assoc. Comput. Mach., 51(6):993–1024, 2004.\\n[Tho05] M. Thorup. Quick k-median, k-center, and facility location for sparse graphs. SIAM Journal\\non Computing, 34(2):405–432, 2005.\\n[VBR+10] K. Voevodski, M.-F. Balcan, H. Roglin, S.-H. Teng, and Y. Xia. Efficient clustering with\\nlimited distance information. In Proc. 26th Conf. Uncertainty in AI, pages 632–641, 2010.\\n[Xia97] Z. Xiang. Color image quantization by minimizing the maximum intercluster distance. ACM\\nTrans. Graph., 16(3):260–276, 1997.\\n17\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd24'), 'authors': 'Martin, Daniel P.', 'year': '2017', 'title': 'Dynamic Shortest Path and Transitive Closure Algorithms: A Survey', 'full_text': 'ar\\nX\\niv\\n:1\\n70\\n9.\\n00\\n55\\n3v\\n2 \\n [c\\ns.D\\nS]\\n  2\\n9 O\\nct \\n20\\n17\\nDynamic Shortest Path and Transitive Closure Algorithms:\\nA Survey\\nDaniel P. Martin\\nSchool of Mathematics, University of Bristol, Bristol, BS8 1TW, UK,\\nand the Heilbronn Institute for Mathematical Research, Bristol, UK\\ndan.martin@bristol.ac.uk\\nAbstract. Algorithms which compute properties over graphs have always been of interest in com-\\nputer science, with some of the fundamental algorithms, such as Dijkstra’s algorithm, dating back\\nto the 50s. Since the 70s there has been interest in computing over graphs which are constantly\\nchanging, in a way which is more efficient than simply recomputing after each time the graph\\nchanges.\\nIn this paper we provide a survey of both the foundational, and the state of the art, algorithms\\nwhich solve either shortest path or transitive closure problems in either fully or partially dynamic\\ngraphs. We balance this with the known conditional lowerbounds.\\nKeywords: Dynamic Graph Algorithms, Shortest Paths, Connectivity\\n1 Introduction\\nGraphs are one of the most fundamental and well studied data structures within computer science.\\nMany problems of interest can be phrased in terms of graphs. In this work we focus on two problems\\nover graphs; connectivity and shortest paths;\\nConnectivity: Given two vertices within a graph, does there exist a path between the two vertices? If the\\ngraph is directed, asking if there is a path between vertex u and vertex v is called transitive closure. If\\na path is required both from u to v and from v to u, the question is asking if u and v are in the same\\nstrongly connected component. If the graph is undirected, two vertices being connected and being in the\\nsame strongly connected component are equivalent.\\nShortest Path: Given two vertices u, v in a graph what is the shortest path from u to v? A slight variant\\nof the problem only requires the length of the shortest path to be returned and not the component itself.\\nThe problems described above are the exact variant; there is also an approximate variant where given\\nα > 1 a path of length α ·∆ must be returned, where ∆ is the length of the shortest path.\\nFor both of these problems there are a few variants. In the single source variant, the queries will\\nonly be asked from a fixed start vertex. A similar variant can be considered for a single sink. In the s-t\\nvariant, the only query asked will be between the vertices s and t.\\nThese two problems occur in a host of areas, including; databases [75], compilers [63] and VLSI\\ndesign [17]. For example, in bioinformatics, shortest path algorithms have been used to identify genes\\nassociated with colorectal cancer [55]. Another example, is that if you represent the possible states of a\\nRubik’s cube as vertices in the graph, where an edge corresponds to a single twist, then a shortest path\\nalgorithm will give an efficient way to solve any cube [49].\\nThe problems defined above are given in a static setting; when the graph is defined, it will remain\\nfixed for the entirety of the graph’s lifetime. However, in practice there may be times when the graph\\nwill change over the course of the algorithm. In these situations it is desirable not to have to start from\\nscratch – dynamic graph algorithms are for these situations. There are three variants of dynamic graph\\nalgorithms; incremental algorithms which only allow edges to be inserted and not deleted, decremental\\nalgorithms which only allow edges to be deleted and fully dynamic algorithms which allow edges to both\\nbe inserted and deleted.1 Some algorithms on weighted graphs, also allow the edge weights to be changed.\\nFully dynamic algorithms have a multitude of clear uses, as in many applications the graph will\\nnaturally change over time. Incremental algorithms are useful in scenarios where links appear but never\\ndisappear. For example, representing the graph of all people on a social network, where the edges are\\nif two people have ever communicated. The road network tends to be constantly expanding, with it\\nbeing extremely rare for a road to be permanently destroyed. The use of decremental algorithms, is\\n1 In this work we will not consider the case where vertices can be inserted or deleted.\\nslightly more subtle but they are of important theoretical interest. This is because they can be used as a\\nbuilding block in a variety of fully dynamic algorithms [40,12,4,5,66]. Lots of decremental algorithms can\\nbe converted into incremental algorithms, while maintaining the same time complexities, such as [13].\\nDecremental algorithms have even been used to give improved static graph algorithms [58].\\nThe are other ways that a graph can be considered to be changing dynamically. For example, Albers\\nand Schraink [7] consider graph colouring where the vertices of the graph arrive one at a time. We will\\nnot focus on this, or other models, in this survey but limit ourselves to the case where edges are added\\nand removed.\\nDynamic algorithms can trivially be constructed from the classic approach. If the algorithm does\\nnothing when an edge is updated, it will have to calculate the answer to any query from scratch. This\\ngives a O(1) update time and O˜(m + n) query time algorithm for all pairs shortest path. However, if\\nafter each change the static algorithms are rerun, this results in a O˜(n ·m+ n2) update time and O(1)\\nquery time all pairs shortest path algorithm. The goal of research into dynamic graph algorithms is to do\\nbetter than these trivial bounds. For incremental connectivity, a set union algorithm [71] can be used to\\ngive a query and update time of O(α(n)), where α is related to the inverse-Ackerman function and thus\\nvery slow growing. In this work we focus mainly on algorithms with a constant query time. However,\\nthere are a host of algorithms which do better than the trivial case without this property [39,50,65,12].\\nFor example, Roddity and Zwick [65] give a fully dynamic (1 + ǫ)-approximation algorithm for dynamic\\nall pairs shortest paths with O˜(m·n\\nt\\n) update time and O(t) query time. Hence, this algorithm provides a\\ntrade-off between update and query time.\\nEven and Shiloach [33] designed the first decremental connectivity algorithm, which is more efficient\\nthan simply recomputing from scratch after the deletion of an edge. A similar result was independently\\ndiscovered by Dinitz [29]. The algorithm of Even and Shiloach will be the starting point for our survey,\\nsince it has had a great deal of influence in the literature, resulting in several variants and multiple algo-\\nrithms being built on top of it [39,50,15,13,41,42]. There have also been a host of algorithms which use\\ntechniques disjoint from those given by Even and Shiloach [15,24,53,64]. In contrast to the efficient algo-\\nrithms to solve these problems, there have been several lower bounds given in the literature [61,2,52,45].\\nTable 1 summarises a large proportion of the results in the field, while the text focuses on a smaller\\nproportion, to go into more detail on.\\nThis paper focuses on surveying the theoretical aspects of dynamic graph algorithms for connectivity\\nand shortest path. However, there has been previous work that compares these algorithms in practical\\nscenarios [21,35,36,22,37,27,8,48,77]. For example, Frigioni et al. [37] implement and compare dynamic\\ntransitive closure algorithms, while Demetrescu and Italiano [27] implement and compare dynamic all\\npairs shortest path algorithms.\\nIn this paper, we have limited ourselves to the aforementioned two problems, on general graphs.\\nHowever, there has been lots of work which considers other dynamic problems. These included how to\\nembed a dynamically changing graph into the plane [46], topological ordering [11,38], matchings [68]\\nand min-cut [72]. It is still an open question as to if fully dynamic max flow can be done faster than\\nrecomputation. There has also been work which gives more efficient solutions when restrictions are made\\nto the graph, such as if the graph is planar [34,4,3,1], has bounded degree [76] or is directed acyclic [47,25].\\nThere are two types of algorithm which can be considered; randomised or deterministic, both of\\nwhich will be discussed in this paper. In general, randomised algorithms tend to be more efficient than\\ndeterministic algorithms. However, in the dynamic graph scenario, the randomised algorithms come with\\na large disadvantage. The randomised algorithms tend to assume that the adversary2 is both oblivious to\\nthe randomness used by the algorithm and non-adaptive in their queries. The non-adaptivity is required\\nbecause otherwise it is possible that queries would allow the adversary learn about the randomness used\\nby the algorithm and then act upon this information. Without this assumption the algorithms tend to\\nlose their benefit. Using the nice example from [13]; If the goal is to maintain a set of approximately√\\nn vertices called centers such that every vertex is at most\\n√\\nn away from a center, in the static case\\nthis can be achieved by simply choosing\\n√\\nn · logn centers uniformly at random. In the deterministic\\nsetting this does not work because the adversary can simply choose to disconnect all the centers (while\\nleaving the rest of the graph intact), by deleting a suitable subset of edges. In the randomised setting\\nwith oblivious adversaries the randomised solution once again holds. Hence, one of the desirable goals\\nfor dynamic algorithms, is to construct deterministic algorithms with the same time complexity as their\\nrandomised counterparts.\\n2 Here we define an adversary as a user of the algorithm who is trying to get the algorithm to run in the worst\\npossible time.\\n1.1 Related Work\\nRecently Madkour et al. [57] published a survey on shortest path algorithms and fitted algorithms into a\\nframework to aid relating algorithms. They give a section providing an overview of dynamic algorithms.\\nHowever, due to the wide subject area, they only touch on dynamic algorithms, while we can give\\nconsiderably more detail.\\nDemetrescu and Italiano [26] provided a survey of dynamic shortest path and transitive closure algo-\\nrithms. The authors abstract out some of the combinatorial, algorithmic and data structure techniques\\nused in the literature and then present a subset of the known algorithms within this unifying framework.\\nHowever, there has been a host of work published since the survey was published over a decade ago.\\n1.2 Outline\\nIn Section 2 we give some preliminaries. Section 3 gives the algorithm of Even and Shiloach [33] and\\nsome variants. Section 4 gives algorithms which are built upon the algorithm of Even and Shiloach, while\\nSection 5 gives algorithms built from other techniques. In Section 6 we describe the known lower bounds\\non these problems, which reduce to other well studied problems. We conclude in Section 7 with some\\nopen problems.\\n2 Preliminaries\\nIn this section we give the notation that will be used throughout this paper, along with the definitions\\nof problems which the given algorithms will try to solve.\\n2.1 Notation\\nLet G = (V,E) be a (possibly directed) graph. We denote n = |V | and m = |E|. If the graph is weighted,\\nwe use the weight function ℓ(u, v) for u, v ∈ V to give the weight, of the edge, between u and v. Given\\nU ⊆ V , we define G[U ] = (U,E′) to be the graph with vertices from U and E′ = E ∩ U2, i.e. all edges\\nfrom G that start and end in U .\\nA series of vertices p = 〈v0, . . . , vk〉 is said to be a path between u and v, of length k if v0 = u, vk = v\\nand (vi, vi+1) ∈ E ∀0 ≤ i < k. The weight of a path ℓ(p) is defined as the sum of the weight of all the\\nedges in the path. A path p is called a shortest path between u and v if ℓ(p) ≤ ℓ(p′) for all other paths\\np′ between u and v.\\nGiven three variables n1, n2, n3 we say a function f(n1, n2, n3) = ˜˜o(n\\nc1\\n1 · nc22 · nc33 ) if there exists a\\nconstant ǫ > 0 such that f(n1, n2, n3) = O(nc1−ǫ1 · nc22 · nc33 + nc1−ǫ1 · nc2−ǫ2 · nc33 + nc11 · nc22 · nc3−ǫ3 ). It can\\nbe defined analogously for an arbitrary number of parameters.\\n2.2 Definitions\\nGiven a graph G, a dynamic algorithm is one which allows changes to the graph intermixed with the\\nqueries, while a static algorithm does not allow changes to the graph. An algorithm which only allows\\nedge deletion is called decremental, an algorithm which only allows edge insertion is called incremental,\\nwhile one which allows both is called fully dynamic.\\nThe type of queries allowed determine which problem the algorithm can solve. In this work we consider\\ntwo types of problem; shortest path and transitive closure. We define both in the static setting but the\\nalgorithms are trivially extended to the dynamic setting by allowing the relevant changes to the graph.\\nDefinition 1 (Transitive Closure Problem). Given a directed graph G = (V,E), the transitive\\nclosure problems require an algorithm to answer questions of the form:\\nTransitive Closure “Given u, v ∈ V , is there a path from u to v?”\\nSingle Source Transitive Closure “Given u ∈ V , is there a path from the fixed vertex s to u?”\\nSingle Sink Transitive Closure “Given u ∈ V , is there a path from u to the fixed vertex t?”\\ns-t Transitive Closure “Is there a path from the fixed vertex s to the fixed vertex t?”\\nNote: When the graph is not directed, transitive closure is referred to as connectivity.\\nDefinition 2 (Shortest Path Problem). Given a directed graph G = (V,E), the shortest path prob-\\nlems require an algorithm to answer questions of the form:\\nAll Pairs Shortest Paths “Given u, v ∈ V , return the shortest path from u to v”\\nSingle Source Shortest Paths “Given u ∈ V , return the shortest path from the fixed vertex s to u”\\nSingle Sink Shortest Paths “Given u ∈ V , return the shortest path from u to the fixed vertex t”\\ns-t Shortest Paths “Return the shortest path from the fixed vertex s to the fixed vertex t”\\nNote: A variation of the shortest path problem only requires the algorithm to return the length of the\\nshortest path, instead of the actual path.\\nDefinition 3 (Strongly Connected Component Problem). Given a directed graph G = (V,E), the\\nstrongly connected component problem requires an algorithm to answer the question:\\n“Given u, v ∈ v are u and v in the same strongly connected component?”\\nwhere a set of vertices v1, . . . , vk are said to form a strongly connected component C if for all vi, vj ∈ C\\nthere is a path from vi to vj and a path from vj to vi.\\nSimilar source and sink variants can also be defined.\\nNote: If the graph is undirected, two vertices being connected is equivalent to them being in the same\\nstrongly connected component.\\nA\\nB\\nC\\nD\\nE\\nA\\nB\\nC\\nD\\nE\\nFig. 1. An example graph (left) with the corresponding shortest path graph HA from A (right).\\nSeveral of the algorithms need the notion of a shortest path graph from source s. This is defined\\nbelow and an example is given in Figure 1.\\nDefinition 4 (Shortest Path Graph). Given a graph G and a source s, the shortest path graph Hs\\nis defined as the union of all shortest paths in G starting from s.\\n3 ES Trees\\nOne of the foundational pieces of work was the ES tree by Even and Shiloach [33], which solves the\\ndecremental connectivity problem, on unweighted, undirected graphs. The algorithm has constant query\\ntime and O(q +m · n) total update time, for q queries. Hence for q > n, the algorithm outperforms the\\nnaive solution of rerunning a static algorithm after each edge deletion (which has total runtime O(q ·m)).\\nThis was the first algorithm to beat the naive solution. It has since had many generalisations [39,50,15].\\nWe begin by discussing the original algorithm before giving the generalisations.\\n3.1 Original [33]\\nIn this section we will discuss the algorithm by Even and Shiloach [33], before giving an example of how\\nit behaves on a small graph.\\nGiven a graph G = (V,E), the algorithm stores an array which states which connected component\\neach vertex is in. This array can be used to answer connectivity queries in O(1) time and thus all that\\nis required is to show that the array can be maintained in O(m · n) total time.\\nThe algorithm will construct a shortest path graph H , starting from an arbitrary root vertex r. We\\nconstruct a distance oracle d such that d(r) = 0. Any vertex v at distance i from r is assigned d(v) = i.\\nThis can be calculated using a Breadth First Search. u is said to be a witness of v, if the edge (u, v) is\\nin H and d(u) = i− 1. If there is a component of the graph which is not connected to r, choose a vertex\\nr′, assign it d(r′) = 1 by adding an artificial edge (r, r′) and continue with the BFS, repeating until the\\nwhole graph is within the structure. This can be achieved in O(m+ n) time.\\nWhen an edge (u, v) is removed, there are two cases: edges u and v continue to belong to the same\\nconnected component, or they now belong to different connected components. Two processes will be run\\nin parallel3, one to deal with each of these cases. Each process is discussed in turn. The processes will\\n‘race’, so the first one to finish, will terminate the other one. Thus, we only need to consider the time\\ncomplexity of the process when it finds the event (if the graph is still connected or not) that it was\\nlooking for.\\nProcess One checks whether removing (u, v) disconnects the two components and handles this case.\\nIt does so by calling two Depth First Searches (DFS) from u and v, on the graph G. If either DFS finds\\nthe other vertex, Process One stops because they are still connected.4 However, if one of the DFS finish\\nwithout finding the other vertex, the two vertices have become disconnected. The smaller component\\n(the one whose DFS finished first) is given a new component name in the array. Since each time the\\nsmaller component is renamed, using a charging argument on the edges, it can be shown that the total\\ntime complexity for this process is O(m · logm).\\nProcess Two handles the second case where removing (u, v) does not disconnect the two components\\nand maintains the shortest path graph H . Process Two runs in parallel with Process One and starts\\nwith the assumption that we are in case two. As we will discuss, its actions will be reversed if Process\\nOne determines that we are in-fact in case one.\\nIf (u, v) ∈ G\\\\H then (u, v) is simply removed from G, hence we only need to consider the case where\\n(u, v) ∈ H . If d(u) = d(v) then removing the edge (u, v) does not change the connected components.\\nTherefore, assume, without loss of generality d(v) = d(u) + 1. The function can only be at most one\\ndifferent for d(u) and d(v) since it is defined as the distance to r and there was an edge (u, v). If there is\\nanother witness w for v, simply remove (u, v) from both H and G. We will now consider the case where\\nu was the final witness for v.\\nWhen (u, v) is removed from H , v must increase d(v) by at least one. As a side effect of removing v,\\nanything rooted at v will also need to be reinserted into H . Starting with i = d(v), repeat the following.\\nFor each w ∈ V with d(w) = i and no incoming edges, remove all outgoing edges from w and increment\\nd(w) by 1. For each w incremented, if there exists a y such that d(y) = i and (y, w) ∈ E, insert the edge\\ninto H , thus adding w back into H . Then increment i and repeat until all vertices have been added back\\nto the tree.\\nClearly if removing (u, v) does not disconnect the graph, then this process will terminate. If (u, v)\\ndoes disconnect the graph, then Process One can detect this and cancel Process Two which can\\nreset the data structure to its original state and simply mark the edge (u, v) as artificial. If the graph\\nremains connected the algorithm runs in total time O(m · n) - each time an edge is processed one of its\\nends drops by a level. Since d(v) < n ∀v ∈ V , each edge can be processed at most O(n) times. This gives\\nthe desired time complexity.\\nFigure 2 gives an example of the construction, on a given graph. The left hand column shows how\\nthe graph G changes over a sequence of deletions. The middle column shows H as it corresponds to G,\\nwhen it is created with A as the root vertex. The dashed lines represent edges which are in the original\\ngraph but sit in the same level of the H . The right hand column shows when only a shortest path tree\\nT is stored instead of the shortest path graph, this will be discussed in more detail below.\\nFigure 3 shows how the data structures change upon the deletion of the edge (A,B) – the final edge\\ndeletion in Figure 2. In the graph G the edge (A,B) is removed and nothing else changes, hence, the\\nfigure focuses on the steps undergone by H and T (which are identical up until the final step).5 The\\nfirst step is to remove the edge (A,B) from the data structure. As B, which is in level 1, has no edges\\nconnecting it to level 0 (A), it is dropped by a level. The data structure H directly has access to this\\ninformation, while T has to use the adjacency information of G. At the next step (looking at level 2),\\nboth B and D need to be considered. The vertex B has no edges to the level above and thus drops a\\nlevel. In H , D is connected to level 1 and thus is done, while in T it is not connected, so the adjacency\\n3 Here parallel refers to running the two processes in an interleaved fashion.\\n4\\nProcess Two is left to finish, since it must maintain its internal data structure.\\n5\\nProcess One will not succeed and thus will not be discussed here.\\ninformation is checked and the edge (C,D) is added to T . Finally, B would be checked in level 3 and is\\nconnected to level 2 and thus the process completes.\\nA B\\nC D\\nA\\nB C D\\nA\\nB C D\\nA B\\nC D\\nA\\nB C\\nD\\nA\\nB C\\nD\\nA B\\nC D\\nA\\nB C\\nD\\nA\\nB C\\nD\\nA B\\nC D\\nA\\nC\\nD\\nB\\nA\\nC\\nD\\nB\\nFig. 2. An example of the data structures under the following series of deletions: (A,D), (B,C), (A,B). The left\\nhand column represents the original graph G, the middle column is the data structure H , and the right hand\\ncolumn is the data structure T which utilises the reduced memory technique.\\n3.2 Generalisation\\nIn this section we discuss how the above can be generalised to other forms of graph or problem.\\nDirected graph Henzinger and King show that a similar approach can be taken for single source\\ntransitivity [39]. It will only consider the connected component the source s is in and construct the ES\\ntree using it as the root. The ES tree will now only contain vertices in the same connected component\\nas s and will not contain any artificial edges. Other than this minor modification the algorithm behaves\\nas previously described.\\nSingle sink transitivity problems can be answered in a similar manner, by reversing the direction\\nof all edges before constructing the ES tree. Transitivity for arbitrary vertex pairs can be answered by\\nstoring an ES tree per vertex.\\nWe use out(x) to denote the ES tree with root x (single source) and in(x) to denote the ES tree with\\nroot x on the graph with all the edge directions reversed (single sink).\\nShortest path Henzinger and King also show how to answer single source shortest path problems [39].\\nAn ES tree can be created using the source s as the root of the tree. However, the ES tree will only be\\nAB C\\nD\\nA\\nB C\\nD\\nA\\nB C\\nD\\nA\\nB C\\nD\\nA\\nC\\nDB\\nA\\nC\\nDB\\nA\\nC\\nD\\nB\\nA\\nC\\nD\\nB\\nFig. 3. An example of the steps taken by the algorithm when the edge (A,B) is deleted from the example graph.\\nThe left hand column representsthe data structure H and the right hand column is the data structure T which\\nutilises the reduced memory technique.\\nconstructed for the connected component s is in, it will not contain any artificial edges or vertices not\\nin the same connected component. The distance between s and a queried vertex u is then simply d(u)\\nand the path can be constructed using the ES tree.\\nSingle sink shortest path problems can be answered in a similar manner, by reversing the direction\\nof all edges before constructing the ES tree. All pairs shortest paths can be answered by storing an ES\\ntree per vertex.\\nWeighted graph King shows how the above descriptions can be adjusted to deal with weighted\\ngraphs [50]. Again here we will focus on a single connected component and can boost the result by\\nstoring multiple ES trees. The function d now represents the distance of a vertex from the root. The\\ngraph will only be stored up to a given depth ∆. This will result in a total update time complexity of\\nO(m ·∆). Note, when ∆ = n we get back the original result.\\nIt may no longer be true that d(v) = d(u) + 1 since ℓ(u, v) might be greater than 1. In fact, d(v) =\\nd(u) + ℓ(u, v), for (u, v) ∈ E. Therefore, when a vertex w is moved down from i to layer i + 1 it won’t\\njust be vertices in layer i which are considered but all vertices y such that y ∈ H, (y, w) ∈ E and seeing\\nif d(y) + ℓ(y, w) = i+1. Note that this does not change the time complexity. It is straightforward to see\\nthe result still holds.\\nWe use out(x,∆) to represent the ES tree when the distance is restricted to ∆, in(x,∆) is defined\\nsimilarly.\\nLimited Insertions While ES trees do not provide a fully dynamic algorithm, Bernstein and Roditty\\nshow how the algorithm can handle a very specific form of insertions [15]. If the insertion of an edge\\n(u, v) does not decrease the distance between the root of the tree and v, then it can be supported by the\\nES Tree. The total update time is now O(m′ ·∆+ d) where m′ is the maximum number of edges ever in\\nG and d is the total number of edge changes. The O(d) arises because O(1) work must be spent per edge\\nchange. This property will become useful in Sect. 4.1 to construct a more efficient decremental shortest\\npath algorithm.\\nThis can also be used to increase the weight of a given edge, by inserting the edge, with the new\\nweight, and then deleting the original edge from the ES tree [50].\\n3.3 Reduced Memory\\nKing and Thorup [51] show how to reduce the space for ES trees, as well as several other algorithms [23,50].\\nThe technique allows the memory (beyond the input) to be reduced from O(m) to O(n). Firstly assume\\nan ordering on the vertices. With such an ordering in place, the algorithm can be tweaked as follows.\\nInstead of storing all of H , the edge (x, v) is stored such that x is first in the vertex ordering of all\\nvertices with the property that (x, v) ∈ H . Therefore, instead of storing all of H , a tree T is stored\\ninstead, which has memory requirement O(n). It just remains to show that the tree can be updated\\nwithout changing the time complexity.\\nAn edge deletion (u, v) will only matter if it was in T , if not it is simply removed from G. If (u, v) ∈ T\\na new edge must be found, the edges leading to v can be scanned, in order, starting at u, stopping if (x, v)\\nis found such that d(x)+ ℓ(x, v) = d(v) and adding (x, v) to T . If not the value of d(v) is incremented. As\\nbefore, each edge into v is only considered once for each value of d(v). Hence, the running time remains\\nthe same.\\nSeveral other works have also given space saving techniques [23,16] but these only allow the distance\\nof the shortest path to be given and can not produce the path. Thus they will not be discussed in detail\\nhere.\\n4 Algorithms Built upon ES Trees\\nIn this section we describe some of the algorithms which use ES trees as a building block to solve the\\ndiscussed dynamic algorithms. While there are a host more, such as [14,43,12], we discuss a subset which\\nmade significant progress or contain interesting ideas.\\n4.1 Approximate Decremental Single Source Shortest Paths [13]\\nIn this section we discuss an approximate algorithm for the decremental single source shortest path prob-\\nlem, for unweighted and undirected graphs, by Bernstein and Chechik [13]. This is the first deterministic\\nalgorithm which manages to have a total update time better than the O(m ·n) of ES trees. The algorithm\\nis a (1 + ǫ) approximation with a total update time of O˜(n2).\\nHere we will formally describe an algorithm that gives a time complexity of O(n2.5) and then discuss\\nhow to improve this to the stated bound.\\nA vertex v ∈ V is called heavy if it has degree at least √n and light otherwise. A path between\\ntwo vertices can contain at most 5\\n√\\nn vertices - intuitively, no two heavy vertices can share a common\\nneighbour else there would be a shorter path. Since a heavy vertex has\\n√\\nn neighbours the result follows.\\nThis will be discussed formally below.\\nThe algorithm works by storing two ES trees one which will store short paths and one which will\\napproximate the long paths. For short paths we will simply create a ES tree on the original graph from\\nsource s up to depth 5\\n√\\nnǫ−1. The remainder of this explanation will discuss long paths.\\nLet H be the set of heavy vertices in G. The auxiliary graph G′ is the graph with an additional vertex\\nc per connected component in G[H ] which is connected to each vertex in the connected component with\\nweight 1/2. The light vertices then have all their edges added to the graph. The graph G′ has at most n1.5\\nedges; 1 per heavy vertex and at most\\n√\\nn per light vertex. We will now show that this graph provides\\nthe following bounds:\\ndistG′(s, v) ≤ distG(s, v) ≤ distG′(s, v) + 5\\n√\\nn\\nThe lowerbound follows from the observation that given an s-t path in G, an s-t path can be con-\\nstructed in G′ as follows. For each (u, v) if either u or v are light then (u, v) ∈ E′ and can be added to\\nthe path. If (u, v) are both heavy then they must be in the same connected component so (u, c), (c, v)\\ncan be added to the path. Since these two edges have weight half the pair have the same weight as the\\noriginal path. Thus the path in G′ has the same weight as the path in G. Note that this path may not\\nbe simple.\\nFor the upper bound let L be the set of light vertices on the shortest path in G′ and let X be the\\nset of non light edges on the path (so either heavy or a center c). We want to show that |X | < 5√n,\\ntherefore showing that ignoring the heavy edges doesn’t cost too much. Let Y be every 5th element of\\nX , therefore |Y | ≥ |X|5 . We know that Ball(X, v, 2) ≥\\n√\\nn for v ∈ Y , since either v is heavy or adjacent\\nto a heavy vertex. For v, w ∈ Y we know Ball(X, v, 2) ∩ Ball(X,w, 2) = ∅ otherwise there would be a\\nshorter path (since v and w are at least distance 5 away from each other on the path). Thus we know\\n| ∪v∈Y Ball(X, v, 2)| ≥\\n√\\nn|Y | but since the graph contains at most n vertices we get the desired result.\\nAn ES tree can be stored for the original graph G up to distance 5\\n√\\nnǫ−1 to respond to short edge\\nqueries, while the ES tree up to distance n on G′ allows us to respond to longer path queries. Both run\\nin time O(n2.5), the first due to its bounded depth and the second because it is sparse.\\nWe now need to show that the distances can be maintained under edge deletions. Any edge incident\\nto a light vertex is easy to maintain as it is in the original graph. Deleting an edge can cause a vertex\\nto go from heavy to light (but this can only happens once). When this happens, all of its edges must be\\nadded to the auxiliary graph. The slightly trickier case is when an edge is deleted between two heavy\\nvertices. A data structure which maintains connectivity information in dynamic graphs can be used to\\nmaintain the auxiliary graph. When edge (u, v) is deleted it must be checked that (u, v) are still in the\\nsame connected component. If yes nothing changes. Else these two edges now need to be connected to\\ndifferent centers cu, cv instead of the same center c. This is done by choosing the smallest center and\\nmoving all vertices adjacent to it over to a newly created center. Hence, the graph can be maintained.\\nTo reduce the time complexity from O˜(n2.5) to O˜(n2), instead of having two ES trees (a “heavy one”\\nand a “light one”), O(log n) heaviness thresholds can be used to handle O(log n) ranges of distance queries\\nand returning the minimum of the O(log n) queries.\\nThe algorithm can be trivially converted to the incremental setting, with the same time complexity.\\n4.2 Fully Dynamic Transitivity [39]\\nHenzinger and King [39] give the first fully dynamic transitive closure algorithm, along with a decremental\\nalgorithm. Both are Monte Carlo algorithms. The fully dynamic algorithm has either; query time O( nlogn )\\nand update time O(mˆ · √n · log2 n + n), or query time O( nlogn ) and update time O(n · mˆ\\nµ−1\\nµ · log2 n)\\nwhere mˆ is the average number of edges in the graph and µ is the exponent for matrix multiplication.\\nNote that, unlike the other algorithms given, these algorithms do not have a constant query time.\\nThe deletions only algorithm takes in a user defined parameter r and for i = 1, . . . , log r stores a set\\nof min(2i · logn, n) distinguished vertices Si. For each distinguished vertex x maintain ES trees in(x, n2i )\\nand out(x, n2i ), where x ∈ Si. Then out(x) can be defined as the union of all out(x, n2i ) where x ∈ Si,\\nwith in(x) being defined similarly. For each vertex v ∈ V also maintain in(v, n\\nr\\n) and out(v, n\\nr\\n)\\nGiven a query (u, v), test if v is in out(u, n\\nr\\n), if yes return true. Else see if there exists a distinguished\\nvertex x such that u ∈ in(x) and v ∈ out(x). If this is the case answer yes, else answer no.\\nIf the path is of length less than n\\nr\\nthen the answer will always be correct, otherwise it will be correct\\nwith high probability.\\nThe two fully dynamic algorithms use similar techniques of using the deletion only data structure\\ndiscussed above and suitably keeping track of inserted edges. Thus only one of the two will be discussed\\nhere. The intuition is that you store the deletion only data structure, for r = n\\nlog2 n\\n, along with storing\\nin(x, n) and out(x, n) each time an edge is inserted. These extra structures, along with the deletion only\\ndata structure are updated each time an edge is removed. To answer a query (u, v), query the deletion\\nonly data structure and query if u ∈ in(x, n) and v ∈ out(x, n) for every newly inserted edge. After every√\\nn updates to the graph the deletion only data structure is rebuilt. This gives the desired result.\\n4.3 Fully Dynamic All Pairs Shortest Paths and Transitivity [50]\\nKing [50] gives the first fully dynamic algorithms for all pairs shortest paths in directed and weighted\\ngraphs, where the weight is bounded by some positive integer b. Three algorithms are given; a (2+ ǫ) ap-\\nproximation, a (1+ǫ) approximation and an exact algorithmwith amortized update times;O(n2·log2 nlog logn ),O(n\\n2·log3 (b·n)\\nǫ2\\n)\\nand O(n2.5√b · logn) respectively. They also give a fully dynamic transitive closure algorithm with up-\\ndate time O(n2 · logn). The update times are amortized over O(m\\nn\\n) operations.\\nHere we describe the transitive closure algorithm, since the others follow a similar strategy. The\\nalgorithm works by keeping k = ⌈lnn⌉ forests, denoted F 1, . . . , F k, such that Fi contains ini(v, 2) and\\nouti(v, 2) for each v ∈ V . The count between two vertices counti(u,w) is the number of vertices v such\\nthat u ∈ ini(v, 2) and w ∈ outi(v, 2). The list listi(u,w) contains all such vertices. The forests (and the\\ngraphs they are built upon) are defined recursively. The edges Ei = {(u,w)|counti−1(u, v) > 0} and then\\nthe forests are the in and out ES trees on top of this. Inserting an edge requires adding it at the bottom\\nlayer and adjusting the edge sets and forests as required. Deleting the edge requires deleting it from the\\nlowest layer and the recursively deleting edges in higher layers where the count has gone from positive\\nto zero (i.e. where there is no longer a path). It follows that u,w ∈ V are connected in G if and only if\\ncountk(u,w) > 0, resulting in a transitive closure algorithm.\\n4.4 Decremental Single Source Shortest Paths [41]\\nHenzinger et al. [41] present an algorithm for (1 + ǫ) decremental single-source shortest path for un-\\nweighted graphs, with total update time O(m1+o(1)). The algorithm works by maintaining a sparse\\n(d, ǫ)-hop set (introduced by Cohen [19]). This allows the distance between any two vertices to be (1+ǫ)-\\napproximated using at most d edges. To maintain the hop set, under deletions, the authors introduce a\\nmonotone bounded-hop Even Shiloach tree.\\nThe high-level idea of the algorithm is to create a hop set and then defining the shortcut graph as\\nthe original graph plus the edges from the hop set. This process is the repeated on the resulting graph\\nup to a suitable depth. Intuitively this works because while the graph gains more edges, the number of\\nhops is being constrained. The final algorithm is slightly more complex, where it has to contain active\\nand inactive vertices. Inactive vertices are those which it would be too expensive to maintain an ES-tree\\nfor but the authors show that not constructing these trees will not change the result. See the paper for\\nthe formal definition of active and inactive vertices.\\n4.5 Decremental Single Source Shortest Paths [42]\\nHenzinger et al. [42] give a (1 + ǫ)-approximation decremental single-source shortest path algorithm for\\ndirected graphs with total update time O˜(m · n0.984) and constant update time. Here we will describe\\nthe s-t reachability algorithm, and the authors show how it can be extended to single source reachability\\nand shortest paths.\\nGiven a set of vertices H called a hub, the hub distance between u and v is the shortest distance\\nbetween u and v such that one of the vertices in the path belongs to H . A path between u and v is called\\na h-hop if it contains at most h edges. The h-hop u-v path union graph is the graph created by taking\\nthe union of all the h-hops between u and v. The goal of the algorithm is to maintain reachability while\\ndist(s, t) ≤ h for some parameter h. The algorithm then maintains the hub distance between s and t\\nwhile the hub distance is less than h and maintains the distance between s and t in the path union graph\\nwhen the distance becomes greater than h.\\nFor each v ∈ H dist(s, v) and dist(v, t) are maintained using ES-trees of depth h. Maintaining the\\ndistance between s and t in the path union graph varies depending on the algorithm given in the paper.\\nThe authors give algorithms for sparse graphs, dense graphs and other graphs. One way to do this is\\nmaintaining an ES tree over the path union graph.\\nThe authors later improved upon this [44] for the single source reachability case, with total update\\ntime O(m · n0.98+o(1)). The algorithm works by extending to multiple layers of path union graphs and\\nhubs from the previous algorithm.\\n5 Algorithms Using Alternate Techniques\\nIn this section we discuss some other algorithms which do not utilise ES trees. While there is a host of\\nwork, such as [62,28,60], we focus on a particular subset which contain interesting concepts.\\n5.1 Approximate Decremental All Pairs Shortest Paths [15]\\nBernstein and Roditty [15] give the first decremental algorithms to beat the O(m · n) update time of ES\\ntrees. These algorithms beat the time complexity only when the graph is “not too sparse”. They present\\ntwo randomised algorithms for shortest path problems on unweighted and undirected graphs.\\nThe first algorithm is a (1 + ǫ) approximation algorithm for single source shortest path, with an\\nexpected total update time of O˜(n2+O( 1√logn )). The second algorithm just returns the distances for the\\nall pairs shortest path problem with total expected update time of O˜(n2+ 1k+O( 1√logn )) for any fixed integer\\nk, where the returned distances are at most a 2 · k − 1 + ǫ approximation of the true distance.\\nBoth algorithms are created using a similar technique. Existing algorithms are run on a sparse sub-\\ngraph of the graph (e.g. spanner or emulator) which is being queried, instead of the graph itself. This\\ntechnique has been used previously to construct more efficient static algorithms [9,18,6,30,20,32,31,78,73].\\nHowever, it is more complex in the decremental only setting. The issue arises, in that a deletion from the\\noriginal graph, can cause an insertion into the emulator. The authors resolve the issue by showing that\\nthe insertions will be limited and “suitably well behaved”. Given this they are able to show that existing\\ndecremental algorithms can support the required insertions, giving the desired result.\\n5.2 Dynamic All Pairs Shortest Paths [24]\\nDemetrescu and Italiano [24] give a, deterministic, fully dynamic algorithm for APSP on directed graphs\\nwith non-negative real-valued edge weights. The algorithm has amortized update time O(n2 · log3 n) and\\nworst case constant time query time. We begin by discussing the algorithm which can only increase the\\nweight edges and has amortized update time O(n2 · logn). This was the first algorithm to do better than\\nsimple recomputation.\\nAn important definition for this problem is that of a local shortest path. A local shortest path is a\\npath where all proper subpaths are shortest paths. Note this does not require that the path itself is a\\nshortest path. The algorithm works by storing priority queues Px,y of locally shortest paths between x\\nand y, and P ∗x,y of shortest paths between x and y. Distance queries are answered by returning elements\\nfrom the correct priority queue. It remains to show that the priority queues can be updated efficiently.\\nThe paper shows that a graph can have at most m ·n locally shortest paths (assuming unique shortest\\npaths) and that at most O(n2) paths can stop being locally shortest per edge weight increase. Using this\\nresult the data structures can be maintained in the claimed time.\\nTo upgrade the algorithm to be fully dynamic in the claimed time, the authors use historically shortest\\npaths. At current time t such that the given path had last been updated at time t′ ≤ t, a path is called\\nhistorical if it has been a shortest path in the interval [t′, t]. A path is locally historical if it contains\\na single vertex or all proper subpaths are historical. The algorithm has an additional step to keep the\\nnumber of historical paths at a reasonable level. If there are either too few or too many, it slows the\\nalgorithm. Otherwise the algorithms behaves in a similar manner.\\n5.3 Decremental Strongly Connected Components [53]\\nŁącki [53] gives a deterministic algorithm for strongly connected components, with total update time\\nO(m ·n). The algorithm works by reducing the problem to solving connectivity in a set of directed acyclic\\ngraphs. The algorithm begins by removing all vertices that are not reachable from the given source s,\\nwhich takes O(m) time. When an edge (u, v) ∈ E is deleted, the graph becomes disconnected if v loses its\\nlast in edge. Not only may this cause v to become disconnected, it may also cause children of v to become\\ndisconnected. The algorithm works using this observation. It starts by taking the vertex v and if it is\\nhas no other incoming edges it declares that v has become disconnected. If v has become disconnected,\\nit must recurse on the all children x of v, removing the edge (v, x) since that no longer connects x to the\\nsource. The runtime is linear in the number of newly disconnected vertices and their incident edges. A\\nsimilar algorithm can be given for the reachability of a sink by reversing all the edges.\\nThe strongly connected components algorithm works by splitting a vertex d into two d1 and d2 where\\nall edges (u, d) ∈ E get replaced with (u, d1) and edges (d, v) ∈ E get replaced with (d2, v). Then\\nthe condensation of the resulting graph is calculated. This graph is denoted Gd. This graph is a DAG\\nand therefore reachability from d2 and to d1 can be maintained using the above algorithm. As soon as\\none of the reachability sets is different from the vertex set then the graph has stopped being strongly\\nconnected. To handle deletion of edges not in Gd this must be applied recursively to each strongly\\nconnected component, this is stored as a tree.\\nRoddity [64] improves on the preprocessing and worst case update time of the above algorithm\\nto O(m · logn), without changing the query time or total update time. This is achieved by giving a\\nnew preprocessing algorithm which generates the tree containing the strongly connected components in\\nO(m · logn). The update function is the original one by Łącki, with the adjustment that if it takes over\\nO(m · logn) time then the process is terminated and the tree is simply built again from scratch.\\n6 Limitations\\nIn this section we discuss work that attempts to give lowerbounds on the complexity of these algo-\\nrithms [61,2,52,45]. They achieve this by giving a reduction to a well studied problem. If these dynamic\\nproblems can be solved “too efficiently” then it would result in a more efficient algorithm for a problem\\nwhich is well studied and conjectured that no algorithm can beat a certain threshold. Here we detail the\\nresult of Henzinger et al. [45] who use online boolean matrix-vector multiplication as their underlying\\nproblem. This work is for undirected and unweighted graphs. We chose to discuss the work of Henzinger\\net al. because lots of the results given subsume previously known results [61,2,52].\\nWe begin by defining the online boolean matrix-vector multiplication problem that they reduce all\\nthe problems to, to construct lower bounds.\\nDefinition 5 (Online Boolean Matrix-Vector Multiplication Problem (OMv) [45]). Let n be\\nan integer. Given an n × n Boolean matrix M , and, for each of n rounds, an n-dimensional column\\nvector vi, compute Mvi. The algorithm must output the result before being given the next column vector.\\nThe OMv problem was chosen because it has been well studied [59,74,70,69,56]. It is widely believed\\nthat (up to logarithmic factors) the OMv problem can not be solved more efficiently than O(n3). This\\nled to the formalisation of the OMv conjecture which is given below.\\nDefinition 6 (Online Boolean Matrix-Vector Multiplication Conjecture [45]). For any con-\\nstant ǫ > 0, there is no O(n3−ǫ) time algorithm that solves OMv with an error probability of at most\\n1\\n3 .\\nFor the remainder of the paper, the authors construct lowerbounds for a host of problems by assuming\\nthe OMv conjecture to be true [45]. We will describe the lowerbounds for the problems we are interested\\nin below. However, many more lowerbounds are given, including bounds for; triangle detection, d-failure\\nconnectivity, Langerman’s problem, Erickson’s problem, approximate diameter and densest subgraph.\\nFor what follows, let the preprocessing time, update time and query time be denoted as p(n), u(n), q(n)\\nrespectively. We are now in a position to detail the lowerbound results of Henzinger et al. [45].\\nTheorem 1 ([45], Corollary 4.2). Assuming the OMv conjecture; for any n and m = O(n2), there is\\nno decremental s-t shortest path with polynomial preprocessing time, u(m) = ˜˜o(m\\n3\\n2 ) (total) and q(m) =\\n˜˜o(m).\\nThis theorem, in particular, shows why it was many years for an improvement to be given to the\\nalgorithm of Even and Shiloach [33]. If m ∈ O(n2) then the two bounds are matching. This shows why it\\nis important to consider approximate algorithms instead of exact algorithms. Since the result is for exact\\nalgorithms, considering approximate algorithms could be a way to skirt around this. However, Henziner\\net al. also provide some bounds for approximate algorithms.\\nNote: The above theorem also holds for incremental algorithms.\\nTheorem 2 ([45], Corollary 3.10). Assuming the OMv conjecture; there is no decremental algorithm\\nfor (3− ǫ)-s-t shortest path with p(n) = poly(n), u(n) = ˜˜o(√n) (in the worst case) and q(m) = ˜˜o(n).\\nNote: The theorem also holds for incremental algorithms, and fully dynamic algorithms with amortized\\nupdate time.\\nWe conclude this section by giving a few more results which are also of relevance.\\nTheorem 3 ([45], Corollary 3.4). Assuming the OMv conjecture; for any n and m ≤ n2, there is\\nno partially dynamic algorithm for Strongly Connected Components or Transitivity with time p(m) =\\npoly(m), u(m) = ˜˜o(\\n√\\nm) (worse case) and q(m) = ˜˜o(m).\\nTheorem 4 ([45], Corollary 4.8). Assuming the OMv conjecture, for any n and m = Θ(n\\n1\\n1−δ ),\\nand constant δ ∈ (0, 12 ], there is no partially dynamic algorithm for the problems listed below with;\\np(m) = poly(m), u(m) = ˜˜o(m · n) (total) and q(m) = ˜˜o(mδ). The problems are:\\n– Single Source Shortest Path\\n– All Pairs Shortest Path (2 vs 4)\\n– Transitive closure\\nThe 2 vs 4 variant of the APSP problem requires you to distinguish if the shortest path between the\\ngiven vertices is less than, or equal to, two or if it is greater than, or equal to, four. This shows that even\\ndecision variants of these problems can’t be solved “too efficiently”.\\nRecently Larsen andWilliams [54] showed that a proof of the OMv conjecture could not be constructed\\nonly using information theoretic arguments. However, it is still possible for the conjecture to hold.\\n7 Conclusion and Open Problems\\nIn this paper we survey, both the foundational and state of the art, algorithms for computing shortest\\npaths and connectivity on graphs which are constantly changing. Table 1 summarises the time complex-\\nities of all the algorithms discussed in this paper, while Table 2 gives the known lower bounds. We now\\nprovide a brief conclusion and describe some of the open problems.6\\nDerandomisation Currently the randomised algorithms tend to perform better than the deterministic\\nalgorithms. However, as discussed in the introduction randomised algorithms require a weaker adversary,\\nwho does not know the randomness and can not make adaptive queries. For real world scenarios, it is\\nimportant to be able to remove these restrictions. This makes deterministic algorithms more desirable,\\nsince they do not have these restrictions. Hence, an important open question is if the randomised al-\\ngorithms can be derandomised or if deterministic algorithms can be constructed with the same time\\ncomplexities as their randomised counterparts.\\nMemory To have a constant query time for all pairs shortest path and transitive closure, at least Ω(n2)\\nmemory is required for the lookup table. For connectivity and single source/sink shortest path only\\nΩ(n) memory is required. However, lots of the dynamic algorithms require more space than this. Thus\\nan important question is if existing algorithms can have their memory reduced to the lowerbound or if\\nnew algorithms can be designed meeting this bound. As mentioned above, there has been work moving\\nalgorithms in this direction [51,23,16]\\nLower Bounds It is an important question to try and prove the online matrix vector multiplication\\nconjecture, or equivalently conjectures from other lowerbound work. As discussed by Henzinger et al. [45],\\nthe lower bound techniques tend to work for both incremental and decremental algorithms. Therefore, it\\nis an interesting question to see if these can be bounded separately, to get tighter bounds. For example,\\nincremental single source connectivity can be solved in time O(m ·α(n)) while the best known result for\\nthe decremental setting is O(m · n0.98+o(1)). In relation to the derandomisation question above, to show\\na seperation between deterministic and randomised algorithms, would require applying new techniques\\nwhich only hold for the deterministic setting.\\nAcknowledgements The author is extremely grateful to Benjamin Sach for proof reading and helpful\\ndiscussions.\\n6 All individual work posed their own open questions but here we try to focus on themes more than individual\\nquestions.\\nAlgorithm Dynamic u(m,n) q(m,n) Weighted Directed Time Complexity Approximation\\nTransitive Closure\\n[33] Dec m · n2 1 ✗ ✗ Total\\nN/A\\n[39] Fully mˆ\\n√\\nn · log2 n+ n n\\nlog n\\n✗ ✗ Amortized\\n[39] Fully n · mˆµ−1µ · log2 n n\\nlog n\\n✗ ✗ Amortized\\n[50] Fully n2 · log n 1 Z+\\n<b\\n✓ Amortized\\nStrongly Connected Components\\n[53] Dec m · n 1 ✓ ✓ Total\\nN/A\\n[64] Dec m · log n 1 ✓ ✓ Worst case (Total as above)\\nSingle Source Shortest Path\\n[33] Dec m · n 1 ✗ ✗ Total 1\\n[13] Dec n2 1 ✗ ✗ Total 1 + ǫ\\n[41] Dec m1+o(1) 1 logW slow down ✗ Total 1 + ǫ\\n[42] Dec m · n0.984 1 Z+\\n2log\\nc n ✓ Total 1 + ǫ\\n[44] Dec m · n0.98+o(1) 1 Z+\\n2log\\nc n ✓ Total 1 + ǫ\\n[15] Dec n\\n2+O( 1√\\nlog n\\n)\\n1 ✗ ✗ Total 1 + ǫ\\nAll Pairs Shortest Path\\n[33] Dec m · n2 1 ✗ ✗ Total 1\\n[50] Fully n\\n2\\n·log2 n\\nlog log n\\n1 Z+\\n<b\\n✓ Amortized over m\\nn\\n2 + ǫ\\n[50] Fully n\\n2\\n·log3 (b·n)\\nǫ2\\n1 Z+\\n<b\\n✓ Amortized over m\\nn\\n1 + ǫ\\n[50] Fully n2.5\\n√\\nb · log n 1 Z+\\n<b\\n✓ Amortized over m\\nn\\n1\\n[15] Dec n\\n2+ 1\\nk\\n+O( 1√\\nlogn\\n)\\n1 ✗ ✗ Total 2 · k − 1 + ǫ\\n[24] Fully n2 · log3 n 1 R+ ✓ Amortized 1\\n[65] Fully m·n\\nt\\nt ✗ ✗ Amortized 1 + ǫ\\n[65] Dec m · n 1 ✗ ✗ Expected 1 + ǫ\\n[10] Dec n\\n10\\n9 1 ✗ ✗ Amortized 3\\n[10] Dec n\\n14\\n13 1 ✗ ✗ Amortized 5\\n[10] Dec n\\n28\\n27 1 ✗ ✗ Amortized 7\\nTable 1. A summary of known upper bounds for various problems.\\nProblem p(m,n) u(m,n) q(m,n) Assumption Remark Citation\\nTransitive Closure\\n(m · n)1−ǫ (m · n)1−ǫ mδ−ǫ BMM\\nδ ∈ (0, 1\\n2\\n],m = Θ(n\\n1\\n1−δ )\\n[30]\\npoly (m · n)1−ǫ mδ−ǫ OMv [45]\\nStrongly Connected Components poly m · n1−ǫ mδ−ǫ OMv δ ∈ (0, 1\\n2\\n],m = Θ(n\\n1\\n1−δ ) [45]\\nSingle Source Shortest Path\\npoly m\\n3\\n2\\n−ǫ m1−ǫ OMv m = O(n2) [45]\\n(m · n)1−ǫ (m · n)1−ǫ mδ−ǫ BMM\\nδ ∈ (0, 1\\n2\\n],m = Θ(n\\n1\\n1−δ )\\n[67]\\npoly (m · n)1−ǫ mδ−ǫ OMv [45]\\nAll Pairs Shortest Path\\n(m · n)1−ǫ (m · n)1−ǫ mδ−ǫ BMM\\nδ ∈ (0, 1\\n2\\n],m = Θ(n\\n1\\n1−δ )\\n[30]\\npoly (m · n)1−ǫ mδ−ǫ OMv [45]\\nTable 2. A summary of known lower bounds for various problems.\\nReferences\\n1. Amir Abboud and Søren Dahlgaard. Popular conjectures as a barrier for dynamic planar graph algorithms.\\n2016.\\n2. Amir Abboud and Virginia Vassilevska Williams. Popular conjectures imply strong lower bounds for dynamic\\nproblems. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on, pages 434–\\n443. IEEE, 2014.\\n3. Ittai Abraham, Shiri Chechik, Daniel Delling, Andrew V Goldberg, and Renato F Werneck. On dynamic\\napproximate shortest paths for planar graphs with worst-case costs. In Proceedings of the Twenty-Seventh\\nAnnual ACM-SIAM Symposium on Discrete Algorithms, pages 740–753. Society for Industrial and Applied\\nMathematics, 2016.\\n4. Ittai Abraham, Shiri Chechik, and Cyril Gavoille. Fully dynamic approximate distance oracles for planar\\ngraphs via forbidden-set distance labels. In Proceedings of the forty-fourth annual ACM symposium on Theory\\nof computing, pages 1199–1218. ACM, 2012.\\n5. Ittai Abraham, Shiri Chechik, and Kunal Talwar. Fully dynamic all-pairs shortest paths: Breaking the o(n)\\nbarrier. In LIPIcs-Leibniz International Proceedings in Informatics, volume 28. Schloss Dagstuhl-Leibniz-\\nZentrum fuer Informatik, 2014.\\n6. Donald Aingworth, Chandra Chekuri, Piotr Indyk, and Rajeev Motwani. Fast estimation of diameter and\\nshortest paths (without matrix multiplication). SIAM Journal on Computing, 28(4):1167–1181, 1999.\\n7. Susanne Albers and Sebastian Schraink. Tight bounds for online coloring of basic graph classes. arXiv\\npreprint arXiv:1702.07172, 2017.\\n8. David Alberts, Giuseppe Cattaneo, and Giuseppe F Italiano. An empirical study of dynamic graph algorithms.\\nJournal of Experimental Algorithmics (JEA), 2:5, 1997.\\n9. Baruch Awerbuch, Bonnie Berger, Lenore Cowen, and David Peleg. Near-linear time construction of sparse\\nneighborhood covers. SIAM Journal on Computing, 28(1):263–277, 1998.\\n10. Surender Baswana, Ramesh Hariharan, and Sandeep Sen. Maintaining all-pairs approximate shortest paths\\nunder deletion of edges. In Proceedings of the fourteenth annual ACM-SIAM symposium on Discrete algo-\\nrithms, pages 394–403. Society for Industrial and Applied Mathematics, 2003.\\n11. Michael A Bender, Jeremy T Fineman, and Seth Gilbert. A new approach to incremental topological ordering.\\nIn Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1108–1115.\\nSIAM, 2009.\\n12. Aaron Bernstein. Fully dynamic (2+ epsilon) approximate all-pairs shortest paths with fast query and close\\nto linear update time. In Foundations of Computer Science, 2009. FOCS’09. 50th Annual IEEE Symposium\\non, pages 693–702. IEEE, 2009.\\n13. Aaron Bernstein and Shiri Chechik. Deterministic decremental single source shortest paths: beyond the\\no(mn) bound. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, pages\\n389–397. ACM, 2016.\\n14. Aaron Bernstein and Shiri Chechik. Deterministic partially dynamic single source shortest paths for sparse\\ngraphs. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages\\n453–469. SIAM, 2017.\\n15. Aaron Bernstein and Liam Roditty. Improved dynamic algorithms for maintaining approximate shortest paths\\nunder deletions. In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms,\\npages 1355–1365. SIAM, 2011.\\n16. Gordon Brown and Valerie King. Space-efficient methods for maintaining shortest paths and transitive\\nclosure; theory and practice. Presentation at Workshop on Efficient Algorithms at Oberwolfach, 2000.\\n17. Danny Z Chen. Developing algorithms and software for geometric path planning problems. ACM Computing\\nSurveys (CSUR), 28(4es):18, 1996.\\n18. Edith Cohen. Fast algorithms for constructing t-spanners and paths with stretch t. SIAM Journal on\\nComputing, 28(1):210–236, 1998.\\n19. Edith Cohen. Polylog-time and near-linear work approximation scheme for undirected shortest paths. Journal\\nof the ACM (JACM), 47(1):132–166, 2000.\\n20. Edith Cohen and Uri Zwick. All-pairs small-stretch paths. Journal of Algorithms, 38(2):335–353, 2001.\\n21. Camil Demetrescu, Pompeo Faruolo, Giuseppe F Italiano, and Mikkel Thorup. Does path cleaning help in\\ndynamic all-pairs shortest paths? In European Symposium on Algorithms, pages 732–743. Springer, 2006.\\n22. Camil Demetrescu, Daniele Frigioni, Alberto Marchetti-Spaccamela, and Umberto Nanni. Maintaining short-\\nest paths in digraphs with arbitrary arc weights: An experimental study. In International Workshop on\\nAlgorithm Engineering, pages 218–229. Springer, 2000.\\n23. Camil Demetrescu and Giuseppe F Italiano. Fully dynamic transitive closure: Breaking through the O(n2)\\nbarrier. In Foundations of Computer Science, 2000. Proceedings. 41st Annual Symposium on, pages 381–389.\\nIEEE, 2000.\\n24. Camil Demetrescu and Giuseppe F Italiano. A new approach to dynamic all pairs shortest paths. Journal\\nof the ACM (JACM), 51(6):968–992, 2004.\\n25. Camil Demetrescu and Giuseppe F Italiano. Trade-offs for fully dynamic transitive closure on DAGs: breaking\\nthrough the O(n2). Journal of the ACM (JACM), 52(2):147–156, 2005.\\n26. Camil Demetrescu and Giuseppe F Italiano. Dynamic shortest paths and transitive closure: Algorithmic\\ntechniques and data structures. Journal of Discrete Algorithms, 4(3):353–383, 2006.\\n27. Camil Demetrescu and Giuseppe F Italiano. Experimental analysis of dynamic all pairs shortest path algo-\\nrithms. ACM Transactions on Algorithms (TALG), 2(4):578–601, 2006.\\n28. Wei Ding and Guohui Lin. Partially dynamic single-source shortest paths on digraphs with positive weights.\\nIn International Conference on Algorithmic Applications in Management, pages 197–207. Springer, 2014.\\n29. Yefim Dinitz. Dinitz’algorithm: The original version and Even’s verion. In Theoretical computer science,\\npages 218–240. Springer, 2006.\\n30. Dorit Dor, Shay Halperin, and Uri Zwick. All-pairs almost shortest paths. SIAM Journal on Computing,\\n29(5):1740–1759, 2000.\\n31. Michael Elkin. Computing almost shortest paths. In Proceedings of the twentieth annual ACM symposium\\non Principles of distributed computing, pages 53–62. ACM, 2001.\\n32. Michael Elkin and David Peleg. (1 + ǫ,β)-spanner constructions for general graphs. SIAM Journal on\\nComputing, 33(3):608–631, 2004.\\n33. Shimon Even and Yossi Shiloach. An on-line edge-deletion problem. Journal of the ACM (JACM), 28(1):1–4,\\n1981.\\n34. Jittat Fakcharoenphol and Satish Rao. Planar graphs, negative weight edges, shortest paths, and near linear\\ntime. In Foundations of Computer Science, 2001. Proceedings. 42nd IEEE Symposium on, pages 232–241.\\nIEEE, 2001.\\n35. Daniele Frigioni, Mario Ioffreda, Umberto Nanni, and Giulio Pasqualone. Experimental analysis of dynamic\\nalgorithms for the single source shortest paths problem. Journal of Experimental Algorithmics (JEA), 3:5,\\n1998.\\n36. Daniele Frigioni, Tobias Miller, Umberto Nanni, Giulio Pasqualone, Guido Schaefer, and Christos Zaroliagis.\\nAn experimental study of dynamic algorithms for directed graphs. In European Symposium on Algorithms,\\npages 368–380. Springer, 1998.\\n37. Daniele Frigioni, Tobias Miller, Umberto Nanni, and Christos Zaroliagis. An experimental study of dynamic\\nalgorithms for transitive closure. Journal of Experimental Algorithmics (JEA), 6:9, 2001.\\n38. Bernhard Haeupler, Telikepalli Kavitha, Rogers Mathew, Siddhartha Sen, and Robert E Tarjan. Incremental\\ncycle detection, topological ordering, and strong component maintenance. ACM Transactions on Algorithms\\n(TALG), 8(1):3, 2012.\\n39. Monika R. Henzinger and Valerie King. Fully dynamic biconnectivity and transitive closure. In Foundations\\nof Computer Science, 1995. Proceedings., 36th Annual Symposium on, pages 664–672. IEEE, 1995.\\n40. Monika R. Henzinger and Valerie King. Maintaining minimum spanning forests in dynamic graphs. SIAM\\nJournal on Computing, 31(2):364–374, 2001.\\n41. Monika R. Henzinger, Sebastian Krinninger, and Danupon Nanongkai. Decremental single-source shortest\\npaths on undirected graphs in near-linear total update time. In Foundations of Computer Science (FOCS),\\n2014 IEEE 55th Annual Symposium on, pages 146–155. IEEE, 2014.\\n42. Monika R. Henzinger, Sebastian Krinninger, and Danupon Nanongkai. Sublinear-time decremental algorithms\\nfor single-source reachability and shortest paths on directed graphs. In Proceedings of the 46th Annual ACM\\nSymposium on Theory of Computing, pages 674–683. ACM, 2014.\\n43. Monika R. Henzinger, Sebastian Krinninger, and Danupon Nanongkai. A subquadratic-time algorithm for\\ndecremental single-source shortest paths. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium\\non Discrete Algorithms, pages 1053–1072. Society for Industrial and Applied Mathematics, 2014.\\n44. Monika R. Henzinger, Sebastian Krinninger, and Danupon Nanongkai. Improved algorithms for decremental\\nsingle-source reachability on directed graphs. arXiv preprint arXiv:1612.03856, 2016.\\n45. Monika R. Henzinger, Sebastian Krinninger, Danupon Nanongkai, and Thatchaphol Saranurak. Unifying\\nand strengthening hardness for dynamic problems via the online matrix-vector multiplication conjecture. In\\nProceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, pages 21–30. ACM,\\n2015.\\n46. Jacob Holm and Eva Rotenberg. Dynamic planar embeddings of dynamic graphs. In LIPIcs-Leibniz Inter-\\nnational Proceedings in Informatics, volume 30. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2015.\\n47. Giuseppe F Italiano. Finding paths and deleting edges in directed acyclic graphs. Information Processing\\nLetters, 28(1):5–11, 1988.\\n48. Raj Iyer, David Karger, Hariharan Rahul, and Mikkel Thorup. An experimental study of polylogarithmic,\\nfully dynamic, connectivity algorithms. Journal of Experimental Algorithmics (JEA), 6:4, 2001.\\n49. Harpreet Kaur. Algorithms for solving the rubik’s cube: A study of how to solve the rubik’s cube using two\\nfamous approaches: The thistlewaite’s algorithm and ida* algorithm., 2015.\\n50. Valerie King. Fully dynamic algorithms for maintaining all-pairs shortest paths and transitive closure in\\ndigraphs. In Foundations of Computer Science, 1999. 40th Annual Symposium on, pages 81–89. IEEE, 1999.\\n51. Valerie King and Mikkel Thorup. A space saving trick for directed dynamic transitive closure and shortest\\npath algorithms. In International Computing and Combinatorics Conference, pages 268–277. Springer, 2001.\\n52. Tsvi Kopelowitz, Seth Pettie, and Ely Porat. Higher lower bounds from the 3sum conjecture. In Proceedings\\nof the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1272–1287. Society for\\nIndustrial and Applied Mathematics, 2016.\\n53. Jakub Łącki. Improved deterministic algorithms for decremental reachability and strongly connected com-\\nponents. ACM Transactions on Algorithms (TALG), 9(3):27, 2013.\\n54. Kasper Green Larsen and Ryan Williams. Faster online matrix-vector multiplication. arXiv preprint\\narXiv:1605.01695, 2016.\\n55. Bi-Qing Li, Tao Huang, Lei Liu, Yu-Dong Cai, and Kuo-Chen Chou. Identification of colorectal cancer related\\ngenes with mrmr and shortest path in protein-protein interaction network. PloS one, 7(4):e33393, 2012.\\n56. Edo Liberty and Steven W Zucker. The mailman algorithm: A note on matrix–vector multiplication. Infor-\\nmation Processing Letters, 109(3):179–182, 2009.\\n57. Amgad Madkour, Walid G. Aref, Faizan Ur Rehman, Mohamed Abdur Rahman, and Saleh Basalamah. A\\nsurvey of shortest-path algorithms. arXiv preprint arXiv:1705.02044, 2017.\\n58. Aleksander Madry. Faster approximation schemes for fractional multicommodity flow problems via dynamic\\ngraph algorithms. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 121–130.\\nACM, 2010.\\n59. TS Motzkin. Evaluation of polynomials and evaluation of rational functions. Bulletin of the AMS, 61(163):10,\\n1955.\\n60. Meghana Nasre, Matteo Pontecorvi, and Vijaya Ramachandran. Decremental all-pairs all shortest paths\\nand betweenness centrality. In International Symposium on Algorithms and Computation, pages 766–778.\\nSpringer, 2014.\\n61. Mihai Patrascu. Towards polynomial lower bounds for dynamic problems. In Proceedings of the forty-second\\nACM symposium on Theory of computing, pages 603–610. ACM, 2010.\\n62. Matteo Pontecorvi and Vijaya Ramachandran. Fully dynamic all pairs all shortest paths. arXiv preprint\\narXiv:1412.3852, 2014.\\n63. Thomas Reps. Program analysis via graph reachability. Information and software technology, 40(11):701–726,\\n1998.\\n64. Liam Roditty. Decremental maintenance of strongly connected components. In Proceedings of the Twenty-\\nFourth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1143–1150. Society for Industrial and\\nApplied Mathematics, 2013.\\n65. Liam Roditty and Uri Zwick. Dynamic approximate all-pairs shortest paths in undirected graphs. In Foun-\\ndations of Computer Science, 2004. Proceedings. 45th Annual IEEE Symposium on, pages 499–508. IEEE,\\n2004.\\n66. Liam Roditty and Uri Zwick. Improved dynamic reachability algorithms for directed graphs. SIAM Journal\\non Computing, 37(5):1455–1471, 2008.\\n67. Liam Roditty and Uri Zwick. On dynamic shortest paths problems. Algorithmica, 61(2):389–401, 2011.\\n68. Piotr Sankowski. Faster dynamic matchings and vertex connectivity. In Proceedings of the eighteenth annual\\nACM-SIAM symposium on Discrete algorithms, pages 118–126. Society for Industrial and Applied Mathe-\\nmatics, 2007.\\n69. Nicola Santoro and Jorge Urrutia. An improved algorithm for boolean matrix multiplication. Computing,\\n36(4):375–382, 1986.\\n70. John E Savage. An algorithm for the computation of linear forms. SIAM Journal on Computing, 3(2):150–158,\\n1974.\\n71. Robert Endre Tarjan. Efficiency of a good but not linear set union algorithm. Journal of the ACM (JACM),\\n22(2):215–225, 1975.\\n72. Mikkel Thorup. Fully-dynamic min-cut. In Proceedings of the thirty-third annual ACM symposium on Theory\\nof computing, pages 224–230. ACM, 2001.\\n73. Mikkel Thorup and Uri Zwick. Approximate distance oracles. Journal of the ACM (JACM), 52(1):1–24,\\n2005.\\n74. Ryan Williams. Matrix-vector multiplication in sub-quadratic time:(some preprocessing required). In SODA,\\nvolume 7, pages 995–1001, 2007.\\n75. Mihalis Yannakakis. Graph-theoretic methods in database theory. In Proceedings of the ninth ACM SIGACT-\\nSIGMOD-SIGART symposium on Principles of database systems, pages 230–242. ACM, 1990.\\n76. Daniel M Yellin. Speeding up dynamic transitive closure for bounded degree graphs. Acta Informatica,\\n30(4):369–384, 1993.\\n77. Christos Zaroliagis. Implementations and experimental studies of dynamic graph algorithms. Experimental\\nAlgorithmics, pages 229–278, 2002.\\n78. Uri Zwick. All pairs shortest paths using bridging sets and rectangular matrix multiplication. Journal of the\\nACM (JACM), 49(3):289–317, 2002.\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd25'), 'authors': 'Helfer, Brian, Kepner, Jeremy, Reuther, Albert, Ricke, Darrell O., Samsi, Siddharth', 'year': '2017', 'title': 'A Linear Algebra Approach to Fast DNA Mixture Analysis Using GPUs', 'full_text': 'A Linear Algebra Approach to Fast DNA Mixture\\nAnalysis Using GPUs\\nSiddharth Samsi, Brian Helfer, Jeremy Kepner, Albert Reuther and Darrell O. Ricke\\nMIT Lincoln Laboratory\\nLexington, MA\\nAbstract—Analysis of DNA samples is an important step in\\nforensics, and the speed of analysis can impact investigations.\\nComparison of DNA sequences is based on the analysis of short\\ntandem repeats (STRs), which are short DNA sequences of 2-5\\nbase pairs. Current forensics approaches use 20 STR loci for\\nanalysis. The use of single nucleotide polymorphisms (SNPs)\\nhas utility for analysis of complex DNA mixtures. The use of\\ntens of thousands of SNPs loci for analysis poses significant\\ncomputational challenges because the forensic analysis scales by\\nthe product of the loci count and number of DNA samples\\nto be analyzed. In this paper, we discuss the implementation\\nof a DNA sequence comparison algorithm by re-casting the\\nalgorithm in terms of linear algebra primitives. By developing an\\noverloaded matrix multiplication approach to DNA comparisons,\\nwe can leverage advances in GPU hardware and algoithms for\\nDense Generalized Matrix-Multiply (DGEMM) to speed up DNA\\nsample comparisons. We show that it is possible to compare 2048\\nunknown DNA samples with 20 million known samples in under\\n6 seconds using a NVIDIA K80 GPU.\\nI. INTRODUCTION\\nDNA forensics is the branch of forensic science that focuses\\non the use of genetic material in criminal investigations [1].\\nShort tandem repeats (STRs) are stretches of DNA containing\\nshort repeat units of of neucleotides that are used in forensic\\nDNA and human identity testing [2]. DNA forensics currently\\nuses STRs for 20 chromosomal locations, referred to as\\nthe Combined DNA Index System (CODIS) loci. Comparing\\nSTR profiles between samples and individuals is the current\\nstandard for justice systems. Samples with more than one\\nDNA contributor are difficult or impossible to analyze using\\nonly STR profiles. Profiling single nucleotide polymorphisms\\n(SNPs) has advantages over STRs for comparisons with mix-\\nture samples [3]. In the United States, the Federal Bureau of\\nInvestigation (FBI) has a database of over 16 million profiles\\nin the National DNA Index System (NDIS). Comparing a large\\nnumber of DNA profiles with this large dataset of known\\nreference DNA profiles is currently a computationally expen-\\nsive process and is typically done in a large datacenter. The\\nFastID [4] method was developed to enable rapid searching\\nof forensic panels with large numbers of loci and runs on x86\\nprocessors. In this paper we cast the FastID method as a dense\\nDISTRIBUTION STATEMENT A. Approved for public release: distribu-\\ntion unlimited. This material is based upon work supported by the Assistant\\nSecretary of Defense for Research and Engineering under Air Force Contract\\nNo. FA8721-05-C-0002 and/or FA8702-15-D-0001. Any opinions, findings,\\nconclusions or recommendations expressed in this material are those of the\\nauthor(s) and do not necessarily reflect the views of the Assistant Secretary\\nof Defense for Research and Engineering.\\nmatrix multiplication operation and use graphics processing\\nunits (GPUs) to enable very fast comparisons between profiles\\nof individuals to individuals, individuals to mixtures, and\\nmixtures to mixtures.\\nThe paper is organized as follows: Section II describes\\nthe process of DNA analysis for forensics applications. Sec-\\ntion II-A gives an overview of the FastID method for DNA\\nmixture comparisons, and in Section II-B we describe the\\nproblem as a dense matrix multiplication algorithm. In Sec-\\ntion III, details of the GPU implementation of the FastID\\nalgorithm and optimizations are described. Finally, in Section\\nIV we present the results of our approach when used to analyze\\nlarge DNA datasets and we summarize in Section V.\\nII. DNA MIXTURE COMPARISON\\nDNA is composed of a series of molecules called nu-\\ncleotides and are encoded as A, C, G and T corresponding\\nto the four types of nucleotides. An allele is a variant of a\\ngene that is located at a specific position on a specific chromo-\\nsome. A single nucleotide polymorphism (SNP) is a genetic\\nvariation between individuals and represents a difference in\\na single nucleotide in a DNA sample. On average there are\\n10 million SNPs in the human genome [5]. SNPs can act as\\nbiological markers of disease and can be used for identifying\\ninheritance within families. In the context of DNA forensics,\\ncomparing SNPs in DNA samples can help identify individuals\\nor relatives.\\nA SNP typically has a major allele that is most common in\\na population of people and a minor allele with a lower allele\\nfrequency than the major allele. Most SNPs have typically\\nonly two alleles but more alleles are possible. Let M represent\\na major allele and m respresent a minor allele. With two alleles\\nfor a SNP, there are four possibilities for the SNP for an\\nindividual: MM, Mm, mM, and mm. To compare a set of SNPs of\\nsize N between two individuals, 2N comparisons are needed\\nto compare all alleles.\\nA. Algorithm for SNP comparison\\nThe FastID DNA mixture comparison algorithm used in\\nthis paper was first developed by Ricke [4]. This algorithm\\ncan be used to compare DNA samples from individuals as\\nwell as mixtures of samples. The algorithm identifies the\\nsimilarity between two samples by first performing a bitwise\\nexclusive-OR (XOR) operation between the reference (known)\\nDNA sample and the query (unknown) DNA sample as shown\\nar\\nX\\niv\\n:1\\n70\\n7.\\n00\\n51\\n6v\\n1 \\n [c\\ns.P\\nF]\\n  3\\n Ju\\nl 2\\n01\\n7\\nQ11 Q12 Q13 Q14 \\nUnknown DNA Samples  \\n(Queries) \\nR11 R12 R13 R14 \\nR21 R22 R23 R24 \\nR31 R32 R33 R34 \\nR41 R42 R43 R44 \\nKnown DNA Samples \\n(References) \\nR11 R12 R13 R14 R21 R22 R23 R24 \\nQ11 Q12 Q13 Q14 Q11 Q12 Q13 Q14 \\nR41 R42 R43 R44 \\nQ11 Q12 Q13 Q14 \\nBitwise XOR, AND \\nPopulation Count \\nPairwise Comparison \\nC1 C2 C3 C4 \\nP(Q1,R1) \\nC1 C2 C3 C4 C1 C2 C3 C4 \\nP(Q1,R2) P(Q1,R3) \\nBitwise XOR, AND Bitwise XOR, AND \\nPopulation Count Population Count \\nFig. 1: Algorithm for DNA mixture analysis: An unknown DNA sample is compared against each known DNA sequence.\\nin Figure 1. The next step is to perform a bitwise AND\\noperation between this result and the reference sample. Finally,\\na count of the number of set bits in the result of the AND\\noperation gives a measure of the similarity between the known\\nand unknown DNA sample. In practice, DNA samples can\\nbe compared by mapping the string SNP alleles to binary\\nrepresentations and comparing the profiles directly with the\\ncomputer hardware XOR instruction. The 1-bits in the result\\nrepresent all positions where there is a difference in the minor\\nalleles between the two individuals. The computer hardware\\npopulation count (POPCOUNT) instruction can then be used\\nto sum the 1-bits in the result to identify all of the minor\\nallele differences between the two profiles. To compare an\\nindividual sample to a mixture, a logical AND operation is\\nperformed between the XOR results and the individual profile\\nto only consider the minor alleles of the individual.\\nLet Ri be the reference DNA sample and Qj be the un-\\nknown DNA sample. The similarity between the two samples\\nas quantified by the population count Pij is given by\\nPij = POPCOUNT (AND(XOR(Ri, Qj), Ri)) (1)\\nIn the implementation of the FastID algorithm, the DNA\\nsamples are first converted from alleles to an array of unsigned\\nintegers. A DNA sample with 512 SNPs can be mapped\\nto 16 unsigned 32-bit integer numbers. A 512 SNP DNA\\nsample is thus represented by a length 16 array of unsigned\\nintegers. For example, let’s consider a DNA sample with\\n32 SNPs: 0x06001440. The binary representation of this\\nSNP is 00000110000000000001010001000000 and the 32-bit\\nunsigned integer decimal equivalent of this is 100668480. This\\nprocedure is used to convert all known and unknown DNA\\nsamples into arrays of 32-bit unsigned integers. The algorithm\\nproceeds by performing the operation in Equation 1 for each\\ninteger in the arrays representing the known and unknown\\nDNA samples. The length of the array depends on the number\\nof SNPs used in the comparison and will be denoted by NW\\nin the rest of the paper. The algorithm for comparing a single\\nunknown DNA mixture of legnth NW with a known sample\\nof the same length is shown in Algorithm 1. This algorithm\\ncan be viewed as an overloaded dot-product of two vectors of\\nlength NW where the multiplication operation is replaced by\\nsequence of logical XOR and AND operations followed by\\nthe population count (POPCOUNT) operation.\\nData: Known DNA sample R of length NW\\nData: Unknown DNA mixture Q of length NW\\nResult: Population count P\\ninitialization;\\nfor i = 0 to NW − 1 do\\nA = XOR(R[i], Q[i])\\nB = AND(A,R[i])\\nPopcount[i] = Popcount[i] + POPC(B)\\nend\\nAlgorithm 1: The core implementation the SNP comparison\\nalgorithm: A single known DNA sample R of length NW is\\ncompared with an unknown mixture Q of the same length.\\nIn practice, law enforcement agencies such as the Federal\\nBureau of Investigation (FBI) have millions of known DNA\\nprofiles and a correspondingly large number of unknown\\nsamples that need identification. Let NR be the number of\\nknown DNA samples and NQ be the number of unknown\\nsamples, each of length NW as described previously. The\\nalgorithm in Algorithm 1 can now be re-written as shown\\nin Algorithm 2. The operation in Equation 1 must now be\\nperformed NR ∗NQ ∗NW times.\\nB. DNA Comparison as Matrix Multiplication\\nGiven NR known DNA samples of length NW and NQ\\nunknown DNA mixtures of length NW , the goal is to compare\\nevery unknown sample with every known sample. In this case,\\nwe can now view this procedure as an overloaded dot product\\nof NQ vectors representing unknown samples with each of\\nthe NR known samples as shown in Figure 2. We cast the\\nproposed algorithm as a dense matrix multiplication operation\\nby organizing the input data into two matrices of size NR\\nx NW and NW x NQ representing the known and unknown\\nsamples, respectively. Thus, the population counts for a given\\nset of DNA samples can be represented by the overloaded\\nmatrix multiplication operation C = AB, where A is of\\ndimension NR x NW , B is of dimension NW x NQ and C is of\\ndimension NR x NQ. The matrix multiplication is overloaded\\nQ11 Q12 Q13 Q14 \\nUnknown DNA Samples  \\n(Queries) \\nR11 R12 R13 R14 \\nR21 R22 R23 R24 \\nR31 R32 R33 R34 \\nR41 R42 R43 R44 \\nKnown DNA Samples \\n(References) \\nXOR, AND, \\nPOPCOUNT \\nQ31 Q32 Q33 Q34 \\nQ21 Q22 Q23 Q24 \\nR\\n11 \\nR\\n12 \\nR\\n13 \\nR\\n14 \\nQ\\n11 \\nQ\\n12 \\nQ\\n13 \\nQ\\n14 \\nR\\n41 \\nR\\n42 \\nR\\n43 \\nR\\n44 \\nQ\\n11 \\nQ\\n12 \\nQ\\n13 \\nQ\\n14 \\nP(Q1,R1) P(Q1,R4) \\nXOR, AND, \\nPOPCOUNT \\nR\\n11 \\nR\\n12 \\nR\\n13 \\nR\\n14 \\nQ\\n21 \\nQ\\n22 \\nQ\\n23 \\nQ\\n24 \\nR\\n41 \\nR\\n42 \\nR\\n43 \\nR\\n44 \\nQ\\n21 \\nQ\\n22 \\nQ\\n23 \\nQ\\n24 \\nP(Q2,R1) P(Q2,R4) \\nXOR, AND, \\nPOPCOUNT \\nR\\n11 \\nR\\n12 \\nR\\n13 \\nR\\n14 \\nQ\\n31 \\nQ\\n32 \\nQ\\n33 \\nQ\\n34 \\nR\\n41 \\nR\\n42 \\nR\\n43 \\nR\\n44 \\nQ\\n31 \\nQ\\n32 \\nQ\\n33 \\nQ\\n34 \\nP(Q3,R1) P(Q3,R4) \\nXOR, AND, \\nPOPCOUNT \\nComparing each unknown DNA sample with each known DNA sample \\nXOR, AND, \\nPOPCOUNT \\nXOR, AND, \\nPOPCOUNT \\nFig. 2: Algorithm for DNA mixture analysis: Each unknown DNA sample is compared against each known DNA sequence.\\nData: NR known DNA samples of length NW\\nData: NQ unknown DNA samples/mixtures length NW\\nResult: PQR Population counts\\ninitialization;\\nfor i = 0 to NQ − 1 do\\nfor j = 0 to NR − 1 do\\nfor k = 0 to NW − 1 do\\nA = XOR(Ri[k], Qj [k])\\nB = AND(A,Ri[k])\\nPopcount[i, j] = Popcount[i, j]+POPC(B)\\nend\\nend\\nend\\nAlgorithm 2: A naı¨ve implementation the SNP comparison\\nalgorithm for NQ individuals and NR mixtures.\\nas shown in Equation 1, where the multiply operation in the\\nmatrix multiplication algorithm is replaced by a logical XOR\\nand AND operations followed by the POPCOUNT operation.\\nIII. DGEMM ON GPU FOR MIXTURE ANALYSIS\\nA. GPU Architecture\\nThe algorithm described in this paper was developed on the\\nNVIDIA TESLA K80 GPU and will be referred to as K80 in\\nGPU Device CPU Serial Code \\nGPU Kernel \\nCPU Serial Code \\nBlock(0,0) \\nShared Memory \\nRegisters Registers \\nConstant Memory \\nThread \\n(0,0) \\nThread \\n(1,0) \\nGlobal Memory \\nBlock(0,1) \\nShared Memory \\nRegisters Registers \\nThread \\n(0,0) \\nThread \\n(1,0) \\nC\\nod\\ne \\nex\\nec\\nut\\nio\\nn \\ntim\\nel\\nin\\ne \\nFig. 3: CUDA program execution and GPU memory architec-\\nture, after [6].\\nthe remainder of the paper. The K80 consists of 4992 NVIDIA\\nCUDA cores in a dual-GPU design with an aggregate 24GB\\nGB of GDDR5 memory [7]. The processing described in this\\npaper used a single GPU in the K80.\\nFigure 3 shows the execution of a program written using the\\nNVIDIA CUDA programming platform and language and the\\nmemory hierarchy of NVIDIA GPUs. The serial code runs on\\nthe CPU and the parallel section of the code, implemented\\nusing the CUDA library is launched on the GPU kernel.\\nThe CUDA programming model enables programmers to run\\nfine-grained parallel code on the GPU on a large number\\nof threads [8]. Threads are organized into grid blocks as\\nshown in Figure 3. A block is a group of threads that runs\\non a single multiprocessor where they have access to 64KB\\nof shared memory on the K80. A collection of threads that\\nrun concurrently on the GPU is called a warp. For detailed\\ndescriptions of the execution of a CUDA program, the reader\\nis referred to Kirk & Wu [6]. The GPU also has several\\ntypes of memory available to each individual thread: global,\\nshared and constant memory. Constant memory is read-only\\nfor the threads whereas the global and shared memories\\ncan be written to and read by the threads. The amount of\\nshared and constant memory on the GPU is significantly\\nsmaller than the global memory but accesses to the shared\\nand constant memory are much faster than global memory. The\\noptimization of CUDA programs involves the management of\\ndata transfers to the GPU, data layout in device memory and\\nthe maximization of computation to global memory transfer\\nratio. These optimizations are discussed in Section III-B.\\nB. Optimizing overloaded matrix multiplication on GPU\\nMatrix multiplication is a widely researched topic and there\\nhas been a significant amount of research towards optimizing\\ndense matrix-matrix multiplication (DGEMM) on the GPU.\\nThe BLAS [10], [11] library provides routines for basic vector\\nand matrix operations, including matrix-matrix multiplication.\\nOptimized libraries such as ATLAS [12] and Intel MKL [13]\\nare also available for a variety of platforms. In addition,\\nlibraries such as MAGMA [14] and NVIDIA cuBLAS [15]\\nFig. 4: Blocked matrix multiplication: Each thread computes\\none element of the output matrix [9].\\nalso offer optimized implementations of matrix-matrix multi-\\nplications that can leverage multi-core processors and GPUs.\\nThe approaches to optimizing dense matrix multiplication\\nalgorithm [6], [16], [17] have been well researched and are\\nutilized in the development of our algorithm as described in\\nthis section.\\nGiven matrices A and B of appropriate dimensions, the\\nnaı¨ve approach to matrix multiplication ported to the GPU\\nis shown in Algorithm 3. A single GPU thread computes\\none output element of the matrix C. In order to compute a\\nsingle output of the output matrix, each thread has to copy\\none row and one column of matrices A and B respectively\\nfrom global memory, compute the overloaded inner product\\nfrom Equation 1 and copy the result back to global memory.\\nData: blockIdx, blockDim, threadIdx - Block and thread\\nidentifiers defined by CUDA\\nData: A, B - 2D Arrays of type 32-bit unsigned integer\\nResult: Popcount as described in Section II-A\\ni = blockIdx.y * blockDim.y + threadIdx.y\\nj = blockIdx.x * blockDim.x + threadIdx.x\\nfor k = 0 to N-1 do\\nTMP = TMP + POPCOUNT( AND( XOR( A[i,k],\\nB[k,j] ), A[i,k] )\\nend\\nC[i,j] = TMP\\nAlgorithm 3: A naı¨ve CUDA based implementation of\\nthe SNP comparison algorithm for NQ individuals and NR\\nmixtures.\\nTiling and Shared Memory usage The naı¨ve approach to\\nmatrix multiplication described earlier is bandwidth\\nbound. The number of global memory transfers can be\\nreduced by improving data locality through tiling and\\nthe use of shared memory. The tiling approach involves\\ncomputing the output for a small block at a time and re-\\nusing the data already fetched from global memory. The\\nGPU threads load a block of data required to compute\\na sub-block Csub of the output matrix C into shared\\nmemory. The required sub-matrices Asub and Bsub are\\nloaded into the shared memory of a given block of threads\\nand are used for computing the output matrix Csub. This\\napproach is illustrated in Figure 4. In this paper, block\\nsizes of 16, 32 and 64 were used depending on the\\nnumber of SNPs in the data being analyzed.\\nCompute optimization In addition to the tiled approach, a\\nsecond optimization technique proposed by Volkov [18]\\nis to compute more elements of the output matrix Csub\\nper thread. This allows the use of fewer threads leading\\nto a greater use of registers and more computations being\\nperformed in parallel. In this paper we compute 16 output\\nelements per thread. We also employ loop unrolling to\\nunroll inner loops in the CUDA kernel that are not\\nunrolled by the NVIDIA compiler by default.\\nMemory access coalescing Two dimensional arrays in\\nC/C++ are stored in row-major format. As a result, the\\nmemory accesses to the matrix A by threads in a block\\nare coalesced; i.e., threads in a wrap access successive\\nmemory locations in the GPU global memory. By\\ncoalescing memory accesses, the number of clock cycles\\nrequired to fetch data from global memory to shared\\nmemory can be minimized. If memory accesses are\\nnot coalesced, the global memory access is effectively\\nserialized. By transposing matrix B in memory before\\ntransferring it to the GPU device, memory access to B\\ncan also be coalesced. The memory layout of matrices\\nA and B is adjusted appropriately while reading in the\\ndata from input files.\\nC. Comparing Large Numbers of DNA Mixtures\\nGPUs have a limited amount of RAM. The experiments\\ndescribed in this paper were conducted using a NVIDIA Tesla\\nK80 GPU with 12GB of RAM. This limits the size of the\\nmatrices that are created in a kernel. For example, comparing\\n1,000,000 known DNA profiles with 2048 unknown profiles,\\neach of length NW , represented using 32-bit unsigned integers,\\ngenerates a result matrix C of size 2048 x 1,000,000 that\\nrequires 65GB of memory. To compare large numbers of DNA\\nmixtures, we break up the computation into a series of smaller\\ncomparisons.\\nMoving data between the GPU memory space and the\\nCPU memory space can be a significant bottleneck in GPU\\ncomputing. One technique for hiding latency in data transfers\\nbetween the GPU and CPU is to overlap compute with the data\\ntransfers. However, in our case, the entire memory available\\non the GPU is used for storing the inputs and the results of the\\nDNA comparison algorithm in order to minimize the number\\nof GPU kernel launches and the number of data transfers\\nbetween the CPU and GPU. As a result, it is not possible to\\noverlap the compute with data transfers. Typically the number\\nof unknown DNA profiles is significantly smaller than the\\nnumber of known reference profiles. In this case, we transfer\\nall the query profiles and a block of known reference profiles\\nto the GPU, followed by a GPU kernel launch to perform the\\ncomparisons. The next batch of known profiles to compare\\nagainst is transferred to the GPU at the same time that the\\nresults from the previous batch are copied back to the CPU.\\n1 5 10 15 20\\nNumber of known samples (millions)\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\nG\\nPU\\n K\\ner\\nne\\nl t\\nim\\ne \\n(m\\ns.) 128 SNPs256 SNPs\\n512 SNPs\\n(a) Number of unknown samples = 512\\n1 5 10 15 20\\nNumber of known samples (millions)\\n0\\n1000\\n2000\\n3000\\n4000\\n5000\\n6000\\nG\\nPU\\n K\\ner\\nne\\nl t\\nim\\ne \\n(m\\ns.) 128 SNPs256 SNPs\\n512 SNPs\\n(b) Number of unknown samples : 1024\\n1 5 10 15 20\\nNumber of known samples (millions)\\n0\\n2000\\n4000\\n6000\\n8000\\n10000\\nG\\nPU\\n K\\ner\\nne\\nl t\\nim\\ne \\n(m\\ns.) 128 SNPs256 SNPs\\n512 SNPs\\n(c) Number of unknown samples : 2048\\nFig. 5: GPU Kernel time for comparing unknown DNA profiles\\nagainst 1M, 5M, 10M, 15M and 20M target profiles using\\nSNPs of length 128, 256 and 512.\\nIV. RESULTS\\nTo test the performance of the proposed algorithm for\\ncomparing DNA mixtures, we compared 512, 1024 and 2048\\nunknown DNA profiles against 1, 5, 10, 15 and 20 Million\\nknown profiles. Because of the large mismatch the number\\nof known and unknown profiles, all unknown profiles were\\ntransferred to the GPU along with a block of known pro-\\nfiles. Depending on the total number of comparisons to be\\nperformed, the number of known reference profiles used in\\na given kernel launch was changed such that all memory on\\nthe GPU was utilized. This also helped minimize the number\\nof data transfers between the CPU and GPU memory. As\\na result of nearly full utilization of GPU memory for each\\nkernel launch, it was not possible to overlap data transfers and\\ncomputation. Experiments were also perfomed to measure the\\nperformance of using pinned and non-pinned memory in the\\nGPU kernel.\\nFigures 5a, 5b and 5c show the cumulative GPU kernel time\\nfor comparing DNA mixtures with 128, 256 and 512 SNPs\\nrespectively. While the total time spent in the GPU kernel\\nis a function of the total number of comparisons between\\nknown and unknown DNA samples, the total time for the\\nalgorithm is dominated by the time required to transfer results\\nback to the GPU. Transfer times for copying the known and\\nunknown DNA samples to the GPU are a significantly smaller\\nfraction of the total time spent in data transfers because of the\\nrelatively small amount of data being copied. Figure 6 shows\\nthe cummulative GPU kernel time and the total time spent in\\ndata transfers between the GPU and the CPU memory. As seen\\nin this figure, the time spent in transferring data between the\\nCPU and GPU tends to dominate. This time can be reduced by\\noffloading additional computations to the GPU or performing\\nadditional reduction operation on the data in GPU memory.\\nAdditionally, the use of pinned memory can reduce the time\\nit takes to copy results back to the CPU memory as shown\\nin Figure 7. Using pinned memory provides a consistently\\nfaster data transfer time as compared with the use of non-\\npinned memory but this comes at the cost of a small added\\noverhead at the time that the memory is allocated for the first\\ntime.\\n1 5 10 15 20\\nNumber of known DNA samples (millions)\\n0\\n2000\\n4000\\n6000\\n8000\\nTi\\nm\\ne \\n(m\\ns.)\\nGPU kernel time\\nData transfer time\\n(a) Number of unknown samples : 512, SNP length : 128\\n1 5 10 15 20\\nNumber of known DNA samples (millions)\\n0\\n0.5\\n1\\n1.5\\n2\\n2.5\\n3\\n3.5\\nTi\\nm\\ne \\n(m\\ns.)\\n×104\\nGPU kernel time\\nData transfer time\\n(b) Number of unknown samples : 2048, SNP length : 512\\nFig. 6: Cummulative GPU kernel time and data transfer time\\nfor comparing known and unknown DNA profiles: As data size\\nincreases, the ratio of compute to data transfers improves.\\nV. SUMMARY\\nIn this paper we discuss the formulation of DNA forensics\\nas a dense linear algebra problem. A GPU based approach\\nis used to speed up computations that involve comparing\\nmillions of known DNA profiles with a few thousand unknown\\nprofiles. Current approaches to DNA forensics employed by\\nthe forensics community require large computing systems\\nand can take hours. By using GPUs and overloaded matrix\\nmultiplication as desribed in this paper, it is possible to reduce\\nthe compute time required to process large amounts of data.\\nIn this paper we use a single NVIDIA K80 for computations\\n1M 5M 10M 15M 20M\\nNumber of known DNA samples\\n0\\n2000\\n4000\\n6000\\nD\\nat\\na \\ntra\\nns\\nfe\\nr t\\nim\\ne \\n(m\\ns.) Pinned memoryNon-pinned memory\\nFig. 7: Comparison of data transfer time using pinned memory\\nand non-pinned memory: Results shown for SNP length of\\n512.\\nbut this approach can be extended to use mulitple GPUs on\\nthe same system for a further reduction in compute times.\\nAdditionally, this implementation can also be run on laptops\\nwith NVIDIA hardware.\\nACKNOWLEDGEMENT\\nThe authors would like to thank Adam Michaleas and\\nMichael Jones for their support with NVIDIA hardware and\\nsoftware configuration. We would also like to thank David\\nMartinez for his support.\\nREFERENCES\\n[1] National Library of Medicine and National Human Genome Institute and\\nNational Institutes of Health, “DNA Forensics - GeneEd - Genetics,\\nEducation, Discovery,” https://geneed.nlm.nih.gov/topic subtopic.php?\\ntid=37, 2017, [Online; accessed 18-May-2017].\\n[2] J. M. Butler et al., “Short tandem repeat typing technologies used in\\nhuman identity testing,” Biotechniques, vol. 43, no. 4, pp. 2–5, 2007.\\n[3] J. Isaacson, E. Schwoebel, A. Shcherbina, D. Ricke, J. Harper, M. Petro-\\nvick, J. Bobrow, T. Boettcher, B. Helfer, C. Zook, and E. Wack, “Robust\\ndetection of individual forensic profiles in DNA mixtures,” Forensic\\nScience International: Genetics, vol. 14, pp. 31 – 37, 2015.\\n[4] D. O. Ricke, “FastID: Extremely Fast Forensic DNA Comparisons,” In\\nprogress, 2017.\\n[5] U.S. National Library of Medicine, “What are single nucleotide poly-\\nmorphisms (SNPs)?” https://ghr.nlm.nih.gov/primer/genomicresearch/\\nsnp, 2017, [Online; accessed 18-May-2017].\\n[6] D. Kirk and W.-m. W. Hwu, Programming Massively Parallel Processors\\n- A hands on approach. Morgan Kaufman, 2013.\\n[7] NVIDIA Corp., “TESLA K80 GPU Accelerator,” http://images.nvidia.\\ncom/content/pdf/kepler/Tesla-K80-BoardSpec-07317-001-v05.pdf,\\n2015, [Online; accessed 19-May-2017].\\n[8] S. Samsi, V. Gadepally, and A. Krishnamurthy, “Matlab for signal\\nprocessing on multiprocessors and multicores,” IEEE Signal Processing\\nMagazine, vol. 27, no. 2, March 2010.\\n[9] NVIDIA Corp., “Programming guide : Cuda toolkit documentation,”\\nhttp://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html,\\n2017, [Online; accessed 25-April-2017].\\n[10] L. S. Blackford, J. Demmel, J. Dongarra, I. Duff, S. Hammarling,\\nG. Henry, M. Heroux, L. Kaufman, A. Lumsdaine, A. Petitet, R. Pozo,\\nK. Remington, and R. C. Whaley, “An updated set of basic linear algebra\\nsubprograms (blas),” ACM Trans. Math. Softw., vol. 28, no. 2, pp. 135–\\n151, Jun. 2002.\\n[11] C. L. Lawson, R. J. Hanson, D. R. Kincaid, and F. T. Krogh, “Basic\\nlinear algebra subprograms for fortran usage,” ACM Trans. Math. Softw.,\\nvol. 5, no. 3, pp. 308–323, Sep. 1979.\\n[12] R. C. Whaley and J. Dongarra, “Automatically Tuned Linear Algebra\\nSoftware,” in Ninth SIAM Conference on Parallel Processing for Scien-\\ntific Computing, 1999, cD-ROM Proceedings.\\n[13] Intel Corp., “Intel Math Kernel Library,” https://software.intel.com/\\nen-us/intel-mkl/, 2016, [Online; accessed 06-Oct-2016].\\n[14] J. Dongarra, M. Gates, A. Haidar, J. Kurzak, P. Luszczek, S. Tomov, and\\nI. Yamazaki, “Accelerating numerical dense linear algebra calculations\\nwith gpus,” Numerical Computations with GPUs, pp. 1–26, 2014.\\n[15] NVIDIA Corp., “cuBLAS,” https://developer.nvidia.com/cublas, 2016,\\n[Online; accessed 06-Oct-2016].\\n[16] V. Volkov and J. W. Demmel, “Benchmarking gpus to tune dense linear\\nalgebra,” in 2008 SC - International Conference for High Performance\\nComputing, Networking, Storage and Analysis, Nov 2008, pp. 1–11.\\n[17] J. Li, S. Ranka, and S. Sahni, Multi- and Many-Core Technologies:\\nArchitectures, Programming, Algorithms, and Applications. Chapman-\\nHall/CRC Press, 2013.\\n[18] V. Volkov, “Better performance at lower occupancy,” in Proceedings of\\nthe GPU technology conference, GTC, vol. 10. San Jose, CA, 2010,\\np. 16.\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd26'), 'authors': 'Bringmann, K., Nakos, V.', 'year': '2022', 'title': 'Top-k-Convolution and the Quest for Near-Linear Output-Sensitive Subset Sum', 'full_text': 'arXiv:2107.13206v1  [cs.DS]  28 Jul 2021Top-k-Convolution andthe Quest for Near-Linear Output-Sensitive Subset Sum∗Karl Bringmann† Vasileios Nakos‡AbstractIn the classical SubsetSum problem we are given a set X and a target t, and the task isto decide whether there exists a subset of X which sums to t. A recent line of research hasresulted in Õ(t)-time algorithms, which are (near-)optimal under popular complexity-theoreticassumptions. On the other hand, the standard dynamic programming algorithm runs in timeO(n · |S(X, t)|), where S(X, t) is the set of all subset sums of X that are smaller than t. Further-more, all known pseudopolynomial algorithms actually solve a stronger task, since they actuallycompute the whole set S(X, t).As the aforementioned two running times are incomparable, in this paper we ask whether onecan achieve the best of both worlds: running time Õ(|S(X, t)|). In particular, we ask whetherS(X, t) can be computed in near-linear time in the output-size. Using a diverse toolkit containingtechniques such as color coding, sparse recovery, and sumset estimates, we make considerableprogress towards this question and design an algorithm running in time Õ(|S(X, t)|4/3).Central to our approach is the study of top-k-convolution, a natural problem of independentinterest: given sparse polynomials with non-negative coefficients, compute the lowest k non-zeromonomials of their product. We design an algorithm running in time Õ(k4/3), by a combinationof sparse convolution and sumset estimates considered in Additive Combinatorics. Moreover,we provide evidence that going beyond some of the barriers we have faced requires either analgorithmic breakthrough or possibly new techniques from Additive Combinatorics on how topass from information on restricted sumsets to information on unrestricted sumsets.∗This work is part of the project TIPEA that has received funding from the European Research Council (ERC)under the European Unions Horizon 2020 research and innovation programme (grant agreement No. 850979).†Saarland University and Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, Ger-many. bringmann@cs.uni-saarland.de‡Saarland University and Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, Ger-many. vnakos@mpi-inf.mpg.de1Contents1 Introduction 11.1 Subset Sum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.2 Top-k-Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.3 Our Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Results and Techniques 32.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32.1.1 Problem Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32.1.2 Covering of Prefix-Restricted Sumsets . . . . . . . . . . . . . . . . . . . . . . 42.2 Formal Statement of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42.2.1 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42.2.2 Additional Results: Relaxing the Upper Bound . . . . . . . . . . . . . . . . . 52.2.3 Additional Results: Interval-Restricted Sumset and Convolution . . . . . . . . 52.3 Overview of Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62.3.1 Reduction from Subset Sum to Prefix-Restricted Sumset Computation . . . . 62.3.2 Restricted Sumset Computation . . . . . . . . . . . . . . . . . . . . . . . . . . 72.4 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 Top-k-Convolution 114 Restricted Sumset Computation: Learning the Output-Size 125 Interval-Restricted Sumset Computation 145.1 An Õ(√mn · out)-Time Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155.2 Hardness Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166 Relaxed Version of Prefix-Restricted Convolution 177 Construction of the Õ(out4/3)-cost Covering 187.1 An Additive Combinatorics Ingredient: Ruzsa’s Triangle Inequality . . . . . . . . . . 197.2 Description of the Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197.3 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208 Lower Bound on Coverings 239 Reducing SubsetSum to Prefix-Restricted Sumset Computation 279.1 Handling Large Elements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299.2 General Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309.3 A Number-Theoretic Lemma for the decrease of Subset Sums . . . . . . . . . . . . . 309.4 Putting Everything Together . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3310 Acknowledgements 3411 Conclusion and Future Work 35References 3511 Introduction1.1 Subset SumSubsetSum is a fundamental problem at the intersection of computer science, mathematical op-timization, and operations research. In this problem, given a set X of n integers and a target t,the task is to decide whether there exists a subset of X that sums to t. The problem belongs toKarp’s initial list of NP-complete problems [29], and it has given rise to a plethora of algorith-mic techniques, see, e.g., the monographs [30, 34]. Apart from being a cornerstone in algorithmdesign, SubsetSum draws its importance from being a special case of many other problems, likeKnapsack or Integer Programming. It has also played a role in cryptography, as Merkle andHellman [35] based their cryptosystem on this problem, see also [43, 13, 18, 40, 26].Several classic algorithms for SubsetSum are typically taught in undergraduate courses, in-cluding the meet-in-the-middle algorithm running in time O(2n/2) [25] and Bellman’s dynamicprogramming algorithm running in pseudopolynomial time O(n · t) [12].Surprisingly, after decades of research, major algorithmic advances were still discovered in thelast 10 years, e.g., [39, 33, 21, 7, 24, 8, 9, 32, 11, 38, 14, 31, 27, 1, 10]. Among these developments, themost relevant for this paper are improvements over Bellman’s O(n·t) algorithm: Koiliaris and Xu [31]designed a deterministic algorithm running in time1 Õ(min{√n·t, t4/3}), and Bringmann [14] deviseda randomized algorithm running in time Õ(t) (which was improved in terms of log factors in [27]).The running time Õ(t) of the randomized algorithms is optimal under the Strong Exponential TimeHypothesis [1] as well as under the SetCover Hypothesis [20].Thus, research on pseudopolynomial algorithms for SubsetSum with respect to parameter t ismore or less finished. However, it remains to study whether the recent improvements generalize toother parameters as well as to variants of SubsetSum. For instance, this has been done for theModularSubsetSum problem in [10]. In this paper, we start from the observation that Bellman’sclassic dynamic programming algorithm can be implemented to run in time O(n · |S(X, t)|), whereS(X, t) is the set of all subset sums of X below t. Since |S(X, t)| can be much smaller than t, sofar the running times Õ(t) and O(n · |S(X, t)|) are incomparable. Thus, despite the running timeÕ(t) being matched by a conditional lower bound, in situations where |S(X, t)| is small Bellman’salgorithm can outperform the recent improved algorithms. To obtain the best of both worlds,it would thus be desirable to consider |S(X, t)|, rather than t, as the parameter to measure thecomputational complexity of the problem, and to similarly shave off the factor n from the runningtime of Bellman’s algorithm. In particular, since all previous pseudopolynomial algorithms forSubsetSum produce all attainable subset sums smaller than t, a natural question is whether onecan design a near-linear output-sensitive algorithm.Question 1.1. Is there an algorithm that computes S(X, t) in time Õ(|S(X, t)|)?Our work struggles to make progress towards understanding SubsetSum under this new com-putational perspective, and it lead us to study a new type of sparse convolution problem.1.2 Top-k-ConvolutionConvolution and Boolean convolution are fundamental computational primitives that frequentlyarise in algorithm design, e.g., when combining solutions of two subproblems. The Boolean con-volution f ©⋆ g of vectors f, g ∈ {0, 1}u is the vector with entries (f ©⋆ g)k =∨0≤i≤k fi ∧ gk−i for1By Õ(T ) we hide factors of the form polylog(T ) as well as factors polylog(u), where u is the universe size, andpolylog(t), where t is the target.10 ≤ k < 2u. This arises when we split a problem into two subproblems, so that the whole problemhas a solution of size k if and only if for some i the left subproblem has a solution of size i and theright subproblem has a solution of size k− i. Moreover, Boolean convolution is equivalent to sumsetcomputation, where we are given sets A,B ⊆ {0, 1, . . . , u − 1} and the task is to compute A + B,the set of all sums a+ b with a ∈ A, b ∈ B.The convolution f ⋆ g of vectors f, g ∈ Ru is the vector with entries (f ⋆ g)k =∑ki=0 fi · gk−i for0 ≤ k < 2u. When we split a problem into two subproblems, and fi and gi count the number ofsize-i solutions of the left and right subproblem, then (f ⋆ g)k counts the number of size-k solutionsof the whole problem. Moreover, convolution is equivalent to polynomial multiplication, where weare given the coefficients of two polynomials and want to compute the coefficients of their product.Boolean convolution can be solved via convolution, and convolution can be solved in timeO(u log u) using Fast Fourier Transform (FFT). However, when the input vectors are sparse, wecan ask for algorithms that compute convolutions much faster than performing FFT. Specifically,the ultimate goal is an algorithm running in near-linear output-sensitive time, i.e., in near-lineartime in terms of the number of non-zero entries of f ⋆ g. This practically and theoretically relevantproblem is called sparse convolution (or sparse polynomial multiplication) and has been studied,e.g., in [19, 41, 36, 46, 6, 17, 42, 37, 15]. The ultimate goal of near-linear output-sensitive time hasbeen achieved by Cole and Hariharan [19] for non-negative vectors by a Las Vegas algorithm, in [37]for general vectors by a Monte Carlo algorithm, and in [15] for non-negative vectors by an almostlinear time deterministic algorithm.In this paper we study a natural variant that we call Top-k-Convolution: Given vectors f, g ∈ Ru,compute the k lowest non-zero entries of f ⋆g. Formally, denoting by i1 < i2 < . . . < iℓ all indices ofnon-zero entries of f ⋆g, our goal is to compute the pairs (i1, (f ⋆g)i1), . . . , (ik, (f ⋆g)ik ). Boolean Top-k-Convolution is defined analogously, i.e., the task is to compute the lowest k indices i1 < . . . < ikof 1-entries of f ©⋆ g.“Top-k” problems, that ask for the k best solutions, are well motivated from a practical perspec-tive, e.g., for displaying search results. Note that in the setting where we split a problem into twosubproblems, Boolean Top-k-Convolution asks for the k smallest solution sizes. In the polynomialmultiplication setting, Top-k-Convolution asks for the k lowest-degree monomials in the productof two given sparse polynomials. The problem is equivalent if we replace “lowest” by “highest” (byreversing f and g), and thus Top-k-Convolution is also equivalent to computing the k highest-degreemonomials in the product of two given sparse polynomials. As an additional major application, wepresent a connection to SubsetSum in this paper.Therefore, sparse Top-k-Convolution is a well-motivated problem with several applications, andit is surprising that to the best of our knowledge it has not been explicitly studied before. Through-out the paper we will assume that f, g are non-negative vectors, since this is satisfied in manyapplications and the possible cancelations resulting from negative entries make the problem consid-erably harder (indeed, sparse convolution with negative entries has been solved much later [37] thanon non-negative vectors [19]). Note that Top-k-Convolution on non-negative vectors can be solvednaively in time O(k2): The lowest k non-zeros of f ⋆ g are among the combinations of the lowest knon-zeros of f and the lowest k non-zeros of g. This allows us to assume that f and g are k-sparse,and we can compute f ⋆ g naively in time O(k2) to obtain its lowest k non-zeros.1.3 Our ContributionWe initiate the study of SubsetSum with respect to the parameter |S(X, t)|, making considerableprogress in this direction. We show an “output-sensitivity preserving” reduction from SubsetSumto Top-k-Convolution, such that if the latter can be solved in near-linear output-sensitive time so can2the former. Then, we investigate upper and lower bounds for Top-k-Convolution on non-negativevectors (as well as related restricted variants of sumset computation and sparse convolution). Inparticular, we present a randomized Õ(k4/3)-time algorithm for Top-k-Convolution, resulting in timeÕ(|S(X, t)|4/3) for SubsetSum. Our algorithms fall into a natural class that we call “rectanglecovering algorithms”, for which we show that they cannot yield near-linear time. Our technicalmachinery draws from a wide range of techniques such as color coding, sparse convolutions, andsumset estimates from Additive Combinatorics.2 Results and Techniques2.1 PreliminariesWe write N = {0, 1, . . .} and [n] = {0, 1, . . . , n} for any n ∈ N. For sets A,B ⊆ N define A + B ={a + b | a ∈ A, b ∈ B}. We also define max(A) = maxx∈A x, and similarly min(A). For any setX ⊆ N we define Σ(X) =∑x∈X x. For t ∈ N we define the set of all subset sums of X below t asS(X, t) = {Σ(Y ) | Y ⊆ X, Σ(Y ) ≤ t}.For integers a, b we write [a, b] = {a, . . . , b}, (a, b) = {a + 1, . . . , b − 1}, and similarly for (a, b] and[a, b).For a vector f ∈ Rd we let ‖f‖0 be its number of non-zero entries. For a set S ⊆ [d] we denoteby fS the vector that is zeroed out outside of S, i.e., (fS)i = fi for i ∈ S and (fS)i = 0 otherwise.We shall use 0-indexed vectors throughout the paper. We also use a non-standard notation of Õ(T )throughout the paper that hides factors of the form polylog(T ) as well as polylog(u), where u is theuniverse size, or polylog(t), where t is the target.We shall need the following result by Cole and Hariharan and an immediate corollary.Theorem 2.1 ([19]). Given non-negative vectors f, g of length d, we can compute f ⋆ g in expectedtime O(‖f ⋆ g‖0 · log2 d).Theorem 2.2. Given A,B ⊆ [u], we can compute A+B in expected time O(|A+B| · log2 u).Proof. Let f be the indicator vector of A, g be the indicator vector of B, d = 2u + 1 and useTheorem 2.1.2.1.1 Problem DefinitionsOur work is concerned with the following problems.Definition 2.3 (Subset Sum). Given a set X ⊆ N and a number t, compute S(X, t) = {Σ(Y ) |Y ⊆ X, Σ(Y ) ≤ t}. We measure the running time in terms of |S(X, t)|. We write n = |X|.Definition 2.4 (Top-k-Convolution). Given two vectors f, g ∈ Ru and a parameter k, compute thefirst k non-zero entries of f ⋆ g. In other words, find (i1, (f ⋆ g)i1), (i2, (f ⋆ g)i2), . . . (ik, (f ⋆ g)ik )where i1 < i2 < . . . < ik are the smallest k indices on which f ⋆ g is non-zero. We measure therunning time in terms of k. We write n = ‖f‖0 and m = ‖g‖0.Top-k-convolution is equivalent to the following problem, as we will show in Lemma 3.1.Definition 2.5 (Prefix-Restricted Convolution). Given a positive integer u and two vectors f, g ∈Ru, compute (f ⋆ g)[u], i.e., compute (a sparse representation of) the first u entries of f ⋆ g. Wemeasure the running time in terms of the output-size out = ‖(f ⋆ g)[u]‖0. We write n = ‖f‖0 andm = ‖g‖0.3The following problem is a Boolean version of prefix-restriced convolution.Definition 2.6 (Prefix-restricted Sumset Computation). Given u ∈ N and A,B ⊆ [u], compute(A+B) ∩ [u]. We measure the running time in terms of the output-size out = |(A+B) ∩ [u]|. Wewrite n = |A| and m = |B|.2.1.2 Covering of Prefix-Restricted SumsetsFor a set A ⊆ N of size n, we implicitly assume that A is sorted and we denote its elements in sortedorder as A1 < A2 < . . . < An. Moreover, for I ⊆ {1, . . . , n} we write AI for {Ai | i ∈ I}.We define the notion of a covering of a restricted sumset (A,B, [u]). Intuitively, we want to cover(A+B)∩[u] by sumsets of the form AI+BJ , that is, we want to have (A+B)∩[u] ⊆⋃(I,J)∈C AI+BJ .Definition 2.7. Let u ∈ N and A,B ⊆ [u] with n = |A|, m = |B|. A covering of (A,B, [u]) is afamily C such that:1. C consists of pairs (I, J) where I ⊆ {1, . . . , n} and J ⊆ {1, . . . ,m}.2. For any 1 ≤ i ≤ n, 1 ≤ j ≤ m with Ai +Bj ∈ [u] there exists (I, J) ∈ C with (i, j) ∈ I × J .We call a covering C unique if the pair in property 2. is unique, i.e., for any 1 ≤ i ≤ n, 1 ≤ j ≤ mwith Ai +Bj ∈ [u] there exists a unique pair (I, J) ∈ C such that (i, j) ∈ I × J .We call a covering C a rectangle covering if for any (I, J) ∈ C the sets I and J are intervals,meaning that they consists of contiguous elements in the sorted order of A and B, respectively.The cost of a covering C is ∑(I,J)∈C|AI +BJ | .This notion is useful because of the following fact.Observation 2.8. Given a covering C of (A,B, [u]) of cost c, we can compute the prefix-restrictedsumset (A+B) ∩ [u] in expected time Õ(c).Proof. Using output-sensitive sumset computation (Theorem 2.2), we can compute AI + BJ inexpected time O(|AI+BJ | log2 u). Thus, we can compute R :=⋃(I,J)∈C AI+BJ in time O(c log2 u).By the covering property, we can simply return R ∩ [u] = (A+B) ∩ [u].We refer to an algorithm making use of Observation 2.8 as a covering algorithm. We call it aunique-rectangle-covering algorithm if the used covering is a unique rectangle covering. This is anatural class of algorithms, as we also explain in Section 2.3.2. All algorithms presented in thispaper are unique-rectangle-covering algorithms.2.2 Formal Statement of Results2.2.1 Main ResultsAs the technical core of our paper, we present an efficient construction of low-cost coverings.Theorem 2.9 (Covering Construction, Section 7). Given A,B ⊆ [u], in expected time Õ(out4/3) wecan compute a unique rectangle covering of (A,B, [u]) of cost Õ(out4/3), where out = |(A+B)∩ [u]|.By Observation 2.8, this yields an Õ(out4/3) Las Vegas algorithm for prefix-restricted sumsetcomputation. Via simple reductions, we obtain similar algorithms for our convolution problems.4Corollary 2.10 (Top-k-Convolution and Related Problems, Section 3). Top-k-convolution on non-negative vectors can be solved in expected time Õ(k4/3). Prefix-restricted sumset computation andprefix-restricted convolution on non-negative vectors can be solved in expected time Õ(out4/3).By carefully adapting a recent pseudopolynomial Õ(n + t)-time SubsetSum algorithm [14] touse prefix-restricted sumset computation as a subroutine, we obtain our main result for SubsetSum.This can be seen as a reduction from SubsetSum to prefix-restricted sumset computation.Theorem 2.11 (SubsetSum, Section 9). Given X ⊆ N and t ∈ N, we can compute the set S(X, t)in time Õ(|S(X, t)|4/3) with high probability.Since all of these results depend on our technical core (Theorem 2.9), we also study limitationsof rectangle-covering algorithms. The following result shows that our approach of using rectanglecoverings to solve top-k-convolution and SubsetSum cannot achieve near-linear output-sensitivealgorithms.Theorem 2.12 (Lower Bound on Rectangle Coverings, Section 8). There exists an infinite sequenceof tuples (A,B, [u]), with u ∈ N and A,B ⊆ [u], such that any rectangle covering of (A,B, [u]) hascost Ω(out1.047), where out = |(A+B) ∩ [u]|.We remark that this result crucially uses rectangle coverings; we do not rule out the existenceof non-rectangle coverings of near-linear cost.In the remainder of Section 2.2 we present additional related results.2.2.2 Additional Results: Relaxing the Upper BoundIn our results so far we have relaxed the ultimate goal of algorithms running in time Õ(out) to timeÕ(out4/3). We can alternatively relax the goal by measuring the running time in terms of a slightlylarger upper bound. Specifically, for prefix-restricted sumset computation so far we measured timein terms of the output-size out = |(A+B)∩ [u]|. Now we will measure the running time in terms of|(A+B)∩ [(1 + ζ)u]| for some ζ > 0, while the task still is to compute the set (A+B)∩ [u]. Sinceone could expect that for “realistic” instances (A,B, u) there are not many sums in the boundaryregion (A+B)∩ [u, (1+ ζ)u], algorithms that perform well with respect to this new measure mightperform well in practice. We show the following.Theorem 2.13 (Relaxed Upper Bound, Section 6). Prefix-restricted sumset computation can besolved in expected time Õ(ζ−1 · |(A+ B) ∩ [(1 + ζ)u]|+ ζ−2). Prefix-restricted convolution on non-negative vectors can be solved in expected time Õ(ζ−1 · ‖(f ⋆ g)[(1+ζ)u]‖0 + ζ−2). SubsetSum can besolved in time Õ(ζ−1 · |S(X, (1 + ζ)t)|+ ζ−2) with high probability.Note that |S(X, (1+ζ)t)| ≤ (1+ζ)t, and thus plugging in any constant ζ > 0 yields an algorithmrunning in time Õ(t), which is as good as [14, 27].2.2.3 Additional Results: Interval-Restricted Sumset and ConvolutionSo far we studied the problem of computing the first u output values of sumset computation andconvolution. More generally, we could ask to compute all output values at given positions P ⊆ N.This is a well-motivated generalization, as it corresponds to computing specific coefficients of theproduct of two polynomials, which comes up in counting problems.However, here we show that even when the desired positions P form an interval then near-linearoutput-sensitive algorithms are unlikely to exist. Specifically, we study the following problems.5Definition 2.14 (Interval-Restricted Sumset Computation). Given positive integers ℓ ≤ u andA,B ⊆ [u], compute (A + B) ∩ [ℓ, u]. We denote the output-size by out = |(A + B) ∩ [ℓ, u]|. Wewrite n = |A| and m = |B|.Definition 2.15 (Interval-Restricted Convolution). Given positive integers ℓ ≤ u and vectors f, g ∈Ru, compute (f ⋆g)[ℓ,u], i.e., compute (a sparse representation of) the entries of (f ⋆g)i for ℓ ≤ i ≤ u.We denote the output-size by out = ‖(f ⋆ g)[ℓ,u]‖0. We write n = ‖f‖0 and m = ‖g‖0.We design algorithms for these problems with the following running time. This uses a naturalgeneralization of coverings, where we simply replace the set [u] by [ℓ, u].Theorem 2.16 (Algorithm for Interval-Restricted). Interval-restricted sumset computation andinterval-restricted non-negative convolution can be solved in expected time Õ(n +m+√nm out).We present conditional lower bounds based on two classical problems: Boolean matrix multipli-cation and sliding window Hamming distance. In Boolean matrix multiplication, we are asked tomultiply two Boolean n× n-matrices. By a reduction to multiplying real-valued matrices, Booleanmatrix multiplication can be solved in time O(nω), where ω ≤ 2.373 is the exponent of fast matrixmultiplication [47, 22]. Limitations of fast matrix multiplication have been recently studied, and itwas shown that known techniques cannot facilitate ω < 2 + 16 [5, 4, 3]. In particular, ω > 2 is abarrier that known techniques cannot overcome, also for Boolean matrix multiplication.In the sliding window Hamming distance problem, we are given a text of length 2n and a patternof length n, both over a (possibly large) alphabet Σ, and want to find the Hamming distance betweenthe pattern and every length-n substring of the text. This problem admits a Õ(n3/2)-time algorithm,and it is a major open question in string algorithms whether a faster algorithm exists [2, 23].Theorem 2.17 (Hardness for Interval-Restricted). Let δ > 0. If interval-restricted sumset compu-tation is in time Õ((n +m+ out)ω/2−δ), then Boolean matrix multiplication is in time Õ(nω−2δ).If interval-restricted convolution on non-negative vectors is in time Õ(n +m+ (nm out)1/2−δ),then sliding window Hamming distance is in time Õ(n3/2−3δ).2.3 Overview of Techniques2.3.1 Reduction from Subset Sum to Prefix-Restricted Sumset ComputationTo reduce SubsetSum to prefix-restricted sumset computation, our starting point is the pseu-dopolynomial Õ(t)-time algorithm of [14], which uses a combination of color coding and FFT. Weobserve that in this algorithm all uses of FFT in fact compute prefix-restricted sumsets, so wecan replace FFT by an algorithm for prefix-restricted sumset computation. This directly yields areduction from SubsetSum to prefix-restricted sumset computation, but it does not directly givethe desired guarantees. Our goal is to make this reduction “output-sensitivity preserving”, that is,if we can compute prefix-restricted sumsets in near-linear output-sensitive time then the reductionyields an algorithm for SubsetSum that runs in near-linear output-sensitive time.To this end, we need to take a closer look at the algorithm in [14]. Given (X, t), this algorithmsplits X into the set X(L) of numbers that are larger than t/polylog(n) and the remaining smallnumbers X(S). Any subset summing to at most t uses at most polylog(n) large numbers, which en-ables a color-coding-based algorithm in [14]. We observe that, without any problems, replacing theusage of FFT in that part of the algorithm by near-linear output-sensitive prefix-restricted sumsetcomputation yields a near-linear output-sensitive algorithm for handling the large numbers X(L).The small numbers X(S) are then further split at random into two subsets X(1),X(2). By concen-tration inequalities, this splits the subset Y into two subsets Y (i) = Y ∩X(i) satisfying with high6probability Σ(Y (i)) ≤ (1 + ǫ)t/2. We can therefore find Σ(Y (1)) and Σ(Y (2)) by recursive calls on(X(1), (1+ǫ)t/2) and (X(2), (1+ǫ)t/2). Since we recurse on two subproblems and the target bound tis roughly split in half, one can argue that the total running time is Õ(t) [14].In this paper, we instead measure the running time in terms of |S(X, t)|. Hence, it does notsuffice that the target bound t is roughly split in half; we also need that the measure |S(X, t)| isroughly split in half. Indeed, if we can show this, then we again obtain two subproblems and themeasure |S(X, t)| is roughly split in half, so we can argue that the running time is Õ(|S(X, t)|). Thisrequires novel insights into the set of all subset sums, to obtain inequalities relating the number ofsubset sums of X to that of its subsets. Specifically, we show that X(1) and X(2) satisfy|S(X(1), (1 + ǫ)t/2)| + |S(X(2), (1 + ǫ)t/2)| ≤ (1 +O(ǫ))|S(X, t)| +O(1).This inequality only holds if every element of X(1),X(2) is sufficiently smaller than the target t.Indeed, in the proof of this inequality we use that all numbers in X(1),X(2) are small, which justifieswhy we removed the large numbers X(L).2.3.2 Restricted Sumset ComputationWe now give an overview of our techniques for restricted sumset computation, and discuss therelation to classical results from Additive Combinatorics.Consider the prefix-restricted sumset computation of (A + B) ∩ [u]. Note that we can assumeA,B ⊆ [u], since larger numbers cannot form sums of at most u. We also always implicitly assumethat for every number a ∈ A there is a number b ∈ B with a+ b ≤ u, since otherwise we can removea from A without changing (A + B) ∩ [u]. In other words, we assume max(A) + min(B) ≤ u, andsymmetrically min(A)+max(B) ≤ u, which can be ensured after a O(|A|+ |B|)-time preprocessing.Standard sumset computation does not suffice. Standard sparse convolution (Theorem 2.2)allows us to compute A + B in near-linear output-sensitive time Õ(|A + B|). In particular, sinceA + B ⊆ [2u], we can compute (A + B) ∩ [2u] in near-linear output-sensitive time. Although thismight seem close at first glance, careful inspection reveals that it can be much larger than thegoal |(A + B) ∩ [u]|. Indeed, we can construct sets A,B ⊆ [u] with |A| = |B| = n satisfying|(A+B)∩ [2u]| = Ω(n2) and |(A+B)∩ [u]| = O(n). Hence, in general we cannot afford to compute(A+B) ∩ [2u] when our goal is to compute (A+B) ∩ [u] in near-linear output-sensitive time.For this construction, assume that u is a sufficiently large even integer and letX := [n] = {0, 1, 2, . . . , n}, Y := n[n] = {0, n, 2n, . . . , n2},A := {0} ∪ ({u2}+X), B := {0} ∪ ({u2 }+ Y ).One can check that max(A)+min(B), min(A)+max(B) ≤ u2 +n2 ≤ u. Since for any x ∈ X, y ∈ Ythe sum (u2 + x) + (u2 + y) is larger than u, we obtain |(A+B) ∩ [u]| ≤ |{0}| + |X|+ |Y | = 2n+ 1.Moreover, one can check that X + Y = [n2 + n], and thus |(A+B) ∩ [2u]| = |A+B| ≥ n2.A natural class of algorithms. We investigate a natural and safe algorithmic approach: Tocompute (A + B) ∩ [u], we make sure that every sum Ai + Bj ≤ u is computed, where 1 ≤ i ≤ n,1 ≤ j ≤ m. Since we can use as a subroutine that standard sumset computation is in near-linear output-sensitive time, it is natural to compute sumsets AI + BJ for some I ⊆ {1, . . . , n}and J ⊆ {1, . . . ,m}. Combining these two arguments, we arrive at our notion of covering. Inparticular, we want to compute a covering C of (A,B, [u]), and then use Observation 2.8 to compute7(⋃(I,J)∈C AI + BJ)∩ [u] = (A + B) ∩ [u] in time proportional to the cost of C. Hence, coveringalgorithms are a natural class of algorithms for prefix-restricted sumset computation.We are particularly interested in unique coverings, because they allow us to compute prefix-restricted convolutions: A pair Ai + Bj being covered exactly once corresponds to adding theproduct fi · gj exactly once to the output value (f ⋆ g)i+j .Moreover, we are interested in rectangle coverings, i.e., the case of I, J being intervals, because(i) they have a more geometric flavour and are thus easier to argue about, (ii) all constructions ofnon-rectangular coverings that we came up with could be adapted to become rectangular, so we arenot aware of any better non-rectangular coverings, and (iii) for rectangular coverings we can show(unconditional) lower bounds.Designing covering algorithms. In what follows, for ease of exposition we shall assume that|A| = |B| = n. Our first algorithmic step is to reduce to a promise version of the problem, wherewe know out = |(A + B) ∩ [u]| up to a constant factor. (In fact, we could even assume to know asuperset of (A+B)∩ [u] whose size is at most a constant factor larger, but we do not know how toexploit such a set in this context.)The easiest possible covering algorithm partitions {1, . . . , n} into k consecutive intervals I1, . . . , Ikfor A, and similarly into J1, . . . , Jk for B. We may ignore pairs (x, y) with min(AIx)+min(BJy) > u,since they do not contain any sum Ai + Bj ≤ u. For all remaining (x, y), we construct the pairs(Ix, Jy) to form a covering C. In the following we discuss two ways of choosing I1, . . . , Ik andJ1, . . . , Jk.Choosing the interval partitioning such that it splits the universe [u] into parts of length u/kensures that any sum s ∈ AI + BJ for (I, J) ∈ C satisfies s ≤ (1 + 2k )u. Moreover, we show thatany s ∈ N appears in at most k sumsets AI +BJ , (I, J) ∈ C (this holds because every “diagonal” ofsubproblems contains distinct sums). In total, we can bound the cost of C by k ·|(A+B)∩[(1+ 2k )u]|,which for ζ = 2/k yields one result of Theorem 2.13, see Section 6.Alternatively, we can choose the interval partitioning such that it splits A and B into partsof size n/k. There are O(k) pairs (I, J) ∈ C along the “boundary”, where min(AI + BJ) ≤ u <max(AI + BJ). For each such pair we bound the cost trivially by (nk )2. The remaining pairs onlycover sums in (A + B) ∩ [u], and each such sum is covered at most k times, as in the previousparagraph. In total, this yields cost at most k · out + n2k , which is O(n · out1/2) by optimizingover k. Since n ≤ out, in particular the cost is at most O(out3/2). Generalizing this idea to theinterval-restricted case yields Theorem 2.16, see Section 5. For the interval-restricted case, this costbound turns out to be (conditionally) optimal, see Theorem 2.17.So far, we used simple charging arguments to obtain a covering of cost O(out3/2). We showthat for prefix-restricted sumset computation we can bypass the hardness results for the interval-restricted case and reduce the exponent further to 4/3. This requires insights from Additive Combi-natorics. Specifically, to obtain an Õ(out4/3) algorithm, we make use of Ruzsa’s triangle inequality.This inequality from Additive Combinatorics is a charging argument that implies|AI +BJ | ≤|AI +BJ ′ ||AI′ +BJ ′ ||AJ ′ +BJ ||I ′||J ′| ,for any sets I, I ′, J, J ′ ⊆ {1, . . . , n}. In particular, if we choose I ′ and J ′ such that the three sumsetson the right hand side are all subsets of [u], then the size of every sumset on the right hand side isbounded from above by out = |(A+B)∩ [u]|. With some care, this inequality can be used to show|AI +BJ | ≤out3min(I)min(J).8This allows us to bound the number of “bad” sums in AI +BJ , namely the sums in (AI +BJ) \\\\ [u],in terms of the output-size and the position of the rectangle (I, J). However, it turns out thatthis bound is not good enough, essentially because it yields a worst-case bound that holds for allrectangles (I, J). We thus replace it with an improved bound that holds for “most” rectangles (I, J).Combining this bound with several tricks from our previous covering constructions allows us toconstruct a covering of cost Õ(out4/3), see Section 7.Connection to the Balog-Szemerédi-Gowers Theorem. Probably the most important resulton restricted sumsets is the Balog-Szemerédi-Gowers (BSG) theorem [45, Theorem 2.13]. Given setsA,B ⊆ Z and a set of pairs G ⊆ A× B, the BSG theorem is used to pass from information aboutthe restricted sumset A +G B = {a + b | (a, b) ∈ G} to information about an unrestricted sumsetAI +BJ , for some I, J ⊆ {1, . . . , n} (where we continue to assume that |A| = |B| = n). Specifically,the BSG theorem states that if |G| = δn2 and |A+GB| = cn, then there exist sets I, J ⊆ {1, . . . , n}of size Ω(δ2n) with |AI +BJ | ≤ O((c/δ)5n).In a breakthrough paper, Chan and Lewenstein [17] algorithmically exploited the BSG theoremin order to solve several problems with additive structure. On a high level, their approach uses theBSG theorem repeatedly, constructing a sequence (I(1), J (1)), . . . , (I(k), J (k)) of subsets I(i), J (i) ⊆{1, . . . , n} and a remainder R ⊆ A×B such thatA+G B =(k⋃i=1(AI(i) +BJ(i)))⋃{a+ b : (a, b) ∈ R} (1)(2)For |A +G B| = cn, this decomposition satisfies that all sumsets are small, i.e., |AI(i) + BJ(i) | =O((kc)5n), and the remainder is not too large, i.e., |R| = O(n2/k). Using this decomposition,one can compute (a superset of) A+G B by computing the unrestricted sumsets AI(i) + BJ(i) andthen iterating over all elements of R. Using near-linear output-sensitive sumset computation, thistakes total time Õ(k6c5n + n2/k). For several applications considered in [17], this approach yieldsnon-trivial improvements. We note, however, that constructing the sequence and the remainder intheir setting requires quadratic time in n.A careful reader could notice that our notion of a covering is very similar to the decompositioncomputed by Chan and Lewenstein [17]. In our situation, we are interested in the set G = {(a, b) ∈A × B | a + b ≤ u}, since our goal is to compute A +G B = (A + B) ∩ [u]. A covering C satisfiesA +G B ⊆⋃(I,J)∈C AI + BJ =: T , from which we can compute A +G B as T ∩ [u]. Exceptfor the remainder set R, this is analogous to Chan and Lewenstein’s decomposition. Thus, bothnotions produce a certain form of covering of A+G B.2 Unfortunately, in our situation using theirdecomposition as a black box is worse than the simple covering of cost O(out3/2) for two reasons.The first is that the time needed to construct their decomposition is Θ(|A||B|), which could be upto out2. The second reason is that the bound they get on the cost of the covering they produce isstrictly worse than out3/2. Thus, even if their decomposition was given to us for free, it would beworse than the easy approach which uses elementary charging arguments.Our approach in this paper can be seen as using the arguments of the proof of the BSG theoremin an ad-hoc manner for our specific set G. Indeed, at the heart of the BSG theorem lies a chargingargument that is analogous to Ruzsa’s triangle inequality (see [17, Section 7.3]), and careful inspec-tion of the algorithmic version of the BSG theorem (see [17, Sections 2 and 7]) shows that it can2We remark that Chan and Lewenstein [17] construct their decomposition for a somewhat different reason, sincethey are not (immediately) concerned with output-sensitivity.9be modified to produce a rectangle covering for our set G with no loss in parameters. We presenta highly optimized variant of this ad-hoc construction in this paper.In summary, our algorithms can be viewed as ad-hoc BSG-type theorems for a special choice ofthe restriction set G, obtaining bounds that seem unreachable in the more general setting.Connection to Freiman-type Theorems. Another celebrated result among combinatorialistsis Freiman’s theorem [45, Theorem 5.33]: If A ⊆ N satisfies |A + A| ≤ c|A|, then A is containedin an f(c)-dimensional arithmetic progression of length g(c) · |A|. Such a result could potentiallybe useful in our situation, since for a rectangle with AI + BJ ⊆ [u] we have |AI + BJ | ≤ out, andthus a small output-size could imply that AI and BJ are essentially arithmetic progressions, whichis easy to exploit. However, the state-of-the-art bounds on f(·), g(·) are exponential and thus notuseful in applications. This is why much of Additive Combinatorics has focused on bypassing theneed for Freiman’s theorem, to prove robust theorems with polynomial parameter dependence.Recently, [44] announced such a robust extension of another theorem by Freiman, called the 3k−4theorem, which roughly states that if |A+GB| is “small” and |G| is “large” (|G| ≥ (1−ǫ)|A||B|), thenone can extract highly non-trivial information about A and B. However, the “small” and “large”conditions are too restrictive for our algorithmic applications, and it is unclear to us at the momentto what extent this result can be exploited algorithmically. We expect that further interaction withthe field of Additive Combinatorics yields more insights that lead to new algorithmic machinery.Selection from X + Y . A problem that seems related at first glance is selecting the k-th elementfrom X + Y and row-sorted matrices, see [28] and references therein. The crucial difference is thatin that line of research X + Y is treated as a multiset, instead of a set. For example, they consider{1, 2} + {1, 2, 3} = {2, 3, 3, 4, 4, 5}, while we define {1, 2} + {1, 2, 3} = {2, 3, 4, 5}. Note that in oursituation the presence of multiplicities is exactly what we want to avoid. It is vital for our algorithmto process every element in the sumset the minimum number of times possible, and not proportionalto the number of times it appears, since we measure running time with respect to the size of thesumset (without multiplicities). For this reason, the ideas of [28] do not seem applicable for top-kand prefix-restricted convolution.Lower Bound on Rectangle Coverings. Let us also briefly discuss the construction of instances(A,B, [u]) on which any rectangle covering has superlinear cost (see Theorem 2.12). We build suchinstances in two steps.First, we construct “many” sets X(1), . . . ,X(g) and Y (1), . . . , Y (g) such that |X(ℓ)+Y (ℓ)| is “large”,while |X(ℓ) + Y (ℓ′)| is “small” for any ℓ 6= ℓ′. To this end, we pick appropriate parameters m, t, andgreedily choose an error-correcting code over {0, 1}t where every codeword has Hamming weightexactly t/2. We define X(ℓ) to be the set of all t-digit numbers in the m-ary number system withthe constraint that their r-th digit is 0 if the ℓ-th codeword has a 1 at position r. Similarly, wedefine Y (ℓ) to be the set of all t-digit numbers in the m-ary system with the constraint that theirr-th digit is 0 if the ℓ-th codeword has a 0 at position r.In the second step, we contruct the set A as a union over several shifted copies of each X(ℓ). Theset B is constructed similarly from the sets Y (ℓ). In this construction, we ensure that (A+B)∩ [u]contains no sumset X(ℓ)+Y (ℓ), so the output-size is “small”. We also ensure that any rectangle (I, J),that covers many parts of (A+B) ∩ [u] at once, needs to cover a sumset X(ℓ) + Y (ℓ), which resultsin a “large” sumset |AI + BJ |. Therefore, any covering either contains a rectangle of “large” costor consists of “many” rectangles; in both cases we obtain a superlinear lower bound on the cost interms of the output-size |(A+B) ∩ [u]|.102.4 OrganizationIn Section 3 we reduce top-k-convolution and related problems to finding a covering, proving Corol-lary 2.10. In Section 4 we show how to learn the output-size up to a constant factor for restrictedconvolution problems. In Section 5 we study interval-restricted convolution, proving Theorems 2.16and 2.17. In Section 6, we relax the upper bound, proving part of Theorem 2.13. In Section 7 wedescribe and analyze our Õ(out4/3)-cost covering for prefix-restricted sumset computation (Theo-rem 2.9) and we prove a lower bound for rectangle coverings (Theorem 2.12). Section 9 then containsthe reduction from SubsetSum to prefix-restricted sumset computation, proving Theorem 2.11 andfinishing the proof of Theorem 2.13.3 Top-k-ConvolutionIn this section, we present some easy reductions among Top-k-Convolution and related problems,proving Corollary 2.10. We start by proving that Top-k-Convolution is equivalent to Prefix-Restricted convolution.Lemma 3.1. If Top-k-Convolution on non-negative vectors can be solved in time Õ(kα), thenPrefix-Restricted convolution on non-negative vectors can be solved in time Õ(outα), and vice versa.Proof. To solve Prefix-Restricted sparse convolution, we run Top-k-Convolution for k = 20, 21, 22, . . .until we reach a value of k so that the k-th non-zero entry lies above u. Then we have found allelements of the Prefix-Restricted convolution, and we only computed at most twice as many non-zeroentries as necessary.To solve Top-k-Convolution on vectors f, g ∈ Rd≥0, we perform binary search over {0, 1, . . . , 2d},to find the smallest u such that Prefix-Restricted convolution on [u] returns a k-sparse vector. Toobtain the desired running time we add a small twist: if the execution time of Prefix-Restrictedsumset computation on [u] is more than kα · polylog d, i.e., more than what it would be for outputsize k, then we abort and look for a smaller u.Both reductions only add a log-factor to the running time.We next show that coverings not only allow us to compute Prefix-Restricted sumsets, but theyeven enable the computation of Prefix-Restricted convolutions, if the covering is unique.Lemma 3.2. Suppose that, given (A,B, u), we can compute a unique covering of (A,B, [u]) with costÕ(outα) in expected time Õ(outα), where out := |(A+B)∩ [u]|. Then Prefix-Restricted convolutionon non-negative vectors can be solved in expected time Õ(outα).Proof. For a vector f we denote by fS the same vector where every entry outside of S is zeroedout. Given non-negative vectors f, g, let A := supp(f) and B := supp(g). The output-size ofPrefix-Restricted convolution on f, g is the number of non-zero entries of (f ⋆g)[u]. This is the sameas the output-size of Prefix-Restricted sumset computation on A,B, namely |(A+B) ∩ [u]|. Thus,the two output-sizes coincide and we denote both by “out”. In particular, we can afford to computea unique covering C of (A,B, [u]).Using output-sensitive convolution of non-negative vectors (Theorem 2.1, [19]), we can computefS ⋆ gT in time Õ(‖fS ⋆ gT ‖0). Therefore, we can computeh :=∑(I,J)∈CfAI ⋆ gBJ ,11in time proportional to the cost of the covering, up to log factors. By the properties of a uniquecovering, any non-zero product fj · gi−j appears exactly once in the definition of h, for any 0 ≤ j ≤i ≤ u. We thus have h[u] = (f ⋆ g)[u].Corollary 2.10 now follows.Proof of Corollary 2.10. To solve Prefix-Restricted sumset computation in time Õ(out4/3), we com-bine our covering construction (Theorem 2.9) with Observation 2.8. To solve Prefix-Restrictedconvolution on non-negative vectors in time Õ(out4/3), we combine our covering construction (The-orem 2.9) with Lemma 3.2. To solve Top-k-Convolution on non-negative vectors in time Õ(k4/3), wecombine the result for Prefix-Restricted convolution with the equivalence shown in Lemma 3.1.4 Restricted Sumset Computation: Learning the Output-SizeIn this section, we show that we can assume to know the output-size up to a constant factor. Wewill later use this for Prefix-Restricted and for Interval-Restricted sumset computation. Here we willdiscuss the more general setting of Interval-Restricted sumset computation; the same constructionworks for the Prefix-Restricted case.Suppose that we are given sets A,B and integers ℓ, u and we want to compute their Interval-Restricted sumset (A + B) ∩ [ℓ, u]. We show that we can assume to know |(A + B) ∩ [ℓ, u]| up toa constant factor. In fact, we reduce Interval-Restricted sumset computation to a seemingly mucheasier variant, where additionally we are given a set T of size Θ(|(A + B) ∩ [ℓ, u]|) which contains(A+B) ∩ [ℓ, u].Definition 4.1 (Interval-Restricted Sumset Computation with a Promise (IR-SMP)). Given setsA,B ⊆ Z, numbers ℓ ≤ u, and a set T of size |T | ≤ 6|(A +B) ∩ [ℓ, u]|+ 9, such that(A+B) ∩ [ℓ, u] ⊆ T,compute the set(A+B) ∩ [ℓ, u].We present a reduction from the problem variant without promise to the variant with promise.Lemma 4.2. We can reduce a given instance (A,B, ℓ, u) of Interval-Restricted sumset computationto O(log(u− ℓ)) instances of IR-SMP, each of the form (A′, B′, ℓ′, u′, T ′) with |A′| ≤ |A|, |B′| ≤ |B|,ℓ′ ≤ ℓ, u′ ≤ u, u′ − ℓ′ ≤ u− ℓ, and |T ′| = O(|(A + B) ∩ [ℓ, u]|). The reduction runs in linear timein its output-size.Proof. In what follows, for integers x, y we define x÷y = ⌊xy ⌋. For a set Z we write Z÷x = {z÷x |z ∈ Z}, and we write xZ = {x · z | z ∈ Z}.Let (A,B, ℓ, u) be a given instance of Interval-Restricted sumset computation. We computebetter and better approximations of the desired output (A+B) ∩ [ℓ, u] by computing the setsS(i) :=((A÷ 2i) + (B ÷ 2i))∩[ℓ÷ 2i, u÷ 2i],for i = r, . . . , 0, where r := ⌈log(u − ℓ)⌉. See Algorithm 1 for pseudocode. Note that S(0) is thedesired output (A+B)∩ [ℓ, u]. We compute each set S(i) by calling an IR-SMP oracle on a suitablepromise T (i), determined as follows.For i = r, the interval [ℓ÷2i, u÷2i] has length at most 2. Thus, the set T (r) := {ℓ÷2i, . . . , u÷2i}is a valid promise from which our assumed IR-SMP oracle can compute the set S(r).12For i < r, we claim that a valid promise for the instance (A÷2i, B÷2i, ℓ÷2i, u÷2i) is given byT (i) :=(2S(i+1) + {0, 1, 2})∪ {(ℓ÷ 2i), (ℓ÷ 2i) + 1, (u÷ 2i)}.If this claim holds, then from S(r) we can compute S(r−1), S(r−2), . . ., S(0), finishing our reduction.Let us also claim that any set S(i) has size |S(i)| = O(|(A+B)∩ [ℓ, u]|). Then in particular theset T (i) has size |T (i)| = O(|S(i)|) = O(|(A + B) ∩ [ℓ, u]|). Hence, all constructed instances satisfythe claimed properties, and we reduced the given instance (A,B, ℓ, u) to O(log(u − ℓ)) calls to anIS-SMP oracle. This finishes the proof, except that it remains to prove the two claims.Claim 4.3. The set S(i) has size |S(i)| ≤ 2|(A +B) ∩ [ℓ, u]| + 2.Proof. It is easy to check that for any integers a, b we have2i((a÷ 2i) + (b÷ 2i))≤ a+ b ≤ 2i((a÷ 2i) + (b÷ 2i))+ 2i, (3)or, equivalently,a+ b2i− 1 ≤ (a÷ 2i) + (b÷ 2i) ≤ a+ b2i. (4)Any number in s ∈ S(i) can be expressed as s = (a÷ 2i) + (b÷ 2i) with(ℓ÷ 2i) ≤ s ≤ (u÷ 2i).Suppose that we have s ∈ [(ℓ÷ 2i) + 1, (u ÷ 2i)− 1]. Then by inequalities (3), it follows thatℓ ≤ 2i((ℓ÷ 2i) + 1) ≤ 2is ≤ a+ b ≤ 2is+ 2i ≤ 2i(u÷ 2i) ≤ u,Thus, we obtain that the number s′ := a+ b lies in (A+B)∩ [ℓ, u]. We say that s charges s′. Frominequalities (4), we infer that any number s′ ∈ (A +B) ∩ [ℓ, u] can only be charged by numbers in[s′/2i − 1, s′/2i], so it is charged by at most two numbers s ∈ S(i). This charging scheme proves∣∣S(i) ∩ [(ℓ÷ 2i) + 1, (u÷ 2i)− 1]∣∣ ≤ 2 ·∣∣(A+B) ∩ [ℓ, u]∣∣.We add 2 to account for the numbers ℓ÷ 2i and u÷ 2i. This yields the claim.Algorithm 1 Reduction of Interval-Restricted Sumset Computation to its Promise Version1: procedure ComputeIntervalRestrictedSumset(A,B, ℓ, u)2: ⊲ A,B ⊆ [u], ℓ ≤ u⊲ We assume oracle access to Interval-Restricted sumset computation with a promise(IR-SMP)3: r ← ⌈log(u− ℓ)⌉4: T (r) ← {ℓ÷ 2r, . . . , u÷ 2r}5: S(r) ← IR-SMP(A÷ 2r, B ÷ 2r, ℓ÷ 2r, u÷ 2r, T (r))6: ⊲ Call to the promise problem7: for i = r − 1 down to 0 do8: T (i) ←(2S(i+1) + {0, 1, 2})9: T (i) ← T (i) ∪ {(ℓ÷ 2i), (ℓ ÷ 2i) + 1, (u÷ 2i)}10: S(i) ← IR-SMP(A÷ 2i, B ÷ 2i, ℓ÷ 2i, u÷ 2i, T (i))11: ⊲ Call to the promise problem12: Return S(0)13Claim 4.4. The set T (i) is a valid promise for the instance (A÷ 2i, B ÷ 2i, ℓ÷ 2i, u÷ 2i). That is,we have T (i) ⊇ S(i) and |T (i)| ≤ 6|S(i)|+ 9.Proof. Since x÷ 2i+1 = (x÷ 2i)÷ 2, it suffices to prove the claim for i = 0. The same proof thenalso works for larger i after replacing A by A÷ 2i, B by B ÷ 2i, ℓ by ℓ÷ 2i, and u by u÷ 2i.We first show that |T (0)| ≤ 6|S(0)|+ 9. Recall that in Claim 4.3 we proved that|S(1)| ≤ 2 · |(A+B) ∩ [ℓ, u]| + 2 = 2|S(0)|+ 2.We combine this with the inequality |T (0)| ≤ 3|S(1)|+3 that we obtain from the construction of T (0).This yields the claimed inequality |T (0)| ≤ 6|S(0)|+ 9.Next we prove T (0) ⊇ S(0). Consider any a ∈ A, b ∈ B with a + b ∈[ℓ + 2, u − 1]. Usinginequalities (4) we obtainℓ÷ 2 ≤ ℓ2≤ a+ b2− 1 ≤ (a÷ 2) + (b÷ 2)≤ a+ b2≤ u− 12≤ u÷ 2.Therefore, the sum (a ÷ 2) + (b ÷ 2) is in S(1). Now, since a + b can be found among the threenumbers2((a÷ 2) + (b÷ 2)), 2((a÷ 2) + (b÷ 2))+ 1, 2((a÷ 2) + (b÷ 2))+ 2,it follows that a+ b ∈ (2S(1) + {0, 1, 2}) ⊆ T (0). Recall that here we assumed a+ b ∈ [ℓ+ 2, u− 1].The boundary numbers ℓ, ℓ+ 1, and u are handled by explicitly adding them to the set T (0) in itsconstruction. We thus obtain S(0) ⊆ T (0).These claims finish the proof of Lemma 4.2.We obtain the following easy corollary where we essentially ignore the superset T and only keepits size |T |, which is a constant-factor approximation of the output-size.Lemma 4.5 (Interval-Restricted Sumset Computation with Approximate Output-size). Supposethat given (A,B, ℓ, u) and an additional input õut satisfying out ≤ õut ≤ 6out + 9, we can computea covering of (A,B, [ℓ, u]) of cost O(c) in time O(T ), where c and T are monotone functions of|A|, |B|, u, out. Then Interval-Restricted sumset computation can be solved in time Õ(c+ T ).An analogous statement holds for Prefix-Restricted sumset computation.Proof. Given an instance (A,B, ℓ, u) of Interval-Restricted sumset computation, we run Lemma 4.2to reduce to O(log(u− ℓ)) promise instances of the form (A′, B′, ℓ′, u′, T ′). By the properties of T ′,for õut′:= |T ′| we have out′ ≤ õut′ ≤ 6out′ + 9, where out′ := |(A′ + B′) ∩ [ℓ′, u′]|. Hence, we canuse our assumed algorithm to compute a covering of (A′, B′, [ℓ′, u′]) of cost O(c) in time O(T ). ByObservation 2.8, we can thus solve the instance (A′, B′, [ℓ′, u′]) in time Õ(c+T ). Over O(log(u− ℓ))many constructed instances, we obtain the same time bound up to log-factors.5 Interval-Restricted Sumset ComputationIn this subsection we prove Theorems 2.16 and 2.17.145.1 An Õ(√mn · out)-time AlgorithmIn this subsection we prove Theorem 2.16.Proof. Given A,B, ℓ, u, we will compute the Interval-Restricted sumset (A+B)∩[ℓ, u] in time Õ(n+m +√mn · out). More precisely, we will compute a covering of (A,B, [ℓ, u]) of cost O(√mn · out)in time O(n + m), assuming that we know an approximation õut of the output-size satisfyingout ≤ õut ≤ O(out) (we can assume this by Lemma 4.5).Set q :=⌈(nm/õut)1/2⌉. Note that since n,m ≤ out ≤ n ·m we have 1 ≤ q = O(min{n,m}).We assume that n and m are divisible by q, which we can ensure by duplicating at most q elementsin A and B (with the slight abuse of transitioning to multi-sets).We split A and B into q subsets by defining for any 1 ≤ i, j ≤ q:Ii := {(i − 1) · n/q + 1, . . . , i · n/q},Jj := {(j − 1) ·m/q + 1, . . . , j ·m/q},A(i) := AIi ,B(j) := BJj .We let C be the set of all pairs (Ii, Jj) withmin(A(i)) + min(B(j)) ≤ u and max(A(i)) + max(B(j)) ≥ ℓ.Observe that C is a unique-rectangle-covering of (A,B, [ℓ, u]). We can easily compute C by bruteforce, by iterating over all 1 ≤ i, j ≤ q and testing whether to put (Ii, Jj) into C in time O(1), seeAlgorithm 2. This takes total time O(q2) = O(nm/out) = O(min{n,m}), assuming that we haverandom access to A and B.Algorithm 2 Covering for Interval-Restricted Sumset Computation1: procedure FindCovering(A,B, ℓ, u, õut)2: q ←⌈(nm/õut)1/2⌉3: C ← ∅4: for i = 1 to q do5: for j = 1 to q do6: I ← {(i− 1) · n/q + 1, . . . , i · n/q}7: J ← {(j − 1) ·m/q + 1, . . . , j ·m/q}8: if min(AIi) + min(BJj) ≤ u then9: if max(AIi) + max(BJj) ≥ ℓ then10: C ← C ∪ {(I, J)}11: Return CRecall that the cost of a covering C is∑(I,J)∈C|AI +BJ |.We split C into two parts, the rectangles in the interior and the rectangles at the boundary:Cint := {(I, J) ∈ C | AI +BJ ⊆ [ℓ, u]},Cbd := C \\\\ Cint.15For the interior rectangles, we split their cost into diagonal sums of the form∑(Ii,Ji+∆)∈Cint|A(i) +B(i+∆)|,for −q < ∆ < q. We claim that each such diagonal sum is bounded from above by out. Indeed, forconsecutive terms along a diagonal we havemax(A(i)) + max(B(i+∆)) < min(A(i+1)) + min(B(i+∆+1)).Therefore, the output-sizes |A(i)+B(i+∆)| are disjoint contributions to out, for fixed ∆ and rangingover all i. It follows that a diagonal sum is bounded by out, and since there are 2q − 1 diagonals,we obtain ∑(I,J)∈Cint|AI +BJ | ≤ (2q − 1)out = O(√nm · out).We argue geometrically about the boundary. Observe that Cbd contains at most two rectanglesper diagonal, which yields |Cbd| ≤ 4q. For each (I, J) ∈ Cbd we use the trivial upper bound|AI +BJ | ≤ |AI | · |BJ | = nm/q2. In total, this yields cost∑(I,J)∈Cbd|AI +BJ | ≤ 4q ·nmq2= O(√nm · out).The contribution from both parts is the same, so in total we bounded the cost of C by O(√nm · out).This finishes the proof.5.2 Hardness ResultsIn this subsection we prove Theorem 2.17.Proof. We want to prove hardness of Interval-Restricted sumset computation. Note that here weanalyze running time in terms of n = |A|, m = |B|, and out, which are all invariant under shifting Aby adding a number q to each a ∈ A; similarly for B. Therefore, we may drop the assumption thatA,B are sets of positive integers and let them be subsets of Z instead. This is the case because we canshift A,B and the interval appropriately, so that every number in the input becomes non-negative.Reduction from Boolean Matrix Multiplicaion to Interval-Restricted Sumset Compu-tation: In Boolean matrix multiplication we are given n× n matrices A,B with entries in {0, 1}and want to compute their product C with Cij =∨r Air ∧Brj.Given matrics A,B, we construct sets A,B asA := {rM2 +Air ·M + i | i, r ∈ [n]},B := {−rM2 +Brj ·M + nj | r, j ∈ [n]},where M is any integer greater than 10(n2 + n). We also setℓ := 2M + n+ 1, u := 2M + n2 + n.We observe the following.1. Every integer of the form (Air +Brj) ·M + (i+ nj) with i, r, j ∈ [n] is contained in A+B.162. For r 6= r′ any sum (rM2 + Air · M + i) + (−r′M2 + Br′j · M + nj) is either less than−M2 + 2M + n2 + n < 0 or at least M2 + n+ 1, and hence outside of [ℓ, u].3. If Air ∧Brj = 1, then (Air +Brj)M + (i+ nj) = 2M + (i+ nj).4. If Air ∧Brj = 0, then (Air +Brj)M + (i+ nj) ≤M + (i+ nj) < 2M .It follows that from (A+B)∩ [2M + n+1, 2M +n2 + n] we can infer all entries of the productmatrix C.Note that the output-size is out ≤ u − ℓ + 1 = n2. Hence, for any δ > 0, an O((|A| + |B| +out)ω/2−δ)-time algorithm for Interval-Restricted sumset computation would yield an O(n2ω−2δ)-time algorithm for Boolean matrix multiplication.Reduction from Sliding Window Hamming Distance to Interval-Restricted Convolu-tion: Note that Interval-Restricted convolution allows us to not only to compute (A+B)∩ [ℓ, u],but also the number of ways an element x ∈ (A + B) ∩ [ℓ, u] can be written as x = a + b witha ∈ A, b ∈ B; let us call this number the multiplicity of x. Indeed, for the indicator vectors of Aand B, the x-th entry of their convolution is the multiplicity of x in A+B.In the sliding window Hamming distance problem we are given a text t of length 2n and apattern p of length n and want to compute the Hamming distance between the pattern and everylength-n substring of the text. Given such an instance t, p, we construct sets A,B asA := {M · ti + i | 1 ≤ i ≤ 2n}, B := {−M · pj − j | 1 ≤ j ≤ n},where M := 100n. We also set ℓ := 1, u := n and compute, as mentioned in the previous paragraph,the Interval-Restricted sumset (A+B) ∩ [ℓ, u] as well as the multiplicity of every x in this set.Fix a 1 ≤ i ≤ n and observe that1. If ti+j = pj then the the pair (i+ j, j) will contribute 1 to the multiplicity of i.2. If ti+j 6= pj then the pair (i+ j, j) will contribute 1 to the multiplicity of a coefficient outsideof the interval [1, n], by the choice of M .3. Every pair (i′, j) with i′ ≤ j contributes 1 to a coefficient outside of the interval [1, n].It follows that we can read off the Hamming distance between the pattern p and the i-th length-nsubstring of t from the multiplicity of i in the output. This completes the reduction from slidingwindow Hamming distance to prefix-restricted convolution. Since out ≤ n, any Õ(|A| + |B| +(|A| · |B| · out)1/2−δ)-time algorithm for Interval-Restricted comvolution would solve sliding windowHamming distance in time Õ(n3/2−3δ).6 Relaxed Version of Prefix-Restricted ConvolutionWe show how to solve Prefix-Restricted convolution on any instance (A,B, u) in time Õ(ζ−1|(A+B)∩ [u(1+ζu)]|+ζ−2), for any ζ ≤ 1, proving Theorem 2.13. More precisely, we prove the followingtheorem, from which we conclude Theorem 2.13 using Observation 2.8.Theorem 6.1. Given sets A,B and a target u we can compute a unique-rectangle-covering of(A,B, [u]) with costO(ζ−1|(A+B) ∩ [(1 + ζ)u]|)in time O(|A|+ |B|+ ζ−2).17Proof. Find the smallest ℓ such that 2−ℓ ≤ ζ. It suffices to solve the problem for ζ = 2−ℓ, since(A+B)∩ [u+2−ℓu] ⊆ (A+B)∩ [u+ ζu], and 2ℓ ≤ 2ζ−1. Thus, we can assume that 1/ζ is a powerof 2. Moreover, by shifting u as well as A by a suitable number we can assume that u is a power of2. We split the set [u] into the intervalsUr := [(r − 1) · (ζu)/2 + 1, r · (ζu)/2] .Moreover, we define the following sets for 1 ≤ i, j ≤ 2/ζ:A(i) := A ∩ Ui,B(j) := B ∩ Uj,Ii := {i′ ∈ [n] : Ai′ ∈ Ui}Jj := {j′ ∈ [m] : Bj′ ∈ Uj}.Let C be the set of all pairs (Ii, Jj) for which (A(i) +B(j))∩ [u] is not empty; this condition canbe easily checked by testing whethermin(A(i)) + min(B(j)) ≤ u.Note that C can be computed in time O(ζ−2), assuming that we have random access to A and B.Observe that C is indeed a unique-rectangle-covering of (A+B) ∩ [u]. Recall that the cost of C is∑(I,J)∈C|AI +BJ |.Similarly to the argument in the proof of Theorem 2.16, this sum can be decomposed into 4/ζdiagonal sums of the form ∑i|A(i) +B(i+∆)|,where the sum is over all i such that (Ii, Ji+∆) ∈ C. We again usemax(A(i)) + max(B(j)) < min(A(i+1)) + min(B(j+1)).Moreover, for any (i, j) ∈ P we now havemax(A(i)) + max(B(j)) ≤(min(A(i)) +ζu2)+(min(B(j)) +ζu2)=(min(A(i)) + min(B(j)))+ ζu≤ u+ ζu.It follows that every diagonal sum contributes at most|(A+B) ∩ [u+ ζu]| .Summing over all diagonals, in total we can bound the cost of C by O(ζ−1 |(A+B) ∩ [u+ ζu]|).7 Construction of the Õ(out4/3)-cost CoveringThis section is devoted to proving the technical core of our Prefix-Restricted sumset algorithm,specifically we prove Theorem 2.9.187.1 An Additive Combinatorics Ingredient: Ruzsa’s Triangle InequalityThe following is a classical result from Additive Combinatorics. We present a self-contained proof.Lemma 7.1 (Ruzsa’s Triangle Inequality, see also [16, Theorem 2]). For any A,B,C ⊆ Z we have|A−B| ≤ |A−C| · |C −B||C| .Proof. We associate every s ∈ A − B with the lexicographically smallest pair (a, b) ∈ A × B suchthat s = a− b, and we denote this pair by (a(s), b(s)). Consider the mapping(A−B)× C → (A− C)× (C −B)(s, c) 7→ (a(s)− c, c − b(s))We claim that this mapping is injective. Indeed, from an image (x, y) = (a(s)− c, c− b(s)) we caninfer s = a(s)−b(s) = x+y. The value s then determines a(s) and b(s), so we can infer c = y+b(s).We thus recovered the corresponding preimage (s, c).Since this mapping is injective, we obtain |A−B| · |C| ≤ |A− C| · |C −B|.We will use the following simple corollary.Lemma 7.2 (Corollary of Ruzsa’s Triangle Inequality). For any X,Y,Z,W ⊆ Z we have|X + Y | ≤ |X + Z| · |Z +W | · |W + Y ||Z| · |W | .Proof. First use Ruzsa’s triangle inequality on A = X, B = −Y, C = −Z to obtain|X + Y | ≤ |X + Z| · |Z − Y ||Z| .Then use Ruzsa’s triangle inequality on A = Z, B = Y, C = −W to obtain|Z − Y | ≤ |Z +W | · |W + Y ||W | .Plugging the latter into the former proves the claim.7.2 Description of the AlgorithmGiven A,B ⊆ [u], we write n = |A|, m = |B|, and out = |(A+B) ∩ [u]|. We describe an algorithmthat computes a unique rectangle covering C of (A,B, [u]) of cost Õ(out4/3) in time Õ(out4/3).However, as a subroutine we will use output-sensitive sumset computation (Theorem 2.2), whichshows that it is hard to completely separate the tasks of computing a covering and computing thePrefix-Restricted sumset itself.Invoking Lemma 4.5, it suffices to solve the promise problem where we are given a value õutguaranteed to satisfy out ≤ õut ≤ O(out). We can assume that õut is larger than some absoluteconstant, since otherwise we have |A|, |B| ≤ out ≤ õut = O(1), so we can compute a trivial coveringof cardinality 1 and cost |A| · |B| = O(1).We maintain families C,D, initialized to C = ∅ and D = {([n], [m])}, with the invariant thatC∪D is a unique rectangle covering of (A,B, [u]). We refer to the rectangles in D as the unprocessed19subproblems, or simply subproblems. The algorithm is finished when there are no more unprocessedsubproblems, i.e., D = ∅, and then we return the unique rectangle covering C as output.We associate to every subproblem (I, J) ∈ D the type (x, y) for x = ⌈log |I|⌉ and y = ⌈log |J |⌉.Initially, there is exactly one subproblem ([n], [m]) of type (⌈log n⌉ , ⌈logm⌉).We define a total order on types: (x, y) ≺ (x′, y′) iff x + y < x′ + y′, or x + y = x′ + y′ andx < x′. Our algorithm processes types in descending order according to this total order. For anytype (x, y), we process all subproblems of type (x, y) in one batch. Upon processing a subproblem,our algorithm may generate further subproblems of strictly smaller type. As we will see below, allsubproblems of the same type that we generate are disjoint, i.e., for any subproblems (I, J), (I ′, J ′)of the same type we have I ∩ I ′ = ∅ and J ∩ J ′ = ∅.Since we want to process all subproblems of a particular type (x, y) in one batch, we need tostore the family D in such a way that we can efficiently enumerate all subproblems of a type (x, y).To this end, we store D in a standard data structure such as a self-adjusting binary search tree,where subproblems are first compared according to their type and then according to the endpointsof I and J . Note that we can store any subproblem (I, J) using O(1) integers, since I and J areintervals.We set the parameter q := ⌈õut1/3⌉.To finish the description of the algorithm, it remains to describe how we process all subproblemsof a particular type (x, y) in one batch. We consider two cases.Case 1: 2x+y ≤ õut. Then for each subproblem (I, J) of type (x, y) we move (I, J) from D to C.Case 2: 2x+y > õut. If D contains more than q subproblems of type (x, y), then we do thefollowing. We start computing AI+BJ for each of these subproblems in parallel (using Theorem 2.2),and we stop once all but q of these calls have finished. For each finished call AI + BJ , we move(I, J) from D to C.At this point, we have at most q subproblems of type (x, y) left. We split each such subproblem(I, J) = ([i1, i2], [j1, j2]) as follows. We set i := ⌊(i1 + i2)/2⌋ and determine the maximum indexj ∈ J with Ai +Bj ≤ u. This splits I into I1 = [i1, i] and I2 = [i+1, i2] and J into J1 = [j1, j] andJ2 = [j + 1, j2]. We add (I1, J1) to the output C. Moreover, we add the subproblems (I1, J2) and(I2, J1) to D and we remove (I, J) from D. Since u < Ai +Bj+1 ≤ Ai+1 +Bj+1, we can ignore thesubproblem (I2, J2).This finishes the description of our algorithm, for pseudocode see Algorithm 3.7.3 AnalysisThe correctness of the algorithm, meaning that the output is a unique rectangle covering, followsfrom the next claim and the fact that the algorithm stops when D = ∅.Claim 7.3 (Invariant that C ∪D is a covering). At any point during the algorithm, the family C ∪Dis a unique rectangle covering of (A,B, [u]).Proof. Moving (I, J) from D to C does not change this property. Therefore, the only crucial step isthe splitting of I into I1, I2 and of J into J1, J2, where we add (I1, J1) to C and add (I1, J2), (I2, J1)to D. Observe that at this point we have u < Ai + Bj+1 ≤ Ai+1 + Bj+1, and thus the rectangle(I2, J2) does not contain any sum below u, so it is unnecessary for a covering. It follows that C ∪Dremains a covering. Moreover, noting that I1, I2, J1, J2 are again intervals, it remains rectangular.Finally, since no pair (i, j) is contained in two subproblems Ib × Jb′ , it remains unique.We will need the following claims to analyze the running time and the cost of the covering.20Algorithm 31: procedure CoveringConstruction(A,B, u, õut)2: n← |A|, m← |B|, q ← õut1/33: Initialize C ← ∅, D ← {([n], [m])}4: while D 6= ∅ do5: Let (x, y) be the largest type such that D contains subproblems of type (x, y)6: // process all subproblems of type (x, y) in one batch:7: if 2x+y ≤ õut then8: for each (I, J) ∈ D of type (x, y): Move (I, J) from D to C9: else10: if D contains more than q subproblems of type (x, y) then11: Compute AI +BJ in parallel for all (I, J) ∈ D of type (x, y)12: Stop once all but q of these calls finished13: for each finished call AI +BJ : Move (I, J) from D to C14: for each remaining (I, J) ∈ D of type (x, y) do15: Split I = [i1, i2] at i = ⌊(i1 + i2)/2⌋ into I1 and I216: Determine the maximum j ∈ J with Ai +Bj ≤ u17: Split J at j into J1 and J218: Add (I1, J1) to C19: Add (I1, J2), (I2, J1) to D20: Remove (I, J) from D21: return CClaim 7.4 (Subproblems of type (x, y) form a staircase). For any distinct subproblems (I, J), (I ′, J ′)of the same type (x, y), we have max(I ′) < min(I) and max(J) < min(J ′) (or vice versa). Moreover,if this holds then AI′ +BJ ⊆ [u].Proof. This is true in the beginning since |D| = 1. Moving (I, J) from D to C cannot violate thisproperty. Therefore, the only crucial step is the splitting of I into I1, I2 and of J into J1, J2, wherewe remove (I, J) from D and add (I1, J2), (I2, J1) to D. In this situation, clearly (I1, J2), (I2, J1)form a staircase, and AI1 +BJ1 ⊆ [u]. Thus, the property is maintained “locally”. It is not hard tosee that the property is also maintained “globally”, when we have distinct subproblems (I, J), (I ′, J ′)that satisfy the property and we split both of them into subproblems.Claim 7.5 (Any subproblem creates at most two subproblems of strictly smaller type). Processinga subproblem (I, J) ∈ D can cause the insertion of at most two new subproblems into D, both ofstrictly smaller type.Proof. The only point at which we add new subproblems to D is the splitting phase. So let (I, J)be a subproblem of type (x, y) and consider the splitting of I into I1, I2 and of J into J1, J2, wherewe remove (I, J) from D and add (I1, J2), (I2, J1) to D. Since we split I at the midpoint, wehave |Ir| ≤ ⌈|I|/2⌉, for any r ∈ {1, 2}. We clearly also have |Jr| ≤ |J |. Hence, the new type(xr, yr) = (⌈log |Ir|⌉, ⌈log |Jr|⌉) satisfies3 xr < x and yr ≤ y. As this implies xr + yr < x + y, thenewly added subproblems have a strictly smaller type.3To be precise, here we use that for any integers z, w the inequalities 2w−1 < z ≤ 2w imply 2w−2 < ⌈z/2⌉ ≤ 2w−1,and thus ⌈log(⌈z/2⌉)⌉ ≤ ⌈log z⌉ − 1.21Claim 7.6 (Invariant on the number of subproblems). At any point during the algorithm, there areat most 2q log(n) log(m) subproblems of any fixed type (x, y).Proof. The claim is immediate for (x, y) = (⌈log n⌉, ⌈logm⌉). Fix a type (x, y) and assume that theclaim is true for every type (x′, y′) with (x, y) ≺ (x′, y′). Note that for any type (x′, y′) at most qsubproblems of type (x′, y′) reached the splitting phase, and each such subproblem gave rise to atmost two newly added subproblems. Since the number of different types is at most log(n) log(m),we obtain the claimed bound on the number of subproblems of type (x, y).The following claim lies at the core of the analysis of our covering construction, as it bounds therunning time and added cost of Case 2.Claim 7.7. Fix a type (x, y) with 2x+y > õut. Lines 11-13 of Algorithm 3 take time Õ(out4/3) andadd rectangles of total cost Õ(out4/3) to C.Proof. We denote by (I1, J1), (I2, J2), . . . , (IR, JR) the subproblems of type (x, y). We can assumeR ≥ q, since otherwise lines 11-13 are not called. Note that R ≤ 2q log(n) log(m) by Claim 7.6. ByClaim 7.4 these subproblems form a staircase, so we can assume thatmax(I1) < min(I2),max(I2) < min(I3), . . . ,max(IR−1) < min(IR)min(J1) > max(J2),min(J2) > max(J3), . . . ,min(JR−1) > max(JR).Claim 7.4 also implies that for any r < ℓ we haveAIr +BJℓ ⊆ [u]. (5)Set δ := 1/(6 log(n) log(m)), so that 3δR ≤ q. For any r ∈ [R] setS(r) :=r−1∑i=1R∑j=r+1|AIr +BJj |+ |AIi +BJj |+ |AIi +BJr |.Claim 7.8. There are at least (1− δ)R indices r with S(r) ≤ 6R out/δ.Proof. By simple counting how often a summand |AIr +BJℓ | can appear, we observe thatR∑r=1S(r) ≤ 3R∑r<ℓ|AIr +BJℓ|.Note that we have |AIr + BJℓ | ≤ out by equation (5). However, we need a stronger property. Wedecompose the sum∑r<ℓ |AIr +BJℓ | into 2R−1 diagonal sums of the form∑r |AIr +BJr+∆|. Sincemax(AIr +BJℓ) < min(AIr+1 +BJℓ+1), all summands in a diagonal sum are disjoint, and thus eachdiagonal sum is bounded from above by out. We thus obtain∑r<ℓ|AIr +BJℓ| ≤ (2R− 1)out,which yieldsR∑r=1S(r) ≤ 6R2out.By Markov’s inequality, it follows that all but δR indices r satisfy S(r) ≤ 6R out/δ.22In the remainder we consider only indices δR ≤ r ≤ (1 − δ)R with S(r) ≤ 6R out/δ; note thatthere are at least (1 − 3δ)R such indices r. For any such r, there are at least δR22 pairs (i, j) with1 ≤ i < r and r < j ≤ R. Hence, there exist i, j with i < r < j and|AIr +BJj |+ |AIi +BJj |+ |AIi +BJr | ≤(6R outδ)/(δR22)=12outδ2R.We continue by invoking the corollary of Ruzsa’s triangle inequality (Lemma 7.2), see Figure 1 foran illustration:|AIr +BJr | ≤|AIr +BJj | · |BJj +AIi | · |AIi +BJr ||AIi | · |BJj |≤123out3/(δ6R3)2x−1 · 2y−1 .By the case assumption 2x+y > õut ≥ out and R ≥ q = ⌈õut1/3⌉ ≥ out1/3, and by our choice ofδ = 1/(6 log(n) log(m)), we obtain|AIr +BJr | ≤ O(out · log12(nm)).Recall that this holds for at least (1− 3δ)R ≥ R− q many indices r.Since we compute all sumsets AIr + BJr in parallel and stop once all but q calls are finished,it follows that we stop after spending time Õ(out) for each call. Over R ≤ 2q log(n) log(m) =Õ(out1/3) calls, this takes total time Õ(out4/3). Moreover, any finished call ran in time Õ(out),so it has output-size Õ(out), so it contributes cost Õ(out). Thus, we add rectangles of total costÕ(out4/3) in every invocation of lines 11-13 of Algorithm 3.Claim 7.9 (Cost Bound). For any type (x, y) we add rectangles of total cost Õ(out4/3) to C.Proof. If 2x+y > õut, then the cost contributed by lines 11-13 of Algorithm 3 is bounded in Claim 7.7.Additionally, in the splitting phase (line 18) we add a rectangle (I1, J1) satisfying AI1 +BJ1 ⊆ [u],and thus |AI1+BJ1 | ≤ out. Since we split at most q subproblems, we add a total cost of Õ(q ·out) =Õ(out4/3).If 2x+y ≤ õut, then by the trivial bound each added rectangle (I, J) has cost at most |I| · |J | ≤2x+y ≤ õut = O(out). By Claim 7.6 we have Õ(q) subproblems, and thus we add a total cost ofÕ(q · out) = Õ(out4/3).Over all types (x, y), we obtain a total cost of Õ(out4/3). The running time bound of Õ(out4/3)follows along the same lines. This finishes the proof of Theorem 2.9.8 Lower Bound on CoveringsThis section is devoted to proving Theorem 2.12. We will use the following notation. For a vector xwe write w(x) for its Hamming weight, i.e., its number of non-zero coordinates. For vectors x, y wewrite dH(x, y) for their Hamming distance, i.e., the number of coordinates in which they differ.For sets X,Y we denote their symmetric difference by X△Y := (X \\\\ Y )∪ (Y \\\\X). We identifysets with their indicator vectors. In particular, note that if x, y are the indicator vectors of setsX,Y , respectively, then dH(x, y) = |X△Y |.Recall the definition of the binary entropy function h(x) = −x log x − (1 − x) log(1 − x) andnote that h(1/2) = 1. Finally, we use notation Oδ(.), Ωδ(.), and Θδ(.) to hide constants that onlydepend on the parameter δ.The following is a standard construction of an error correcting code.23A(1)B(1)A(2)B(2)A(3)B(3)A(4)B(4)A(5)B(5)A(6)B(6)A(7)B(7)A(8)B(8)Figure 1: Illustration of the proof of Claim 7.8. The thick gray squares depict the unprocessedsubproblems (I1, J1), (I2, J2), . . . of a specific type (x, y). The light gray squares depict pairs (Ir, Jℓ)for r < ℓ. The vertices of the blue rectangle correspond to (starting from the top-right corner andgoing clockwise) (Ir, Jr), (Ir, Jj), (Ii, Jj), (Ii, Jr). From such a 4-tuple, where all sumsets |AIr +BJj |, |AIi +BJj |, |AIr +BJj | are “small”, Ruzsa’s triangle inequality (Lemma 7.2) allows us to bound|AIr +BJr | from above.Lemma 8.1 (Constant-weight Binary Code). Fix 0 ≤ δ < 1/2. For any even integer t there existsa code E ⊆ {0, 1}t such that:• Each codeword x ∈ E has weight w(x) = t/2,• Any two codewords x, y ∈ E with x 6= y satisfy dH(x, y) ≥ δt, and• The number of codewords is |E| = Ωδ(2(1−h(δ))t).Proof. We pick E greedily among all( tt/2)many vectors x ∈ {0, 1}t with Hamming weight t/2.Whenever we pick a vector x, we mark all vectors y within distance δt of x as unpickable. Notethat we mark at most∑δti=0(ti)many vectors y. In total, this process picks |E| ≥( tt/2)/(∑δti=0(ti))vectors. The claim now follows from the following facts about binomial coefficients:•∑αni=0(ni)= Θα(( nαn))for any 0 ≤ α < 1/2,•(nαn)= Θα(2h(α)n/√n).Indeed, with these facts we obtain |E| ≥( tt/2)/(∑δti=0(ti))= Ωδ(( tt/2)/( tδt))= Ωδ(2(1−h(δ))t).We next lift the above code to a family of sets where the sumset X(i)+Y (j) has large cardinalityif i = j, and small cardinality if i 6= j.Lemma 8.2. Fix 0 ≤ δ < 1/2. For any integers m, t ≥ 2, where t is even, forσ := mt, α := 2δt/2m(1−δ/2)t, and some g = Ωδ(2(1−h(δ))t),there exist sets X(1), ..,X(g), Y (1), .., Y (g) ⊆ [σ] satisfying241. |X(i)| = |Y (i)| = σ1/2 for any 1 ≤ i ≤ g,2. |X(i) + Y (i)| = σ for any 1 ≤ i ≤ g, and3. |X(i) + Y (j)| ≤ α for any i 6= j.Proof. Let E ⊆ {0, 1}t be the code given by Lemma 8.1 and set g := |E|. Let I(1), . . . , I(g) ⊆{0, 1, . . . , t− 1} be sets such that the codewords in E are the indicator vectors of I(1), . . . , I(g). Forany 1 ≤ ℓ ≤ g, we writeI(ℓ):= {0, . . . , t− 1} \\\\ I(ℓ).Moreover, for any I ⊆ {0, .., t − 1} we setS(I) :={∑i∈Iηi ·mi∣∣∣ 0 ≤ ηi < m},where ηi ranges over all integers between 0 and m−1. Note that S(I) is the set of all numbers witht digits in the m-ary system for which every digit outside of I is 0. Alternatively, it can be viewedas the sumset of |I| many arithmetic progressions of length m and step sizes {mi}i∈I . Finally, wesetX(ℓ) := S(I(ℓ)) and Y (ℓ) = S(I(ℓ)).Note that any number in S(I) is bounded from above by∑t−1i=0(m− 1) ·mi = mt − 1, and thuswe have X(ℓ), Y (ℓ) ⊆ [mt] for any ℓ. It remains to verify the three claims.For (1.), note that for any ℓ we have |X(ℓ)| = |S(I(ℓ))| = m|I(ℓ)| = mt/2, since the code E hasconstant weight t/2. The same holds for Y (ℓ).For the remaining claims, for any ℓ, h we write∣∣X(ℓ) + Y (h)∣∣ =∣∣S(I(ℓ)) + S(I(h))∣∣ =∣∣∣∣{ ∑i∈I(ℓ)ηi ·mi +∑j∈I(h)η′j ·mj∣∣∣∣ 0 ≤ ηi, η′j < m}∣∣∣∣. (6)For (2.), we use that I(ℓ) ∪ I(ℓ) is a partitioning of {0, . . . , t− 1} to obtain for any ℓ∣∣X(ℓ) + Y (ℓ)∣∣ =∣∣∣∣{ t−1∑i=0ηi ·mi∣∣∣∣ 0 ≤ ηi < m}∣∣∣∣ = mt.For (3.), for any ℓ 6= h we write I := I(ℓ) and J := I(h) and express the right hand side of (6) interms of the intersection I ∩ J and the symmetric difference I△J as∣∣X(ℓ) + Y (h)∣∣ =∣∣∣∣{ ∑i∈I∩Jηi ·mi +∑j∈I△Jη′j ·mj∣∣∣∣ 0 ≤ ηi ≤ 2m− 2, 0 ≤ η′j < m}∣∣∣∣.This allows us to bound∣∣X(ℓ) + Y (h)∣∣ ≤ (2m− 1)|I∩J | ·m|I△J | ≤ 2|I∩J | ·m|I∩J |+|I△J | = 2|I∩J | ·m|I∪J | = 2t(m2)|I∪J |,where we have used inclusion-exclusion |I ∪ J | = |I| + |J | − |I ∩ J | combined with |I| = |J | = t/2in the last step. We now use the fact that for any S, T ⊆ {0, . . . , t− 1} we have∣∣S ∪({0, . . . , t− 1} \\\\ T)∣∣ = 12(2t+ |S| − |T | − |S△T |).25Plugging in the bounds of |I(ℓ)| = t/2 and |I(ℓ)△I(h)| ≥ δt for any ℓ 6= h by the properties of thecode E , we obtain the bound|I ∪ J | =∣∣I(ℓ) ∪ I(h)∣∣ ≤(1− δ2)t.Together, this yields∣∣X(ℓ) + Y (h)∣∣ ≤ 2t(m2)(1−δ/2)t= 2δt/2m(1−δ/2)t,finishing the proof.Lemma 8.3. With parameters σ, α, g as in Lemma 8.2, there exist sets A,B ⊆ N and an integer usuch that1. |A|, |B| = g · σ1/2,2. out := |(A+B) ∩ [u]| = O(g2 · α+ σ), and3. Any rectangle covering of (A,B, [u]) has cost Ω(g · σ).Proof. Let X(1), ..,X(g), Y (1), .., Y (g) be the sets constructed in Lemma 8.2. For i ∈ N we letE(i) :={i, if i is even0, otherwise.Moreover, we let M be a sufficiently large integer; setting M := 100(σ+g) suffices. With this setup,for any 1 ≤ i, j ≤ g we define A(i) and B(j) as appropriate shifts of X(i) and Y (j), respectively, moreprecisely we setA(i) := X(i) +{i ·M2 + E(i) ·M}and A :=g⋃i=1A(i),B(j) := Y (j) +{(g − j) ·M2}and B :=g⋃j=1B(j).Finally, we setu := g ·M2 + 2σ.We now verify the three claims. The size bound |A|, |B| = g ·σ1/2 is immediate from the property|X(i)|, |Y (j)| = σ1/2.For the output-size, we use the following claim.Claim 8.4. The following properties hold.1. For any i 6= j we have |A(i) +B(j)| ≤ α,2. For any even i we have (A(i) +B(i)) ∩ [u] = ∅, and3. For any odd i we have A(i) +B(i) ⊆ [g ·M2, g ·M2 + 2σ] ⊆ [u].26Proof. Claim 1. is immediate from Lemma 8.2.3, since A(i) and B(j) are just shifts of X(i) and Y (j).For even i we have min(A(i)) ≥ i·M2+M and min(B(i)) ≥ (g−i)·M2 and thus min(A(i)+B(i)) ≥g ·M2 +M > u, which shows claim 2.For odd i we have min(A(i)) ≥ i ·M2 and thus min(A(i) + B(i)) ≥ g ·M2. Similarly, we havemax(A(i)) ≤ i ·M2+σ and max(B(j)) ≤ (g−i) ·M2+σ and thus max(A(i)+B(i)) ≤ g ·M2+2σ.The above claim allows us to boundout =(A ∩B)∩ [u] ≤(∑i 6=j|A(i) +B(j)|)+∣∣∣∣⋃odd i(A(i) +B(i))∩ [u]∣∣∣∣ ≤ g2 · α+ 2σ + 1.It remains to analyze coverings. So let C be a rectangle covering of (A,B, [u]). See Figure 2for an illustration. We construct a graph G with vertex set V (G) consisting of all odd integers1 ≤ i ≤ g. We put an edge (i, i + 2) into E(G) if there exists a rectangle in C that contains a pairin A(i) × B(i) as well as a pair in A(i+2) × B(i+2). Note that such a rectangle contains all pairs inA(i+1) ×B(i+1) and thus has cost at least∣∣A(i+1) +B(i+1)∣∣ = σ.We now consider two cases, depending on the number of edges in G.If G has more than g/4 edges, then for at least g/4 even integers i all pairs in A(i) × B(i) arecovered by C. Since A(i) +B(i) ⊆ [g ·M2 + i ·M,g ·M2 + i ·M +2σ] has size σ, and M is large, allsumsets A(i) +B(i) for even i are disjoint, and therefore any covering of g/4 such sumsets has costat least gσ/4.Otherwise, if G has at most g/4 edges, then it has at least g/4 components. Note that eachcomponent must be covered by distinct rectangles in C. Since each component requires cost at least|A(i) +B(i)| = σ, the covering has a total cost of at least gσ/4. This finishes the proof.We are now ready to prove Theorem 2.12.Proof. Lemma 8.3 yields for any δ > 0 and integers m, t, where t is even, a tuple (A,B, [u]) without :=∣∣(A+B) ∩ [u]∣∣ = Oδ(2(2−2h(δ)+δ/2)tm(1−δ/2)t +mt),and any rectangle covering of (A,B, [u]) has cost Ωδ(2(1−h(δ))tmt). Observe that we can write thiscost bound in the form Ωδ(outc) forc :=(1− h(δ))t + t logmmax{(2− 2h(δ) + δ/2)t + (1− δ/2)t logm, t logm} .Note that t cancels in this expression. We numerically optimize c = c(δ,m) by setting m := 10and δ := 0.2709, obtaining c ≥ 1.047. In particular, we constructed an infinite sequence of tuples(A,B, [u]) for which any rectangle covering has cost Ω(out1.047), which proves Theorem 2.12.9 Reducing SubsetSum to Prefix-Restricted Sumset ComputationThis section is devoted to proving Theorems 2.11 and 2.13. In particular, we give an output-sensitivity-preserving reduction from SubsetSum to top-k-convolution. For that, we need thefollowing definition.27A(1) A(2) A(3) A(4) A(5) A(6) A(7) A(8) A(9)B(1)B(2)B(3)B(4)B(5)B(6)B(7)B(8)B(9)000002468Figure 2: A tuple (A,B, [u]) constructed in the proof of Theorem 2.12. In gray we mark all thepairs (i, j) such that A(i) +B(j) is contained in [u]. On the diagonal only every second pair belongsto the output. The numbers along the diagonal indicate the shift E(i). The five boxes markedas “0” contribute to (A + B) ∩ [u] exactly the same numbers. The rectangles marked with 2, 4, 6,and 8 have disjoint sumsets. By definition, a covering algorithm should cover all the boxes markedwith 0. The indicated blue rectangle covers numbers from two boxes marked by 0, and thus alsofully contains the box corresponding to pair (A(6), B(6)) in between, which does not belong to theoutput.28Definition 9.1 ((α, ζ)-effective algorithm). Let sets A,B ⊆ [u] with |A| = n, |B| = m. An (α, ζ)-effective algorithm is an algorithm for prefix-restricted sumset computation algorithm on instance(A,B, [u]) which runs in timeÕ(m+ n+ |(A+B) ∩ [(1 + ζ)u]|1+α).The reduction is based on randomly dividing the input and conquering with a prefix-restrictedsumset computation. In order to prove correctness, apart from the argumentation in [14], weadditionally we need number-theoretic condition on how the set of attainable subset sums decreasesunder partition; this is captured in Lemma 9.3. The base cases of the algorithm are those instanceswhere either there is only one element or every element is sufficiently large with respect to thetarget.9.1 Handling Large ElementsIn this subsection we treat one of the base instances of the more general algorithm, which are theinstances where all elements are large with respect to the target. For a small technical reason inlater subsections (in particular, in order to afford to take a union bound over all recursive calls), weneed to define small numbers with respect to two parameters u, t. We shall analyze Algorithm 4.We classify an element as “heavy” if it is larger than uβ log3 t.Algorithm 41: procedure SubsetSumforLargeElements(X,u, t, δ) ⊲ x ∈[uβ log3 t, u],∀x ∈ X;β = Θ(1)2: O ← ∅3: R← Θ(log(t/δ))4: B ← 2β2 log6 t5: for r ∈ [R] do6: Color X randomly with B different colors.7: for b ∈ [B] do8: X(r,b) ← (elements of X which have received color b)⋃{0}.9: O(r) ← ∅10: for b ∈ [B] do11: O(r) ←(O(r) +X(r,b)))⋂[u] ⊲ Oracle call with failure probability δ2BR12: O ← O ∪O(r)13: Return OThe instance consisting solely of large items is much easier to solve, since only a polylogarithmicnumber of elements can participate in a subset sum which is at most t. Our algorithm in the nextsubsection will be recursive, and the next lemma will serve as one of the the bases of the recursion.Lemma 9.2 (Guarantee for large elements). Suppose there exists an (α, ζ)-effective algorithm forsolving (A,B, u) prefix-resticted sumset computation with probability 9/10. Let parameters u ≤ t.Let X ⊆[uβ log3 t, u]a set of positive integers, where β is a sufficiently large absolute constant. Wecan find S(X,u) using Algorithm 4 in timeÕ(|S(X,u + ζ · u)|1+α),with probability 1− δ. We remind the reader that Õ(·) hides factors also in log t.29Algorithm 51: procedure SubsetSumReduction(X,u, t) ⊲ u ≤ t2: X(S) ← X ∩[uβ log3 t]⊲ Partition X to small and large element3: O ← SubsetSumforLargeElements(X \\\\X(S), u, t, 1/poly(u))4: X(1) ← Sample X(S) at rate 1/25: X(2) ← X(S) \\\\X(1)6: ǫ← 1log t7: O1 ← SubsetSumReduction(X(1), (1 + ǫ)u2 , t)8: O2 ← SubsetSumReduction(X(2), (1 + ǫ)u2 , t)9: Return (O + (O1 +O2) ∩ [u]) ∩ [u] ⊲ Oracle call with failure probability 1/poly(t)Proof. First of all, to accommodate the call in line 11 we note that any (a, ζ)-effective algorithmwith success probability 9/10 can be turned to an (a, ζ)-effective algorithm with success probability1 − δ by running in parallel Θ(log(1/δ)) independent copies and stopping when a 7/10 fraction ofthem has halted. Then at least half of them will have returned the same answer, and we can returnthis. Fix x ∈ S(X, t) and let I ⊆ X such that Σ(I) = x. Since every element of X is at leastu/(β log3 t), it should be the case that |I| ≤ β log3 t. For a fixed r ∈ [R], the probability that allelements in I are colored with a different color is at least 12 by standard arguments. Conditioningon the latter event happening, the computation in Lines 10, 11 will contain x with probability, bythe same argument as the one appearing in [14, Lemma 3.1]. In particular, x ∈ X(r,1)+ . . .+X(r,B),by the fact that every element of I belongs to a distinct set X(r,b), and those sets contain 0 ∈ X(r,b)for all b.All calls to prefix-restricted sumset computation will succeed with probability δ/2 by a unionbound. Repeating R = Θ(log(u/δ)) times ensures that the coloring in line 6 will color everyelement of I with a different color, i.e. for some r it hold that x ∈ X(r,1) + . . . + X(r,B) exceptwith probability(12)R= δ/(2u). In turn, this implies that x will be found with probability 1− δ2u .Taking a union-bound over the at most u possible values of x we obtain that O shall contain everyx ∈ S(X,u) with probability 1−(δ2 +δ2)= 1− δ. The running time of the algorithm follows by thefact that each computation involved is of the form (A+B)∩ [u] with A,B ∈ S(X,u), and there areR ·B = O(log6 t · log(u/δ)) such computations. This finishes the proof.9.2 General AlgorithmThe reduction is presented in Algorithm 5. As mentioned in the overview, the algorithm partitionsthe set X to small elements (set X(S)) and large elements (set X \\\\ X(S)). The large elementsare handled by the algorithm presented in the previous subsection, Line 3. Next, X(S) is split toX(1),X(2) randomly in Line 4, and recursion takes places in each part with the appropriate changeof the target (Lines 7,8).9.3 A Number-Theoretic Lemma for the decrease of Subset SumsThe crux of the analysis is the following technical lemma, which postulates that the number ofsubsets sums of a set decreases in a suitable way when halving the set. This will allows us to controlthe running time of Algorithm 5. Note that this holds for any partitioning, not only for a randomone.30Lemma 9.3 (Number of subset sums decreases appropriately). Let Z ⊆ [u] be partitioned intoZ = Z(1) ∪ Z(2). Set µ := max(Z)/u and assume µ ≤ 116 . Then for any 0 ≤ ǫ ≤ 14 we have∣∣∣S(Z(1), (1 + ǫ)u2)∣∣∣+∣∣∣S(Z(2), (1 + ǫ)u2)∣∣∣ ≤ |S(Z, u)| + 11− 2ǫ− 4µProof. We denoteA := S(Z(1), (1 + ǫ)u2),B := S(Z(2), (1 + ǫ)u2),C := S(Z, u),A′ := A ∩((1− ǫ− 2µ)u2, (1 + ǫ)u2].With this notation, our goal is to show|C| ≥ (|A|+ |B|)(1− 2ǫ− 4µ)− 1. (7)By symmetry, without loss of generality we can assume max(A) ≤ max(B).It remains to consider the case max(A),max(B) > (1− ǫ)u2 .The statement is trivial if max(A) ≤ (1− ǫ)u2 (or max(B) ≤ (1− ǫ)u2 ), since then for x ∈ B \\\\{0}we can map IY (x) to IY (x) ∪W , yielding the following: Σ(IY (x) ∪W ) = x + Σ(W ) > max(A),obtaining |B|−1 distinct subset sums above max(A), from which we conclude that |C| ≥ |A|+|B|−1.Let us now assume that this is not the case, and thus max(A) ≥ (1−ǫ)u2 and max(B) ≥ (1−ǫ)u2 .We shall need the following two claims.Claim 9.4. |C| ≥ |A|+ |B| − |A′| − 1Proof. Note that∣∣∣C ∩[0, (1 − ǫ− 2µ)u2]∣∣∣ ≥∣∣∣A ∩[0, (1 − ǫ− 2µ)u2]∣∣∣ = |A| − |A′|Moreover, since all items are bounded by µu, we can choose P ⊆W such thatΣ(P ) ∈[(1− ǫ− 2µ)u2, (1− ǫ)u2].To see that, let any ordering of elements of W , initialize P to the empty set, and start addingelements to it one by one; clearly at some point Σ(P ) will fall inside the aforementioned interval.Now, for every x ∈ B \\\\ {0}, map IY (x) to IY (x) ∪ P to obtain number Σ(IY (x) ∪ P ) = x+ Σ(P );this yields |B| − 1 different sums, all in the interval[(1− ǫ− 2µ)u2 + 1, u], and thus disjoint fromthe numbers in A counted above. Thus, we obtain at least (|A|− |A′|)+ (|B|− 1) different numbersin C, yielding the proof of the claim.Claim 9.5. |C| ≥ |A|+ |A′|(12ǫ+4µ − 1).31Proof. In order to prove the desired lower bound, we shall look at two disjoint intervals[0, (1 + ǫ)u2],and[(1 + ǫ)u2 + 1, t].In the interval[0, (1 + ǫ)u2]we shall simply count the numbers in A,∣∣A ∩[0, (1 + ǫ)u2]∣∣ ≤ |A|.For the other interval we argue as follows. There exists a sequence of setsPi0 ⊆ Pi0+1 ⊆ Pi0+2 ⊆ . . . ⊆ YsatisfyingΣ(Pi) ∈ [i(ǫ+ 2µ)u+ 1, i(ǫ+ 2µ)u+ µu)] ,for all i ≥ 1 such thati(ǫ+ 2µ)u+ µu ≤ max(B). (8)Note that i0 is the smallest non-zero i such that the above inequality holds. Call such i good.To see the existence of such a sequence, intialize P to the empty set and starting adding elementsof Y one by one. Since every element is at most µu and the [i(ǫ+ 2µ)u+ 1, i(ǫ + 2µ)u+ µu)] is oflength µ we obtain that existence of such a sequence.Since max(B) ≥ (1− ǫ)u2 all i smaller than(1− ǫ)u2 − µu(ǫ+ 2µ)u=1− ǫ− 2µ2(ǫ+ 2µ)=12ǫ+ 4µ− 12.are good.For any x ∈ A′ ⊆ S(X, (1 + ǫ)u2 ) map IW (x) to IW (x) ∪ Pi, to obtain |A′| different numbers inthe interval[i(ǫ+ 2µ)u+ 1, i(ǫ+ 2µ)u+ µu)] +[(1− ǫ− 2µ)u2, (1 + ǫ)u2]=[i(ǫ+ 2µ)u+ (1− ǫ− 2µ)u2+ 1, (i + 1)(ǫ + 2µ)u+ (1− ǫ− 2µ)u2].The collection of those intervals across all i are pairwise disjoint as well as disjoint from the initialinterval[0, (1 + ǫ)u2].In order for all generated sums to be at most u, we also need (i+1)(ǫ+2µ)u+(1−ǫ−2µ)u2 ≤ u,which boils down to i + 1 ≤ 1+ǫ+2µ2ǫ+4µ = 12 + 12ǫ+4µ . Since i is an integer, we have at least 12ǫ+4µ − 1valid i’s. Hence, we obtain|C| ≥ |A|+ |A′|(12ǫ+ 4µ− 1).To finish the proof of the lemma we combine the two claims, by considering two cases:• Case 1: |A′| ≤ (2ǫ+ 4µ)|B|.Then Claim 9.3 yields|C| ≥ |A|+ |B| − |A′| − 1 ≥ |A|+ |B|(1− 2ǫ− 4µ)− 1 ≥ (|A|+ |B|)(1 − 2ǫ− 4µ)− 1.32• Case 2: |A′| ≥ (2ǫ+ 4µ) · |B|Then Claim 9.3 yields|C| ≥ |A|+ |A|′(12ǫ+ 4µ)≥|A|+ |B|(2ǫ+ 4µ)(12ǫ+ 4µ)≥(|A| + |B|)(1 − 2ǫ− 4µ).9.4 Putting Everything TogetherWe finish the reduction and the proofs of Theorems 2.13 and 2.11. The algorithms is one call toSubsetSumReduction(S, t, t).Proof of correctness. We show that SubsetSumReduction(X, t, t) returns S(X, t) with con-stant probability. This will require that all recursive calls succeed. In particular, for every call intheClaim 9.6. Consider the execution of SubsetSumReduction(X,u, t). Fix x ∈ S(X,u) and setI = X(S)∩ IX(x), i.e. the “part” of the representation x which is formed by small elements. It holdsthatP{Σ(I ∩X(1)) /∈[(1 + ǫ)u2]}≤ 1poly(t).In words, the sum of elements of I which belong to X(1) will be at most (1+ǫ)u2 with high probability.The analogous statement holds with X(1) replaced with X(2).Proof. This claim easily follows by concentration bounds for bounded random variables. We shalluse Bernstein’s inequality which postulates that for a collection C of random variables {Ze}e∈C suchthat all Ze ∈ [0,K], it holds thatP{∣∣∣∣∣∑eZe − E∑eZe∣∣∣∣∣ ≥ λ}≤ e− cλK + e−cλ2σ2 ,where σ2 =∑e E{(Ze − EZe)2}, λ ≥ 0 and c is some absolute constant.We apply the inequality the collection C := I of independent random variables {Ze}e∈I ={e · 1X(1)(e)}e∈I and λ = ǫu2 . In words, Ze is e with probability 1/2, and 0 otherwise. We have1. E [∑e Ze] ≤ x2 ≤ u2 .2. K = uβ·log3 tby definition of X(S), and3. σ2 ≤ Kx2 sinceσ2 ≤∑e∈Ie2 · E {1X(1)(e)} ≤ K∑e∈Ie · E {1X(1)(e)} =12K∑e∈Ie2≤ Kx2.33We thus obtainP{Σ(I ∩X(1)) ≥ ǫu2}≤ e− cβ·ǫ log3 t4 + e−cβ·ǫ2u2·log3 t2ux ≤ 1/poly(t),as long as β is sufficiently large compared to c. This finishes the proof of the claim.Equipped with the above claim, we can now prove correctness of the reduction. Fix x ∈ S(X, t)and let I = IX(x). Let also I0 = I ∩ (X \\\\X(S)), I1 = I ∩X(1), I2 = I ∩X(2). Set also x = y+ z+w,where y = Σ(I0), z = Σ(I1), w = Σ(I2). It holds that y will be returned by Algorithm 5 withprobability 1− 1/poly(t) due to the call in Line 3. If the conclusion of Claim 9.6 holds for I, thenthis means that both Σ(I∩X(1)) and Σ(I∩X(2)) are at most (1+ǫ)u2 and hence z ∈ S(X(1), (1+ǫ)u2 ),w ∈ S(X(2), (1 + ǫ)u2 ). Thus, if the recursive calls in Lines 7 and 8 as well as the call 3 succeed,then x = y+ z+w will be inserted to the output. This means that the correctness of the algorithmis guaranteed on the conclusion of Claim 9.6 holding in all recursive calls and on every call toSubsetSumforLargeElements being correct. Since in each recursive call t does not change andremains the same, each recursive splitting succeeds with probability 1− 1/poly(t), while every callto SubsetSumforLargeElements succeeds also with probability 1− 1/poly(t). This allows fora union-bound over all splitting and all calls to SubsetSumforLargeElements.Proof of Desired Running Time. Due to the splitting lemma 9.3, the fact that ǫ = 1log t ≤1log u ≤ 14 at all times, and a straightforward induction, we have that the total output size for all prob-lems in the ℓth level of the recursion tree during the execution of SubsetSumReduction(S, u, t)is upper bounded by|S(X, t)|(1− 2ǫ− 2µ)ℓ = Õ(|S(X, t)|),since ℓ ≤ log n ≤ log t and ǫ = 1log t , µ = 1β log3 t . Thus, over all recursion levels the total outputsize is still Õ(|S(X, t)|). This shows that if we had the ideal (0, 0)-effective algorithm, we wouldobtain a SubsetSum running in time Õ(S(X, t)). A similar analysis yields the desired reductionwhen we plug in a (α, ζ)-effective algorithm.Obtaining Theorem 2.11. We plug in the Õ(out4/3)-time algorithm for prefix-restricted sumsetcomputation (Theorem 2.9).Obtaining Theorem 2.13. We plug in the algorithm guaranteed by the first part of Theorem 2.13and Observation 2.8.10 AcknowledgementsWe are grateful to Shachar Lovett for the resolution of an Additive Combinatorics question in anearly stage of this work, which gave the core idea for Theorem 2.12, and for allowing us to includehis construction in this paper.3411 Conclusion and Future WorkWe initiated a line of research which strives for a SubsetSum algorithm that computes the setS(X, t), consisting of all subsets sums of X below t, in near-linear output-sensitive time Õ(|S(X, t)|).Our approach lead us to studying a new type of convolution problem: In top-k-convolution the taskis to compute the lowest k monomials in the product of two sparse polynomials. Many open problemsare spawned by our work; here we present questions that are of particular interest to us.Question 11.1. Understand our notion of covering for prefix-restricted sumset computation, eitherby constructing a better covering or by proving a higher lower bound. Specifically, for non-rectangularcoverings so far we have no superlinear lower bounds.Question 11.2. Design any non-trivial algorithm that is not based on coverings, and thus exploitsthe additive structure in a different way.Question 11.3. Are covering algorithms universal? More precisely, can we transform any algorithminto a covering algorithm, with a reasonable blow-up in the running time?Question 11.4. So far we have no algorithm that always beats Bellman’s algorithm with runningtime O(n · |S(X, t)|). Specifically, can we solve SubsetSum in time O(n1−ǫ · |S(X, t)|) for anyǫ > 0? A possible approach to this question is to design faster algorithms for prefix-restricted sumsetcomputation in the special situation where A = S(X(1), t), B = S(X(2), t). Do these sets offerexploitable structure, e.g., giving rise to more sophisticated sumset estimates?References[1] Amir Abboud, Karl Bringmann, Danny Hermelin, and Dvir Shabtay. SETH-based lower boundsfor subset sum and bicriteria path. In Proceedings of the Thirtieth Annual ACM-SIAM Sym-posium on Discrete Algorithms, pages 41–57. SIAM, 2019.[2] Karl R. Abrahamson. Generalized string matching. SIAM J. Comput., 16(6):1039–1051, 1987.[3] Josh Alman. Limits on the universal method for matrix multiplication. In 34th ComputationalComplexity Conference, CCC 2019, July 18-20, 2019, New Brunswick, NJ, USA., pages 12:1–12:24, 2019.[4] Josh Alman and Virginia Vassilevska Williams. Further limitations of the known approachesfor matrix multiplication. In 9th Innovations in Theoretical Computer Science Conference,ITCS 2018, January 11-14, 2018, Cambridge, MA, USA, pages 25:1–25:15, 2018.[5] Josh Alman and Virginia Vassilevska Williams. Limits on all known (and some unknown)approaches to matrix multiplication. In 59th IEEE Annual Symposium on Foundations ofComputer Science, FOCS 2018, Paris, France, October 7-9, 2018, pages 580–591, 2018.[6] Andrew Arnold and Daniel S Roche. Output-sensitive algorithms for sumset and sparse polyno-mial multiplication. In Proceedings of the 2015 ACM on International Symposium on Symbolicand Algebraic Computation, pages 29–36. ACM, 2015.[7] Per Austrin, Petteri Kaski, Mikko Koivisto, and Jussi Määttä. Space–time tradeoffs for SubsetSum: An improved worst case algorithm. In Proc. of the 40th International Colloquium onAutomata, Languages, and Programming (ICALP), pages 45–56, 2013.35[8] Per Austrin, Petteri Kaski, Mikko Koivisto, and Jesper Nederlof. Subset Sum in the absenceof concentration. In Proc. of the 32nd International Symposium on Theoretical Aspects ofComputer Science (STACS), pages 48–61, 2015.[9] Per Austrin, Petteri Kaski, Mikko Koivisto, and Jesper Nederlof. Dense Subset Sum may be thehardest. In Proc. of the 33rd Symposium on Theoretical Aspects of Computer Science (STACS),pages 13:1–13:14, 2016.[10] Kyriakos Axiotis, Arturs Backurs, Ce Jin, Christos Tzamos, and Hongxun Wu. Fast modularsubset sum using linear sketching. In Proc. of the 30th Annual ACM-SIAM Symposium onDiscrete Algorithms (SODA), pages 58–69. SIAM, 2019.[11] Nikhil Bansal, Shashwat Garg, Jesper Nederlof, and Nikhil Vyas. Faster space-efficient al-gorithms for Subset Sum, k-Sum and related problems. In Proc. of the 49th Annual ACMSIGACT Symposium on Theory of Computing (STOC), pages 198–209, 2017.[12] Richard E. Bellman. Dynamic programming. Princeton University Press, 1957.[13] Ernest F. Brickell and Andrew M. Odlyzko. Cryptanalysis: A survey of recent results. Pro-ceedings of the IEEE, 76(5):578–593, 1988.[14] Karl Bringmann. A near-linear pseudopolynomial time algorithm for Subset Sum. In Proc. ofof the 28th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1073–1084,2017.[15] Karl Bringmann and Vasileios Nakos. Fast n-fold boolean convolution via additive combina-torics. Under submission, 2019.[16] Boris Bukh. Walk through combinatorics:sumset inequalities. 2018.[17] Timothy M. Chan and Moshe Lewenstein. Clustered integer 3SUM via additive combinatorics.In Proc. of the 47th Annual ACM Symposium on Theory of Computing (STOC), pages 31–40,2015.[18] Benny Chor and Ronald R. Rivest. A knapsack-type public key cryptosystem based on arith-metic in finite fields. IEEE Transactions on Information Theory, 34(5):901–909, 1988.[19] Richard Cole and Ramesh Hariharan. Verifying candidate matches in sparse and wildcardmatching. In Proc. of the 34th Annual ACM Symposium on Theory of Computing (STOC),pages 592–601. ACM, 2002.[20] Marek Cygan, Holger Dell, Daniel Lokshtanov, Dániel Marx, Jesper Nederlof, Yoshio Okamoto,Ramamohan Paturi, Saket Saurabh, and Magnus Wahlström. On problems as hard as CNF-SAT. ACM Transactions on Algorithms, 12(3):41, 2016.[21] Itai Dinur, Orr Dunkelman, Nathan Keller, and Adi Shamir. Efficient dissection of compositeproblems, with applications to cryptanalysis, knapsacks, and combinatorial search problems. InProc. of the 32nd Annual Conference on Advances in Cryptology (CRYPTO), pages 719–740,2012.[22] François Le Gall. Powers of tensors and fast matrix multiplication. In International Symposiumon Symbolic and Algebraic Computation, ISSAC ’14, Kobe, Japan, July 23-25, 2014, pages296–303, 2014.36[23] Pawel Gawrychowski and Przemyslaw Uznanski. Optimal trade-offs for pattern matching withk mismatches. arXiv preprint abs/1704.01311, 2017.[24] Omer Gold and Micha Sharir. Improved bounds for 3SUM, k-SUM, and linear degeneracy. InProc. of the 25th Annual European Symposium on Algorithms (ESA), pages 42:1–42:13, 2017.[25] Ellis Horowitz and Sartaj Sahni. Computing partitions with applications to the knapsackproblem. Journal of the ACM, 21(2):277–292, 1974.[26] Russell Impagliazzo and Moni Naor. Efficient cryptographic schemes provably as secure assubset sum. Journal of Cryptology, 9(4):199–216, 1996.[27] Ce Jin and Hongxun Wu. A simple near-linear pseudopolynomial time randomized algorithmfor subset sum. In 2nd Symposium on Simplicity in Algorithms, SOSA@SODA 2019, volume 69of OASICS, pages 17:1–17:6, 2019.[28] Haim Kaplan, László Kozma, Or Zamir, and Uri Zwick. Selection from heaps, row-sorted matri-ces, and X+Y using soft heaps. In 2nd Symposium on Simplicity in Algorithms, SOSA@SODA2019, volume 69 of OASICS, pages 5:1–5:21, 2019.[29] Richard M. Karp. Reducibility among combinatorial problems. In Complexity of ComputerComputations, pages 85–103. Springer, 1972.[30] Hans Kellerer, Ulrich Pferschy, and David Pisinger. Knapsack problems. Springer, 2004.[31] Konstantinos Koiliaris and Chao Xu. A faster pseudopolynomial time algorithm for SubsetSum. In Proc. of the 28th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),pages 1062–1072, 2017.[32] Andrea Lincoln, Virginia Vassilevska Williams, Joshua R. Wang, and R. Ryan Williams. De-terministic time-space trade-offs for k-SUM. In Proc. of the 43rd International Colloquium onAutomata, Languages, and Programming (ICALP), pages 58:1–58:14, 2016.[33] Daniel Lokshtanov, Dániel Marx, and Saket Saurabh. Known algorithms on graphs on boundedtreewidth are probably optimal. In Proc. of the 27th 2nd Annual ACM-SIAM Symposium onDiscrete Algorithms (SODA), pages 777–789, 2011.[34] Silvano Martello and Paolo Toth. Knapsack problems: algorithms and computer implementa-tions. John Wiley & Sons, Inc., 1990.[35] Ralph Merkle and Martin Hellman. Hiding information and signatures in trapdoor knapsacks.IEEE Transactions on Information Theory, 24(5):525–530, 1978.[36] Michael Monagan and Roman Pearce. Parallel sparse polynomial multiplication using heaps.In Proceedings of the 2009 international symposium on Symbolic and algebraic computation,pages 263–270. ACM, 2009.[37] Vasileios Nakos. Nearly optimal sparse polynomial multiplication. arXiv preprintarXiv:1901.09355, 2019.[38] Jesper Nederlof. A short note on Merlin-Arthur protocols for subset sum. Information Pro-cessing Letters, 118:15–16, 2017.37[39] Kevin O’Bryant. Sets of integers that do not contain long arithmetic progressions. ElectronicJournal of Combinatorics, 18(1):P59, 2011.[40] Andrew M. Odlyzko. The rise and fall of knapsack cryptosystems. Cryptology and Computa-tional Number Theory, 42:75–88, 1990.[41] Daniel S. Roche. Adaptive polynomial multiplication. Proc. Milestones in Computer Algebra(MICA’08), pages 65–72, 2008.[42] Daniel S. Roche. What can (and can’t) we do with sparse polynomials? arXiv preprintarXiv:1807.08289, 2018.[43] Adi Shamir. A polynomial-time algorithm for breaking the basic Merkle-Hellman cryptosystem.IEEE Transactions on Information Theory, 30(5):699–704, 1984.[44] Xuancheng Shao and Wenqiang Xu. A robust version of Freiman’s 3k–4 theorem and applica-tions. In Mathematical Proceedings of the Cambridge Philosophical Society, volume 166, pages567–581. Cambridge University Press, 2019.[45] Terence Tao and Van H. Vu. Additive combinatorics, volume 105. Cambridge University Press,2006.[46] Joris Van Der Hoeven and Grégoire Lecerf. On the complexity of multivariate blockwise poly-nomial multiplication. In Proceedings of the 37th International Symposium on Symbolic andAlgebraic Computation, pages 211–218. ACM, 2012.[47] Virginia Vassilevska Williams. Multiplying matrices faster than Coppersmith-Winograd. InProc. of the 44th Symposium on Theory of Computing (STOC), pages 887–898, 2012.38'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd27'), 'authors': 'Dhulipala, Laxman, Durfee, David, Kulkarni, Janardhan, Peng, Richard, Sawlani, Saurabh, Sun, Xiaorui', 'year': '2020', 'title': 'Parallel Batch-Dynamic Graphs: Algorithms and Lower Bounds', 'full_text': 'ar\\nX\\niv\\n:1\\n90\\n8.\\n01\\n95\\n6v\\n1 \\n [c\\ns.D\\nS]\\n  6\\n A\\nug\\n 20\\n19\\nParallel Batch-Dynamic Graphs: Algorithms and Lower Bounds\\nDavid Durfee\\nLinkedIn\\nddurfee@linkedin.com\\nLaxman Dhulipala\\nCMU\\nldhulipa@cs.cmu.edu\\nJanardhan Kulkarni\\nMicrosoft Research\\njakul@microsoft.com\\nRichard Peng\\nGeorgia Tech\\nrichard.peng@gmail.com\\nSaurabh Sawlani\\nGeorgia Tech\\nsawlani@gatech.edu\\nXiaorui Sun\\nUniversity of Illinois at Chicago\\nxiaorui@uic.edu\\nAbstract\\nIn this paper we study the problem of dynamically maintaining graph properties under\\nbatches of edge insertions and deletions in the massively parallel model of computation. In this\\nsetting, the graph is stored on a number of machines, each having space strongly sublinear with\\nrespect to the number of vertices, that is, nǫ for some constant 0 < ǫ < 1. Our goal is to\\nhandle batches of updates and queries where the data for each batch fits onto one machine in\\nconstant rounds of parallel computation, as well as to reduce the total communication between\\nthe machines. This objective corresponds to the gradual buildup of databases over time, while\\nthe goal of obtaining constant rounds of communication for problems in the static setting has\\nbeen elusive for problems as simple as undirected graph connectivity.\\nWe give an algorithm for dynamic graph connectivity in this setting with constant communi-\\ncation rounds and communication cost almost linear in terms of the batch size. Our techniques\\ncombine a new graph contraction technique, an independent random sample extractor from cor-\\nrelated samples, as well as distributed data structures supporting parallel updates and queries\\nin batches.\\nWe also illustrate the power of dynamic algorithms in the MPC model by showing that the\\nbatched version of the adaptive connectivity problem is P-complete in the centralized setting,\\nbut sub-linear sized batches can be handled in a constant number of rounds. Due to the wide\\napplicability of our approaches, we believe it represents a practically-motivated workaround to\\nthe current difficulties in designing more efficient massively parallel static graph algorithms.\\n1 Introduction\\nParallel computation frameworks and storage systems, such as MapReduce, Hadoop and Spark,\\nhave been proven to be highly effective methods for representing and analyzing the massive datasets\\nthat appear in the world today. Due to the importance of this new class of systems, models of\\nparallel computation capturing the power of such systems have been increasingly studied in re-\\ncent years, with the Massively Parallel Computation (MPC) model [KSV10] now serving as the\\ncanonical model. In recent years the MPC model has seen the development of algorithms for fun-\\ndamental problems, including clustering [EIM11, BMV+12, BBLM14, YV18], connectivity prob-\\nlems [RMCS13, KLM+14, ASS+18, ASW19, ASZ19], optimization [MKSK13, EN15, BENW16],\\ndynamic programming [IMS17, BBD+18], to name several as well as many other fundamental graph\\nand optimization problems [BKV12, ANOY14, KMVV15, AG18, AK17, Ass17, ASS+18, BFU18,\\nC LM+18,  LMW18, Ona18, ABB+19, ACK19, AKZ19, BDE+19, BHH19, GKMS18, GKU19, GU19,\\nHSS19]. Perhaps the main goal in these algorithms has been solving the problems in a constant\\nnumber of communication rounds while minimizing the total communication in a round. Obtaining\\nlow round-complexity is well motivated due to the high cost of a communication round in practice,\\nwhich is often between minutes and hours [KSV10]. Furthermore, since communication between\\nprocessors tends to be much more costly than local computation, ensuring low communication per-\\nround is also an important criteria for evaluating algorithms in the MPC model [SASU13, BKS13].\\nPerhaps surprisingly, many natural problems such as dynamic programming [IMS17] and sub-\\nmodular maximization [BENW16] can in fact be solved or approximated in a constant number\\nof communication rounds in MPC model. However, despite considerable effort, we are still far\\nfrom obtaining constant-round algorithms for many natural problems in the MPC setting where\\nthe space-per-machine is restricted to be sublinear in the number of vertices in the graph (this\\nsetting is arguably the most reasonable modeling choice, since real-world graphs can have tril-\\nlions of vertices). For example, no constant round algorithms are known for a problem as simple\\nas connectivity in an undirected graph, where the current best bound is O(log n) rounds in gen-\\neral [KSV10, RMCS13, KLM+14, ASS+18,  LMW18, ASW19]. Other examples include a O(\\n√\\nlog n)\\nround algorithm for approximate graph matching [Ona18, GU19], and a O(\\n√\\nlog log n)-round algo-\\nrithm for (∆+1) vertex coloring [CFG+19]. Even distinguishing between a single cycle of size n and\\ntwo cycles of size n/2 has been conjectured to require Ω(log n) rounds [KSV10, RMCS13, KLM+14,\\nRVW18, YV18, GKU19, IM19]. Based on this conjecture, recent studies have shown that several\\nother graph related problems, such as maximum matching, vertex cover, maximum independent\\nset and single-linkage clustering cannot be solved in a constant number of rounds [YV18, GKU19].\\nOn the other hand, most large-scale databases are not formed by a single atomic snapshot, but\\nform rather gradually through an accretion of updates. Real world examples of this include the\\nconstruction of social networks [LZY10], the accumulation of log files [HI15], or even the gradual\\nchange of the Internet itself [DG08, KTF09, MAB+10]. In each of these examples, the database is\\ngradually formed over a period of months, if not years, of updates, each of which is significantly\\nsmaller than the whole database. It is often the case that the updates are grouped together, and are\\nperiodically processed by the database as a batch. Furthermore, it is not uncommon to periodically\\nre-index the data structure to handle a large number of queries between sets of updates.\\nIn this paper, motivated by the gradual change in real-world datasets through batches of up-\\ndates, we consider the problem of maintaining graph properties in dynamically changing graphs\\nin the MPC model. Our objective is to maintain the graph property for batches of updates, while\\nachieving a constant number of rounds of computation in addition to also minimizing the total\\ncommunication between machines in a given round.\\nSpecifically, we initiate the study of parallel batch-dynamic graph problems in MPC, in which\\n1\\nan update contains a number of mixed edge insertions and deletions. We believe that batch-\\ndynamic algorithms in MPC capture the aforementioned real world examples of gradually changing\\ndatabases, and provide an efficient distributed solution when the size of the update is large compared\\nto single update dynamic algorithms. We note that a similar model for dynamic graph problems in\\nMPC was recently studied by Italiano et al. [ILMP19]. However, they focus on the scenario where\\nevery update only contains a single edge insertion or deletion. Parallel batch-dynamic algorithms\\nwere also recently studied in the shared-memory setting by Tseng et al. [TDB19] for the forest-\\nconnectivity problem and Acar et al. [AABD19] for dynamic graph connectivity. However, the\\ndepth of these algorithms is at least Ω(log n), and it is not immediately clear whether these results\\ncan be extended to low (constant) round-complexity batch-dynamic algorithms in the MPC setting.\\nWe also study the power of dynamic algorithms in the MPC setting by considering a natural\\n“semi-online” version of the connectivity problem which we call adaptive connectivity. We show that\\nthe adaptive connectivity problem is P-complete, and therefore in some sense inherently sequential,\\nat least in the centralized setting. In contrast to this lower bound in the centralized setting, we\\nshow that in the MPC model there is a batch-dynamic algorithm that can process adaptive batches\\nwith size proportional to the space per-machine in a constant number of rounds. Note that such\\nan algorithm in the centralized setting (even one that ran in slightly sublinear depth per batch)\\nwould imply an algorithm for the Circuit Value Problem with polynomial speedup, thus solving a\\nlongstanding open problem in the parallel complexity landscape.\\n1.1 Our Results\\nSince graph connectivity proves to be an effective representative for the aforementioned difficulty\\nof graph problems in the MPC model, the focus of this paper is studying graph connectivity and\\nadaptive graph connectivity in the batch-dynamic MPC model.\\nGraph Connectivity\\nThe dynamic connectivity problem is to determine if a given pair of vertices belongs to same\\nconnected component in the graph as the graph undergoes (batches of) edge insertions and deletions.\\nThe dynamic connectivity algorithm developed in this paper is based on a hierarchical partitioning\\nscheme that requires a more intricate incorporation of sketching based data structures for the\\nsequential setting. Not only does our scheme allow us to achieve a constant number of rounds, but\\nit also allows us to achieve a total communication bound that is linear with respect to the batch\\nsize with only an additional no(1) factor.\\nTheorem 1.1. In the MPC model with memory per machine s = O˜(nǫ) we can maintain a dynamic\\nundirected graph on m edges which, for constants δ, α, and integer k such that k ·nα+δ ·polylog(n) ≤\\ns, can handle the following operations with high probability:\\n1. A batch of up to k edge insertions/deletions, using O(1/(δα)) rounds.\\n2. Query up to k pairs of vertices for 1-edge-connectivity, using O(1/α) rounds.\\nFurthermore, the total communication for handling a batch of k operations is O˜(knα+δ), and the\\ntotal space used across all machines is O˜(m).\\nAdaptive Connectivity and Lower-Bounds in the Batch-Dynamic MPC Model\\nIn the adaptive connectivity problem, we are given a sequence of query/update pairs. The problem is\\nto process each query/update pair in order, where each query determines whether or not a given pair\\n2\\nof vertices belongs to the same connected component of the graph, and applies the corresponding\\ndynamic update to the graph if the query succeeds. We obtain the following corollary by applying\\nour batch-dynamic connectivity algorithm, Theorem 1.1.\\nCorollary 1.2. In the MPC model with memory per machine s = O˜(nǫ) we can maintain a dynamic\\nundirected graph on m edges which for constants δ, α, and integer k such that k·nα+δ ·polylog(n) ≤ s\\ncan handle the following operation with high probability:\\n1. An adaptive batch of up to k (query, edge insertions/deletions) pairs, using O(1/(δα)) rounds.\\nFurthermore, the total communication for handling a batch of k operations is O˜(knα+δ), and the\\ntotal space used across all machines is O˜(m).\\nWe also provide a lower-bound for the adaptive connectivity problem in the centralized setting,\\nshowing that the problem is P-complete under NC1 reduction. P-completeness is a standard notion\\nof parallel hardness [KRS90, GHR+95, BM96]. As a consequence of our reduction, we show that the\\nadaptive connectivity algorithm does not admit a parallel algorithm in the centralized setting with\\npolynomial speedup, unless the (Topologically-Ordered) Circuit Value Problem admits a parallel\\nalgorithm with polynomial speedup, which is a long-standing open problem in parallel complexity\\nliterature.\\nTheorem 1.3. The adaptive connectivity problem is P-complete under NC1 reductions.\\nBy observing that our reduction, and the NC1 reductions proving the hardness for the Circuit\\nValue Problem can be done in O(1) rounds of MPC, we have the following corollary in the MPC\\nsetting.\\nCorollary 1.4. In the MPC model with memory per machine s = O˜(nǫ) for some constant ǫ, if\\nadaptive connectivity on a sequence of size O(n) can be solved in O(k) rounds, then every problem\\nin P can be solved in O(k) rounds.\\n1.2 Batch-Dynamic MPC Model\\nIn this section, we first introduce the massively parallel computation (MPC) model, followed by the\\nbatch-dynamic MPC model which is the main focus of this paper.\\nMassively Parallel Computation (MPC) Model. The Massively Parallel Computation (MPC)\\nmodel is a widely accepted theoretical model for parallel computation [KSV10]. Here, the in-\\nput graph G has n vertices and at most m edges at any given instant. We are given p proces-\\nsors/machines, each with local memory for storage s = Θ˜(m/p).1 Note that we usually assume\\nm1−δ ≥ p ≥ mδ, for some δ > 0. This is because the model is relevant only when the number of\\nmachines and the local memory per machine are significantly smaller than the size of the input.\\nThe computation in the MPC model proceeds via rounds. Initially, the input data is distributed\\nacross the processors arbitrarily. During each round, each processor runs a polynomial-time al-\\ngorithm on the data which it contains locally. Between rounds, each machine receives at most µ\\namount of data from other machines. The total data received by all machines between any two\\nrounds is termed as the communication cost. Note that no computation can occur between rounds,\\nand equivalently, no communication can occur during a round.\\n1Throughout this paper, Θ˜ and O˜ hide polylogarithmic terms in the size of the input.\\n3\\nThe aim for our algorithms in this model is twofold. Firstly and most importantly, we want to\\nminimize the number of rounds required for our algorithm, since this cost is the major bottleneck of\\nmassively parallel algorithms in practice. Ideally, we would want this number to be as low as O(1).\\nSecondly, we want to decrease the maximum communication cost over all rounds, since the costs\\nof communication between processors in practice are massive in comparison to local computation.\\nBatch-Dynamic MPC Model. At a high-level, our model works as follows. Similar to recent\\nworks by Acar et al. [AABD19] and Tseng et al. [TDB19], we assume that the graph undergoes\\nbatches of insertions and deletions, and in the initial round of each computation, an update or\\nquery batch is distributed to an arbitrary machine. The underlying computational model used\\nis the MPC model, and assume that space per machine is strongly sublinear with respect to the\\nnumber of vertices of the graph, that is, O(nα) for some constant 0 < α < 1.\\nMore formally, we assume there are two kinds of operations in a batch:\\n1. Update: A set of edge insertions/deletions of size up to k.\\n2. Query: A set of graph property queries of size up to k.\\nFor every batch of updates, the algorithm needs to properly maintain the graph according to\\nthe edge insertions/deletions such that the algorithm can accurately answer a batch of queries\\nat any instant. We believe that considering batches of updates and queries most closely relates\\nto practice where often multiple updates occur in the examined network before another query is\\nmade. Furthermore, in the MPC model there is a distinction between a batch of updates and a\\nsingle update, unlike the standard model, because it is possible for the batch update to be made in\\nparallel, and handling batch updates or queries is as efficient as handling a single update or query,\\nespecially in terms of the number of communication rounds.\\nWe use two criteria to measure the efficiency of parallel dynamic algorithms: the number of com-\\nmunication rounds and the total communication between different machines. Note that massively\\nparallel algorithms for static problems are often most concerned with communication rounds. In\\ncontrast, we also optimize the total communication in the dynamic setting, since the total commu-\\nnication becomes a bottleneck for practice when overall data size is very huge, especially when the\\nupdate is much smaller than the total information of the graph. Ideally, we want to handle batches\\nof updates and queries in constant communication rounds and sublinear total communication with\\nrespect to the number of vertices in the graph.\\nThe key algorithmic difference between the dynamic model we introduce here and the MPC\\nmodel is that we can decide how to partition the input into processors as updates occur to the\\ngraph.\\nDynamic problems in the MPC model were studied in the very recent paper by Italiano et\\nal. [ILMP19]. Their result only explicitly considers the single update case. In the batch-dynamic\\nscenario, the result of [ILMP19] generalizes but has higher dependencies on batch sizes in both\\nnumber of rounds and total communication.Our incorporation of graph sketching, fast contraction,\\nand batch search trees are all critical for obtaining our optimized dependencies on batch sizes.\\n1.3 Our Techniques\\nIn this section we give in-depth discussion of the primary techniques used to achieve the results\\npresented in the previous section.\\n4\\nConnectivity\\nWithout loss of generality, we assume that the batch of updates is either only edge insertions or\\nonly edge deletions. For a mixed update batch with both insertions and deletions, we can simply\\nhandle the edge deletions first, and then the edge insertions. In case the same edge is being inserted\\nand deleted, we simply eliminate both operations.\\nSimilar to previous results on dynamic connectivity [Fre85, GI92, HK99, HDLT01, AGM12,\\nKKM13, GKKT15, NS17, Wul17, NSW17], we maintain a maximal spanning forest. This forest\\nencodes the connectivity information in the graph, and more importantly, undergoes few changes\\nper update to the graph. Specifically:\\n1. An insert can cause at most two trees in F to be joined to form a single tree.\\n2. A delete may split a tree into two, but if there exists another edge between these two resulting\\ntrees, they should then be connected together to ensure that the forest is maximal.\\nOur dynamic trees data structure adapts the recently developed parallel batch-dynamic data\\nstructure for maintaining a maximal spanning forest in the shared-memory setting by Tseng et\\nal. [TDB19] to the MPC model. Specifically, [TDB19] give a parallel batch-dynamic algorithm that\\nruns in O(log n) depth w.h.p. to insert k new edges to the spanning forest, to remove k existing\\nedges in the spanning forest, or to query the IDs of the spanning tree containing the given k vertices.\\nWe show that the data structure can be modified to achieve O(1/α) round-complexity and O(k ·nα)\\ncommunication for any small constant α satisfying k · nα · polylog(n) ≤ s in the MPC setting. In\\naddition, if we associate with each vertex a key of length ℓkey, then we can query and update a\\nbatch of k key values in O(1/α) round-complexity and O(k · ℓkey·) communication.\\nWith a parallel batch-dynamic data structure to maintain a maximal spanning forest, a batch\\nof edge insertions or edge queries for the dynamic connectivity problem can be handled in O(1/α)\\nround-complexity and O(k ·nα) communication for any constant α. Our strategy for insertions and\\nqueries is similar to the dynamic connectivity algorithm of Italiano et al. [ILMP19]: A set of edge\\nqueries can be handled by querying the IDs of the spanning tree of all the vertices involved. Two\\nvertices are in the same connected component if and only if their IDs are equal. To process a batch\\nof edge insertions, we maintain the maximal spanning forest by first identifying the set of edges\\nin the given batch that join different spanning trees without creating cycles using ID queries, and\\nthen inserting these edges to the spanning forest, by linking their respective trees.\\nHandling a set of edge deletions, however, is more complex. This is because if some spanning\\nforest edges are removed, then we need to find replacement edges which are in the graph, but\\npreviously not in the spanning forest, that can be added to the spanning forest without creating\\ncycles. To facilitate this, we incorporate developments in sketching based sequential data structures\\nfor dynamic connectivity [AGM12, KKM13].\\nTo construct a sketch of parameter 0 < p < 1 for a graph, we first independently sample every\\nedge of the graph with probability p, and then set the sketch for each vertex to be the XOR of the\\nIDs for all the sampled edges which are incident to the vertex. A sketch has the property that for\\nany subset of vertices, the XOR of the sketches of these vertices equals to the XOR of the IDs for\\nall the sampled edges leaving the vertex subset. In particular, if there is only a single sampled edge\\nleaving the vertex subset, then the XOR of the sketches of these vertices equals to the ID of the\\nedge leaving the vertex subset.\\nThe high level idea of [AGM12, KKM13] is to use sketches for each current connected compo-\\nnent to sample previous non-tree edges going out of the connected component using sketches with\\ndifferent parameters, and use these edges to merge connected components that are separated after\\n5\\ndeleting some tree edges. We visualize this process by representing each connected component as\\na vertex in a multigraph, and finding a replacement non-tree edge between the two components\\nas the process of merging these two vertices. At first glance, it seems like we can translate this\\napproach to the MPC model by storing all the sketches for each connected component in a single\\nmachine. However, directly translating such a data structure leads to either polylog(n) commu-\\nnication rounds or Ω(m) total communication per update batch. To see this, let us look at some\\nintuitive ideas to adapt this data structure to the MPC model, and provide some insight into why\\nthey have certain limitations:\\n1. Sketch on the original graph: For this case, once we use the sketch to sample an edge\\ngoing out of a given connected component, we only know the ID of the two vertices of the\\nedge, but not the two connected components the edge connects. Obtaining the information\\nabout which connected components the endpoints belong to requires communication, because\\na single machine cannot store the connected component ID of each vertex in the graph. Hence,\\nto contract all the connected components using sampled edges for each connected component,\\nwe need one round of communication. Since we may need to reconnect as many as k connected\\ncomponents (k is the number of deletions, i.e., the batch size), this approach could possibly\\nrequire log k = Θ(log n) communication rounds.\\n2. Sketch on the contracted graph where every connected component is contracted\\nto a single vertex: To do this, each edge needs to know which connected components its\\nendpoints belong to. If we split a connected component into several new connected com-\\nponents after deleting some tree edges, the edges whose vertices previously belong to same\\nconnected component may now belong to different connected components. To let each edge\\nknow which connected components its endpoints belong to, we need to broadcast the map-\\nping between vertices and connected components to all the related edges. Hence, the total\\ncommunication can be as large as Ω(m). To further illustrate this difficulty via an example,\\nconsider the scenario that the current maximal spanning forest is a path of n vertices, and\\na batch of k edge deletions break the path into k + 1 short paths. In this case, almost all\\nthe vertices change their connected component IDs. In order to find edges previously not in\\nthe maximal spanning forest to link these k + 1 path, every edge needs to know if the two\\nvertices of the edge belong to same connected component or not, and to do this, the update\\nof connected component ID for vertices of every edge requires Ω(m) communication.\\nThe high level idea of our solution is to speed up the “contraction” process such that constant\\niterations suffice to shrink all the connected components into a single vertex. To do this, sampling\\nO˜(1) edges leaving each connected component in each iterations (as previous work) is not enough,\\nbecause of the existence of low conductance graph. Hence, we need to sample a much larger number\\nof edges leaving each connected component. Following this intuition, we prove a fast contraction\\nlemma which shows that picking nα edges out of each component finds all connecting non-tree\\nedges between components within O(1/α) iterations.\\nHowever, a complication that arises with the aforementioned fast contraction lemma is that it\\nrequires the edges leaving a component to be independently sampled. But the edges sampled by\\na single sketch are correlated. This correlation comes from the fact that a sketch outputs an edge\\nleaving a connected component if and only if there is only one sampled edge leaving that connected\\ncomponent. To address this issue, we construct an independent sample extractor to identify\\nenough edges that are eventually sampled independently based on the sketches and show that these\\nedges are enough to simulate the independent sampling process required by the fast contraction\\nlemma.\\n6\\nWe discuss these two ideas in depth below. In the rest of this section, we assume without loss\\nof generality that every current connected component is contracted into a single vertex, since the\\nsampled edges are canceled under the XOR operation for sketches.\\nFast Contraction Lemma. We first define a random process for edge sampling (which we term\\nContractionSampling ) in Definition 1.5. The underlying motivation for such a definition is that the\\nedges obtained from the sketch are not independently sampled. So, we tweak the sampling process\\nvia an independent sample extractor, which can then produce edges which obey the random process\\nContractionSampling . Before discussing this independent sample extractor, we will first outline how\\nedges sampled using ContractionSampling suffice for fast contraction.\\nDefinition 1.5 (ContractionSampling process). The random process ContractionSampling for a\\nmultigraph G = (V,E) and an integer k is defined as follows: each vertex v independently draws\\ntv samples Sv,1, Sv,2, . . . Sv,tv for some integer tv ≥ k such that\\n1. the outcome of each Sv,i can be an either an edge incident to v or ⊥;\\n2. for every edge e incident to vertex v,\\ntv∑\\ni=1\\nPr[Sv,i = e] ≥ Ω\\n(\\nk log2 n\\ndG(v)\\n)\\n.\\nWe show that in each connected component, if we contract edges sampled by the Contrac-\\ntionSampling process, the number of edges remaining reduces by a polynomial factor with high\\nprobability by taking k = poly(n).\\nLemma 1.6. Consider the following contraction scheme starting with a multigraph G(V,E) on n\\nvertices and m < poly(n) (multi) edges: For a fixed integer k,\\n1. let E′ be a set of edges sampled by the ContractionSampling process;\\n2. contract vertices belonging to same connected component of graph G′ = (V,E′) into a new\\ngraph G⋆ = (V ⋆, E⋆) as follows: each vertex of V ⋆ represents a connected component in the\\nsampled graph G′ = (V,E′), and there is an edge between two vertices x, y ∈ V ⋆ iff there is\\nan edge in G between the components corresponding to x and y, with edge multiplicity equal\\nto the sum of multiplicity of edges in G between the components corresponding to x and y.\\nThen the resultant graph has at most O˜(mk−1/3) (multi) edges with high probability.\\nBased on Lemma 1.6, if we iteratively apply the ContractionSampling process with k = nα\\nand shrink connected components using sampled edges into a single vertex, then every connected\\ncomponent of the multigraph becomes a singleton vertex in O(1/α) rounds with high probability.\\nLemma 1.6 can be shown using a straightforward argument for simple graphs. However, in the\\ncase of multigraphs (our graphs are multigraphs because there can be more than one edge between\\ntwo components), this argument is not as easy. It is possible that for a connected component C1, a\\nlarge number of edges leaving C1 will go to another connected component C2. Hence, in one round,\\nthe sampled nδ edges leaving C1 may all go to C2. From this perspective, we cannot use a simple\\ndegree-based counting argument to show that every connected component merges with at least nδ\\nother connected components if it connected to at least nδ other connected components.\\nTo deal with parallel edges, and to prove that the contraction occurs in constant, rather than\\nO(log n) rounds, we make use of a more combinatorial analysis. Before giving some intuition about\\nthis proof, we define some useful terminology.\\n7\\nDefinition 1.7 (Conductance). Given a graph G(V,E) and a subset of vertices S ⊆ V , the\\nconductance of S w.r.t. G is defined as\\nφG(S)\\ndef\\n= min\\nS′⊆S\\n|E(S′, S \\\\ S′)|\\nmin\\n{∑\\nu∈S′ dG(u),\\n∑\\nu∈S\\\\S′ dG(u)\\n} .\\nThe conductance of a graph is a measure of how “well-knit” a graph is. Such graphs are of\\nconsequence to us because the more well-knit the graph is, the faster it contracts into a singleton\\nvertex. We use the expander decomposition lemma from [ST11], which says that any connected\\nmultigraph G can be partitioned into such subgraphs.\\nLemma 1.8 ([ST11], Section 7.1.). Given a parameter k > 0, any graph G with n vertices and m\\nedges can be partitioned into groups of vertices S1, S2, . . . such that\\n• the conductance of each Si is at least 1/k;\\n• the number of edges between the Si’s is at most O(m log n/k).\\nFor each such “well-knit” subgraph H to collapse in one round of sampling, the sampled edges\\nin H must form a spanning subgraph of H. One way to achieve this is to generate a spectral\\nsparsifier of H [SS11] - which can be obtained by sampling each edge with a probability at least\\nO(log n) times its effective resistance. The effective resistance of an edge is the amount of current\\nthat would pass through it when unit voltage difference is applied across its end points, which is a\\nmeasure of how important it is to the subgraph being well-knit.\\nAs the last piece of the puzzle, we show that the edges sampled by the ContractionSampling\\nprocess do satisfy the required sampling constraint to produce a spectral sparsifier of H. Since\\neach such subgraph collapses, Lemma 1.8 also tells us that only a small fraction of edges are\\nleftover in G, as claimed in Lemma 1.6.\\nIt is important to note that although we introduce sophisticated tools such as expander parti-\\ntioning and spectral sparsifiers, these tools are only used in the proof and not in the actual algorithm\\nto find replacement edges.\\nFrom Sketches to Independent Samples. On a high level, our idea to achieve fast contraction\\nis based on using O(k · polylog(n)) independent sketches. However, we cannot directly claim that\\nthese sketches simulate a ContractionSampling procedure, as required by the fast contraction lemma\\n(Lemma 1.6). This is because ContractionSampling requires the edges being sampled independently.\\nInstead, each sketch as given by [AGM12, KKM13] gives a set of edges are constructed as follows:\\n1. Pick each edge independently with probability p, where p is the parameter of the sketch.\\n2. For each vertex which has exact one sampled edge incident to it, output the sampled incident\\nedge.\\nThe second step means the samples picked out of two vertices are correlated. Given a vertex v,\\nlet Ev be the random variable for the edge picked in Step 2 of above sketch construction process.\\nConsider an example with two adjacent vertices v1 and v2. If the outcome of Ev1 is the edge v1v2,\\nthen the outcome of Ev2 cannot be an edge other than v1v2. Hence two random variables Ev1 and\\nEv2 are correlated.\\nThis issue is a direct side-effect of the faster contraction procedure. Previous uses of sketching\\nonly needs to find one edge leaving per component, which suffices for O(log n) rounds. However,\\n8\\nour goal is to terminate in a constant number of rounds. This means we need to claim much larger\\nconnected components among the sampled edges. For this purpose, we need independence because\\nmost results on independence between edges require some correlation between the edges picked.\\nInstead, we show that each sketch still generates a large number of independent edge samples.\\nThat is, while all the samples generated by a copy of the sketch are dependent on each other, a\\nsufficiently large subset of it is in fact, independent. Furthermore, observe that contractions can\\nonly make more progress when more edges are considered. So it suffices to show that this particular\\nsubset we choose makes enough progress. Formally, we prove the following lemma.\\nLemma 1.9. Given an integer k and a multigraph G of n vertices, O(k log3 n) independent sketches\\nsimulates a ContractionSampling process. Furthermore, for every edge sampled by the Contraction-\\nSampling process, there exists a sketch and a vertex such that the value of the sketch on the vertex\\nis exactly the ID of that edge.\\nOur starting observation is that for a bipartite graph, sketching process gives independent edge\\nsamples for vertices from the same side: For a bipartite graph (A,B), the process of sampling edges,\\nand picking all edges incident to degree one vertices of A satisfies the property that all the edges\\npicked are independent.\\nTo extend this observation to general graph, we consider a bipartition of the graph, (A,B), and\\nview the random sampling of edges from the sketch as a two-step process:\\n1. First, we sample all edges within each bipartition (A,A) and (B,B).\\n2. Then we sample the (A,B) edges independently.\\nAfter first step, we remove vertices from A that have some sampled edges incident to. The second\\nstep gives a set of edges, from which we keep ones incident to some degree one vertices from A.\\nBased on the observation of bipartite graph, the edges kept in the second step are independent\\n(condition on the outcome of the first step).\\nTo bound the probability of picking an edge crossing the bipartition, we will first lower bound\\nthe probability that the incident vertex from A remains after the first step, and then check that\\nthe second step on the bipartite graph is equivalent to an independent process on the involved\\nedges. The overall lower bound on the probability of an edge picked then follows from combining\\nthe probability of an edge being picked in one of these processes with the probability that the\\ncorresponding vertices remain after the first step and the initial pruning of vertices. With this\\nprobability estimation, we show that O(k ·polylogn) independent sketches are enough to boost the\\nprobability of picking the edge to the required lower bound by the ContractionSampling process.\\nAt the end, we show that O(log n) random bipartition of the graph is enough to make sure that\\nevery edge appears in at least one of the bipartition, and then Lemma 1.9 follows.\\nAdaptive Connectivity and Lower-Bounds in the Batch-Dynamic MPC Model\\nThe adaptive connectivity problem is the “semi-online” version of the connectivity problem where\\nthe entire adaptive batch of operations is given to the algorithm in advance, but the algorithm\\nmust apply the query/update pairs in the batch in order, that is each pair on the graph defined by\\napplying the prefix of updates before it. We note that the problem is closely related to offline dy-\\nnamic problems, for example for offline dynamic minimum spanning tree and connectivity [Epp94].\\nThe main difference is that in the offline problem the updates (edge insertions/deletions) are not\\nadaptive, and are therefore not conditionally run based on the queries. We also note here that every\\nproblem that admits a static NC algorithm also admits an NC algorithm for the offline variant of\\n9\\nthe problem. The idea is to run, in parallel for each query, the static algorithm on the input graph\\nunioned with the prefix of the updates occuring before the query. Assuming the static algorithm is\\nin NC, this gives a NC offline algorithm (note that obtaining work-efficient parallel offline algorithms\\nfor problems like minimum spanning tree and connectivity is an interesting problem that we are\\nnot aware of any results for).\\nCompared to this positive result in the setting without adaptivity, the situation is very different\\nonce the updates are allowed to adaptively depend on the results of the previous query, since the\\nsimple black-box reduction given for the offline setting above is no longer possible. In particular, we\\nshow the following lower bound for the adaptive connectivity problem which holds in the centralized\\nsetting: the adaptive connectivity problem is P-complete, that is unless P = NC, there is no NC\\nalgorithm for the problem. The adaptive connectivity problem is clearly in P since we can just run\\na sequential dynamic connectivity algorithm to solve it. To prove the hardness result, we give a low-\\ndepth reduction from the Circuit Value Problem (CVP), one of the canonical P-complete problems.\\nThe idea is to take the gates in the circuit in some topological-order (note that the version of CVP\\nwhere the gates are topologically ordered is also P-complete), and transform the evaluation of the\\ncircuit into the execution of an adaptive sequence of connectivity queries. We give an NC1 reduction\\nwhich evaluates a circuit using adaptive connectivity queries as follows. The reduction maintains\\nthat all gates that evaluate to true are contained in a single connected component connected to\\nsome root vertex, r. Then, to determine whether the next gate in the topological order, g = ga∧gb,\\nevaluates to true the reduction runs a connectivity query testing whether the vertices corresponding\\nto ga and gb are connected in the current graph, and adds an edge (g, r), thereby including it in the\\nconnected component of true gates if the query is true. Similarly, we reduce evaluating g = ga ∨ gb\\ngates to two queries, which check whether ga (gb) is reachable and add an edge from (g, r) in either\\ncase if so. A g = ¬ga gate is handled almost similarly, except that the query checks whether ga\\nis disconnected from s. Given the topological ordering of the circuit, generating the sequence of\\nadaptive queries can be done in O(log n) depth and therefore the reduction works in NC1.\\nIn contrast, in the MPC setting, we show that we can achieve O(1) rounds for adaptive batches\\nwith size proportional to the space per machine. Our algorithm for adaptive connectivity follows\\nnaturally from our batch-dynamic connectivity algorithm based on the following idea: we assume\\nthat every edge deletion in the batch actually occurs, and compute a set of replacement edges\\nin G for the (speculatively) deleted edges. Computing the replacement edges can be done in the\\nsame round-complexity and communication cost as a static batch of deletions using Theorem 1.1.\\nSince the number of replacement edges is at most O(k) = O(s), all of the replacements can be\\nsent to a single machine, which then simulates the sequential adaptive algorithm on the graph\\ninduced by vertices affected by the batch in a single round. We note that the upper-bound in MPC\\ndoes not contradict the P-completeness result, although achieving a similar result for the depth of\\nadaptive connectivity in the centralized setting for batches of size O(s) = O(nǫ) would be extremely\\nsurprising since it would imply a polynomial-time algorithm for the (Topologically Ordered) Circuit\\nValue Problem with sub-linear depth and therefore polynomial speedup.\\n1.4 Organization\\nSection 2 describes the full version of the high level idea for graph connectivity. Section 3 contains a\\ndiscussion of the data structure we used to handle batch-update in constant round. Section 4 gives\\na proof of our fast contraction lemma. Section 5 gives a proof of our independent sample extractor\\nfrom sketches. Section 6 presents the algorithm for graph connectivity and the correctness proof.\\nLastly, we present our lower and upper bounds for the adaptive connectivity problem in Section 7.\\n10\\n2 1-Edge-Connectivity\\nIn this section we prove our result for 1-edge-connectivity, restated here:\\nTheorem 1.1. In the MPC model with memory per machine s = O˜(nǫ) we can maintain a dynamic\\nundirected graph on m edges which, for constants δ, α, and integer k such that k ·nα+δ ·polylog(n) ≤\\ns, can handle the following operations with high probability:\\n1. A batch of up to k edge insertions/deletions, using O(1/(δα)) rounds.\\n2. Query up to k pairs of vertices for 1-edge-connectivity, using O(1/α) rounds.\\nFurthermore, the total communication for handling a batch of k operations is O˜(knα+δ), and the\\ntotal space used across all machines is O˜(m).\\nParallel Batch-Dynamic Data Structure. Similar to previous results on dynamic connectiv-\\nity [Fre85, GI92, HK99, HDLT01, AGM12, KKM13, GKKT15, NS17, Wul17, NSW17], our data\\nstructure is based on maintaining a maximal spanning forest, which we denote using F . Formally,\\nwe define it as follows.\\nDefinition 2.1 (Maximal spanning forest). Given a graph G, we call F a maximal spanning forest\\nof G if F is a subgraph of G consisting of a spanning tree in every connected component of G.\\nNote that this is more specific than a spanning forest, which is simply a spanning subgraph of\\nG containing no cycles. This forest encodes the connectivity information in the graph, and more\\nimportantly, undergoes few changes per update to the graph. Specifically:\\n1. An insert can cause at most two trees in F to be joined to form a single tree.\\n2. A delete may split a tree into two, but if there exists another edge between these two resulting\\ntrees, they should then be connected together to ensure that the forest is maximal.\\nNote that aside from identifying an edge between two trees formed when deleting an edge from some\\ntree, all other operations are tree operations. Specifically, in the static case, these operations can be\\nentirely encapsulated via tree data structures such as dynamic trees [ST83] or Top-Trees [AHLT05].\\nWe start by ensuring that such building blocks also exist in the MPC setting. In Section 3, we show\\nthat a forest can also be maintained efficiently in O(1) rounds and low communication in the MPC\\nmodel (Theorem 2.2). In this section, we build upon this data structure and show how to process\\nupdates and 1-edge-connectivity queries while maintaining a maximal spanning forest of G.\\nLet T (v) indicate the tree (component) in F to which a vertex v belongs. We define the\\ncomponent ID of v as the as the ID of this T (v). We represent the trees in the forest using the\\nfollowing data structure. We describe the data structure in more detail in Section 3.\\nTheorem 2.2. In the MPC model with memory per machine s = O˜(nǫ) for some constant ǫ, for\\nany constant 0 < α < 1 and a key length ℓkey such that n\\nα · ℓkey ≤ s, we can maintain a dynamic\\nforest F in space O˜(n), with each vertex v augmented with a key x v of length ℓkey(x v is a summable\\nelement from a semi-group),\\n• Link(u1v1, . . . , ukvk): Insert a batch of k edges into F .\\n• Cut(u1v1, . . . , ukvk): Delete k edges from F .\\n• ID(v1, . . . , vk): Given a batch of k vertices, return their component IDs in F .\\n11\\n• UpdateKey((v1, x̂ ′1), . . . , (vk, x̂ ′k)): For each i, update the value of ~x vi to ~x ′i.\\n• GetKey(v1, . . . , vk): For each i, return the value of ~x vi .\\n• ComponentSum(v1 . . . , vk): Given a set of k vertices, compute for each vi,∑\\nw : w∈T (vi)\\nxw\\nunder the provided semi-group operation.\\nMoreover, all operations can be performed in O(1/α) rounds and\\n• Link and Cut operations can be performed in O˜(k · ℓkey · nα) communication per round,\\n• ID can be performed in O˜(k) communication per round,\\n• UpdateKey, GetKey and ComponentSum operations can be performed in O˜(k · ℓkey ·nα)\\ncommunication per round.\\nEdge insertions and queries can be handled by above dynamic data structure: for a set of\\nedge queries, we use the ID operation to query the ID of all the vertices. Two vertices are in the\\nsame connected component if and only if their IDs are same. For a batch of edge insertions, we\\nmaintain the spanning forest by first identifying all the inserted edges that join different connected\\ncomponents using ID operation, and then using the Link operations to put these edges into the\\nforest.\\nThe process of handling a set of edge deletions is more complex. This is because, if some\\nspanning forest edges are removed, then we need to find replacement edges in the graph which were\\npreviously not in the spanning forest, but can be added to maintain the desired spanning forest.\\nTo do this, we use the the augmentation of tree nodes with xu and the ComponentSum operation\\nto accommodate each vertex storing “sketches” in order to find replacement edges upon deletions.\\nSketching Based Approach Overview. At the core of the Delete operation is an adaptation\\nof the sketching based approach for finding replacement edges by Ahn et al. [AGM12] and Kapron\\net al. [KKM13]. Since we rely on these sketches heavily, we go into some detail about the approach\\nhere. Without loss of generality, we assume every edge has a unique O(log n)-bit ID, which is\\ngenerated by a random function on the two vertices involved.\\nFor a vertex v, this scheme sets x v to the XOR of the edge IDs of all the edges incident to v\\n(which we assume to be integers):\\nx v\\ndef\\n=\\n⊕\\ne : e∼v\\ne.\\nFor a subset of vertices S, we define ∂(S) as the set of edges with exactly one endpoint in S. Then,\\ntaking the total XOR over all the vertices in S gives (by associativity of XOR)\\n⊕\\nv∈S\\nx v =\\n⊕\\nv∈S\\n⊕\\ne : e∼v\\ne =\\n⊕\\ne∈E\\n\\uf8eb\\uf8ed ⊕\\nv : v∈S,e∼v\\ne\\n\\uf8f6\\uf8f8 = ⊕\\ne∈∂(S)\\ne.\\nSo if there is only one edge leaving S, this XOR over all vertices in S returns precisely the ID of\\nthis edge. To address the case with multiple edges crossing a cut, Ahn et al. [AGM12] and Kapron\\net al. [KKM13] sampled multiple subsets of edges at different rates to ensure that no matter how\\n12\\nmany edges are actually crossing, with high probability one sample picks only one of them. This\\nredundancy does not cause issues because the edge query procedures also serve as a way to remove\\nfalse positives.\\nWe formally define the sketch as follows:\\nDefinition 2.3 (Graph Sketch from [AGM12, KKM13]). A sketch with parameter p of a graph\\nG = (V,E) is defined as follows:\\n1. Every edge is sampled independently with probability p. Let E′ be the set of sampled edges.\\n2. For every vertex v ∈ V , let\\nx v\\ndef\\n=\\n⊕\\ne∈E′ : e∼v\\ne.\\nWe say a sketch generates edge e if there exists a vertex v such that x v = e. The variant of\\nthis sketching result that we will use is stated as follows in Lemma 2.4.\\nLemma 2.4 (Graph Sketch from [AGM12, KKM13]). Assume we maintain a sketch for each of\\np ∈ {1, 1/2, 1/4, . . . , 1/2⌈2 ln⌉−1}, and let ~x v denote the sketches on vertex v,\\n• upon insertion/deletion of an edge, we can maintain all ~x v’s in O(log2 n) update time;\\n• for any subset of vertices S, from the value⊕\\nv∈S\\n~x v,\\nwe can compute O(log n) edge IDs so that for any edge e ∈ ∂(S), the probability that one of\\nthese IDs is e is at least 1/|∂(S)|.\\nFast Contraction Lemma. As XOR is a semi-group operation, we can use these sketches in\\nconjunction with the dynamic forest data structure given in Theorem 2.2 to check whether a tree\\nresulting from an edge deletion has any outgoing edges. In particular, O(log n) copies of this sketch\\nstructure allow us to find a replacement edge with high probability after deleting a single edge in\\nO(1/ǫ) rounds and O(nǫ) total communication. Our algorithm then essentially “contracts” these\\nedges found, thus essentially reconnecting temporarily disconnected trees in F .\\nHowever, a straightforward generalization of the above method to deleting a batch of k edges\\nresults in an overhead of Θ(log k), because it’s possible that this random contraction process may\\ntake up to Θ(log k) rounds. Consider for example a length k path: if we pick O(1) random edges\\nfrom each vertex, then each edge on the path is omitted by both of its endpoints with constant\\nprobability. So in the case of a path, we only reduce the number of remaining edges by a constant\\nfactor in expectation, leading to a total of about Θ(log k) rounds. With our assumption of s = O(nǫ)\\nand queries arriving in batches of k ≤ s, this will lead to a round count that’s up to Θ(log n).\\nWe address this with a natural modification motivated by the path example: instead of keeping\\nO(log n) independent copies of the sketching data structures, we keep O˜(nδ) copies, for some small\\nconstant δ, which enables us to sample nδ random edges leaving each connected component at\\nany point. As this process only deals with edges leaving connected components, we can also view\\nthese connected components as individual vertices. The overall algorithm then becomes a repeated\\ncontraction process on a multi-graph: at each round, each vertex picks nδ random edges incident\\nto it, and contracts the graph along all picked edges. Our key structural result is a lemma that\\nshows that this process terminates in O(1/δ) rounds with high probability. To formally state the\\nlemma, we first define a random process of sampling edges in a graph.\\n13\\nDefinition 1.5 (ContractionSampling process). The random process ContractionSampling for a\\nmultigraph G = (V,E) and an integer k is defined as follows: each vertex v independently draws\\ntv samples Sv,1, Sv,2, . . . Sv,tv for some integer tv ≥ k such that\\n1. the outcome of each Sv,i can be an either an edge incident to v or ⊥;\\n2. for every edge e incident to vertex v,\\ntv∑\\ni=1\\nPr[Sv,i = e] ≥ Ω\\n(\\nk log2 n\\ndG(v)\\n)\\n.\\nBelow is our structural lemma, which we prove in Section 4.\\nLemma 1.6. Consider the following contraction scheme starting with a multigraph G(V,E) on n\\nvertices and m < poly(n) (multi) edges: For a fixed integer k,\\n1. let E′ be a set of edges sampled by the ContractionSampling process;\\n2. contract vertices belonging to same connected component of graph G′ = (V,E′) into a new\\ngraph G⋆ = (V ⋆, E⋆) as follows: each vertex of V ⋆ represents a connected component in the\\nsampled graph G′ = (V,E′), and there is an edge between two vertices x, y ∈ V ⋆ iff there is\\nan edge in G between the components corresponding to x and y, with edge multiplicity equal\\nto the sum of multiplicity of edges in G between the components corresponding to x and y.\\nThen the resultant graph has at most O˜(mk−1/3) (multi) edges with high probability.\\nIndependent Sample Extractor From Sketches. On a high level, our idea is to use O(k ·\\npolylog(n)) independent sketches to simulate the required ContractionSampling process, and then\\napply Lemma 1.6. However, we cannot do this naively, because ContractionSampling requires the\\nedges being sampled independently, whereas the sketch from Lemma 2.4 does not satisfy this\\nproperty. Recall that the sketch generated at a vertex v can correspond to an edge (say uv) if no\\nother edge adjacent to v was sampled in the same sketch. Consider an example where two edges\\nuv and uw are sampled by the graph. This means that no other edge from v or w can be sampled\\nin that same sketch, implying the sampling process is not independent.\\nWe would like to remark that this is not an issue for previous sketching based connectivity\\nalgorithms (e.g. [AGM12, KKM13]), because in [AGM12, KKM13], each time, any current con-\\nnected component only needs to find an arbitrary edge leaving the connected component. In this\\nway, if most current connected components find an arbitrary edge leaving the component, then\\nafter contracting connected components using sampled edges, the total number of connected com-\\nponents reduce by at least a constant factor. In this way, after O(log n) iterations, each connected\\ncomponent shrinks into a single vertex. But in our case the contraction lemma requires edges being\\nsampled independently. Hence, we cannot directly apply Lemma 1.6 on sketches.\\nTo get around this issue, we construct an independent edge sample extractor from the sketches\\nand show that with high probability, this extractor will extract a set of independent edge samples\\nthat are equivalent to being sampled from a ContractionSampling random process, as required by\\nLemma 1.6. One key observation is that if the graph is bipartite, then sketch values on the vertices\\nfrom one side of the bipartite graph are independent, because every edge sample is only related\\nto one sketch value. The high level idea of our extractor is then to extract bipartite graphs from\\nsketches, such that each edge appears in many bipartite graphs with high probability. For each\\nsketch, consider the following random process:\\n14\\n1. For each vertex of the graph, randomly assign a color of red or yellow. Then we can construct\\na bipartite graph with red vertices on one side, yellow vertices on the other side, and an edge\\nis in the bipartite graph if and only if the color of one endpoint is red, and the other endpoint\\nis yellow. Note that this step is not related to the process of sketch construction.\\n2. Independently sample every edge not in the bipartite graph with probability same as the\\nprobability of sampling used in the sketch.\\n3. For each red vertex whose incident edges were not sampled in Step 2, independently sample\\nevery edge incident to the vertex in the bipartite graph with probability same as that used\\nin the sketch.\\n4. Choose all the edges sampled in Step 3 which do not share a red vertex with any other sampled\\nedge.\\nWe show that the edges obtained in Step 4 are sampled independently (conditioned on the\\noutcome of Step 2). Another way to see this independence is to partition all the independent\\nrandom variables in the process of generating all the sketches into two random processes R1 and\\nR2 (based on the bipartite graph generated for each sketch) such that R1 and R2 are independent\\nand simulate a required ContractionSampling process in the following sense:\\n1. After implementing the random process R1 and based on the outcome of R1, define a Con-\\ntractionSampling process as required by Lemma 1.6.\\n2. The random process R2 simulates the defined ContractionSampling process in the following\\nsense: there is a partition of the independent random variables of random process R2 into\\ngroups satisfying the following conditions:\\n(a) There is a bijection between groups and random variables of the ContractionSampling\\nprocess.\\n(b) For each group, there exists a function of the random variables in the group such that the\\nfunction is equivalent to the corresponding random variable of the ContractionSampling\\nprocess.\\nFurthermore, all the edges sampled by the defined ContractionSampling process are generated by\\nthe sketches (meaning that there exist a vertex and a sketch such that sketch on the vertex is the\\nID of the sampled edge). In this way, we argue that the edges generated by all the sketches contains\\na set of edges generated by a ContractionSampling process so that we can apply Lemma 1.6.\\nMore formally, we define the simulation between two random processes as follows.\\nDefinition 2.5. We say a set of independent random variables E1, E2, . . . , Et simulates another\\nset of independent random variables F1, F2, . . . , Fℓ if there exists a set of random variables U ⊆\\n{E1, E2, . . . , Et} such that with constant probability, after fixing all the random variables of U ,\\nthere are ℓ subsets T1, T2, . . . Tℓ ⊆ {E1, E2, . . . , Et} \\\\ U (depending on the outcome of the random\\nprocess for U) satisfying\\n1. T1, . . . , Tℓ are mutually disjoint.\\n2. For every i ∈ [ℓ], there exist a random variable which is a function of random variables in Ti,\\ndenoted as fi(Ti), such that f(Ti) is same to the random variable Fi.\\n15\\nAnd we show that the process of generating O(k log3 n) sketches simulates the random process\\nin the contraction lemma.\\nLemma 1.9. Given an integer k and a multigraph G of n vertices, O(k log3 n) independent sketches\\nsimulates a ContractionSampling process. Furthermore, for every edge sampled by the Contraction-\\nSampling process, there exists a sketch and a vertex such that the value of the sketch on the vertex\\nis exactly the ID of that edge.\\n3 Batch-Dynamic Trees in MPC\\nIn this section we describe a simple batch-dynamic tree data structure in the MPC setting. Our\\ndata structure is based on a recently developed parallel batch-dynamic data structure in the shared-\\nmemory setting [TDB19]. Specifically, Tseng et al. give a parallel batch-dynamic tree that supports\\nbatches of k links, cuts, and queries for the representative of a vertex in O(k log(n/k+1)) expected\\nwork and O(log n) depth w.h.p. Their batch-dynamic trees data structure represents each tree in\\nthe forest using an Euler-tour Tree (ETT) structure [HK99], in which each tree is represented as\\nthe cyclic sequence of its Euler tour, broken at an arbitrary point. The underlying sequence rep-\\nresentation is a concurrent skip list implementation that supports batch join and split operations.\\nAugmented trees are obtained by augmenting the underlying sequence representation.\\nWe show that the structure can be modified to achieve low round-complexity and communication\\nin the MPC setting. We now define the batch-dynamic trees interface and describe how to extend\\nthe data structure into the MPC setting. The main difficulty encountered in the shared-memory\\nsetting is that nodes are stored in separate memory locations and refer to each other via pointers.\\nTherefore, when traversing the skip list at some level i to find a node’s ancestor at level i + 1, it\\nrequires traversing all nodes that occur before (or after) it at level i. We show that by changing\\nthe sampling probability to 1/nǫ, we can ensure that each level has size O˜(nǫ), each level can be\\nstored within a single machine and thus this search can be done within a single round. The new\\nsampling probability also ensures that the number of levels is O(1/ǫ) w.h.p. which is important for\\nachieving our bounds.\\nBatch-Dynamic Trees Interface. A batch-parallel dynamic trees data structure represents a\\nforest G = (V,E) as it undergoes batches of links, cuts, and connectivity queries. A Link links two\\ntrees in the forest. A Cut deletes an edge from the forest, breaking one tree into two trees. A ID\\nquery returns a unique representative for the tree containing a vertex. Formally the data structure\\nsupports the following operations:\\n• Link({{u1, v1} , . . . , {uk, vk}}) takes an array of edges and adds them to the graph G. The\\ninput edges must not create a cycle in G.\\n• Cut({{u1, v1} , . . . , {uk, vk}}) takes an array of edges and removes them from the graph G.\\n• ID({u1, . . . , uk}) takes an array of vertex ids and returns an array containing the represen-\\ntative of each ui. The representative of a node, r(u) is a unique value s.t. r(u) = r(v) iff u\\nand v are in the same tree.\\nFurthermore, the trees can be augmented with values ranging over a domain D, and a commu-\\ntative function f : D2 → D. The trees can be made to support queries for the sum according to\\nf on arbitrary subtrees, but for the purposes of this paper queries over the entire tree suffice. The\\ninterface is extended with the following two primitives:\\n16\\n• UpdateKey({{u1, xˆ1} , . . . , {uk, xˆk}}) takes an array of vertex id, value pairs and updates\\nthe value for ui to xˆi.\\n• GetKey({u1, . . . , uk}) takes an array of vertex ids and returns an array containing the value\\nof each ui, xˆi.\\n• ComponentSum({u1, . . . , uk}) takes an array of vertex ids and returns an array containing∑\\nw:w∈T (ui)\\nxˆw where T (ui) is the tree containing ui, xˆw is the value for node w, and the sum\\nis computed according to f .\\nWe show the following theorem in this section. Let δ be a parameter controlling the size of\\nthe keys stored at each node and let α be a parameter controlling the size of the blocks stored\\ninternally within a single machine.\\nTheorem 3.1. Let δ be a parameter controlling the keysize, and α be a constant controlling the\\nblocksize s.t. δ + α < ǫ and 0 < α. Then, in the MPC model with memory per machine s = O˜(nǫ)\\nthere is an augmented batch-dynamic tree data structure in MPC that supports batches of up to k\\nLink, Cut, ID, UpdateKey, GetKey, and ComponentSum operations in O(1/α) rounds per\\noperation w.h.p. where k = O(nα).\\nFurthermore, the batch operations cost\\n• O˜(knδ) communication per round w.h.p. for UpdateKey, GetKey, and ComponentSum\\n• O˜(knδnα) communication per round w.h.p. for Link and Cut and\\n• O(k) communication per round for ID.\\n3.1 Augmented Batch-Dynamic Sequences in MPC\\nIn order to obtain Theorem 3.1, we first show how to implement augmented batch-dynamic se-\\nquences in few rounds of MPC. In particular, we will show the following lemma. Note that\\nachieving a similar bound on the round-complexity for large batches, e.g., batches of size O(n),\\nwould disprove the 2-cycle conjecture. We refer to [TDB19] for the precise definition of the sequence\\ninterface.\\nLemma 3.2. Let δ be a parameter controlling the keysize, and α be a constant controlling the\\nblocksize s.t. δ + α < ǫ and 0 < α. Then, in the MPC model with memory per machine s = O˜(nǫ)\\nthere is an augmented batch-dynamic sequence data structure in MPC that supports batches of up\\nto k Split, Join, ID, UpdateKey, GetKey, and SequenceSum operations in O(1/α) rounds\\nper operation w.h.p. where k = O(nα).\\nFurthermore, the batch operations cost\\n• O˜(knδ) communication per round w.h.p. for UpdateKey, GetKey, and SequenceSum\\n• O˜(knδnα) communication per round w.h.p. for Split and Join and\\n• O(k) communication per round for ID.\\nFor the sake of simplicity we discuss the case where δ = 0 and 0 < α < ǫ (i.e. values that\\nfit within a constant number of machine words), and describe how to generalize the idea to larger\\nvalues at the end of the sub-section.\\nSequence Data Structure. As in Tseng et al. [TDB19] we use a skip list as the underlying\\nsequence data structure. Instead of sampling nodes with constant probability to join the next level,\\n17\\nwe sample them with probability 1/nα. It is easy to see that this ensures that the number of\\nlevels in the list is O(1/α) w.h.p. since α is a constant greater than 0. Furthermore, the largest\\nnumber of nodes in some level i that “see” a node at level i + 1 as their left or right ancestor is\\nO(nα log n) w.h.p. We say that the left (right) block of a node belonging to level i are all of its\\nsiblings to the left (right) before the next level i + 1 node. As previously discussed, in the MPC\\nsetting we should intuitively try to exploit the locality afforded by the MPC model to store the\\nblocks (contiguous segments of a level) on a single machine. Since each block fits within a single\\nmachine w.h.p., operations within a block can be done in 1 round, and since there are O(1/α)\\nlevels, the total round complexity will be O(1/α) as desired. Since the ideas and data structure\\nare similar to Tseng et al. [TDB19], we only provide the high-level details and refer the reader to\\ntheir paper for pseudocode.\\nJoin. The join operation takes a batch of pairs of sequence elements to join, where each pair\\ncontains the rightmost element of one sequence and the leftmost element of another sequence. We\\nprocess the levels one by one. Consider a join of (ri, li). We scan the blocks for ri and li to find\\ntheir left and right ancestors, and join them. In the subsequent round, these ancestors take the\\nplace of (ri, li) and we recursively continue until all levels are processed. Observe that at each level,\\nfor each join we process we may create a new block, with O˜(nα) elements. In summary, the overall\\nround-complexity of the operation is O(1/α) w.h.p., and the amount of communication needed is\\nO˜(knα) w.h.p.\\nSplit. The split operation takes a batch of sequence elements at which to split the sequences\\nthey belong to by deleting the edge to the right of the element. We process the levels one by one.\\nConsider a split at a node ei. On each level, we first find the left and right ancestors as in case of\\njoin. We then send all nodes splitting a given block to the machine storing that block, and split\\nit in a single round. Then, we recurse on the next level. If the left and right ancestors of ei were\\nconnected, we call split on the left right ancestor at the next level. The overall round-complexity\\nis O(1/α) w.h.p., and the amount of communication needed is O˜(knα) w.h.p.\\nAugmentation and Other Operations. Each node in the skip list stores an augmented value\\nwhich represents the sum of all augmented values of elements in the block for which it is a left\\nancestor. Note that these values are affected when performing splits and joins above, but are easily\\nupdated within the same round-complexity by computing the correct sum within any block that\\nwas modified and updating its left ancestor. SetKey operations, which take a batch of sequence\\nelements and update the augmented values at these nodes can be handled similarly in the same\\nround-complexity as join and split above. Note that this structure supports efficient range queries\\nover the augmented value, but for the purposes of this paper, returning the augmented value for an\\nentire sequence (SequenceSum) is sufficient, and this can clearly be done in O(1/α) rounds and\\nO(k) communication. Similarly, returning a representative node (ID) for the sequence can be done\\nin O(1/α) rounds w.h.p. and O(k) communication by finding the top-most level for the sequence\\ncontaining the queried node, and returning the lexicographically first element in this block.\\nHandling Large Values. Note that if the values have super-constant size, i.e. size O(nδ) for\\nsome δ s.t. δ + α < ǫ we can recover similar bounds as follows. Since the blocks have size O˜(nα)\\nand each value has size O(nδ) the overall size of the block is O˜(nα+δ) = O˜(nǫ). Therefore blocks\\ncan still be stored within a single machine without changing the sampling parameter. Storing large\\nvalues affects the bounds as follows. First, the communication cost of performing splits and joins\\ngrows by a factor of O(nδ) due to the increased block size. Second, the cost of getting, setting, and\\nperforming a component sum grows by a factor of O(nδ) as well, since k values are returned, each of\\nsize O(nδ). Therefore the communication cost of all operations other than finding a represntative\\nincrease by a multiplicative O(nδ) factor. Finally, note that the bounds on round-complexity are\\n18\\nnot affected, since nodes are still sampled with probability 1/nα.\\n3.2 Augmented Batch-Dynamic Trees in MPC\\nWe now show how to implement augmented batch-dynamic trees in MPC, finishing the proof of\\nTheorem 3.1. We focus on the case where δ = 0 (we are storing constant size words) and explain\\nhow the bounds are affected for larger δ.\\nForest Data Structure. We represent trees in the forest by storing the Euler tour of the tree\\nin a sequence data structure. If the forest is augmented under some domain D and commutative\\nfunction f : D2 → D, we apply this augmentation to the underlying sequences.\\nLink. Given a batch of link operations (which are guaranteed to be acyclic) we update the forest\\nstructure as follows. Consider a link (ui, vi). We first perform a batch split operation on the\\nunderlying sequences at all ui, vi for 1 ≤ i ≤ k, which splits the Euler tours of the underlying trees\\nat the nodes incident to a link. Next, we send all of the updates to a single machine to establish\\nthe order in which joins incident to a single vertex are carried out. Finally, we perform a batch\\njoin operation using the order found in the previous round to link together multiple joins incident\\nto a single vertex. Since we perform a constant number of batch-sequence operations with batches\\nof size O(k), the overall round complexity is O(1/α) w.h.p. by our bounds on sequences, and the\\noverall communication is O˜(knα) w.h.p.\\nCut. Given a batch of cut operations, we update the forest structure as follows. Consider a cut\\n(ui, vi). The idea is to splice this edge out of the Euler tour by splitting before and after (ui, vi) and\\n(vi, ui) in the tour. The tour is then repaired by joining the neighbors of these nodes appropriately.\\nIn the case of batch cuts, we perform a batch split for the step above. For batch cuts, notice that\\nmany edges incident to a node could be deleted, and therefore we may need to traverse a sequence of\\ndeleted edges before finding the next neighbor to join. We handle this by sending all deleted edges\\nand their neighbors to a single machine, which determines which nodes should be joined together\\nto repair the tour. Finally, we repair the tours by performing a batch join operation. Since we\\nperform a constant number of batch-sequence operations with batches of size O(k) the overall round\\ncomplexity is O(1/α) w.h.p. by our bounds on sequences, and the overall communication is O˜(knα)\\nw.h.p.\\nAugmentation, Other Operations and Large Values. Note that the underlying sequences\\nhandle updating the augmented values, and that updating the augmented values at some nodes\\ntrivially maps to an set call on the underlying sequences. Therefore the bounds for GetKey and\\nSetKey are identical to that of sequences. Similarly, the bounds for ID are identical to that of the\\nsequence structure. For super-constant size values, the bounds are affected exactly as in the case\\nfor augmented sequences with large values. The communication costs for all operations other than\\nID grow by an O(nδ) factor and the round-complexity is unchanged. This completes the proof of\\nTheorem 3.1.\\n4 Fast Contraction\\nThe aim of this section is to prove Lemma 1.6, which is pivotal in proving the correctness of the\\nmain algorithm from Section 2.\\nLemma 1.6 is important in proving that our algorithm can find replacement edges in the span-\\nning forest quickly in the event of a batch of edges being deleted. The proof idea is as follows. We\\nfirst show that there exists a partitioning of the vertices such that the edges within the partitions\\ncollapse in a single iteration.\\n19\\nTo do this, we first need to define a few terms relating to expansion criteria of a graph. Let\\ndG(v) denote the degree of a vertex v in graph G. For edges in a partition to collapse in a single\\niteration, we need each partition to be sufficiently “well-knit”. This property can be quantified\\nusing the notion of conductance.\\nDefinition 1.7 (Conductance). Given a graph G(V,E) and a subset of vertices S ⊆ V , the\\nconductance of S w.r.t. G is defined as\\nφG(S)\\ndef\\n= min\\nS′⊆S\\n|E(S′, S \\\\ S′)|\\nmin\\n{∑\\nu∈S′ dG(u),\\n∑\\nu∈S\\\\S′ dG(u)\\n} .\\nThe following lemma proves the existence of a partitioning such that each partition has high\\nconductance.\\nLemma 1.8 ([ST11], Section 7.1.). Given a parameter k > 0, any graph G with n vertices and m\\nedges can be partitioned into groups of vertices S1, S2, . . . such that\\n• the conductance of each Si is at least 1/k;\\n• the number of edges between the Si’s is at most O(m log n/k).\\nNow that we have a suitable partitioning, we want to find a strategy of picking edges in a\\ndecentralized fashion such that all edges within a partition collapse with high probability. One\\nway to do this is to pick edges which form a spectral sparsifier of Si. The following lemma by\\nSpielman and Srivastava [SS11] helps in this regard: we use more recent interpretations of it that\\ntake sampling dependencies into account.\\nLemma 4.1 ([SS11, Tro12, KLP16, KPPS17]). On a graph G, let E1 . . . Ek be independent random\\ndistributions over edges such that the total probability of an edge e being picked is at least Ω(log n)\\ntimes its effective resistance, then a random sample from H = E1+E2+ . . .+Ek is connected with\\nhigh probability.\\nNow we want to show that the random process ContractionSampling (Defintion 1.5) where\\neach vertex draws k log2 n samples actually satisfies the property mentioned in Lemma 4.1, i.e., all\\nedges are picked with probability at least their effective resistance. To show this, we first need the\\nfollowing inequality given by Cheeger.\\nLemma 4.2 ([AM85]). Given a graph G, for any subset of vertices S with conductance φ, we have\\nλ2\\n(\\nD\\n−1/2\\nS LSD\\n−1/2\\nS\\n)\\n≥ 1\\n2\\nφ2,\\nwhere LS is the Laplacian matrix of the subgraph of G induced by S. DS is the diagonal matrix\\nwith degrees of vertices in S.\\nLemma 4.3. Let S be a subset of vertices of G such that φG(S) ≥ 1/2α1/3 for some α > 0. For\\nan edge e = uv, where u, v ∈ S, the effective resistance of e measured in S, ERS(e), satisfies\\nERS (e) ≤ α\\n(\\n1\\ndG(u)\\n+\\n1\\ndG(v)\\n)\\n.\\n20\\nProof. From Lemma 4.2, we get that\\nLS \\x17 1\\n2\\n(φG(S))\\n2Π⊥~1SDSΠ⊥~1S .\\nUsing this, along with the definition ERS(u, v)\\ndef\\n= χTuvL\\n†\\nSχuv, gives us that\\nERS(u, v) ≤ 1\\n2\\n(φG(S))\\n−2\\n(\\n1\\ndS(u)\\n+\\n1\\ndS(v)\\n)\\n. (1)\\nWe have for any subset S′ ⊆ S that:\\nE(S′, S \\\\ S′)\\nmin\\n{∑\\nu∈S′ dG(u),\\n∑\\nu∈S\\\\S′ dG(u)\\n} ≥ φG(S).\\nFurthermore, for every vertex v ∈ S, we get\\ndS(v)\\ndG(v)\\n≥ φG(S),\\nwhich when substituted into Equation 1 gives\\nERS(u, v) ≤ 1\\n2\\n(φG(S))\\n−3\\n(\\n1\\ndG(u)\\n+\\n1\\ndG(v)\\n)\\n.\\nSubstituting for φG(S) ≥ 1/2α1/3 completes the proof.\\nNow, we have enough ammunition to prove Lemma 1.6.\\nProof of Lemma 1.6. From Lemma 1.8, we know that our graph can be partitioned into expanders\\nwith conductance at least Ω(k−1/3 log1/3 n). Now, let S be one such partition and let e = uv be an\\nedge contained in S. From the definition of the random process in Definition 1.5, we know that for\\nan edge uv, the probability that it is sampled by either u or v is at least\\nk log2 n\\n(\\n1\\ndG(u)\\n+\\n1\\ndG(v)\\n)\\n≥ ERS(uv) · Ω(log n),\\nwhere the inequality follows from Lemma 4.3. Since each such edge uv is chosen with probability\\ngreater than Ω(log n) times its effective resistance w.r.t. S, from Lemma 4.1, we know that the\\nedges chosen within S are connected with high probability.\\nThus, we are left with the edges between the partitions, the number of which is bounded by\\nO(m log4/3 n · k−1/3) edges,\\n5 Independent Sample Extractor From Sketches\\nIn this section, we prove Lemma 1.9, which shows how we extract independent edge samples from\\nthe sketches, which are inherently dependent.\\nWe start with the definition of an induced bipartite multigraph. Given a multigraph G = (V,E)\\nof n vertices, we say B = (R,Y,EB) is an induced bipartite multigraph of G if V is partitioned\\ninto two disjoint vertex sets R and Y and an edge of G belongs to EB if and only if the edge\\ncontains one vertex from R and one vertex from Y .\\nFor a fixed multigraph G and an induced bipartite multigraph B of G, we conceptually divide\\nthe process of generating a sketch with parameter p into two phases:\\n21\\nPhase 1. Independently sample each edge not in the bipartite graph with probability p.\\nPhase 2. Independently sample each edge in the bipartite graph with probability p.\\nLemma 5.1. Given a multigraph G = (V,E) and an induced bipartite multigraph B = (R,Y,EB)\\nof G, with probability at least 1− 1n , O(k log2 n) independent sketches simulate the following random\\nprocess: Every vertex v ∈ R is associated with tv independent variables Sv,1, Sv,2, . . . Sv,tv for some\\ninteger tv ≥ k satisfying\\n1. The outcome of each Sv,i can be an edge incident to v or ⊥.\\n2. For every edge e incident to vertex v,\\ntv∑\\ni=1\\nPr[Sv,i = e] ≥ 2k\\ndG(v)\\n.\\nFurthermore, for every edge sampled by the above random process, there exists a sketch and a vertex\\nsuch that the value of the sketch on the vertex equals the edge ID.\\nProof. Assume for\\np ∈\\n{\\n1\\n2\\n,\\n1\\n4\\n, . . . ,\\n1\\n2⌈log2 n⌉ + 1\\n}\\n,\\nthat there are t = 8000k log n sketches corresponding to each p. Let pi denote the parameter of\\ni-th sketch.\\nLet m denote the number of edges in G. We use Ei,e1 , Ei,e2 , . . . , Ei,em to denote the random\\nvariables denoting edges being present in the i-th sketch. Hence, the random process of generating\\nall the sketches corresponds to sampling random variables {Ei,ej}i∈[t],j∈[m].\\nLet U ⊆ {Ei,ej}i∈[t],j∈[m] be the set of random variables in Phase 1 of all the sketches. We\\ndefine another random process based on the outcome of U as follows: For i-th sketch and any\\nvertex v ∈ R, if no edge incident to vertex v was sampled in Phase 1 of the i-th sketch, then we\\ndefine a new independent random variable Sv,i such that\\nPr[Sv,i = e] = pi(1− pi)dB(v)−1\\nif e is in graph B and incident to vertex v, and\\nPr[Sv,i = ⊥] = 1− pi(1− pi)dB(v)−1 · dB(v).\\nIf at least one edge incident to vertex v was sampled in Phase 1 of the i-th sketch, then we do not\\ndefine random variable Sv,i.\\nNow, for an arbitrary v ∈ R, let\\npv\\ndef\\n=\\n1\\n2⌈log2 dG(v)⌉+1\\n.\\nFor a single sketch with parameter pv, the probability that no edge incident to v was sampled in\\nPhase 1 is\\n(1− pv)dG(v)−dB(v) ≥ (1− pv)dG(v) > 0.1.\\n22\\nApplying Chernoff bound, with probability 1 − 1\\nn3\\n, at least 80k random variables Sv,i are defined\\nsuch that with probability pv(1 − pv)dB(v)−1, Sv,j equals exactly equals edge e for every e in B\\nincident to v. Hence, for any edge e incident to v in graph B, we have∑\\nSv,i:Sv,i is defined\\nPr[Sv,i = e] ≥ 80k · pv(1− pv)dB(v)−1 > 80k · pv(1− pv)dG(v) ≥ 2k\\ndG(v)\\n.\\nBy union bound, with probability 1− 1n , all the defined random variables Sv,j’s form the required\\nrandom process.\\nIn the rest of this proof, we show that Phase 2 of each sketch simulates the generation of the\\ndefined random varibles {Sv,i}v∈R,i∈[t]. For every defined random variable Sv,i, we let\\nTv,i = {Ei,e : e ∈ B and is incident to vertex v}\\ndenote the random variable for the i-th sketch which corresponds to edges incident to vertex v in\\ngraph B. It is easy to verify that Tv,i ∩ U = ∅. Furthermore, all the Tv,i’s are mutually disjoint.\\nWe define a function\\nfv,i(Tv,i) =\\n{\\ne if\\n∑\\nEi,e′∈Tv,i\\nEi,e′ = 1 and Ei,e = 1\\n⊥ if ∑Ei,e′∈Tv,i Ei,e′ 6= 1.\\nSince all the random variables in Tv,i are independent, we have\\nPr[fv,i(Tv,i) = e] = pi(1− pi)dB(v)−1\\nfor any edge e incident to v in B, and\\nPr[fv,i(Tv,i) = ⊥] = 1− pi(1− pi)dB(v)−1 · dB(v).\\nThen the lemma follows.\\nUsing the above lemma, we can now prove Lemma 1.9.\\nProof of Lemma 1.9. We repeat the following process 10 log n times:\\n1. Every vertex is independently assigned the color red with probability 1/2, or is assigned yellow\\notherwise.\\n2. Let R be the vertices with red color and Y be all the vertices with yellow color. Construct\\nthe induced bipartite multigraph B = (R,Y,EB), where EB contains all the edges of G with\\none red vertex and one yellow vertex.\\nBy Chernoff bound and union bound, with probability at least 1− 1n , for every edge e and a vertex\\nv contained by the edge e, there is a sampled bipartite multigraph B = (R,Y,EB) such that v ∈ R\\nand e ∈ EB .\\nAssuming every edge belongs to at least one sampled bipartite graph. For each sampled bipartite\\nmultigraph, we assign O(k log2 n) sketches. The lemma follows by applying Lemma 1.9 for every\\nbipartite multigraph and its assigned sketches,\\n23\\n6 Connectivity Algorithms and Correctness\\nWe give the algorithms for batch edge queries, batch edge insertions, and batch edge deletions and\\nprove the correctness in Section 6.1, Section 6.2 and Section 6.3 respectively. Putting together\\nLemmas 6.1, 6.3 and 6.2 then gives the overall result as stated in Theorem 1.1.\\nThroughout this section, we will use the batch-dynamic tree data structure discussed in Section 3\\nto maintain\\n1. a maximal spanning forest F of the graph,\\n2. a key ~x v for every vertex v, where ~x v is a vector of O˜(n\\nδ) sketch values on vertex v,\\n3. an edge list data structure which can be used to check if an edge is in the graph given an\\nedge ID.\\n6.1 Algorithm for Batch Edge Queries\\nSince F is a maximal spanning tree, the query operations are directly provided by calling ID on\\nall involved vertices. Pseudocode of this routine is in Algorithm 6.1.\\nQuery((u1, v1), (u2, v2), . . . , (uk, vk))\\nInput: Pairs of vertices (u1, v1), (u2, v2), . . . , (uk, vk)\\nOutput: For each 1 ≤ i ≤ k, yes if ui and vi are connected in G, and no otherwise.\\n1. Call ID(u1, v1, u2, v2, . . . , uk, vk).\\n2. For each i, output yes if ui and vi have the same component ID, and no otherwise.\\nAlgorithm 6.1: Querying the connectivity between a batch of vertex pairs\\nLemma 6.1. The algorithm Query (Algorithm 6.1) correctly answers connectivity queries and\\ntakes O(1/α) rounds, each with total communication at most O˜(k).\\nProof. The correctness and performance bounds follow from the fact that F is a maximal spanning\\nforest of F and from Theorem 2.2.\\n6.2 Algorithm for Batch Edge Insertions\\nGiven a batch of k edge insertions, we want to identify a subset of edges from the batch that are\\ngoing to add to F to maintain the invariant that F is a maximal spanning forest. To do this, we use\\nID operation to find IDs of all the involved vertices in the edge insertion batch. Then we construct\\na graph Glocal which initially contains all the edges in the edge insertion batch, and then contracts\\nvertices from same connected component of F to a single vertex. Since this graph contains k edges,\\nwe can put this graph into a single machine, and compute a spanning forest Flocal of Glocal. We\\nmaintain the maximal spanning forest F by adding edges in Flocal to F . We also maintain the\\nedge list data structure by adding inserted edges to the list, and maintain the sketches for the\\ninvolved vertices by the UpdateKey operation. Pseudocode of the batched insertion routine is in\\nAlgorithm 6.2.\\n24\\nInsert(u1v1, u2v2, . . . , ukvk)\\nInput: new edges e1 = u1v1, e2 = u2v2, . . . , ek = ukvk.\\n1. Add all k edges to the edge list data structure.\\n2. Run GetKey(u1, v1, . . . , uk, vk).\\n3. For every sketch, sample every inserted edge with probability equal to the parameter of\\nthe sketch, and compute the updated key value for vertices ~x ′u1 , ~x\\n′\\nv1 , . . . , ~x\\n′\\nuk\\n, ~x ′vk .\\n4. Run UpdateKey((u1, ~x\\n′\\nu1), (v1, ~x\\n′\\nv1), . . . , (uk, ~x\\n′\\nuk\\n), (vk, ~x\\n′\\nvk\\n)).\\n5. Run ID({u1, v1, u2, v2 . . . uk, vk}).\\n6. Using these IDs as vertex labels, construct a graph Glocal among the inserted edges, on a\\nlocal machine.\\n7. Find a maximal spanning forest Flocal of Glocal locally on this machine.\\n8. Run Link(E(Flocal)).\\nAlgorithm 6.2: Pseudocode for maintaining the data structure upon a batch of insertions.\\nLemma 6.2. The algorithm Insert in Algorithm 6.2 correctly maintains a maximal spanning\\nforest of G and takes O(1/α) rounds, each with total communication at most O˜(knα+δ).\\nProof. To show the correctness, notice that since we add only a forest on the components as a\\nwhole, there is never an edge added between two already connected components. Additionally,\\nsince the forest is spanning, we do not throw away any necessary edges.\\nFrom Theorem 2.2, using GetKey, UpdateKey, ID and Link falls under the claimed bound\\nfor rounds and communication, whereas the rest of the steps are performed only locally.\\n6.3 Algorithm for Batch Edge Deletions\\nPseudocode of the batched deletion routine is in Algorithm 6.3.\\nLemma 6.3. The algorithm Delete (Algorithm 6.3) correctly maintains a maximal spanning\\nforest of G and takes O(1/δα) rounds, each with total communication at most O˜(knα+δ).\\nProof. Note that F remains a maximal spanning forest if the deleted edges are from outside of F .\\nSo, we only need to deal with the complementary case. Consider some tree T ∈ F , from which\\nwe deleted kˆ − 1 edges. T is now separated into kˆ trees, T1, T2, . . . , Tkˆ. We need to show that the\\nalgorithm eventually contracts all Ti using the edges stored in the sketches. For this, note that\\nthe guarantees of Lemma 1.9 imply that from the O˜(nδ) copies of sketches, we can sample edges\\nleaving a group of Tis in ways that meet the requirements of Lemma 1.6. These trees will collapse\\ninto singleton vertices in O(1/δ) rounds with high probability by applying Lemma 1.6 iteratively.\\nThus the result is correct.\\nSteps 1-6 only require O(1/α) rounds of communication, from Theorem 2.2. Step 7 loops O(1/δ)\\ntimes, and its bottleneck is step 7b, the verification of the locations of the endpoints in the trees.\\n25\\nDelete(e1, e2, . . . , ek)\\nInput: edges e1 = u1v1, e2 = u2v2, . . . , ek = ukvk that are currently present in the graph.\\n1. Update the global edge index structure.\\n2. Run GetKey(u1, v1, . . . , uk, vk).\\n3. For every sketch, compute the updated key value ~x ′u1 , ~x\\n′\\nv1 , . . . , ~x\\n′\\nuk\\n, ~x ′vk for vertices\\nu1, v1, . . . , uk, vk by removing the IDs of edges e1, . . . , ek.\\n4. Run UpdateKey((u1, ~x\\n′\\nu1), (v1, ~x\\n′\\nv1), . . . , (uk, ~x\\n′\\nuk\\n), (vk, ~x\\n′\\nvk\\n)).\\n5. Run Cut for all edges that are in the spanning forest. Let u1 . . . ut be representative\\nvertices from the resulting trees\\n6. Run ComponentSum({u1 . . . ut}) to extract the total XOR values from each of the trees.\\n7. Repeat O(1/δ) rounds:\\n(a) From the XOR values of the current components, deduce a list of potential replace-\\nment edges, ER\\n(b) Identify the subset of edges with endpoints between current components given by\\nID(u1) . . . ID(ut) using a call to Query.\\n(c) Find TR, a maximal spanning forest of the valid replacement edges, via local compu-\\ntation.\\n(d) Link(E(TR)).\\n(e) Update u1 . . . ut and their XOR values, either using another batch of queries, or by a\\nlocal computation.\\nAlgorithm 6.3: Pseudocode for maintaining the data structure upon a batch of deletions.\\nOnce again by the guarantees of Theorem 2.2, this takes O(1/α) rounds for each iteration, and at\\nmost O˜(knδ+α) communication per round.\\nLastly, we call Link on the edges in ER across various iterations. Since at most k edges are\\ndeleted from F , there can only be at most k replacement edges, so the total communication caused\\nby these is O˜(knα+δ).\\n7 Adaptive Connectivity\\nThe adaptive connectivity problem is a “semi-online” version of the dynamic connectivity, where\\nwe are given a sequence of query/update pairs, and each update (an edge insertion or deletion) is\\nonly applied if its corresponding query evaluates to true on the graph resulting from all operations\\nbefore this pair. We say that the problem is semi-online because although the entire input is known\\nin advance, the algorithm must answer each query taking into account all operations that occur\\nbefore it.\\nIn this section, we show that this natural problem is P-complete under NC1 reductions. In\\n26\\nthe context of MPC algorithms, our result implies that if there exists an O(1) round low-memory\\nMPC algorithm solving the problem, then every problem in P can be solved in O(1) rounds in the\\nlow-memory MPC model.\\nOn the positive side, in Subsection 7.2 we give an upper-bound based on the batch-dynamic\\nconnectivity algorithm from Section 2, which shows that the adaptive connectivity problem can be\\nsolved in O(1) rounds for batches with size proportional to the space per machine.\\nWe first give a formal definition of the adaptive connectivity problem.\\nDefinition 7.1 (Adaptive Connectivity). The input to the Adaptive Connectivity problem is an\\ninput graph G on n vertices, and a sequence of query and update pairs: [(q1, u1), . . . , (qm, um)].\\nEach query, qi, is of the form Connected(u, v) or ¬Connected(u, v), and each update, ui, is either\\nan edge insertion (Insert(e = (u, v))) or an edge deletion (Delete(e = (u, v))). The problem is to\\nrun each qi, i ∈ [1,m] on the graph Gi, and apply ui to Gi to produce Gi+1 if and only if qi = true.\\nThe output of the problem is qm.\\n7.1 A Lower Bound for Adaptive Connectivity\\nWe now prove our lower-bound, showing that the adaptive connectivity problem is P-complete.\\nThe idea is that we can use the adaptivity in the problem to encode a circuit evaluation problem,\\nwhich are well known to be hard for P. Our reduction will be from the Circuit Value Problem,\\ndefined below:\\nDefinition 7.2 (Circuit Value Problem). The input to the Circuit Value Problem is an encoding\\nof a circuit C consisting of binary-fanin ∧ (and) and ∨ (or) gates, and unary-fanin ¬ (not) gates,\\ndefined over n boolean inputs x1, . . . , xn with truth assignments. Additionally there is a single\\nspecified output gate, y. The problem is to evaluate C and emit the value of the output gate, y.\\nOur reduction makes use of a topological ordering of the input circuit. A topological ordering\\nof a DAG (e.g., circuit) is a numbering ρ of its vertices so that for every directed edge (u, v),\\nρ(u) < ρ(v). Although we can topologically order a DAG in NC2, there is no known NC1 algorithm\\nfor the problem, which would mean that our reduction would use a (stronger) NC2 reduction. To\\nbypass this issue, we use the fact that the Topologically-Ordered Circuit Value Problem is still P-\\ncomplete [GHR+95]. Therefore, in what follows we assume that the circuit value problem instance\\nprovided to the reduction is topologically ordered.\\nTheorem 1.3. The adaptive connectivity problem is P-complete under NC1 reductions.\\nProof. The adaptive connectivity problem is clearly contained in P since a trivial O(poly(m)) work\\nalgorithm can first run a connectivity query using BFS or DFS on Gi to check whether the vertices\\nare connected or not, and then apply the update ui depending on the result of the query.\\nFor hardness we give a reduction from the Topologically-Ordered Circuit Value Problem. We\\nassume the circuit C, is equipped with the ability to query for the i-th gate in the specified\\ntopological order in O(1) time. Let n be the number of variables in the circuit, and k be the\\nnumber of gates.\\nThe reduction builds the initial graph G on n + k + 1 vertices, where there are n vertices\\ncorresponding to the variables, k vertices corresponding to the gates, where gate gi corresponds to\\na vertex vi, and a single distinguished root vertex, r. In the initial graph, each variable that is set\\nto true is connected to r.\\nThe reduction constructs a query/update sequence inductively as follows. Consider the i-th\\ngate in the topological ordering of C.\\n27\\n• If the gate is of the form gi = ga ∧ gb, we append the following query/update pair to the\\nsequence:\\n(Connected(va, vb), Insert(r, vi))\\nThat is, if the vertex corresponding to gate ga is connected to the vertex corresponding to\\ngate gb in G, then add an edge between the root r and the vertex corresponding to the i-th\\ngate.\\n• Similarly, if the gate is gi = ga ∨ gb, we append the following query/update pairs to the\\nsequence:\\n(Connected(r, va), Insert(r, vi))\\n(Connected(r, vb), Insert(r, vi))\\n• Finally, if the gate is of the form gi = ¬ga, we append the following query/update pair to the\\nsequence:\\n(¬Connected(r, va), Insert(r, vi))\\nIn this way a simple proof by induction shows that after executing all query/update pairs in\\nthe sequence, the connected component in G containing the root r contains all vertices (gates) that\\nevaluate to true. By making the final query of the form\\n(Connected(r, y), )\\nwhere y is the desired output gate, the output of the adaptive connectivity instance returns the\\nanswer to the to input circuit.\\nIt is easy to see that we can construct the query/update sequence in NC1 as we can access the\\ni-th gate independently in parallel, and each gate can be made to emit exactly two update/query\\npairs (for ∧ and ¬ gates we can simply insert a second noop query/update pair).\\nWe have the following corollary in the MPC setting.\\nCorollary 1.4. In the MPC model with memory per machine s = O˜(nǫ) for some constant ǫ, if\\nadaptive connectivity on a sequence of size O(n) can be solved in O(k) rounds, then every problem\\nin P can be solved in O(k) rounds.\\nProof. The proof follows by observing that each of the NC1 reductions starting with the reduction\\nfrom an arbitrary polynomial-time Turing machine, to the Topologically-Ordered Circuit Value\\nProblem can be carried out in O(1) rounds of MPC. Therefore, by applying these reductions, we\\ncan transform any problem in P to an adaptive connectivity instance in O(1) rounds of MPC.\\nRemark 7.3. We note that there are no known polynomial-time algorithms for the (Topologically-\\nOrdered) Circuit Value Problem with depth O(n1−ǫ), i.e., achieving polynomial speedup, and\\nthat finding such an algorithm has been a longstanding open question in parallel complexity the-\\nory [VS86, Con94, Rei97]. A parallel algorithm for adaptive connectivity in the centralized set-\\nting achieving even slightly sub-linear depth, e.g., O(nǫ−c) depth to process adaptive batches of\\nsize O(nǫ) for any constants ǫ, c > 0 would imply by our reduction above an algorithm for the\\n(Topologically-Ordered) Circuit Value Problem with depth O(n1−c), and therefore give an upper-\\nbound with polynomial speedup.\\n28\\nHardness for Other Adaptive Problems. Note that the reduction given above for adaptive\\nconnectivity immediately extends to problems related to connectivity, such as directed reachability,\\nand shortest-path problems. For adaptive directed connectivity, when we add an (x, y) edge in the\\nundirected case, we repeat the query twice and add both the x→ y and y → x edges. For adaptive\\nunweighted shortest paths, if the queries are of the form DistanceLessThan(u, v, d) then we reduce\\nthese queries to connectivity/reachability queries by setting d to an appropriately large value (in\\nthe reduction above, setting d to 2 suffices).\\n7.2 An Upper Bound for Adaptive Connectivity\\nWe now show that the static batch-parallel 1-Edge-Connectivity algorithm given in Theorem 1.1\\ncan be used to solve the adaptive connectivity problem. The bounds on the largest batch sizes that\\nthe algorithm handle are identical to those in Theorem 1.1.\\nGiven an adaptive batch of size k, the idea is to first take all deletion updates in the batch,\\nand “apply” them on G using a modified version of Theorem 1.1. Instead of permanently inserting\\nthe newly discovered replacement edges into G, we temporarily insert them to find all replacement\\nedges that exist if all deletions in the adaptive batch actually occur. This can be done by first\\ndeleting the edges in the adaptive batch using Theorem 1.1, finding all replacement edges, and\\nthen undoing these operations to restore G. The algorithm them collects the adaptive batch, and\\nall replacement edges (which have size at most equal to the size of the adaptive batch) on a single\\nmachine, and simulates the sequential adaptive algorithm on the contracted graph corresponding\\nto vertices active in the batch in 1 round. The insertions and deletions that ensue from processing\\nthe adaptive batch can be applied in the same bounds as Theorem 1.1. Therefore, we have an\\nalgorithm for adaptive connectivity with the following bounds:\\nCorollary 1.2. In the MPC model with memory per machine s = O˜(nǫ) we can maintain a dynamic\\nundirected graph on m edges which for constants δ, α, and integer k such that k·nα+δ ·polylog(n) ≤ s\\ncan handle the following operation with high probability:\\n1. An adaptive batch of up to k (query, edge insertions/deletions) pairs, using O(1/(δα)) rounds.\\nFurthermore, the total communication for handling a batch of k operations is O˜(knα+δ), and the\\ntotal space used across all machines is O˜(m).\\nReferences\\n[AABD19] Umut A. Acar, Daniel Anderson, Guy E. Blelloch, and Laxman Dhulipala. Parallel\\nbatch-dynamic graph connectivity. In ACM Symposium on Parallelism in Algorithms\\nand Architectures (SPAA), pages 381–392, 2019.\\n[ABB+19] Sepehr Assadi, MohammadHossein Bateni, Aaron Bernstein, Vahab Mirrokni, and Cliff\\nStein. Coresets meet edcs: algorithms for matching and vertex cover on massive graphs.\\nIn ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1616–1635, 2019.\\n[ACK19] Sepehr Assadi, Yu Chen, and Sanjeev Khanna. Sublinear algorithms for (∆+ 1) vertex\\ncoloring. In ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 767–786,\\n2019.\\n[AG18] Kook Jin Ahn and Sudipto Guha. Access to data and number of iterations: Dual primal\\nalgorithms for maximum matching under resource constraints. ACM Transactions on\\nParallel Computing (TOPC), 4(4):17, 2018.\\n29\\n[AGM12] Kook Jin Ahn, Sudipto Guha, and Andrew McGregor. Analyzing graph structure\\nvia linear measurements. In ACM-SIAM symposium on Discrete Algorithms (SODA),\\npages 459–467, 2012.\\n[AHLT05] Stephen Alstrup, Jacob Holm, Kristian De Lichtenberg, and Mikkel Thorup. Maintain-\\ning information in fully dynamic trees with top trees. ACM Transactions on Algorithms,\\n1(2):243–264, 2005.\\n[AK17] Sepehr Assadi and Sanjeev Khanna. Randomized composable coresets for matching\\nand vertex cover. In ACM Symposium on Parallelism in Algorithms and Architectures\\n(SPAA), pages 3–12, 2017.\\n[AKZ19] Sepehr Assadi, Nikolai Karpov, and Qin Zhang. Distributed and streaming linear pro-\\ngramming in low dimensions. In ACM Symposium on Principles of Database Systems\\n(PODS), pages 236–253, 2019.\\n[AM85] Noga Alon and Vitali D Milman. λ1, isoperimetric inequalities for graphs, and super-\\nconcentrators. Journal of Combinatorial Theory, Series B, 38(1):73–88, 1985.\\n[ANOY14] Alexandr Andoni, Aleksandar Nikolov, Krzysztof Onak, and Grigory Yaroslavtsev. Par-\\nallel algorithms for geometric graph problems. In ACM Symposium on Theory of Com-\\nputing (STOC), pages 574–583, 2014.\\n[Ass17] Sepehr Assadi. Simple round compression for parallel vertex cover. arXiv preprint\\narXiv:1709.04599, 2017.\\n[ASS+18] Alexandr Andoni, Zhao Song, Clifford Stein, Zhengyu Wang, and Peilin Zhong. Parallel\\ngraph connectivity in log diameter rounds. In IEEE Symposium on Foundations of\\nComputer Science (FOCS), pages 674–685, 2018.\\n[ASW19] Sepehr Assadi, Xiaorui Sun, and Omri Weinstein. Massively parallel algorithms for\\nfinding well-connected components in sparse graphs. To appear in ACM Symposium\\non Principles of Distributed Computing (PODC), 2019.\\n[ASZ19] Alexandr Andoni, Clifford Stein, and Peilin Zhong. Log diameter rounds algorithms\\nfor 2-vertex and 2-edge connectivity. arXiv preprint arXiv:1905.00850, 2019.\\n[BBD+18] MohammadHossein Bateni, Soheil Behnezhad, Mahsa Derakhshan, MohammadTaghi\\nHajiaghayi, and Vahab Mirrokni. Massively parallel dynamic programming on trees.\\narXiv preprint arXiv:1809.03685, 2018.\\n[BBLM14] MohammadHossein Bateni, Aditya Bhaskara, Silvio Lattanzi, and Vahab Mirrokni.\\nDistributed balanced clustering via mapping coresets. In Advances in Neural Informa-\\ntion Processing Systems, pages 2591–2599, 2014.\\n[BDE+19] Soheil Behnezhad, Laxman Dhulipala, Hossein Esfandiari, Jakub  La¸cki, Vahab Mir-\\nrokni, and Warren Schudy. Massively parallel computation via remote memory access.\\nIn ACM Symposium on Parallelism in Algorithms and Architectures (SPAA), pages\\n59–68, 2019.\\n[BENW16] Rafael da Ponte Barbosa, Alina Ene, Huy L Nguyen, and Justin Ward. A new frame-\\nwork for distributed submodular maximization. In 2016 IEEE 57th Annual Symposium\\non Foundations of Computer Science (FOCS), pages 645–654. Ieee, 2016.\\n30\\n[BFU18] Sebastian Brandt, Manuela Fischer, and Jara Uitto. Matching and MIS for uniformly\\nsparse graphs in the low-memory MPC model. arXiv preprint arXiv:1807.05374, 2018.\\n[BHH19] Soheil Behnezhad, MohammadTaghi Hajiaghayi, and David G Harris. Exponentially\\nfaster massively parallel maximal matching. arXiv preprint arXiv:1901.03744, 2019.\\n[BKS13] Paul Beame, Paraschos Koutris, and Dan Suciu. Communication steps for parallel\\nquery processing. In ACM Symposium on Principles of Database Systems (PODS),\\npages 273–284, 2013.\\n[BKV12] Bahman Bahmani, Ravi Kumar, and Sergei Vassilvitskii. Densest subgraph in stream-\\ning and mapreduce. Proceedings of the VLDB Endowment, 5(5):454–465, 2012.\\n[BM96] Guy E Blelloch and Bruce M Maggs. Parallel algorithms. ACM Computing Surveys\\n(CSUR), 28(1):51–54, 1996.\\n[BMV+12] Bahman Bahmani, Benjamin Moseley, Andrea Vattani, Ravi Kumar, and Sergei Vas-\\nsilvitskii. Scalable k-means++. Proceedings of the VLDB Endowment, 5(7):622–633,\\n2012.\\n[CFG+19] Yi-Jun Chang, Manuela Fischer, Mohsen Ghaffari, Jara Uitto, and Yufan Zheng. The\\ncomplexity of (∆ + 1) coloring in congested clique, massively parallel computation,\\nand centralized local computation. To appear in the ACM Symposium on Principles of\\nDistributed Computing (PODC), 2019.\\n[C LM+18] Artur Czumaj, Jakub  La¸cki, Aleksander Ma¸dry, Slobodan Mitrovic´, Krzysztof Onak,\\nand Piotr Sankowski. Round compression for parallel matching algorithms. In ACM\\nSymposium on Theory of Computing (STOC), pages 471–484, 2018.\\n[Con94] Anne Condon. A theory of strict P-completeness. Computational Complexity, 4(3):220–\\n241, 1994.\\n[DG08] Jeffrey Dean and Sanjay Ghemawat. Mapreduce: simplified data processing on large\\nclusters. Communications of the ACM, 51(1):107–113, 2008.\\n[EIM11] Alina Ene, Sungjin Im, and Benjamin Moseley. Fast clustering using mapreduce. In\\nACM International Conference on Knowledge Discovery and Data Mining (SIGKDD),\\npages 681–689, 2011.\\n[EN15] Alina Ene and Huy Nguyen. Random coordinate descent methods for minimizing\\ndecomposable submodular functions. In International Conference on Machine Learning\\n(ICML), pages 787–795, 2015.\\n[Epp94] David Eppstein. Offline algorithms for dynamic minimum spanning tree problems.\\nJournal of Algorithms, 17(2):237–250, 1994.\\n[Fre85] Greg N. Frederickson. Data structures for on-line updating of minimum spanning trees,\\nwith applications. SIAM J. Comput., 14(4):781–798, 1985.\\n[GHR+95] Raymond Greenlaw, H James Hoover, Walter L Ruzzo, et al. Limits to parallel com-\\nputation: P-completeness theory. Oxford University Press on Demand, 1995.\\n[GI92] Z. Galil and G. Italiano. Fully dynamic algorithms for 2-edge connectivity. SIAM\\nJournal on Computing, 21(6):1047–1069, 1992.\\n31\\n[GKKT15] David Gibb, Bruce M. Kapron, Valerie King, and Nolan Thorn. Dynamic graph\\nconnectivity with improved worst case update time and sublinear space. CoRR,\\nabs/1509.06464, 2015. Availabel at: http://arxiv.org/abs/1509.06464.\\n[GKMS18] Buddhima Gamlath, Sagar Kale, Slobodan Mitrovic´, and Ola Svensson. Weighted\\nmatchings via unweighted augmentations. arXiv preprint arXiv:1811.02760, 2018.\\n[GKU19] Mohsen Ghaffari, Fabian Kuhn, and Jara Uitto. Conditional hardness results for mas-\\nsively parallel computation from distributed lower bounds. To appear in IEEE Sympo-\\nsium on Foundations of Computer Science (FOCS), 2019.\\n[GU19] Mohsen Ghaffari and Jara Uitto. Sparsifying distributed algorithms with ramifications\\nin massively parallel computation and centralized local computation. In ACM-SIAM\\nSymposium on Discrete Algorithms (SODA), pages 1636–1653, 2019.\\n[HDLT01] Jacob Holm, Kristian De Lichtenberg, and Mikkel Thorup. Poly-logarithmic determin-\\nistic fully-dynamic algorithms for connectivity, minimum spanning tree, 2-edge, and\\nbiconnectivity. Journal of the ACM, 48(4):723–760, 2001. Announced at STOC’98.\\n[HI15] Hemant Hingave and Rasika Ingle. An approach for mapreduce based log analysis\\nusing hadoop. In IEEE International Conference on Electronics and Communication\\nSystems (ICECS), pages 1264–1268, 2015.\\n[HK99] Monika Rauch Henzinger and Valerie King. Randomized fully dynamic graph algo-\\nrithms with polylogarithmic time per operation. J. ACM, 46(4):502–516, 1999.\\n[HSS19] MohammadTaghi Hajiaghayi, Saeed Seddighin, and Xiaorui Sun. Massively parallel\\napproximation algorithms for edit distance and longest common subsequence. In ACM-\\nSIAM Symposium on Discrete Algorithms (SODA), pages 1654–1672, 2019.\\n[ILMP19] Giuseppe F Italiano, Silvio Lattanzi, Vahab S Mirrokni, and Nikos Parotsidis. Dynamic\\nalgorithms for the massively parallel computation model. In ACM Symposium on\\nParallelism in Algorithms and Architectures (SPAA), pages 49–58, 2019.\\n[IM19] Sungjin Im and Benjamin Moseley. A conditional lower bound on graph connectivity\\nin mapreduce. CoRR, abs/1904.08954, 2019.\\n[IMS17] Sungjin Im, Benjamin Moseley, and Xiaorui Sun. Efficient massively parallel methods\\nfor dynamic programming. In ACM Symposium on Theory of Computing (STOC),\\npages 798–811, 2017.\\n[KKM13] Bruce M. Kapron, Valerie King, and Ben Mountjoy. Dynamic graph connectivity in\\npolylogarithmic worst case time. In Annual ACM-SIAM Symposium on Discrete Algo-\\nrithms (SODA), 2013.\\n[KLM+14] Raimondas Kiveris, Silvio Lattanzi, Vahab Mirrokni, Vibhor Rastogi, and Sergei Vas-\\nsilvitskii. Connected components in mapreduce and beyond. In ACM Symposium on\\nCloud Computing (SOCC), pages 1–13, 2014.\\n[KLP16] Ioannis Koutis, Alex Levin, and Richard Peng. Faster spectral sparsification and nu-\\nmerical algorithms for SDD matrices. ACM Trans. Algorithms, 12(2):17:1–17:16, 2016.\\nAvailable at http://arxiv.org/abs/1209.5821.\\n32\\n[KMVV15] Ravi Kumar, Benjamin Moseley, Sergei Vassilvitskii, and Andrea Vattani. Fast greedy\\nalgorithms in mapreduce and streaming. ACM Transactions on Parallel Computing\\n(TOPC), 2(3):14, 2015.\\n[KPPS17] Rasmus Kyng, Jakub Pachocki, Richard Peng, and Sushant Sachdeva. A framework\\nfor analyzing resparsification algorithms. In ACM-SIAM Symposium on Discrete Al-\\ngorithms (SODA), pages 2032–2043, 2017.\\n[KRS90] Clyde P Kruskal, Larry Rudolph, and Marc Snir. A complexity theory of efficient\\nparallel algorithms. Theoretical Computer Science, 71(1):95–132, 1990.\\n[KSV10] Howard Karloff, Siddharth Suri, and Sergei Vassilvitskii. A model of computation for\\nmapreduce. In ACM-SIAM symposium on Discrete Algorithms (SODA), pages 938–\\n948, 2010.\\n[KTF09] U Kang, Charalampos E Tsourakakis, and Christos Faloutsos. Pegasus: A peta-scale\\ngraph mining system implementation and observations. In IEEE International Con-\\nference on Data Mining (ICDM), pages 229–238, 2009.\\n[ LMW18] Jakub  La¸cki, Vahab Mirrokni, and Micha l W lodarczyk. Connected components at scale\\nvia local contractions. arXiv preprint arXiv:1807.10727, 2018.\\n[LZY10] Guojun Liu, Ming Zhang, and Fei Yan. Large-scale social network analysis based on\\nmapreduce. In IEEE International Conference on Computational Aspects of Social\\nNetworks (CASoN), pages 487–490, 2010.\\n[MAB+10] Grzegorz Malewicz, Matthew H Austern, Aart JC Bik, James C Dehnert, Ilan Horn,\\nNaty Leiser, and Grzegorz Czajkowski. Pregel: a system for large-scale graph pro-\\ncessing. In ACM International Conference on Management of Data (SIGMOD), pages\\n135–146, 2010.\\n[MKSK13] Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause. Distributed\\nsubmodular maximization: Identifying representative elements in massive data. In\\nAdvances in Neural Information Processing Systems, pages 2049–2057, 2013.\\n[NS17] Danupon Nanongkai and Thatchaphol Saranurak. Dynamic spanning forest with worst-\\ncase update time: adaptive, las vegas, and O(n1/2−ǫ)-time. In Symposium on Theory\\nof Computing (STOC), pages 1122–1129, 2017.\\n[NSW17] Danupon Nanongkai, Thatchaphol Saranurak, and Christian Wulff-Nilsen. Dynamic\\nminimum spanning forest with subpolynomial worst-case update time. In Symposium\\non Foundations of Computer Science (FOCS), pages 950–961, 2017.\\n[Ona18] Krzysztof Onak. Round compression for parallel graph algorithms in strongly sublinear\\nspace. arXiv preprint arXiv:1807.08745, 2018.\\n[Rei97] Klaus Reinhardt. Strict sequential P-completeness. In Symposium on Theoretical As-\\npects of Computer Science (STACS), pages 329–338, 1997.\\n[RMCS13] Vibhor Rastogi, Ashwin Machanavajjhala, Laukik Chitnis, and Anish Das Sarma. Find-\\ning connected components in map-reduce in logarithmic rounds. In IEEE Conference\\non Data Engineering (ICDE), pages 50–61, 2013.\\n33\\n[RVW18] Tim Roughgarden, Sergei Vassilvitskii, and Joshua R Wang. Shuffles and circuits (on\\nlower bounds for modern parallel computation). Journal of the ACM (JACM), 65(6):41,\\n2018.\\n[SASU13] Anish Das Sarma, Foto N Afrati, Semih Salihoglu, and Jeffrey D Ullman. Upper and\\nlower bounds on the cost of a map-reduce computation. In Proceedings of the VLDB\\nEndowment, volume 6, pages 277–288, 2013.\\n[SS11] D. Spielman and N. Srivastava. Graph sparsification by effective resis-\\ntances. SIAM Journal on Computing, 40(6):1913–1926, 2011. Available at\\nhttp://arxiv.org/abs/0803.0929.\\n[ST83] Daniel D. Sleator and Robert Endre Tarjan. A data structure for dynamic trees. J.\\nComput. Syst. Sci., 26(3):362–391, June 1983.\\n[ST11] D. Spielman and S. Teng. Spectral sparsification of graphs. SIAM Journal on Com-\\nputing, 40(4):981–1025, 2011. Available at http://arxiv.org/abs/0808.4134.\\n[TDB19] Thomas Tseng, Laxman Dhulipala, and Guy Blelloch. Batch-parallel Euler tour trees.\\nAlgorithm Engineering and Experiments (ALENEX), pages 92–106, 2019.\\n[Tro12] Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Found. Comput.\\nMath., 12(4):389–434, August 2012. Available at http://arxiv.org/abs/1004.4389.\\n[VS86] Jeffrey Scott Vitter and Roger A. Simons. New classes for parallel complexity: A study\\nof unification and other complete problems for P. IEEE Transactions on Computers,\\n(5):403–418, 1986.\\n[Wul17] Christian Wulff-Nilsen. Fully-dynamic minimum spanning forest with improved worst-\\ncase update time. In ACM Symposium on Theory of Computing (STOC), pages 1130–\\n1143, 2017.\\n[YV18] Grigory Yaroslavtsev and Adithya Vadapalli. Massively parallel algorithms and hard-\\nness for single-linkage clustering under ℓp-distances. In International Conference on\\nMachine Learning (ICML), 2018.\\n34\\n'}\n",
      "{'_id': ObjectId('6815e619dd259dc7e6e1cd28'), 'authors': 'Chakraborty, Diptarka, Das, Debarati, Goldenberg, Elazar, Koucky, Michal, Saks, Michael', 'year': '2019', 'title': 'Approximating Edit Distance Within Constant Factor in Truly\\n  Sub-Quadratic Time', 'full_text': 'ar\\nX\\niv\\n:1\\n81\\n0.\\n03\\n66\\n4v\\n1 \\n [c\\ns.D\\nS]\\n  8\\n O\\nct \\n20\\n18\\nApproximating Edit Distance Within Constant Factor in Truly\\nSub-Quadratic Time∗\\nDiptarka Chakraborty†1, Debarati Das‡2, Elazar Goldenberg§3, Michal Koucký¶4, and\\nMichael Saks‖5\\n1,2,4Computer Science Institute of Charles University, Malostranské náměstí 25, 118 00\\nPraha 1, Czech Republic\\n3The Academic College Of Tel Aviv-Yaffo, School of Computer Science, Tel Aviv-Yaffo,\\nIsrael\\n5Department of Mathematics, Rutgers University, Piscataway, NJ, USA\\nAbstract\\nEdit distance is a measure of similarity of two strings based on the minimum number of\\ncharacter insertions, deletions, and substitutions required to transform one string into the other.\\nThe edit distance can be computed exactly using a dynamic programming algorithm that runs\\nin quadratic time. Andoni, Krauthgamer and Onak (2010) gave a nearly linear time algorithm\\nthat approximates edit distance within approximation factor poly(log n).\\nIn this paper, we provide an algorithm with running time O˜(n2−2/7) that approximates the\\nedit distance within a constant factor.\\n∗The research leading to these results has received funding from the European Research Council under the Euro-\\npean Union’s Seventh Framework Programme (FP/2007-2013)/ERC Grant Agreement no. 616787.\\n†diptarka@iuuk.mff.cuni.cz\\n‡debaratix710@gmail.com\\n§elazargo@mta.ac.il\\n¶koucky@iuuk.mff.cuni.cz\\n‖msaks30@gmail.com\\n1 Introduction\\nExact computation of edit distance. The edit distance (aka Levenshtein distance) [16] between\\nstrings x, y, denoted by dedit(x, y), is the minimum number of character insertions, deletions, and\\nsubstitutions needed to convert x into y. It is a widely used distance measure between strings\\nthat finds applications in fields such as computational biology, pattern recognition, text processing,\\nand information retrieval. The problems of efficiently computing dedit(x, y), and of constructing an\\noptimal alignment (sequence of operations that converts x to y), are of significant interest.\\nEdit distance can be evaluated exactly in quadratic time via dynamic programming (Wagner\\nand Fischer [20]). Landau et al. [15] gave an algorithm that finds an optimal alignment in time\\nO(n+dedit(x, y)\\n2), improving on a previous O(n ·dedit(x, y)) algorithm of Ukkonen [19]. Masek and\\nPaterson [17] obtained the first (slightly) sub-quadratic O(n2/ log n) time algorithm, and the current\\nasymptotically fastest algorithm (Grabowski [14]) runs in time O(n2 log log n/ log2 n). Backurs and\\nIndyk [7] showed that a truly sub-quadratic algorithm (O(n2−δ) for some δ > 0) would imply a\\n2(1−γ)n time algorithm for CNF-satisfiabilty, contradicting the Strong Exponential Time Hypothesis\\n(SETH). Abboud et al. [3] showed that even shaving an arbitrarily large polylog factor from n2\\nwould have the plausible, but apparently hard-to-prove, consequence that NEXP does not have\\nnon-uniform NC1 circuits. For further “barrier” results, see [2, 13].\\nApproximation algorithms. There is a long line of work on approximating edit distance. The\\nexact O(n+k2) time algorithm (where k is the edit distance of the input) of Landau et al. [15] yields\\na linear time\\n√\\nn-factor approximation. This approximation factor was improved, first to n3/7 [8],\\nthen to n1/3+o(1) [10] and later to 2O˜(\\n√\\nlogn) [6], all with slightly superlinear runtime. Batu et al. [9]\\nprovided an O(n1−α)-approximation algorithm with runtime O(nmax{\\nα\\n2\\n,2α−1}). The strongest result\\nof this type is the (log n)O(1/ǫ) factor approximation (for every ǫ > 0) with running time n1+ǫ of\\nAndoni et al. [4]. Abboud and Backurs [1] showed that a truly sub-quadratic deterministic time\\n1 + o(1)-factor approximation algorithm for edit distance would imply new circuit lower bounds.\\nIndependent of our work, Boroujeni et al. [11] obtained a truly sub-quadratic quantum algorithm\\nthat provides a constant factor approximation. Their latest results [12] are a (3 + ǫ) factor with\\nruntime O˜(n2−4/21/ǫO(1)) and a faster O˜(n1.708)-time with a larger constant factor approximation.\\nAndoni and Nguyen [5] found a randomized algorithm that approximates Ulam distance of\\ntwo permutations of {1, . . . , n} (edit distance with only insertions and deletions) within a (large)\\nconstant factor in time O˜(\\n√\\nn+n/k), where k is the Ulam distance of the input; this was improved\\nby Naumovitz et al. [18] to a (1 + ε)-factor approximation (for any ε > 0) with similar runtime.\\nOur results. We present the first truly sub-quadratic time classical algorithm that approximates\\nedit distance within a constant factor.\\nTheorem 1.1. There is a randomized algorithm ED-UB that on input strings x, y of length n over\\nany alphabet Σ outputs an upper bound on dedit(x, y) in time O˜(n\\n12/7) that, with probability at least\\n1− n−5, is at most a fixed constant multiple of dedit(x, y).\\nIf the output is U , then the algorithm has implicitly found an alignment of cost at most U . The\\nalgorithm can be modified to explicitly output such an alignment.\\nThe approximation factor proved in this preliminary version is 1680, can be greatly improved\\nby tweaking parameters. We believe, but have not proved, that with sufficient care the algorithm\\ncan be modified (with no significant increase in runtime) to get (3 + ǫ) approximation.\\nTheorem 1.1 follows from:\\n1\\nTheorem 1.2. For every θ ∈ [n−1/5, 1], there is a randomized algorithm GAP-UBθ that on input\\nstrings x, y of length n outputs u = GAP-UBθ(x, y) such that: (1) dedit(x, y) ≤ u and (2) on any\\ninput with dedit(x, y) ≤ θn, u ≤ 840θn with probability at least 1−n−7. The runtime of GAP-UBθ\\nis O˜(n2−2/7θ4/7).\\nThe name GAP-UBθ reflects that this is a \"gap algorithm\", which distinguishes inputs with\\ndedit(x, y) ≤ θn (where the output is at most 840θn), and those with dedit(x, y) > 840θn (where the\\noutput is greater than 840θn).\\nTheorem 1.1 follows via a routine construction of ED-UB from GAP-UBθ, presented in Sec-\\ntion 5. The rest of the paper is devoted to proving Theorem 1.2.\\nThe framework of the algorithm. We use a standard two-dimensional representation of edit\\ndistance. Visualize x as lying on a horizontal axis and y as lying on a vertical axis, with horizontal\\ncoordinate i ∈ {1, . . . , n} corresponding to xi and vertical component j corresponding to yj. The\\nwidth µ(I) of interval I ⊆ {0, 1, . . . , n} is max(I)−min(I) = |I|−1. Also, xI denotes the substring\\nof x indexed by I \\\\ {min(I)}. (Note: xmin(I) is not part of xI , e.g., x = x{0,...,n}. This convention is\\nmotivated by Proposition 1.3.) We refer to I as an x-interval to indicate that it indexes a substring\\nof x, and J as a y-interval to indicate that it indexes a substring of y. A box is a set I × J where\\nI is a x-interval and J is a y-interval; I × J corresponds to the substring pair (xI , yJ). I × J is a\\nw-box if µ(I) = µ(J) = w. We often abbreviate dedit(xI , yJ) by dedit(I, J). A decomposition of an\\nx-interval I is a sequence I1, . . . , Iℓ of subintervals with min(I1) = min(I), max(Iℓ) = max(I) and\\nfor j ∈ [ℓ− 1], max(Ij) = min(Ij+1).\\nAssociated to x, y is a directed graph Gx,y with edge costs called a grid graph with vertex set\\n{0, . . . , n} × {0, . . . , n} and all edges of the form (i − 1, j) → (i, j) (H-steps), (i, j − 1) → (i, j)\\n(V -steps) and (i− 1, j− 1)→ (i, j) (D-steps). Every H-step or V-step costs 1, and D-steps cost 1 if\\nxi 6= yj and 0 otherwise. There is a 1-1 correspondence that maps a path from (0, 0) to (n, n) to an\\nalignment from x to y, i.e. a set of character deletions, insertions and substitutions that changes x\\nto y, where an H-step (i− 1, j) → (i, j) means \"delete xi\", a V-step (i, j− 1)→ (i, j) means \"insert\\nyj between xi and xi+1\" and a D-step (i− 1, j − 1)→ (i, j) means replace xi by yj, unless they are\\nalready equal. We have:\\nProposition 1.3. The cost of an alignment, cost(τ), is the sum of edge costs of its associated path\\nτ , and dedit(x, y) is equal to cost(Gx,y), the min cost of an alignment path from (0, 0) to (n, n).\\nFor I, J ⊆ {0, . . . , n}, Gx,y(I×J) ∼= GxI ,yJ is the grid graph induced on I×J , and dedit(I, J) =\\ncost(Gx,y(I × J)). The natural high-level idea of GAP-UBθ appears (explicitly or implicitly) in\\nprevious work. The algorithm has two phases. First, the covering phase identifies a set R of\\ncertified boxes which are pairs (I × J, κ), where κ is an upper bound on the normalized edit distance\\n∆edit(xI , yJ) = dedit(xI , yJ)/µ(I). (∆edit(I, J) is more convenient than dedit(I, J) for the covering\\nphase.) Second, the min-cost path phase, takes input R and uses a straightforward customized\\nvariant of dynamic programming to find an upper bound U(R) on dedit(x, y) in time quasilinear in\\n|R|. The central issue is to ensure that the covering phase outputs R that is sufficiently informative\\nso that U(R) ≤ c · dedit(x, y) for constant c, while running in sub-quadratic time.\\nSimplifying assumptions. The input strings x, y have equal length n. (It is easy to reduce to this\\ncase: pad the shorter string to the length of the longer using a new symbol. The edit distance of\\nthe new pair is between the original edit distance and twice the original edit distance. This factor 2\\nincrease in approximation factor can be avoided by generalizing our algorithm to the case |x| 6= |y|,\\nbut we won’t do this here.) We assume n is a power of 2 (by padding both strings with a new\\n2\\nsymbol, which leaves edit distance unchanged). We assume that θ is a (negative) integral power of\\n2. The algorithm involves integer parameters w1, w2, d, all of which are chosen to be powers of 2.\\nOrganization of the paper. Section 2 is a detailed overview of the covering phase algorithm\\nand its analysis. Section 3 presents the pseudo-code and analysis for the covering phase. Section 4\\npresents the min-cost path phase algorithm. Section 5 summarizes the full algorithm and discusses\\nimprovements in runtime via recursion.\\n2 Covering algorithm: Detailed overview\\nWe give a detailed overview of the covering phase and its time analysis and proof of correctness,\\nignoring minor technical details. The pseudo-code in Section 3 corresponds to the overview, with\\ntechnical differences mainly to improve runtime. We will illustrate the sub-quadratic time analysis\\nwith the sample input parameter θ = n−1/50 and algorithm parameters w1 = n1/10, w2 = n3/10 and\\nd = n1/5.\\nThe covering phase outputs a set R of certified boxes. The goal is that R includes an adequate\\napproximating sequence for some min-cost path τ in Gx,y, which is a sequence σ of certified boxes\\n(I1 × J1, κ1), . . . , (Iℓ × Jℓ, κℓ) that satisfies:\\n1. I1, . . . , Iℓ is a decomposition of {0, . . . , n}.\\n2. Ii × Ji is an adequate cover of τi, where τi = τIi denotes the minimal subpath of τ whose\\nprojection to the x-axis is Ii, and adequate cover means that the (vertical) distance from the\\nstart vertex (resp. final vertex) of τi and the lower left (resp. upper right) corner of Ii × Ji,\\nis at most a constant multiple of cost(τi) + θ.\\n3. The sequence σ is adequately bounded, i.e.,\\n∑\\ni µ(Ii)κi ≤ c(cost(τ) + θn), for a constant c.\\nThis is a slight oversimplification of Definition 3 of (k, ζ)-approximation of τ by a sequence of\\ncertified boxes.\\nThe intuition for the second condition is that τi is \"almost\" a path between the lower left and\\nupper right corners of Ii × Ji. Now τi might have a vertical extent J ′ that is much larger than its\\nhorizontal extent Ii, in which case it is impossible to place a square Ii × Ji with corners close to\\nboth endpoints of τi. But in that case, τi has a very high cost (at least |µ(J ′)−µ(Ii)|. The closeness\\nrequired is adjusted based on cost(τi), with relaxed requirements if cost(τi) is large.\\nThe output of the min-cost path phase should satisfy the requirements of GAP-UBθ. Lemma\\n4.1 shows that if the min-cost path phase receivesR that contains a (k, θ)-approximating sequence to\\nsome min-cost path τ , then it will output an upper bound to dedit(x, y) that is at most k\\n′(dedit(x, y)+\\nθn) for some k′. So that on input x, y with dedit(x, y) ≤ θn, the output is at most 2k′θn, satisfying\\nthe requirements of GAP-UBθ. This formalizes the intuition that an adequate approximating\\nsequence captures enough information to deduce a good bound on cost(τ).\\nOnce and for all, we fix a min-cost path τ . Our task for the covering phase is that, with high\\nprobability, R includes an adequate approximating sequence for τ .\\nA τ -match for an x-interval I is a y-interval J such that I × J is an adequate cover of τI . It\\nis easy to show (Proposition 3.3) that this implies dedit(I, J) ≤ (cost(τI) + θµ(I)). A box I × J is\\nsaid to be τ -compatible if J is a τ -match for I and a box sequence is τ -compatible if every box is\\nτ -compatible. A τ -compatible certified box sequence whose distance upper bounds are (on average)\\n3\\nwithin a constant factor of the actual cost, satisfies the requirements for an adequate approximating\\nsequence. Our cover algorithm will ensure that R contains such a sequence.\\nA natural decomposition is Iw1 , with all parts of width w1 (think of w1 as a power of 2 that is\\nroughly n1/10) so ℓ = n/w1 and Ij = {(j − 1)w1, · · · , (j)w1}. The naïve approach to building R is\\nto include certified boxes for enough choices of J to guarantee a τ -match for each Ij . An interval of\\nwidth w1 is δ-aligned if its upper and lower endpoints are both multiples of δw1 (which we require\\nto be an integral power of 2). We restrict attention to x-intervals in Iw1 , called x-candidates and\\nθ-aligned y-intervals of width w1 called y-candidates. It can be shown (see Proposition 3.4) that an\\nx-interval I always has a τ -match J that is θ-aligned. (In this overview we will fix δ to θ; the actual\\nalgorithm has O(log n) iterations during which the value of δ varies, giving improvements in runtime\\nthat are unimportant in this overview.) For each x-candidate I, designate one such τ -match as the\\ncanonical τ -match, Jτ (I) for I, and I × Jτ (I) is the canonical τ -compatible box for I.\\nIn the exhaustive approach, for each (x-candidate, y-candidate)-pair (I, J), its edit distance is\\ncomputed in time O(w21), and the certified box (I × J,∆edit(I, J)) is included. There are nw1 nθw1\\nboxes, so the time for all edit distance computations is O(n\\n2\\nθ ), which is worse than quadratic. (The\\nfactor 1θ can be avoided by standard techniques, but this is not significant to the quest for a sub-\\nquadratic algorithm, so we defer this until the next section.) Note that |R| is n2\\nθ(w1)2\\n(which is n1.82\\nfor our sample parameters) so at least the min-cost path phase (which runs in time quasi-linear in\\nR) is truly sub-quadratic.\\nTwo natural goals that will improve the runtime are: (1) Reduce the amortized time per box\\nneeded to certify boxes significantly below (w1)\\n2 and (2) Reduce the total number of certified boxes\\ncreated significantly below n\\n2\\nθ(w1)2\\n. Neither goal is always achievable, and our covering algorithm\\ncombines them. In independent work [11, 12], versions of these two goals are combined, where the\\nsecond goal is accomplished via Grover search, thus yielding a constant factor sub-quadratic time\\nquantum approximation algorithm.\\nReducing amortized time for certifying boxes: the dense case algorithm. We aim\\nto reduce the amortized time per certified box to be much smaller than (w1)\\n2. We divide our\\nsearch for certified boxes into iterations i ∈ {0, . . . , log n}. For iteration i, with ǫi = 2−i, our goal\\nis that for all candidate pairs I, J with ∆edit(I, J) ≤ ǫi, we include the certified box (I × J, cǫi)\\nfor a fixed constant c. If we succeed, then for each Ij and its canonical τ -match J\\nτ (Ij), and for\\nthe largest index i for which ∆edit(Ij , J\\nτ (Ij)) ≤ ǫi, iteration i will certify (Ij × Jτ (Ij), κj) with\\nκj ≤ cǫi ≤ 2c∆edit(Ij , Jτ (Ij)), as needed.\\nFor a string z of size w1, let H(z, ρ) be the set of x-candidates I with ∆edit(z, xI) ≤ ρ and\\nV(z, ρ) be the set of y-candidates J with ∆edit(z, yJ) ≤ ρ. In iteration i, for each x-candidate I, we\\nwill specify a set Qi(I) of y-candidates that includes V(xI , ǫi) and is contained in V(xI , 5ǫi). The\\nset of certified boxes (I×J, 5ǫi) for all x-candidates I and J ∈ Qi(I) satisfies the goal of iteration i.\\nIteration i proceeds in rounds. In each round we select an x-candidate I, called the pivot,\\nfor which Qi(I) has not yet been specified. Compute ∆edit(xI , yJ) for all y-candidates J and\\n∆edit(xI , xI′) for all x-candidates I\\n′; these determine H(xI , ρ) and V(xI , ρ) for any ρ. For all\\nI ′ ∈ H(xI , 2ǫi), set Qi(I ′) = V(xI , 3ǫi). By the triangle inequality, for each I ′ ∈ H(xI , 2ǫi),\\nV(xI , 3ǫi) includes V(xI′ , ǫi) and is contained in V(xI′ , 5ǫi) so we can certify all the boxes with\\nupper bound 5ǫi. Mark intervals in H(xI , 2ǫi) as fulfilled and proceed to the next round, choosing\\na new pivot from among the unfulfilled x-candidates.\\nThe number of certified boxes produced in a round is |H(xI , 2ǫi)| × |V(xI , 3ǫi)|. If this is much\\nlarger than O( nθw1 ), the number of edit distance computations, then we have significantly reduced\\n4\\namortized time per certified box. (For example, in the trivial case i = 0, every candidate box will\\nbe certified in a single round.) But in worst case, there are nw1 rounds each requiring Ω(\\nnw1\\nθ ) time,\\nfor an unacceptable total time Θ(n2/θ).\\nHere is a situation where the number of rounds is much less than nw1 . Since any two pivots are\\nnecessarily greater than 2ǫi apart, the sets V(xI , ǫi) for distinct pivots are disjoint. Now for some\\nparameter d (think of d = n1/5) an x-candidate is d-dense for ǫi if |V(xI , ǫi)| ≥ d, i.e., xI is ǫi-close\\nin edit distance to at least d y-candidates; it is d-sparse otherwise. If we manage to select a d-dense\\npivot I in each round, then the number of rounds is O( nw1dθ ) and the overall time will be Θ(\\nn2\\ndθ2\\n).\\nFor the sample parameters this is Θ(n1.84). But there’s no reason to expect that we’ll only choose\\ndense pivots; indeed there need not be any dense pivot.\\nLet’s modify the process a bit. When choosing potential pivot I, first test whether or not it is\\n(approximately) d-dense. This can be done with high probability, by randomly sampling Θ˜( nθw1d )\\ny-candidates and finding the fraction of the sample that are within ǫi of xI . If this fraction is less\\nthan θw1d2n then I is declared sparse and abandoned as a pivot; otherwise I is declared dense, and\\nused as a pivot. With high probability, all d-dense intervals that are tested are declared dense, and\\nall tested intervals that are not d/4-dense are declared sparse, so we assume this is the case. Then\\nall pivots are processed (as above) in time O( n\\n2\\ndθ2\\n) (under sample parameters: O(n1.84)). We pay\\nO˜( nw1dθ )(w1)\\n2 to test each potential pivot (at most nw1 of them) so the overall time to test potential\\npivots is O˜(n\\n2\\ndθ ) (with sample parameters: O˜(n\\n1.82)).\\nEach iteration i (with different ǫi) splits x-candidates into two sets, Si of intervals that are\\ndeclared sparse, and all of the rest for which we have found the desired set Qi(I). With high\\nprobability every interval in Si is indeed d-sparse, but a sparse interval need not belong to Si, since\\nit may belong to H(xI , 2ǫi) for some selected pivot I.\\nFor every x-candidate I 6∈ Si we have met the goal for the iteration. If Si is very small for all\\niterations, then the set of certified boxes will suffice for the min-cost path algorithm to output a\\ngood approximation. But if Si is not small, another approach is needed.\\nReducing the number of candidates explored: the diagonal extension algorithm. For\\neach x-candidate I, although it suffices to certify the single box (I, Jτ (I)) with a good upper bound,\\nsince τ is unknown, the exhaustive and dense case approaches both include certified boxes for all\\ny-candidates J . The potential savings in the dense case approach comes from certifying many boxes\\nsimultaneously using a relatively small number of edit distance computations.\\nHere’s another approach: for each x-candidate I try to quickly identify a relatively small subset\\nY(I) of y-candidates that is guaranteed to include Jτ (I). If we succeed, then the number of boxes\\nwe certify is significantly reduced, and even paying quadratic time per certified box, we will have a\\nsub-quadratic algorithm.\\nWe need the notion of diagonal extension of a box. Themain diagonal of box I×J , is the segment\\njoining the lower left and upper right corners. The square box I ′ × J ′ is a diagonal extension of a\\nsquare subbox I × J if the main diagonal of I × J is a subsegment of the main diagonal of I ′ × J ′.\\n(see Definition 2.) Given square box I×J and I ′ ⊂ I the diagonal extension of I×J with respect to\\nI ′ is the unique diagonal extension of I × J having x-interval I ′. The key observation (Proposition\\n3.5) is: if I × J is an adequate cover of τI then any diagonal extension I ′ × J ′ is an adequate cover\\nof τI′ .\\nNow let w1, w2 be two numbers with w1|w2 and w2|n. (Think of w1 = n1/10 and w2 = n3/10.) We\\nuse the decomposition Iw2 of {0, . . . , n} into intervals of width w2. The set of y-candidates consists\\nθ-aligned vertical intervals of width w2 and has size\\nn\\nθw2\\n. To identify a small set of potential matches\\n5\\nfor I ′ ∈ Iw2 , we will identify a set (of size much smaller than nw2 ) of w1-boxes B(I ′) having x-interval\\nin Iw1(I ′) (the decomposition of I ′ into width w1 intervals). For each box in B(I ′) we determine the\\ndiagonal extension I ′× J ′ with respect to I ′, compute κ = ∆edit(I ′, J ′) and certify (I ′× J ′, κ). Our\\nhope is that B(I ′) includes a τ -compatible w1-box I ′′ × Jτ (I ′′), then the observation above implies\\nthat its diagonal extension provides an adequate cover for τI′ .\\nHere’s how to build B(I ′): Randomly select a polylog(n) size set H(I ′) of w1-intervals from\\nIw1(I ′). For each I ′′ ∈ H(I ′) compute ∆edit(I ′′, J ′′) for each y-candidate J ′′, and let J (I ′′) consist\\nof the d candidates J ′′ with smallest edit distance to I ′′. Here d is a parameter; think of d = n1/5\\nas before. B(I ′) consists of all I ′′ × J ′′ where I ′′ ∈ H(I ′) and J ′′ ∈ J (I ′′).\\nTo bound runtime: Each I ′ ∈ Iw2 requires O˜( nθw1 ) width-w1 ∆edit() computations, taking time\\nO˜(nw1θ ). Diagonal extension step requires O˜(d) width-w2 ∆edit() computations, for time O˜(dw\\n2\\n2).\\nSumming over nw2 choices for I\\n′ gives time O˜(n2 w1θw2 + ndw2) (with sample parameters: O˜(n\\n1.82)).\\nWhy should B(I ′) include a box that is an adequate approximation to τI′? The intuition behind\\nthe choice of B(I ′) is that an adequate cover for τI′ should typically be among the cheapest boxes\\nof the form I ′ × J ′, and if I ′ × J ′ is cheap then for a randomly chosen w1-subinterval I ′′, we should\\nalso have I ′′ × Jτ (I ′′) is among the cheapest boxes for I ′′.\\nClearly this intuition is faulty: I ′ may have many inexpensive matches J ′ such that I ′×J ′ is far\\nfrom τI′ , which may all be much cheaper than the match we are looking for. In this bad situation,\\nthere are many y-intervals J ′ such that ∆edit(I ′, J ′) is smaller than the match we are looking, and\\nthis is reminiscent of the good situation for the dense case algorithm, where we hope that I ′ has\\nlots of close matches. This suggests combining the two approaches, and leads to our full covering\\nalgorithm.\\nThe full covering algorithm. Given the dense case and diagonal extension algorithms, the full\\ncovering algorithm is easy to describe. The parameters w1, w2, d are as above. We iterate over\\ni ∈ {0, . . . , log n} with ǫi = 2−i. In iteration i, we first run the dense case algorithm, and let Si\\nbe the set of intervals declared sparse. Then run the diagonal extension algorithm described earlier\\n(with small modifications): For each w2-interval I\\n′, select H(I ′) = Hi(I ′) to consist of θ(log2 n)\\nindependent random selections from Si. For each I ′′ ∈ Hi(I ′), find the set of vertical candidates J ′′\\nfor which ∆edit(I\\n′′, J ′′) ≤ ǫi. Since I ′′ is (almost certainly) d-sparse, the number of such J ′′ is at\\nmost d. Proceeding as in the diagonal extension algorithm, we produce a set Pi(I ′) of O˜(d) certified\\nw2-boxes with x-interval I\\n′. Let RD (resp. RE) be the set of all certified boxes produced by the\\ndense case iterations, resp. diagonal extension iterations. The output is R = RD∪RE. (See Figure\\n1 for an illustration of the output R.)\\nThe runtime is the sum of the runtimes of the dense case and diagonal extension algorithms, as\\nanalyzed above. Later, we will give a more precise runtime analysis for the pseudo-code.\\nTo finish this extended overview, we sketch the argument that R satisfies the covering phase\\nrequirements.\\nClaim 2.1. Let I ′ be an interval in the w2-decomposition. Either (1) the output of the dense case\\nalgorithm includes a sequence of certified w1-boxes that adequately approximates the subpath τI′,\\nor (2) with high probability the output of the sparse case algorithm includes a single w2-box that\\nadequately approximates τI′.\\n(This claim is formalized in Claim 3.12.) Stitching together the subpaths for all I ′ implies that\\nR will contain a sequence of certified boxes that adequately approximates τ .\\n6\\nTo prove the claim, we establish a sufficient condition for each of the two conclusion and show\\nthat if the sufficient condition for the second conclusion fails, then the sufficient condition for the\\nfirst holds.\\nJ\\nI w2 w1\\nFigure 1: Illustration of the Covering Algorithm: Blue boxes are low cost boxes in dense w1-strips,\\nwhile the yellow ones are in sparse w1-strips. The red line corresponds to the path τ that we are\\ntrying to cover. In each w2-strip, τ is covered by either a collection of many w1-boxes or it is covered\\nby a diagonal extension of a low cost w1-box. The various boxes might overlap vertically which is\\nnot shown in the picture.\\nLet I ′ denote the w1-decomposition Iw1(I ′) of I ′. Every interval I ′′ ∈ I ′ has a θ-aligned τ -match\\nJτ (I ′′). It will be shown (see Proposition 3.4), that ∆edit(I ′′, Jτ (I ′′)) ≤ 2 cost(τI′′ )µ(I′′) + θ. Let u(I ′′)\\ndenote this upper bound. Consider the first alternative in the claim. During the dense case iteration\\ni = 0, every interval is declared dense, and (I ′′ × Jτ (I ′′), 5) is in RD for all I ′′. To get an adequate\\napproximation, we try to show that later iterations provide much better upper bounds on these\\nboxes, i.e., (I ′′ × Jτ (I ′′), γ(I ′′)) ∈ RD for a small enough value of γ(I ′′). By definition of adequate\\napproximation, it is enough that\\n∑\\nI′′∈I′ γ(I\\n′′) ≤ c∑I′′∈I′ u(I ′′), for some c. Let t(I ′′) be the last\\n(largest) iteration for which ǫt(I′′) ≥ u(I ′′) and I ′′ 6∈ St(I′′) (which is well defined since S0 = ∅). Let\\nb(I ′′) = ǫt(I′′). Since b(I ′′) ≥ u(I ′′) ≥ ∆edit(I ′′, Jτ (I ′′)), the box (I ′′ × Jτ (I ′′), 5b(I ′′)) is certified.\\nThe collection {(I ′′ × Jτ (I ′′), 5b(I ′′))} is a sequence of certified boxes that satisfies the first two\\nconditions for an adequate approximation of τ . The third condition will follow if:∑\\nI′′∈I′\\n5b(I ′′) ≤ c\\n∑\\nI′′∈I′\\nu(I ′′) (1)\\nso this is sufficient to imply the first condition of the claim.\\nNext consider what we need for the second alternative to hold. Let Si(I ′) be the set of in-\\ntervals declared sparse in iteration i. An interval I ′′ ∈ Si(I ′) is a winner (for iteration i) if\\n7\\n∆edit(I\\n′′, Jτ (I ′′)) ≤ ǫi, and Wi(I ′) is the set of winners. In iteration i of the diagonal extension\\nalgorithm, we sample θ(log2 n) elements of Si(I ′). If for at least one iteration i our sample includes\\na winner I ′′ then the second condition of the claim will hold: I ′′× Jτ (I ′′) is extended diagonally to\\na w2-box, and by the diagonal extension property, the extension is an adequate cover of τI′ , which\\nwe will certify with its exact edit distance.\\nThus for the second alternative to fail with nonnegligible probability:\\nFor all i, |Wi(I ′)| < |Si(I ′)−Wi(I ′)|, (2)\\nWe argue that if the failure condition (2) holds, then the success condition (1) holds. Multiply\\n(2) by ǫi and sum on i to get:∑\\nI′′∈I′\\n∑\\ni:I′′∈Wi(I′)\\nǫi <\\n∑\\nI′′∈I′\\n∑\\ni:I′′∈Si(I′)−Wi(I′)\\nǫi. (3)\\nFor a given interval I ′′ ∈ Iw1(I ′), consider the iterations i for which I ′′ ∈ Wi(I ′) and those for\\nwhich I ′′ ∈ Si(I ′)−Wi(I ′). First of all if ǫi ≥ u(I ′′) and I ′′ ∈ Si(I ′) then since ∆edit(I ′′, Jτ (I ′′)) ≤\\nu(I ′′) ≤ ǫi we conclude I ′′ ∈ Wi(I ′). So I ′′ ∈ Si(I ′) −Wi(I ′) implies that ǫi < u(I ′′), so the inner\\nsum of the right side of (3) is at most 2u(I ′′) (by summing a geometric series).\\nFurthermore, for i with u(I ′′) ≤ ǫi < b(I ′′), I ′′ ∈ Si by the choice of t(I ′′). Either b(I ′′)/2 ≤ u(I ′′)\\nor u(I ′′) < b(I ′′)/2. The latter implies I ′′ ∈ Wt(I′′)+1(I ′), and then b(I ′′)/2 is upper bounded by\\nthe inner sum on the left of (3). Therefore:\\n∑\\nI′′\\nb(I ′′) ≤\\n∑\\nI′′\\n\\uf8eb\\uf8ed2u(I ′′) + ∑\\ni:I′′∈Wi(I′)\\n2ǫi\\n\\uf8f6\\uf8f8\\n<\\n∑\\nI′′\\n\\uf8eb\\uf8ed2u(I ′′) + 2 ∑\\ni:I′′∈Si(I′)−Wi(I′)\\nǫi\\n\\uf8f6\\uf8f8\\n≤ 6\\n∑\\nI′′\\nu(I ′′),\\nas required for (1).\\nThis completes the overview of the covering algorithm.\\n3 Covering Algorithm:pseudo-code and analysis\\nThe pseudo-code consists of CoveringAlgorithm which calls procedures DenseStripRemoval (the\\ndense case algorithm) and SparseStripExtensionSampling (the diagonal extension algorithm). These\\nare abbreviated, respectively by CA, DSR and SSES. The technical differences between the pseudo-\\ncode and the informal description, are mainly to improve runtime analysis.\\n3.1 Pseudo-code\\nThe parameters of CA are as described in the overview: x, y are input strings of length n, θ comes\\nfrom GAP-UBθ, w1 < w2 < n and d < n are integral powers of 2, as are the auxiliary input\\n8\\nparameters. The output is a set R of certified boxes. The algorithm uses global constants c0 ≥ 0\\nand c1 ≥ 120, where the former one is needed for Proposition 3.8.\\nWe use a subroutine SMALL-ED which takes strings z1, z2 of length w and parameter κ and\\noutputs∞ if ∆edit(z1, z2) > κ and otherwise outputs ∆edit(z1, z2). The algorithm of [19] implements\\nSMALL-ED in time O(κw2).\\nOne technical difference from the overview, is that the pseudo-code saves time by restricting the\\nsearch for certified boxes to a portion of the grid close to the main diagonal. Recall that GAP-UBθ\\nhas two requirements, that the output upper bounds dedit(x, y) (which will be guaranteed by the\\nrequirement that R contains no falsely certified boxes), and that if dedit(x, y) ≤ θn, the output is\\nat most cθn for some constant c. We therefore design our algorithm assuming dedit(x, y) ≤ θn, in\\nwhich case every min-cost Gx,y-path τ consists entirely of points within\\nθ\\n2n steps from the main\\ndiagonal, i.e. |i− j| ≤ θ2n. So we restrict our search for certified boxes as follows: set m = 14θn, and\\nconsider the nm overlapping equally spaced boxes of width 8m = 2θn lying along the main diagonal.\\nTogether these boxes cover all points within θn of the main diagonal.\\nThe algorithm of the overview is executed separately on each of these n/m boxes. Within each\\nof these executions, we iterate over i ∈ {0, . . . , log 1θ} (rather than {0, . . . , log n} as in the overview).\\nIn each iteration we apply the dense case algorithm and the diagonal extension algorithm as in the\\noverview. The output is the union over all n/m boxes and all iterations, of the boxes produced.\\nIn the procedures DSR and SSES, the input G is an induced grid graph corresponding to a box\\nIG×JG, as described in the \"framework\" part of Section 1. The procedure DSR on input G, sets T\\nto be the w1-decomposition of IG (the x-candidates) and B to be the set of ǫi8 -aligned y-candidates.\\nAs in the overview, the dense case algorithm produces a set of certified boxes (called R1 in the\\npseudo-code) and a set S of intervals declared sparse. SSES is invoked if S 6= ∅ and iterates over\\nall x-intervals I ′ in the decomposition Iw2(IG). The algorithm skips I ′ if S contains no subset of\\nI ′, and otherwise selects a sample H of θ(log2 n) subintervals of I ′ from S. For each sample interval\\nI ′′ it finds the vertical candidates J ′′ for which ∆edit(I ′′, J ′′) ≤ ǫi, does a diagonal extension to I ′\\nand certifies each box with an exact edit distance computation.\\nThere are a few parameter changes from the overview that provide some improvement in the\\ntime analysis: During each iteration i, rather than take our vertical candidates to be from a θ-\\naligned grid, we can afford a coarser grid that is ǫi/8-aligned. Also, the local parameter d in DSR\\nand SSES is set to d/ǫi during iteration i.\\nThere is one counterintuitive quirk in SSES: each certified box is replicated O(log n) times\\nwith higher distance bounds. This is permissible (increasing the distance bound cannot decertify\\na box), but seems silly (why add the same box with a higher distance bound?). This is just a\\nconvenient technical device to ensure that the second phase min-cost path algorithm gives a good\\napproximation.\\nFor the analysis we must prove that R contains an \"adequate approximation\" of some min-cost\\nalignment path τ . To state this precisely, we start with definitions and observations that formalize\\nintuitive notions from the overview.\\nCost and normalized cost. The cost of a path τ , cost(τ), from (u1, u2) to (v1, v2) in a grid-\\ngraph (see Section 1), is the sum of the edge costs, and the normalized cost is ncost(τ) = cost(τ)v1−u1 .\\ncost(G(I×J)) (or simply cost(I×J)), the cost of subgraph G(I×J), is the min-cost of a path from\\nthe lower left to the upper right corner. The normalized cost is ncost(I × J) = 1µ(I)cost(I × J).\\nWe note the following simple fact without proof:\\n9\\nAlgorithm 1 CA(x, y, n,w1, w2, d, θ)\\nCoveringAlgorithm\\nInput: Strings x, y of length n, w1, w2, d ∈ [n], w1 < w2 < θn/4, and θ ∈ [0, 1]. n,w1, w2, θ are\\npowers of 2.\\nOutput: A set R of certified boxes in G.\\n1: Initialization: G = Gx,y, RD = RE = ∅.\\n2: Let m = θn4\\n3: for k = 0, . . . , 4θ do\\n4: Let I = J = {km, km+ 1, . . . , (k + 8)m}.\\n5: for i = ⌈log 1/θ⌉, . . . , 0 do\\n6: Set ǫi = 2\\n−i.\\n7: Invoke DSR(G(I × J), n, w1, dǫi ,\\nǫi\\n8 , ǫi) to get S and R1.\\n8: if S 6= ∅ then\\n9: Invoke SSES(G(I × J),S, n, w1, w2, dǫi ,\\nǫi\\n8 , ǫi, θ) to get R2.\\n10: else\\n11: R2 = ∅.\\n12: end if\\n13: Add items from R1 to RD and from R2 to RE .\\n14: end for\\n15: end for\\n16: Output R = RD ∪RE .\\nProposition 3.1. For I, J, J ′ ⊆ {0, . . . , n}, |dedit(xI , yJ)−dedit(xI , yJ ′)| ≤ |J∆J ′|, where ∆ denotes\\nsymmetric difference.\\nProjections and subpaths. The horizontal projection of a path τ = (i1, j1), . . . , (iℓ, jℓ) is the\\nset of {i1, . . . , iℓ}. We say that τ crosses box I × J if the vertices of τ belong to I × J and its\\nhorizontal projection is I. If the horizontal projection of τ contains I ′, τI′ denotes the (unique)\\nminimal subpath of τ whose projection is I ′.\\nProposition 3.2. Let τ be a path with horizontal projection I, and let I1, . . . , Iℓ be a decomposition\\nof I. Then the τIj are edge-disjoint and so:\\ncost(τ) ≥\\nℓ∑\\ni=1\\ncost(τIi)\\nncost(τ) ≥\\nℓ∑\\ni=1\\nµ(Ii)\\nµ(I)\\nncost(τIi).\\nDefinition 1. (1 − δ)-cover. Let τ be a path with horizontal projection I and let I ′ × J ′ be a\\n(not necessarily square) box with I ′ ⊆ I. For δ ∈ [0, 1] the box I ′ × J ′ (1 − δ)-covers τ if the\\ninitial, resp. final, vertex of the subpath τI′ is within δµ(I\\n′) vertical units of (min(I ′),min(J ′)),\\nresp. (max(I ′),max(J ′)).\\nProposition 3.3. Let I ′ × J ′ be a (not necessarily square) box that (1− δ)-covers path τ .\\n10\\nAlgorithm 2 DSR(G,n,w, d, δ, ǫ)\\nDenseStripRemoval\\nInput: G = Gx,y(IG × JG) for some IG, JG ⊆ {0, 1, . . . , n}, w, d ∈ [n], the endpoints of IG and JG\\nare multiples of w and δ, ǫ ∈ [0, 1].\\nOutput: Set S which is a subset of the w-decomposition of IG and a set R of δ-aligned certified\\nw-boxes all with distance bound 5ǫi.\\n1: Initialization: S = R = ∅. T = Iw(IG).\\n2: B, the set of y-candidates, is the set of width w δ-aligned subintervals of JG (having endpoints\\na multiple of δw.)\\n3: while T is non-empty do\\n4: Pick I ∈ T\\n5: Sample c0|B|1d log n intervals J ∈ B uniformly at random and for each test if ∆edit(xI , yJ) ≤ ǫ.\\n6: if for at most c02 log n sampled J ’s, SMALL-ED(xI , yJ , ǫ) <∞ then\\n7: S = S ∪ {I}; T = T − {I}. (I is declared sparse)\\n8: else\\n9: (I is declared dense and used as a pivot)\\n10: Compute:\\n11: Y = {J ∈ B; SMALL-ED(xI , yJ , 3ǫ) <∞}.\\n12: X = {I ′ ∈ T ; SMALL-ED(xI , xI′ , 2ǫ) <∞}.\\n13: Add (I ′ × J ′, 5ǫ) to R for all pairs (I ′, J ′) ∈ X × Y.\\n14: T = T − X .\\n15: end if\\n16: end while\\n17: Output S and R.\\n11\\nAlgorithm 3 SSES(G,S, n, w1, w2, d, δ, ǫ, θ)\\nSparseStripExtensionSampling\\nInput: G = Gx,y(IG, JG) with IG, JG ⊆ {0, 1, . . . , n}, w1, w2, d, n are powers of 2, with w1, w2, d < n\\nand w1 < w2. Endpoints of IG and JG are multiples of w2, S is a subset of the w1-decomposition\\nof IG and δ, ǫ, θ are non-positive integral powers of 2.\\nOutput: A set R of certified w2-boxes in G.\\n1: Initialization: R = ∅.\\n2: B, the set of y-candidates, is the set of width w δ-aligned subintervals of JG (endpoints are\\nmultiples of δw.)\\n3: for I ′ ∈ Iw2(IG) do\\n4: if S includes a subset of I ′ then\\n5: Select c1 log\\n2 n intervals I ∈ S independently and uniformly at random from Iw1(I ′) ∩ S,\\nto obtain H.\\n6: for each I ∈ H and each J ∈ B do\\n7: if SMALL-ED(xI , yJ , ǫ) <∞ then\\n8: Let J ′ be such that I ′ × J ′ is the diagonal extension of I × J in I ′ × JG.\\n9: Let p = SMALL-ED(xI′ , yJ ′ , 3ǫ)\\n10: if p <∞ then\\n11: For k = 0, . . . , log n, add (I ′ × J ′, p+ θ + 2−k) to R.\\n12: end if\\n13: end if\\n14: end for\\n15: end if\\n16: end for\\n17: Output R.\\n12\\n1. ncost(I ′ × J ′) ≤ ncost(τI′) + 2δ.\\n2. If J ′′ is any vertical interval, then I ′ × J ′′ (1− δ − |J ′∆J ′′|/µ(I ′)) covers τ .\\nProof. For the first part, let J0 be the vertical projection of τI′ . Then ncost(I\\n′ × J0) ≤ ncost(τI′)\\nsince τI′ joins the lower left corner of I\\n′ × J0 to the upper right corner. Since I ′ × J ′ (1− δ)-covers\\nτ , |J ′∆J0| ≤ 2δµ(I ′), and by Proposition 3.1, ncost(I ′ × J ′) ≤ ncost(τI′) + 2δ.\\nFor the second part, observe that the vertical distance between the lower (resp. upper) corners\\nof I ′ × J ′ and I ′ × J ′′ is at most |J ′∆J ′′|.\\nδ-aligned boxes A y-interval J of width w is δ-aligned for δ ∈ (0, 1) if its endpoints are multiples\\nof δw (which we require to be an integer).\\nProposition 3.4. Let τ be a path that crosses I × J . Suppose that I ′ ⊆ I has width w, and\\nµ(J) ≥ w.\\n1. There is an interval J1 with µ(J1) = µ(I ′) so that ncost(I ′ × J1) ≤ 2ncost(τI′) and I ′ × J1\\n(1− ncost(τI′))-covers τ .\\n2. There is a δ-aligned interval J ′ ⊆ J of width w so that ncost(I ′ × J ′) ≤ 2ncost(τI′) + δ and\\nI ′ × J ′ (1 − ncost(τI′)− δ)-covers τ.\\n(J1, J ′ are “τ -matches” for I ′, in the sense of the overview.)\\nProof. Let τ ′ = τI′ be the min-cost subpath of τ that projects to I ′. Let J0 be the vertical projection\\nof τ ′. Note that |µ(J0) − µ(I ′)| ≤ cost(τ ′). Arbitrarily choose an interval J1 of width µ(I ′) that\\neither contains or is contained in J0. Then |J0∆J1| = |µ(J0)− µ(I ′)| ≤ cost(τ ′), so by Proposition\\n3.1 ncost(I ′×J1) ≤ 2ncost(τ ′). Furthermore I ′×J1 (1−ncost(τ ′)) covers τ ′. Let J ′ be the closest δ-\\naligned interval to J1, so |J ′∆J1| ≤ δµ(I ′) and so ncost(I ′×J ′) ≤ ncost(I ′×J1)+δ ≤ 2ncost(τ ′)+δ.\\nFinally since I ′ × J ′ is a vertical shift of I ′ × J1 of normalized length at most δ, we have I ′ × J ′\\n(1− ncost(τ ′)− δ) covers τ ′.\\nDefinition 2. 1. The main diagonal of a box is the segment joining the lower left and upper\\nright corners.\\n2. For a square box I ′ × J ′, and I ′ ⊆ I, the true diagonal extension of I ′ × J ′ to I is the square\\nbox I × Jˆ whose main diagonal contains the main diagonal of I ′ × J ′.\\n3. For a w-box I ′× J ′ contained in strip I × J , the adjusted diagonal extension of I ′× J ′ within\\nI×J is the box I×J ′′ obtained from the true diagonal extension of I ′×J ′ to I by the minimal\\nvertical shift so that it is a subset of I × J . (The adjusted diagonal extension is the true\\ndiagonal extension if the true diagonal extension is contained in I × J ; otherwise it’s lower\\nedge is min(J) or its upper edge is max(J).)\\nProposition 3.5. Suppose path τ crosses I × J and ncost(τI) ≤ ǫ. Let w = µ(I). Let I ′ × J ′ be a\\nw′-box that (1− δ)-covers τI′. Then the adjusted diagonal extension I × J ′′ of I ′ × J ′ within I × J\\n(1− (ǫ+ δw′w ))-covers τ and satisfies ncost(I × J ′′) ≤ 3ǫ+ 2δw\\n′\\nw .\\n13\\nProof. It suffices to show that I × J ′′ (1 − (ǫ + δw′/w))-covers τ , since then Proposition 3.3 gives\\nus the needed upper bound on ncost(I × J ′′).\\nCase 1. I × J ′′ is equal to the true diagonal extension. If ǫ ≥ 1, the claim follows trivially, so we\\ncan assume ǫ < 1. Let τI , τI′ be the min-cost subpath of τ that projects on I and I\\n′ respectively.\\nWe will give an upper bound on the vertical distance from the final vertex of τ to the upper\\nright corner of I × J ′′. Let τu be the subpath of τ that starts at the final vertex of τI′ and ends\\nat the final vertex of τI . Let Iu and Ju be the horizontal and vertical projections of τu. The start\\nvertex of τu has vertical distance at most δw\\n′ from the main diagonal of I × J ′′. The final vertex of\\nτu therefore has vertical distance at most δw\\n′+ |µ(Iu)−µ(Ju)| from the upper corner of I×J ′′, and\\nthis is at most δw′ + ǫw, since cost(τ) ≥ |µ(Iu)− µ(Ju)|. A similar argument gives the same upper\\nbound on the vertical distance between the start vertex of τI and the lower left corner of I × J ′′, so\\nG′′(I × J ′′) (1− (ǫ+ δw′/w))-covers τ .\\nCase 2. I × J ′′ is not the true diagonal extension. Extend the set J to J¯ by adding µ(I) elements\\nbefore and after. (It is possible that J¯ is not a subset of {0, . . . , n}; in this case we imagine that y is\\nextended to a sequence y∗ by adding µ(I) new symbols to the beginning and end of y and that we\\nare in the grid graph Gx,y∗ .) Let I×J ′′′ be the adjusted diagonal extension of I ′×J ′ to I× J¯ . This\\nis equal to the true diagonal extension, and so by Case 1, I × J ′′′ (1 − (ǫ + δw′/w))-covers τ . We\\nclaim that I × J ′′ does also. Assume J ′′′ falls below min(J) (the case that J ′′′ is above max(J) is\\nsimilar). Then I × J ′′ is obtained by shifting I × J ′′′ up until the lower edge coincides with min(J).\\nThe lower vertex of τI has y-coordinate at least min(J).\\nIf the y-coordinate of the upper vertex of τI is at most max(J\\n′′), then J ′′ contains vertical\\nprojection of τI , and I × J ′′ (1 − ǫ)-covers τ . If the y-coordinate of the upper vertex of τI is\\ngreater than max(J ′′), shifting I × J ′′′ up to I × J ′′ can only decrease the vertical distance from\\nthe the lower left corner to the start of τI and from the upper corner to the end of τI , so I × J ′′\\n(1− (ǫ+ δw′/w))-covers τ .\\n(k, ζ)-approximation of a path. This formalizes the notion of adequate approximation of a\\npath by a certified box sequence.\\nDefinition 3. Let G be a grid graph on I × J . Let ζ, ǫ ∈ [0, 1]. Let τ be a path that crosses G.\\nA sequence of certified boxes σ = {(I1 × J1, ǫ1), (I2 × J2, ǫ2), . . . , (Iℓ × Jℓ, ǫℓ)} (k, ζ)-approximates τ\\nprovided that:\\n1. I1, . . . , Iℓ is a decomposition of I.\\n2. For each i ∈ [ℓ], Ii × Ji (1− ǫi)-covers τ .\\n3.\\n∑\\ni∈[ℓ] ǫiµ(Ii) ≤ (k · ncost(τ) + ζ)µ(I).\\nProposition 3.6. Suppose path τ crosses I × J and I1, . . . , Im is a decomposition of I, and for i ∈\\n[m], σi is a certified box sequence that (k, ζ)-approximates τIi. Then σ1, . . . , σm (k, ζ)-approximates\\nτ .\\nProof. It is obvious that σ is a sequence of certified boxes, that the horizontal projections of all\\nthe boxes form a decomposition of I and that each box (Ii, Ji, ǫi) (1 − ǫi)-covers τ . The final\\ncondition is verified by splitting the sum on the left into m sums where the jth sum includes terms\\nfor Ii ⊆ Ij, and is bounded above by (k · ncost(τIj + ζ)µ(Ij). Summing the latter sum over j and\\nusing Proposition 3.2 we get that σ (k, ζ)-approximates the path τ .\\n14\\n(d, δ, ǫ)-dense and -sparse. Fix a box I × J . An interval I ′ ⊆ I of width w is (d, δ, ǫ)-sparse (wrt\\nI × J) for integer d and ǫ, δ ∈ (0, 1] if there are at most d δ-aligned w-boxes in I ′ × J of ncost at\\nmost ǫ, and is (d, δ, ǫ)-dense otherwise.\\nThe sets Si and Si(I ′). For fixed k in the outer loop of CA, the set S created in iteration i of CA\\nis denoted by Si. For any interval I ′, Si(I ′) is the set of subintervals of I ′ belonging to Si.\\nSuccessful Sampling. The algorithm uses random sampling in two places, in the i loop inside\\nCA and within the conditional on S containing a set from Iw1(I ′) in SSES. We now specify what\\nwe need from the random sampling.\\nDefinition 4. A run of the algorithm has successful sampling provided that for all k ∈ {0, . . . , 4/θ}\\nand i ∈ {0, . . . , log 1θ} in the nested CA loops:\\n• For every w1 interval I with endpoints a multiple of w1, if I is ( dǫi ,\\nǫi\\n8 , ǫi)-dense interval (in\\nterms of global parameters), DSR does not assign I to S and if I is ( d4ǫi ,\\nǫi\\n8 , ǫi)-sparse, DSR\\nplaces I in S.\\n• On all calls to SSES, for every w2 interval I with endpoints a multiple of w2, if |Wi(I)| has size\\nat least |Si(I) −Wi(I)|/32 then the sample H selected contains an element of Wi(I). (Here\\nSi(I) and Wi(I) are that defined in the proof of Claim 3.12, whose definitions don’t depend\\non the randomness used to select H.)\\nWe will need the following variant of the Chernoff bound.\\nProposition 3.7 (Chernoff bound). There is a constant c0 such that the following is true. Let\\n1 ≤ d ≤ n be integers, B be a set and E ⊆ B. Let us sample c0 |B|d log n samples from B independently\\nat random with replacement.\\n1. If |E| ≥ d then the probability that less than c02 log n samples are from E is at most 1/n10.\\n2. If |E| ≤ d/4 then the probability that at least c02 log n samples are from E is at most 1/n10.\\nProposition 3.8. For large enough n, a run of CA has successful sampling with probability at least\\n1− n−7.\\nProof. By Proposition 3.7, the probability that the first condition fails for a particular k, i, I is at\\nmost n−10. The number of choices for k, i, I is at most (4θ +1) · (1+ log 1θ ) nw1 ≤ n2 (for large enough\\nn) so the overall probability that (1) fails is at most n−8.\\nThe probability that the second condition fails for a particular k, i, I is (1 − 132)c1 log\\n2 n ≤ n−10.\\nThe number of k, i, I is less than n2 (for large enough n), so the overall failure probability is at most\\nn−8 .\\nWe assume that coins are fixed in a way that gives successful sampling.\\n3.2 Properties of the covering algorithm\\nThe main property of CA to be proved is:\\n15\\nTheorem 3.9. Let x, y be strings of length n, 1/n ≤ θ ≤ 1 be a real. Let w1, w2, d satisfy w1 ≤ θw2,\\nw2 ≤ θn4 and 1 ≤ d ≤ θnw1 . Assume n,w1, w2, d, θ are powers of 2. Let R be the set of weighted\\nboxes obtained by running CA(x, y, n,w1, w2, d, θ) with c1 > 120. Then (1) Every (I × J, ǫ) ∈ R\\nis correctly certified, i.e., ∆edit(xI , yJ) ≤ ǫ, and (2) In a run that satisfies successful sampling, for\\nevery path τ from the source to the sink in G = Gx,y of cost at most θ there is a subset of R that\\n(45, 15θ)-approximates τ .\\nProof. All boxes output are correctly certified: Each box in RE comes from SSES which only\\ncertifies boxes with atleast their exact edit distance. For (I × J, ǫ) ∈ RD, there must be an I ′ such\\nthat ∆edit(xI′ , yJ) ≤ 35 · ǫ and ∆edit(xI′ , xI) ≤ 25 · ǫ and so by triangle inequality ∆edit(xI , yJ) ≤ ǫ.\\nIt remains to establish (2). Fix a source-sink path τ of normalized cost κ. By Proposition 3.6\\nit is enough to show that for each I ′ ∈ Iw2 , R contains a box sequence that (45, 15θ)-approximates\\nτI′ . So we fix I\\n′ ∈ Iw2.\\nThe main loop (on k) of CA processes G in overlapping boxes. Since ncost(τ) ≤ θ, one of these\\nboxes, which we’ll call I × J , must contain τI′ .\\nClaim 3.10. Let I ′ ∈ Iw2. There exist intervals I, J ⊆ N, I = J that are enumerated in the main\\nloop of CA such that I ′ ⊆ I and τI′ crosses G(I ′ × J).\\nProof. Since τ is of cost at most θ, it cannot use more than θn/2 horizontal edges as for each\\nhorizontal edge of cost 1, it must use one vertical edge of cost 1. Similarly for vertical edges. So τ is\\nconfined to diagonals {−θn/2, . . . , 0, . . . , θn/2} of G. By the choice ofm in CA, there will be I and J\\nconsidered in the main loop of the algorithm such that I ′ ⊆ I and τI′ crosses G(I×J). In particular,\\nI = J = {km, km + 1, . . . , (k + 8)m}, where k is the largest integer such that km ≤ min(I ′) − θn2\\nhas the desired property.\\nLet I, J be as provided by the claim. Let I ′ be the w1-decomposition of I ′. We will show\\none of the following must hold: (1) RD contains a sequence of certified w1-boxes that (45, 15θ)-\\napproximates τI′ , or (2) There is a single certified w2-box in RE that (45, 15θ)-approximates τI′ .\\nLet t = log 1θ . For i = t, . . . , 0, let ǫi = 2\\n−i and let Si be the set S obtained at the iteration i of\\nCA(x, y, n,w1, w2, d, θ).\\nWe note:\\nClaim 3.11. Let i ∈ {0, . . . , log 1/θ}. Suppose I ′′ ∈ Iw1(I) and J ′′ ⊆ J is ǫi/8-aligned. If I ′′ 6∈ Si\\nand cost(I ′′ × J ′′) ≤ ǫi then (I ′′ × J ′′, 5ǫi) ∈ RD.\\nProof. If I ′′ 6∈ Si then in the call to DSR(G(I × J), n, w1, d/ǫi, ǫi/8, ǫi) there is an iteration of\\nthe main loop,where the selected interval I˜ from T is declared dense and ∆edit(xI˜ , xI′′) ≤ 2ǫi.\\nSince ∆edit(xI′′ , yJ ′′) ≤ ǫi, ∆edit(xI˜ , yJ ′′) ≤ 3ǫi and so I ′′ ∈ X and J ′′ ∈ Y. Thus, DSR certifies\\n(I ′′ × J ′′, 5ǫi), which is added to RD.\\nThe theorem follows from:\\nClaim 3.12. For an interval I ′ ∈ Iw2, assuming successful sampling RE or RD contains a (45, 15θ)-\\napproximation of τI′.\\nThe proof is similar to that of Claim 2.1, with adjustments for some technicalities.\\n16\\nProof. Let τ ′ = τI′ and κ = ncost(τ ′). Let I ′ = Iw1(I ′). For I ′′ ∈ I ′, let κI′′ = ncost(τI′′). By\\nProposition 3.4, for all I ′′ ∈ I ′ and ǫi ≥ κI′′ there is an ǫi/8-aligned vertical interval Jτi (I ′′), such\\nthat ncost(I ′′ × Jτi (I ′′)) ≤ 2κI′′ + ǫi/8 and I ′′ × Jτi (I ′′) (1− κI′′ − ǫi/8)-covers τI′ .\\nLet s(I ′′) be the largest integer such that ǫs(I′′) ≥ 3κI′′ + κ+ θ. Let t(I ′′) ≤ s(I ′′) be the largest\\ninteger such that I ′′ 6∈ St(I′′). (Since θn/w1 ≥ d, S0 = ∅, so t(I ′′) is well-defined.) Let a(I ′′) = ǫs(I′′)\\n(this plays a similar role to u(I ′′) in Section 2) and b(I ′′) = ǫt(I′′).\\nFor all ǫi ∈ [a(I ′′), b(I ′′)], ncost(I ′′ × Jτi (I ′′)) ≤ ǫi and I ′′ × Jτi (I ′′) (1 − ǫi)-covers τ ′. By the\\ndefinition of b(I ′′) and Claim 3.11, RD contains the certified box (I ′′ × Jτt(I′′)(I ′′), 5bI′′). So RD\\ncontains a (45, 15θ)-approximation of τ ′ provided that:∑\\nI′′∈I′\\n5b(I ′′) ≤ 45\\n8\\n∑\\nI′′∈I′\\na(I ′′) (4)\\nsince a(I ′′) ≤ 2(3κI′′ + κ+ θ).\\nNext we determine a sufficient condition that RE contain a box sequence (consisting of a single\\nbox) that (5, 4θ)-approximates τ ′. Let Si(I ′) = Si∩I ′. Interval I ′′ ∈ Si(I ′) is a winner for iteration\\ni if ǫi ≥ a(I ′′). This set of winners is denoted by Wi(I ′). It suffices that during iteration i, the\\nset of c1 log\\n2 n samples taken in SSES includes a winner I ′′; then since ∆edit(I ′′, Jτi (I\\n′′)) ≤ ǫi, the\\n(adjusted) diagonal extension I ′ × J˜ of I ′′ × Jτi (I ′′) will be certified. By Proposition 3.5, I ′ × J˜\\nhas normalized cost at most 3κ + 2ǫiw1/w2 ≤ 3κ + 2θ ≤ 3ǫi and it (1 − (κ + θ))-covers τ ′. If\\nκ = 0 then (I ′ × J˜ ,ncost(I ′ × J˜) + θ + 2− logn) is in RE by the behavior of SSES and it (5, 4θ)-\\napproximates τ ′. Otherwise κ ≥ 1/n; so set k = ⌊log 1/κ⌋. Thus, k ≤ log n and 2−k ∈ [κ, 2κ). Then\\n(I ′ × J˜ ,ncost(I ′ × J˜) + θ + 2−k) is in RE and it (5, 4θ)-approximates τ ′.\\nUnder successful sampling if |Wi(I ′)| ≥ 132 |Si(I ′) − Wi(I ′)|, at least one interval from Wi(I ′)\\nwill be included in our c1 log\\n2 n samples during SSES and RE will contain a (5, 4θ)-approximation\\nof τ ′ as above. So suppose this fails:\\nFor all i, |Wi(I ′)| < 1\\n32\\n|Si(I ′)−Wi(I ′)|. (5)\\nWe show that this implies (4). Multiplying (5) by ǫi and summing on i yields:∑\\nI′′∈I′\\n∑\\ni:I′′∈Wi(I′)\\nǫi <\\n1\\n32\\n∑\\nI′′∈I′\\n∑\\ni:I′′∈Si(I′)−Wi(I′)\\nǫi. (6)\\nI ′′ ∈ Si(I ′)−Wi(I ′) implies ǫi < a(I ′′). Summing the geometric series:∑\\ni:I′′∈Si(I′)−Wi(I′)\\nǫi ≤ 2a(I ′′). (7)\\nEither a(I ′′) = b(I ′′) or a(I ′′) < b(I ′′). If the latter, then I ′′ ∈ Wi(I ′) for ǫi = b(I ′′)/2. So:∑\\nI′′∈I′\\nb(I ′′) ≤\\n∑\\nI′′\\n(\\na(I ′′) +\\n∑\\ni:I′′∈Wi(I′)\\n2ǫi\\n)\\n<\\n∑\\nI′′\\n(\\na(I ′′) +\\n1\\n16\\n∑\\ni:I′′∈Si(I′)−Wi(I′)\\nǫi\\n)\\n≤ 9\\n8\\n∑\\nI′′∈I′\\na(I ′′)\\n17\\nwhich implies Equation 4. (The second inequality follows from (6) and the last inequality from\\n(7).)\\n3.3 Time complexity of CA\\nWe write t(w, ǫ) for the time of SMALL-ED(z1, z2, ǫ) on strings of length w. We assume t(w, ǫ) ≥\\nw, and that for k ≥ 1, there is a constant c(k) such that for all ǫ ∈ [0, 1] and all w > 1, t(w, kǫ) ≤\\nc(k) · t(w, ǫ) + c(k). As mentioned earlier, by [19], we can use t(w, ǫ) = O(w2ǫ).\\nTheorem 3.13. Let n be a sufficiently large power of 2 and θ ∈ [1/n, 1] be a power of 2. Let x, y\\nbe strings of length n. Let log n ≤ w1 ≤ w2 ≤ θn/4, 1 ≤ d ≤ n be powers of 2, where w1|w2 and\\nw2|n, and w1/w2 ≤ θ. The size of the set R output by CA is O(( nw1 )2 log2 n) and in any run that\\nsatisfies successful sampling, CA runs in time:\\nO\\n(\\n|R|+\\n∑\\nk=log 1/θ,...,0\\nǫ=2−k\\n(θn2 log n\\ndǫw21\\n· t(w1, ǫ)+ θn\\n2 log2 n\\nw1w2ǫ\\n· t(w1, ǫ) + nd log\\n2 n\\nw2ǫ\\n· t(w2, ǫ)\\n))\\n.\\nProof. To bound |R| note that for each choice of k, i in the outer and inner loops of CA, the set of\\ncandidate boxes of width w1 has size O(\\nθn\\nw1\\nθn\\nw1ǫi\\n). This upper bounds the number of boxes certified\\nby DSR. The call to SSES constructs at most one diagonal extension for each such candidate box,\\nand each diagonal extension gives rise to at most O(log n) certified boxes. Thus, for each (k, i) there\\nare O(θ\\n2n2 logn\\n(w1)2ǫi\\n) certified boxes. Summing the geometric series over i, noting that min(ǫi) = θ, and\\nsumming over O(1/θ) values of k gives the required bound on |R|.\\nThe steps in the algorithm that actually construct certified boxes (13 of DSR, 11 of SSES, 13\\nof CA) cost O(1) per box giving the first term in the time bound.\\nWe next bound the other contributions to runtime. The outer loop of CA has 4θ + 1 iterations\\non k’s. The inner loop has 1+ log 1θ iterations on i. Each iteration invokes DSR and SSES on I × J\\nwith I and J of width at most 4θn.\\nWe bound the time of a call to DSR. To distinguish between local variables of DSR and global\\nvariables of CA, we denote local input variables as Gˆ, nˆ, wˆ, dˆ, δˆ, ǫˆ. For B and T as in DSR, |B| ≤\\nµ(I\\nGˆ\\n)\\nδˆwˆ\\n. since µ(IGˆ) = µ(JGˆ). The main while loop of DSR repeatedly picks intervals I ∈ T and\\nsamples c0|B| log nˆdˆ ≤\\nc0µ(IGˆ) log nˆ\\ndˆδˆwˆ\\nvertical intervals J and tests whether ∆edit(xI , yJ) ≤ ǫˆ. Each such\\ntest takes time t(wˆ, ǫˆ). This is done at most once for each of the µ(IGˆ)/wˆ horizontal candidates\\nfor a total time of O(\\nµ(I\\nGˆ\\n)2 log nˆ\\nwˆ2δˆdˆ\\n)t(wˆ, ǫˆ). We next bound the cost of processing a pivot I. This\\nrequires testing ∆edit(xI , yJ) ≤ 3ǫˆ for J ∈ B and ∆edit(xI , xI′) ≤ 2ǫˆ for I ′ ∈ T . Each test costs\\nO(t(wˆ, ǫˆ)) (by our assumption on t(·, ·)), and since |T | ≤ |B| = µ(IGˆ)\\nwˆδˆ\\n, I is processed in time\\nO(\\nµ(I\\nGˆ\\n)\\nwˆδˆ\\nt(wˆ, ǫˆ)). This is multiplied by the number of intervals declared dense, which we now upper\\nbound. If I is declared dense then at the end of processing I, X is removed from T . This ensures\\n∆edit(I, I\\n′) > 2ǫ for any two intervals I, I ′ declared dense. By the triangle inequality the sets\\nB(I) = {J ∈ B; ∆edit(xI , yJ) ≤ ǫ} are disjoint for different pivots. By successful sampling, for each\\npivot I, |B(I)| ≥ dˆ4 , and thus at most |B|/(dˆ/4) =\\n4µ(I\\nGˆ\\n)\\ndˆδˆwˆ\\nintervals are declared dense, so all intervals\\ndeclared dense are processed in time O(\\nµ(I\\nGˆ\\n)2\\nwˆ2dˆδˆ2\\n)t(wˆ, ǫˆ).\\n18\\nThe time for dense/sparse classification of intervals and for processing intervals declared dense\\nis at most O(\\nµ(I\\nGˆ\\n)2 log nˆ\\nwˆ2dˆδˆ2\\n)t(wˆ, ǫˆ). During iteration i of the inner loop of CA, the local variables of\\nDSR are set as nˆ = n, µ(IGˆ) ≤ 4θn, wˆ = w1, dˆ = d/ǫi, δˆ = ǫi/8. Substituting these parameters\\nyields time O(θ\\n2n2 logn\\n(w1)2dǫi\\n)t(w1, ǫi). Multiplying by the O(1/θ) iterations on k gives the first summand\\nof the theorem.\\nNext we turn to SSES. The local input variables n,w1, w2,S, θ are set to their global values so\\nwe denote them without ˆ . The other local input variables are denoted as Gˆ, dˆ, δˆ, ǫˆ. The local\\nvariable B has size µ(IGˆ)\\nδˆw1\\n. By successful sampling, we assume that on every call, every interval in S\\nis (dˆ, δˆ, ǫˆ)- sparse. The outer loop enumerates the µ(IGˆ)/w2 intervals I\\n′ of Iw2(IGˆ). We select H to\\nbe c1 log\\n2 n random subsets from subsets of I ′ belonging to S. For each I ∈ H and J ∈ B, we call\\nSMALL-ED(xI , yJ , ǫˆ), taking time t(w1, ǫˆ). The total time of all tests is O(\\nµ(I\\nGˆ\\n)2 log2 n\\nδˆw1w2\\n)t(w1, ǫˆ).\\nUsing dˆ = d/ǫi, δˆ = ǫi/8 and ǫˆ = ǫi from the ith call to SSES gives O(\\nθ2n2 log2 n\\nǫiw1w2\\n)t(w1, ǫi). Multiplying\\nby the O(1/θ) iterations on k gives the second summand in the theorem.\\nAssuming successful sampling, all intervals in the set S passed from DSR to SSES are (dˆ, δˆ, ǫˆ)-\\nsparse. Therefore, for each sampled I, at most dˆ intervals J are within ǫˆ of I. For each of these\\nwe do a diagonal extension of I × J to a w2-box I ′ × J ′, and call SMALL-ED(xI′ , yJ ′ , 3ǫˆ) at cost\\nO(t(w2, ǫˆ)) for each call. The number of such calls is O(\\nµ(I\\nGˆ\\n)dˆ log2 n\\nw2\\n). Using the parameter dˆ = d/ǫi\\nin the ith call of the inner iteration of CA, we get a cost of O(θnd log\\n2 n\\nǫiw2\\n)t(w2, ǫi) and multiplying by\\nthe O(1/θ) gives the third summand in the theorem.\\nChoosing the parameters to minimize the maximum term in the time bound, subject to the\\nrestrictions of the theorem and using t(w, ǫ) = O(ǫw2) we have:\\nCorollary 3.14. For all sufficient large n, and for θ ≥ n−1/5 (both powers of 2) choosing w1, w2,\\nand d to be the largest powers of two satisfying: w1 ≤ θ−2/7n1/7, w2 ≤ θ1/7n3/7, and d ≤ θ3/7n2/7,\\nwith probability at least 1− n−1/7, CA runs in time O˜(n12/7θ4/7), and outputs the set R of size at\\nmost O˜(n12/7θ4/7).\\nProof. Set w1, w2, and d to be the largest powers of two satisfying: w1 ≤ θ−2/7n1/7, w2 ≤ θ1/7n3/7,\\nand d ≤ θ3/7n2/7.\\nUse the algorithm of [19] that gives t(w, ǫ) = O(ǫw2). It is routine to check that these choices\\nsatisfy the requirements of Theorem 3.13, and also that all three terms in the time analysis, and\\nthe number of boxes are all bounded by the claimed bound.\\n4 Min-cost Paths in Shortcut Graphs\\nWe now describe the second phase of our algorithm, which uses the set R output by CA to upper\\nbound dedit(x, y). A shortcut graph on vertex set {0, . . . , n} × {0, . . . , n} consists of the H and V\\nedges of cost 1, together with an arbitrary collection of shortcut edges (i, j) → (i′, j′) where i < i′\\nand j < j′, also denoted by eI,J where I = {i, . . . , i′} and J = {j, . . . , j′}, along with their costs.\\nA certified graph (for x, y) is a shortcut graph where every shortcut edge eI,J has cost at least\\ndedit(xI , yJ). The min cost path from (0, 0) to (n, n) in a certified graph upper bounds dedit(x, y).\\nThe second phase algorithm uses R to construct a certified graph, and computes the min cost path\\nto upper bound on dedit(x, y).\\n19\\nA certified box (I × J, κ) corresponds to the eI,J with cost κµ(I). (In the certified graph we use\\nnon-normalized costs.) However, the certified graph built from R in this way may not have a path of\\ncost O(dedit(x, y)+θn). We need a modified conversion of (I×J, κ). If κ ≥ 1/2 we add no shortcut.\\nOtherwise (I × J, κ) converts to the edge eI,J ′ with cost 3κµ(I) where J ′ is obtained by shrinking\\nJ : min(J ′) = min(J) + ℓ and max(J ′) = max(J ′)− ℓ where ℓ = ⌊κµ(I)⌋. By Proposition 3.1, this\\nis a certified edge. Call the resulting graph G˜. We claim:\\nLemma 4.1. Let τ be a path from source to sink in Gx,y. If R contains a sequence σ that (k, θ)-\\napproximates τ then there is a source-sink path τ ′ in G˜ that consists of the shortcuts corresponding\\nto σ together with some H and V edges with costG˜(τ\\n′) ≤ 5(k · costGx,y(τ) + θn).\\nProof. We will modify the path τ in Gx,y to a path τ\\n′ in G˜ of comparable cost. Let {(I1×J1, ǫ1), (I2×\\nJ2, ǫ2), . . . , (Im×Jm, ǫm)} be the set of certified boxes that (k, θ)-approximates τ . Let ℓi = µ(Ii) ·ǫi.\\nLet L be the subset [m] for which ǫi ≤ 1/2. For i ∈ L, let ei = eIi,J ′i be the shortcut edge with\\nweight 3ǫi. We claim (1) there is a source-sink path in G˜ that consists of {ei : i ∈ L} together with\\na horizontal path Hi whose projection to the x-axis is Ii for each i ∈ [m] − L, and a collection of\\n(possibly empty) vertical paths V0, V1, . . . , Vm where the x-coordinate of Vi for i > 0 is max(Ii) and\\n0 for V0, and (2) its cost satisfies the bound of the Lemma.\\nFor the first claim, define for h ∈ [m] ph = (ih, jh) to be the first point in τIh and define pm+1 =\\n(n, n). We will define τ ′ to pass through all of the ph. In preparation, observe that for h ∈ L, since\\nIh×Jh (1− ǫh) covers τ , we have min(J ′h) = min(Jh)+ ℓh ≥ jh and max(J ′h) = max(J)− ℓh ≤ jh+1.\\nDefine the portion τ ′h between ph and ph+1 by climbing vertically from ph to (ih,min(J\\n′\\nh)) and if\\nh ∈ L traversing eIh,J ′h and climbing to ph+1 and if h 6∈ L then move horizontally from (ih,min(J ′h))\\nto (ih+1,min(J\\n′\\nh)) and then climb to ph+1.\\nFor the second claim, we upper bound cost(τ ′). For h ∈ L, eIh,Jh costs 3ℓh, and for h 6∈ L, the\\nhorizontal path that projects to Ih costs µ(Ih) ≤ 2ℓh; the total is at most\\n∑\\nh 3ℓh. The cost of vertical\\nedges is n−∑h∈L µ(J ′h) =∑h∈L(µ(Jh)−µ(J ′h))+∑h 6∈L µ(Jh) =∑h∈L 2ℓh+∑h 6∈L µ(Jh) ≤∑h 2ℓh,\\nsince\\n∑\\nh µ(Jh) =\\n∑\\nh µ(Ih) = n. So cost(τ\\n′) ≤ ∑h 5ℓh. Since ∑mi=1 ℓi ≤ k · costGx,y(τ) + θ · n by\\ndefinition of (k, θ)-approximation, the lemma follows.\\nComputing the min-cost. We present an O(n+m log(mn)) algorithm to find a min cost source-\\nsink path in a shortcut graph G˜ with m shortcuts. It’s easier to switch to the max-benefit problem:\\nLet H˜ be the same graph with cost ce of e = (i, j) → (i′, j′) replaced by benefit be = (i′ − i) + (j′ −\\nj) − ce, (so H and V edges have benefit 0). The min-cost path of G˜ is 2n minus the max-benefit\\npath of H˜. To compute the max-benefit path of H˜, we use a binary tree data structure with leaves\\n{1, . . . , n}, where each node v stores a number bv, and a collection of lists L1,. . . ,Ln, where Li stores\\npairs (e, q(e)) where the head of e has x-coordinate i and q(e) is the max benefit of a path that\\nends with e.\\nWe proceed in n−1 rounds. Let the set Ai consist of all the shortcuts whose tail has x-coordinate\\ni. The preconditions for round i are: (1) for each leaf j, the stored value bj is the max benefit path\\nto (i, j) that includes a shortcut whose head has y-coordinate j (or 0 if there is no such path),\\n(2) for each internal node v, bv = max{bj : j is a leaf in the subtree of v}. and (3) for every edge\\ne = (i′, j′)→ (i′′, j′′) with i′ < i, the value q(e) has been computed and (e, q(e)) is in list Li′′ . During\\nround i, for each shortcut e = (i, j) → (i′, j′) in Ai, q(e) equals the max of bv + be over tree leaves\\nv with v ≤ j. This can be computed in O(log n) time as max bv + be, over {j} union the set of left\\nchildren of vertices on the root-to-j path that are not themselves on the path. Add (e, q(e)) to list\\n20\\nLi′ . After processing Ai, update the binary tree: for each (e, q(e)) ∈ Li+1, let j be the y-coordinate\\nof the head of e and for all vertices v on the root-to-j path, replace bv by max(bv, q(e)). The tree\\nthen satisfies the precondition for round i+1. The output of the algorithm is bn at the end of round\\nn− 1. It takes O(n) time to set up the data structure, O(m logm) time to sort the shortcuts, and\\nO(log n) processing time per shortcut (computing q(e) and later updating the data structure).\\n5 Summing up and speeding up\\nTo summarize, the algorithm GAP-UBθ runs CoveringAlgorithm of Section 3, converts the output\\ninto a shortcut graph, and runs the min-cost path algorithm of Section 4. By Corollary 3.14, and\\nthe quasilinear runtime (in the number of shortcuts) of the min-cost path algorithm, the algorithm\\nGAP-UBθ runs in time O˜(n\\n12/7θ4/7). The construction of the main algorithm ED-UB from\\nGAP-UB is standard:\\nProof of Theorem 1.1 from Theorem 1.2. Given GAP-UBθ, we construct ED-UB: Run the afore-\\nmentioned exact algorithm of [15] with runtime O(n + k2) time on instances of edit distance k,\\nfor O(n + n2−2/5) time. If it terminates then it outputs the exact edit distance. Otherwise, the\\nfailure to terminate implies dedit(x, y) ≥ n4/5. Now run GAP-UBθj(x, y) for θj = (1/2)j for\\nj = {0, . . . , logn5 } and output the minimum of all upper bounds obtained. Let j be the largest\\nindex with θjn ≥ dedit(x, y) (such an index exists since j = 0 works). The output is at most\\n840θjn ≤ 1680dedit(x, y). We run at most O(log n) iterations, each with runtime O˜(n2−2/7).\\nSpeeding up the algorithm. The runtime of ED-UB is dominated by the cost of SMALL-ED(\\nz1, z2, ǫ) on pairs of strings of length w ∈ {w1, w2}. We use Ukkonen’s algorithm [19] with t(w, ǫ) =\\nO(ǫw2). In the full paper we describe a revised algorithm ED-UB1, replacing the Ukkonen’s\\nalgorithm with ED-UB. This worsens the approximation factor (roughly multiplying it by the\\napproximation factor of ED-UB) but improves runtime. The internal parameters w1, w2, d are\\nadjusted to maximize savings. One can iterate this process any constant number of times to get\\nfaster algorithms with worse (but still constant) approximation factors. Because of the dependence\\nof the analysis on θ, we do not get a faster edit distance algorithm for all θ ∈ [0, 1] but only for θ\\nclose to 1. (This may be an artifact of our analysis rather than an inherent limitation.)\\nTheorem 5.1. For ǫ > 0, there are constants c > 1 and β ∈ (0, 1) and an algorithm with runtime\\nO(n\\n1+\\n√\\n5\\n2\\n+ǫ) that on input x, y of length n, outputs u such that dedit(x, y) ≤ u ≤ c · dedit(x, y) +n1−β\\nwith probability at least 1− 1/n.\\nIn the special case of one level of the recursion we obtain an algorithm for the full range of\\nθ ∈ [0, 1] that runs in time O˜(n2−98/277θ54/277) = O˜(n1.647θ0.195) if dedit(x, y) ≤ θn. Notice, 1+\\n√\\n5\\n2 =\\n1.618 . . . .\\nAcknowledgements. We thank the FOCS 2018 committee and especially several anonymous\\nreferees for helpful comments and suggestions. Michael Saks thanks C. Seshadhri for insightful\\ndiscussions on the edit distance problem.\\n21\\nReferences\\n[1] Amir Abboud and Arturs Backurs. Towards hardness of approximation for polynomial time\\nproblems. In 8th Innovations in Theoretical Computer Science Conference, ITCS 2017, January\\n9-11, 2017, Berkeley, CA, USA, pages 11:1–11:26, 2017.\\n[2] Amir Abboud, Arturs Backurs, and Virginia Vassilevska Williams. Tight hardness results for\\nLCS and other sequence similarity measures. In IEEE 56th Annual Symposium on Foundations\\nof Computer Science, FOCS 2015, Berkeley, CA, USA, 17-20 October, 2015, pages 59–78, 2015.\\n[3] Amir Abboud, Thomas Dueholm Hansen, Virginia Vassilevska Williams, and Ryan Williams.\\nSimulating branching programs with edit distance and friends: or: a polylog shaved is a lower\\nbound made. In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of\\nComputing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pages 375–388, 2016.\\n[4] Alexandr Andoni, Robert Krauthgamer, and Krzysztof Onak. Polylogarithmic approximation\\nfor edit distance and the asymmetric query complexity. In 51th Annual IEEE Symposium on\\nFoundations of Computer Science, FOCS 2010, October 23-26, 2010, Las Vegas, Nevada, USA,\\npages 377–386, 2010.\\n[5] Alexandr Andoni and Huy L. Nguyen. Near-optimal sublinear time algorithms for Ulam\\ndistance. In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete\\nAlgorithms, SODA 2010, Austin, Texas, USA, January 17-19, 2010, pages 76–86, 2010.\\ndoi:10.1137/1.9781611973075.8.\\n[6] Alexandr Andoni and Krzysztof Onak. Approximating edit distance in near-linear time. In\\nProceedings of the Forty-first Annual ACM Symposium on Theory of Computing, STOC ’09,\\npages 199–204, New York, NY, USA, 2009. ACM.\\n[7] Arturs Backurs and Piotr Indyk. Edit distance cannot be computed in strongly subquadratic\\ntime (unless SETH is false). In Proceedings of the Forty-Seventh Annual ACM on Symposium\\non Theory of Computing, STOC ’15, pages 51–58, New York, NY, USA, 2015. ACM.\\n[8] Z. Bar-Yossef, T.S. Jayram, R. Krauthgamer, and R. Kumar. Approximating edit distance effi-\\nciently. In Foundations of Computer Science, 2004. Proceedings. 45th Annual IEEE Symposium\\non, pages 550–559, Oct 2004.\\n[9] Tugkan Batu, Funda Ergün, Joe Kilian, Avner Magen, Sofya Raskhodnikova, Ronitt Rubinfeld,\\nand Rahul Sami. A sublinear algorithm for weakly approximating edit distance. In Proceedings\\nof the Thirty-fifth Annual ACM Symposium on Theory of Computing, STOC ’03, pages 316–\\n324, New York, NY, USA, 2003. ACM.\\n[10] Tuğkan Batu, Funda Ergun, and Cenk Sahinalp. Oblivious string embeddings and edit distance\\napproximations. In Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete\\nAlgorithm, SODA ’06, pages 792–801, Philadelphia, PA, USA, 2006. Society for Industrial and\\nApplied Mathematics.\\n[11] Mahdi Boroujeni, Soheil Ehsani, Mohammad Ghodsi, Mohammad Taghi Hajiaghayi, and Saeed\\nSeddighin. Approximating edit distance in truly subquadratic time: Quantum and MapReduce.\\n22\\nIn Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms,\\nSODA 2018, New Orleans, LA, USA, January 7-10, 2018, pages 1170–1189, 2018.\\n[12] Mahdi Boroujeni, Soheil Ehsani, Mohammad Ghodsi, Mohammad Taghi Hajiaghayi, and Saeed\\nSeddighin. Approximating edit distance in truly subquadratic time: Quantum and MapReduce\\n(extended version of [11]). 2018.\\n[13] Karl Bringmann and Marvin Künnemann. Quadratic conditional lower bounds for string prob-\\nlems and dynamic time warping. In IEEE 56th Annual Symposium on Foundations of Computer\\nScience, FOCS 2015, Berkeley, CA, USA, 17-20 October, 2015, pages 79–97, 2015.\\n[14] Szymon Grabowski. New tabulation and sparse dynamic programming based techniques for\\nsequence similarity problems. Discrete Applied Mathematics, 212:96–103, 2016.\\n[15] Gad M. Landau, Eugene W. Myers, and Jeanette P. Schmidt. Incremental string comparison.\\nSIAM J. Comput., 27(2):557–582, April 1998.\\n[16] VI Levenshtein. Binary Codes Capable of Correcting Deletions, Insertions and Reversals. Soviet\\nPhysics Doklady, 10:707, 1966.\\n[17] William J. Masek and Michael S. Paterson. A faster algorithm computing string edit distances.\\nJournal of Computer and System Sciences, 20(1):18 – 31, 1980.\\n[18] Timothy Naumovitz, Michael E. Saks, and C. Seshadhri. Accurate and nearly optimal sublinear\\napproximations to Ulam distance. In Proceedings of the Twenty-Eighth Annual ACM-SIAM\\nSymposium on Discrete Algorithms, SODA 2017, Barcelona, Spain, Hotel Porta Fira, January\\n16-19, pages 2012–2031, 2017.\\n[19] Esko Ukkonen. Algorithms for approximate string matching. Inf. Control, 64(1-3):100–118,\\nMarch 1985.\\n[20] Robert A. Wagner and Michael J. Fischer. The string-to-string correction problem. J. ACM,\\n21(1):168–173, January 1974.\\n23\\n'}\n",
      "{'_id': ObjectId('6815e61add259dc7e6e1cd29'), 'authors': 'Abboud, Amir, Georgiadis, Loukas, Italiano, Giuseppe F., Krauthgamer, Robert, Parotsidis, Nikos, Trabelsi, Ohad, Uznanski, Przemyslaw, Wolleb-Graf, Daniel', 'year': '2018', 'title': 'Faster Algorithms for All-Pairs Bounded Min-Cuts', 'full_text': 'Faster Algorithms for All-Pairs Bounded Min-CutsAmir AbboudIBM Almaden Research Center, California, USAamir.abboud@ibm.comLoukas GeorgiadisUniversity of Ioannina, Greeceloukas@cs.uoi.grGiuseppe F. ItalianoLUISS University, Rome, Italygitaliano@luiss.itRobert KrauthgamerWeizmann Institute of Science, Israelrobert.krauthgamer@weizmann.ac.ilNikos ParotsidisUniversity of Copenhagen, Denmarknipa@di.ku.dkOhad TrabelsiWeizmann Institute of Science, Israelohad.trabelsi@weizmann.ac.ilPrzemysław UznańskiUniversity of Wrocław, Polandpuznanski@cs.uni.wroc.plDaniel Wolleb-GrafETH Zürich, Switzerlanddaniel.graf@inf.ethz.chAbstractThe All-Pairs Min-Cut problem (aka All-Pairs Max-Flow) asks to compute a minimum s-t cut(or just its value) for all pairs of vertices s, t. We study this problem in directed graphs with unitedge/vertex capacities (corresponding to edge/vertex connectivity). Our focus is on the k-boundedcase, where the algorithm has to find all pairs with min-cut value less than k, and report only those.The most basic case k = 1 is the Transitive Closure (TC) problem, which can be solved in graphswith n vertices and m edges in time O(mn) combinatorially, and in time O(nω) where ω < 2.38 isthe matrix-multiplication exponent. These time bounds are conjectured to be optimal.We present new algorithms and conditional lower bounds that advance the frontier for larger k,as follows:A randomized algorithm for vertex capacities that runs in time O((nk)ω). This is only a factorkω away from the TC bound, and nearly matches it for all k = no(1).Two deterministic algorithms for edge capacities (which is more general) that work in DAGsand further reports a minimum cut for each pair. The first algorithm is combinatorial (doesnot involve matrix multiplication) and runs in time O(2O(k2) ·mn). The second algorithm canbe faster on dense DAGs and runs in time O((k log n)4k+o(k)· nω). Previously, Georgiadis et al.[ICALP 2017], could match the TC bound (up to no(1) factors) only when k = 2, and now ourtwo algorithms match it for all k = o(√log n) and k = o(log log n).The first super-cubic lower bound of nω−1−o(1)k2 time under the 4-Clique conjecture, whichholds even in the simplest case of DAGs with unit vertex capacities. It improves on the previous(SETH-based) lower bounds even in the unbounded setting k = n. For combinatorial algorithms,our reduction implies an n2−o(1)k2 conditional lower bound. Thus, we identify new settingswhere the complexity of the problem is (conditionally) higher than that of TC.EATCS© Amir Abboud, Loukas Georgiadis, Giuseppe F. Italiano, Robert Krauthgamer,Nikos Parotsidis, Ohad Trabelsi, Przemysław Uznański, and Daniel Wolleb-Graf;licensed under Creative Commons License CC-BY46th International Colloquium on Automata, Languages, and Programming (ICALP 2019).Editors: Christel Baier, Ioannis Chatzigiannakis, Paola Flocchini, and Stefano Leonardi;Article No. 7; pp. 7:1–7:15Leibniz International Proceedings in InformaticsSchloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany7:2 Faster Algorithms for All-Pairs Bounded Min-CutsOur three sets of results are obtained via different techniques. The first one adapts the networkcoding method of Cheung, Lau, and Leung [SICOMP 2013] to vertex-capacitated digraphs. Thesecond set exploits new insights on the structure of latest cuts together with suitable algebraictools. The lower bounds arise from a novel reduction of a different structure than the SETH-based constructions.2012 ACM Subject Classification Theory of computation → Graph algorithms analysis; Theory ofcomputation → Network flowsKeywords and phrases All-pairs min-cut, k-reachability, network coding, Directed graphs, fine-grained complexityDigital Object Identifier 10.4230/LIPIcs.ICALP.2019.7Category Track A: Algorithms, Complexity and GamesRelated Version A full version of the paper is available at arXiv:1807.05803.Funding Robert Krauthgamer : Work supported in part by ONR Award N00014-18-1-2364, Israel Sci-ence Foundation grant #1086/18, a Minerva Foundation grant, and a Google Faculty Research Award.Nikos Parotsidis: The author is supported by Grant Number 16582, Basic Algorithms ResearchCopenhagen (BARC), from the VILLUM Foundation.Ohad Trabelsi: Work partly done at IBM Almaden Research Center, USA.Acknowledgements We thank Paweł Gawrychowski, Mohsen Ghaffari, Atri Rudra and Peter Wid-mayer for the valuable discussions on this problem.1 IntroductionConnectivity-related problems are some of the most well-studied problems in graph theoryand algorithms, and have been thoroughly investigated in the literature. Given a directedgraph G = (V,E) with n = |V | vertices and m = |E| edges,1 perhaps the most fundamentalsuch problem is to compute a minimum s-t cut, i.e., a set of edges E′ of minimum-cardinalitysuch that t is not reachable from s in G \\\\ E′. This minimum s-t cut problem is well-knownto be equivalent to maximum s-t flow, as they have the exact same value [17]. Currently, thefastest algorithms for this problem run in time Õ(m√n logO(1) U) [24] and Õ(m10/7U1/7)(faster for sparse graphs) [26], where U is the maximum edge capacity (aka weight).2The central problem of study in this paper is All-Pairs Min-Cut (also known as All-PairsMax-Flow), where the input is a digraph G = (V,E) and the goal is to compute the minimums-t cut value for all s, t ∈ V . All our graphs will have unit edge/vertex capacities, in whichcase the value of the minimum s-t cut is just the maximum number of disjoint paths from sto t (aka edge/vertex connectivity), by [28]. We will consider a few variants: vertex capacitiesvs. edge capacities,3 reporting only the value vs. the cut itself (a witness), or a generaldigraph vs. a directed acyclic graph (DAG). For all these variants, we will be interested in thek-bounded version (aka bounded min-cuts, hence the title of the paper) where the algorithmneeds to find which minimum s-t cuts have value less than a given parameter k, and report1 We sometimes use arcs when referring to directed edges, or use nodes instead of vertices.2 The notation Õ(·) hides polylogarithmic factors.3 The folklore reduction where each vertex v is replaced by two vertices connected by an edge vin → voutshows that in all our problems, vertex capacities are no harder (and perhaps easier) than edge capacities.Notice that this is only true for directed graphs.A. Abboud et al. 7:3only those. Put differently, the goal is to compute, for every s, t ∈ V , the minimum betweenk and the actual minimum s-t cut value. Nonetheless, some of our results (the lower bounds)are of interest even without this restriction.The time complexity of these problems should be compared against the fundamentalspecial case that lies at their core – the Transitive Closure problem (aka All-Pairs Reach-ability), which is known to be time-equivalent to Boolean Matrix Multiplication, and insome sense, to Triangle Detection [34]. This is the case k = 1, and it can be solved in timeO(min{mn/ logn, nω}), where ω < 2.38 is the matrix-multiplication exponent [14, 23, 33, 11];the latter term is asymptotically better for dense graphs, but it is not combinatorial.4 Thistime bound is conjectured to be optimal for Transitive Closure, which can be viewed asa conditional lower bound for All-Pairs Min-Cut; but can we achieve this time boundalgorithmically, or is All-Pairs Min-Cut a harder problem?The naive strategy for solving All-Pairs Min-Cut is to execute a minimum s-t cut algorithmO(n2) times, with total running time Õ(n2m10/7) [26] or Õ(n2.5m) [24]. For not-too-densegraphs, there is a faster randomized algorithm of Cheung, Lau, and Leung [13] that runs intime O(mω). For smaller k, some better bounds are known. First, observe that a minimum s-tcut can be found via k iterations of the Ford-Fulkerson algorithm [17] in time O(km), whichgives a total bound of O(n2mk). Another randomized algorithm of [13] runs in better timeO(mnkω−1) but it works only in DAGs. Notice that the latter bound matches the runningtime of Transitive Closure if the graphs are sparse enough. For the case k = 2, Georgiadis etal. [18] achieved the same running time as Transitive Closure up to sub-polynomial factorno(1) in all settings, by devising two deterministic algorithms, whose running times are Õ(mn)and Õ(nω).Other than the lower bound from Transitive Closure, the main previously known result isfrom [21], which showed that under the Strong Exponential Time Hypothesis (SETH),5 All-Pairs Min-Cut requires, up to sub-polynomial factors, time Ω(mn) in unit edge capacitateddigraphs of any edge density, and even in the simpler case of (unit) vertex capacities and ofDAGs. As a function of k their lower bound becomes Ω(n2−o(1)k) [21]. Combining the two,we have a conditional lower bound of (n2k + nω)1−o(1).Related Work. There are many other results related to the edge capacitated All-PairsMin-Cut problem, let us mention a few. Other than DAGs, the problem has also beenconsidered in the special cases of planar digraphs [6, 22], sparse digraphs and digraphs withbounded treewidth [6].In undirected graphs, the problem was studied extensively following the seminal work ofGomory and Hu [19] in 1961, which introduced a representation of All-Pairs Min-Cuts via aweighted tree, commonly called a Gomory-Hu tree, and further showed how to compute itusing n− 1 executions of maximum s-t flow. Bhalgat et al. [8] designed an algorithm thatcomputes a Gomory-Hu tree in unit edge capacitated undirected graphs in Õ(mn) time, andthis upper bound was recently improved [4]. The case of bounded min-cuts (small k) inundirected graphs was studied by Hariharan et al. [20], motivated in part by applications inpractical scenarios. The fastest running time for this problem is Õ(mk) [31], achieved bycombining results from [20] and [8]. On the negative side, there is an n3−o(1) lower boundfor All-Pairs Min-Cut in sparse capacitated digraphs [21], and very recently, a similar lowerbound was shown for undirected graphs with vertex capacities [4].4 Combinatorial is an informal term to describe algorithms that do not rely on fast matrix-multiplicationalgorithms, which are infamous for being impractical. See [1, 3] for further discussions.5 These lower bounds hold even under the weaker assumption that the 3-Orthogonal Vectors problemrequires n3−o(1) time.ICALP 20197:4 Faster Algorithms for All-Pairs Bounded Min-Cuts1.1 Our ContributionThe goal of this work is to reduce the gaps in our understanding of the All-Pairs Min-Cutproblem (see Table 1 for a list of known and new results). In particular, we are motivated bythree high-level questions. First, how large can k be while keeping the time complexity thesame as Transitive Closure? Second, could the problem be solved in cubic time (or faster)in all settings? Currently no Ω(n3+ε) lower bound is known even in the hardest settings ofthe problem (capacitated, dense, general graphs). And third, can the actual cuts (witnesses)be reported in the same amount of time it takes to only report their values? Some of theprevious techniques, such as those of [13], cannot do that.New Algorithms. Our first result is a randomized algorithm that solves the k-boundedversion of All-Pairs Min-Cut in a digraph with unit vertex capacities in time O((nk)ω). Thisupper bound is only a factor kω away from that of Transitive Closure, and thus matchesit up to polynomial factors for any k = no(1). Moreover, any poly(n)-factor improvementover our upper bound would imply a breakthrough for Transitive Closure (and many otherproblems). Our algorithm builds on the network-coding method of [13], and in effect adaptsthis method to the easier setting of vertex capacities, to achieve a better running time thanwhat is known for unit edge capacities. This algorithm is actually more general: Givena digraph G = (V,E) with unit vertex capacities, two subsets S, T ⊆ V and k > 0, itcomputes for all s ∈ S, t ∈ T the minimum s-t cut value if this value is less than k, all intime O((n+ (|S|+ |T |)k)ω + |S||T |kω). Due to space constraints, we overview these resultsin Section 3.1, with full details in the full version.Three weaknesses of this algorithm and the ones by Cheung et al. [13] are that theydo not return the actual cuts, they are randomized, and they are not combinatorial. Ournext set of algorithmic results deals with these issues. More specifically, we present twodeterministic algorithms for DAGs with unit edge (or vertex) capacities that compute, forevery s, t ∈ V , an actual minimum s-t cut if its value is less than k. The first algorithm iscombinatorial (i.e., it does not involve matrix multiplication) and runs in time O(2O(k2) ·mn).The second algorithm can be faster on dense DAGs and runs in time O((k logn)4k+o(k) · nω).These algorithms extend the results of Georgiadis et al. [18], which matched the runningtime of Transitive Closure up to no(1) factors, from just k = 2 to any k = o(√logn) (in thefirst case) and k = o(log logn) (in the second case). We give an overview of these algorithmsin Section 3.2, and the formal results are in the full version.New Lower Bounds. Finally, we present conditional lower bounds for our problem, thek-bounded version of All-Pairs Min-Cut. As a result, we identify new settings where theproblem is harder than Transitive Closure, and provide the first evidence that the problemcannot be solved in cubic time. Technically, the main novelty here is a reduction from the4-Clique problem. It implies lower bounds that apply to the basic setting of DAGs with unitvertex capacities, and therefore immediately apply also to more general settings, such as edgecapacities, capacitated inputs, and general digraphs, and they in fact improve over previouslower bounds [5, 21] in all these settings.6 We prove the following theorem in Section 4.I Theorem 1.1. If for some fixed ε > 0 and any k ∈ [n1/2, n], the k-bounded version ofAll-Pairs Min-Cut can be solved on DAGs with unit vertex capacities in time O((nω−1k2)1−ε),then 4-Clique can be solved in time O(nω+1−δ) for some δ = δ(ε) > 0.6 It is unclear if our new reduction can be combined with the ideas in [4] to improve the lower bounds inthe seemingly easier case of undirected graphs with vertex capacities.A. Abboud et al. 7:5Moreover, if for some fixed ε > 0 and any k ∈ [n1/2, n] that version of All-PairsMin-Cut can be solved combinatorially in time O((n2k2)1−ε), then 4-Clique can be solvedcombinatorially in time O(n4−δ) for some δ = δ(ε) > 0.To appreciate the new bounds, consider first the case k = n, which is equivalent tonot restricting k. The previous lower bound, under SETH, is n3−o(1) and ours is largerby a factor of nω−2. For combinatorial algorithms, our lower bound is n4−o(1), which isessentially the largest possible lower bound one can prove without a major breakthrough infine-grained complexity. This is because the naive algorithm for All-Pairs Min-Cuts is toinvoke an algorithm for Max-Flow O(n2) times, hence a lower bound larger than Ω(n4) forour problem would imply the first non-trivial lower bound for minimum s-t cut. The latteris perhaps the biggest open question in fine-grained complexity, and in fact many expertsbelieve that near-linear time algorithms for minimum s-t cut do exist, and can even beconsidered “combinatorial” in the sense that they do not involve the infamous inefficienciesof fast matrix multiplication. If such algorithms for minimum s-t cut do exist, then our lowerbound is tight.Our lower bound shows that as k exceeds n1/2−o(1), the time complexity of k-boundedof All-Pairs Min-Cut exceeds that of Transitive Closure by polynomial factors. The lowerbound is super-cubic whenever k ≥ n2−ω/2+ε.Table 1 Summary of new and known results. Unless mentioned otherwise, all upper and lowerbounds hold both for unit edge capacities and for unit vertex capacitities.Time Input Output ReferenceO(mn), Õ(nω) deterministic digraphs cuts, only k = 2 [18]O(n2mk) deterministic digraphs cuts [17]O(mω) randomized digraphs cut values [13]O(mnkω−1) randomized DAGs cut values [13]O((nk)ω) randomized, vertex capacities digraphs cut values full version2O(k2)mn deterministic DAGs cuts full version(k log n)4k+o(k) · nω deterministic DAGs cuts full version(mn + nω)1−o(1) based on Transitive Closure DAGs cut valuesn2−o(1)k based on SETH DAGs cut values [21]nω−1−o(1)k2 based on 4-Clique DAGs cut values Theorem 1.12 PreliminariesWe start with some terminology and well-known results on graphs and cuts. Next we willbriefly introduce the main algebraic tools that will be used throughout the paper. We notethat although we are interested in solving the k-bounded All-Pairs Min-Cut problem, wherewe wish to find the all-pairs min-cuts of size at most k − 1, for the sake of using simplernotation we compute the min-cuts of size at most k (instead of less than k) solving this waythe (k + 1)-bounded All-Pairs Min-Cut problem.Directed graphs. The input of our problem consists of an integer k ≥ 1 and a directedgraph, digraph for short, G = (V,A) with n := |V | vertices and m := |A| arcs. Every arca = (u, v) ∈ A consists of a tail u ∈ V and a head v ∈ V . By G[S], we denote the subgraphof G induced by the set of vertices S, formally G[S] = (S,A∩ (S×S)). By N+(v), we denoteICALP 20197:6 Faster Algorithms for All-Pairs Bounded Min-Cutsthe out-neighborhood of v consisting of all the heads of the arcs leaving v. We denote byoutdeg(v) the number of outgoing arcs from v. All our results extend to multi-digraphs,where each pair of vertices can be connected with multiple (parallel) arcs. For parallel arcs,we always refer to each arc individually, as if each arc had a unique identifier. So wheneverwe refer to a set of arcs, we refer to the set of their unique identifiers, i.e., without collapsingparallel arcs, like in a multi-set.Flows and cuts. We follow the notation used by Ford and Fulkerson [17]. Let G = (V,A) bea digraph, where each arc a has a nonnegative capacity c(a). For a pair of vertices s and t, ans-t flow of G is a function f on A such that 0 ≤ f(a) ≤ c(a), and for every vertex v 6= s, t theincoming flow is equal to outgoing flow, i.e.,∑(u,v)∈A f(u, v) =∑(v,u)∈A f(v, u). If G hasvertex capacities as well, then f must also satisfy∑(u,v)∈A f(u, v) ≤ c(v) for every v 6= s, t,where c(v) is the capacity of v. The value of the flow is defined as |f | =∑(s,v)∈A f(s, v). Wedenote the existence of a path from s to t by s t and by s6 t the lack of such a path. Anyset M ⊆ A is an s-t-cut if s6 t in G \\\\M . M is a minimal s-t-cut if no proper subset of Mis s-t-cut. For an s-t-cut M , we say that its source side is SM = {x | s x in G \\\\M} andits target side is TM = {x | x t in G \\\\M}. We also refer to the source side and the targetside as s-reachable and t-reaching, respectively. An s-t k-cut is a minimal cut of size k. AsetM of s-t cuts of size at most k is called a set of s-t ≤ k-cuts. We can define vertex cutsanalogously.Order of cuts. An s-t cut M is later (respectively earlier) than an s-t cut M ′ if and only ifTM ⊆ TM ′ (resp. SM ⊆ SM ′), and we denote it M ≥M ′ (resp. M ≤M ′). Note that thoserelations are not necessarily complementary if the cuts are not minimal (see Figure 1 for anexample).s tM1SM1 SM1s tM2SM2 SM2s tM3SM3 SM3stM3TM3TM3s tM3M2M1Figure 1 A digraph with three s-t-cuts M1, M2, M3. While M1 and M2 are minimal, M3 is not.Hence, the source side and target side differ only for M3. This illustrates that the earlier and laterorders might not be symmetric for non-minimal cuts. We have M3 < M2 yet M2 ≯ M3 (and alsoM3 ≤M2 yet M2 \\x03 M3). Additionally, M1 ≮ M3 yet M3 > M1 (yet both M1 ≤M3 and M3 ≥M1).We make these inequalities strict (i.e., ‘>’ or ‘<’) whenever the inclusions are proper. Wecompare a cut M and an arc a by defining a > M whenever both endpoints of a are in TM .Additionally, a ≥M includes the case where a ∈M . Definitions of the relations ‘≤’ and ‘<’follow by symmetry. We refer to Figure 2 for illustrations.s tM1 M4M3M2Figure 2 A digraph with several s-t cuts. Bold arcs represent parallel arcs which are too expensiveto cut. M1 is the earliest s-t min-cut and M3 is the latest s-t min-cut. M2 is later than M1, but M2is not s-t-latest, as M4 is later and not larger than M2.A. Abboud et al. 7:7This partial order of cuts also allows us to define cuts that are extremal with respect toall other s-t cuts in the following sense:I Definition 2.1 (s-t-latest cuts [27]). An s-t cut is s-t-latest (resp. s-t-earliest) if and onlyif there is no later (resp. earlier) s-t cut of smaller or equal size.Informally speaking, a cut is s-t-latest if we would have to cut through more arcs wheneverwe would like to cut off fewer vertices. This naturally extends the definition of an s-t-latestmin-cut as used by Ford and Fulkerson [17, Section 5]. The notion of latest cuts has first beenintroduced by Marx [27] (under the name of important cuts) in the context of fixed-parametertractable algorithms for multi(way) cut problems, but has been independently studied (or itsequivalent formulations), [7, 29]. Since we need both earliest and latest cuts, we do not referto latest cuts as important cuts. Additionally, we use the term s-t-extremal cuts to refer tothe union of s-t-earliest and s-t-latest cuts.3 Overview of Our Algorithmic Approach3.1 Randomized Algorithms on General GraphsWe will now briefly recap the framework of Cheung et al. [13] as we will modify them laterfor our purposes. In this framework, edges are encoded as vectors, so that the vector ofeach edge e = (u, v) is a randomized linear combination of the vectors correspond to edgesincoming to u, the source of e. One can compute all these vectors for the whole graph,simultaneously, using some matrix manipulations. The bottleneck is that one has to invert acertain m×m matrix with an entry for each pair of edges. Just reading the matrix that isoutput by the inversion requires Ω(m2) time, since most entries in the inverted matrix areexpected to be nonzero even if the graph is sparse.To overcome this barrier, while using the same framework, we define the encoding vectorson the nodes rather than the edges. We show that this is sufficient for the vertex-capacitatedsetting. Then, instead of inverting a large matrix, we need to compute the rank of certainsubmatrices which becomes the new bottleneck. When k is small enough, this turns out tolead to a significant speed up compared to the running time in [13].3.2 Deterministic Algorithms with Witnesses on DAGsHere we deal with the problem of computing certificates for the k-bounded All-Pairs Min-Cutproblem. Our contribution here is twofold. We first prove some properties of the structure ofthe s-t-latest k-cuts and of the s-t-latest ≤k-cuts, which might be of independent interest.This gives us some crucial insights on the structure of the cuts, and allows us to developan algorithmic framework which is used to solve the k-bounded All-Pairs Min-Cut problem.As a second contribution, we exploit our new algorithmic framework in two different ways,leading to two new algorithms which run in O(mn1+o(1)) time for k = o(√logn) and inO(nω+o(1)) time for k = o(log logn).Let G = (V,A) be a DAG. Consider some arbitrary pair of vertices s and t, and anys-t-cut M . For every intermediate vertex v, M must be either a s-v-cut, or a v-t-cut. Theknowledge of all s-v and all v-t min-cuts does not allow us to convey enough information forcomputing an s-t min-cut of size at most k quickly, as illustrated in Figure 3.However, we are able to compute an s-t min-cut by processing all the s-v-earliest cuts andall the v-t-latest cuts, of size at most k. We build our approach around this insight. We notethat the characterization that we develop is particularly useful, as it has been shown thatthe number of all earliest/latest u-v ≤k-cuts can be upper bounded by 2O(k), independentlyof the size of the graph.ICALP 20197:8 Faster Algorithms for All-Pairs Bounded Min-Cutss tabcdefgh2222333 3332222Figure 3 A digraph where each arc appears in at least one s-v or one v-t min-cut. The numberson the arcs denote the number of parallel arcs. Note that neither of the two s-t min-cuts of size 9(marked in yellow) are contained within the union of any two s-v or v-t min-cuts. Thus, finding allthose min-cuts and trying to combine them in pairs in a divide-and-conquer-style approach is notsufficient to find an s-t min-cut.For a more precise formulation on how to recover a min-cut (or extremal ≤k-cuts) fromcuts to and from intermediate vertices, consider the following. Let A1, A2 be an arc split,that is a partition of the arc set A with the property that any path in G consists of a(possibly empty) sequence of arcs from A1 followed by a (possibly empty) sequence of arcsfrom A2. Assume that for each vertex v we know all the s-v-earliest ≤k-cuts in G1 = (V,A1)and all the v-t-latest ≤k-cuts in G2 = (V,A2). We show that a set of arcs M that containsas a subset one s-v-earliest ≤k-cut in G1, or one v-t-latest ≤k-cut in G2 for every v, is as-t-cut. Moreover, we show that all the s-t-cuts of arcs with the above property includeall the s-t-latest ≤k-cuts. Hence, in order to identify all s-t-latest ≤k cuts, it is sufficientto identify all sets M with that property. We next describe how we use these structuralproperties to compute all s-t-extremal ≤k-cuts.We formulate the following combinatorial problem over families of sets, which is indepen-dent of graphs and cuts, that we can use to compute all s-t-extremal ≤k-cuts. The inputto our problem is c families of sets F1,F2, . . . ,Fc, where each family Fi consists of at mostK sets, and each set F ∈ Fi contains at most k elements from a universe U . The goal is tocompute all minimal subsets F ∗ ⊂ U, |F ∗| ≤ k, for which there exists a set F ∈ Fi such thatF ⊆ F ∗, for all 1 ≤ i ≤ c. We refer to this problem as Witness Superset. To create aninstance (s, t, A1, A2) of the Witness Superset problem, we set c = |V | and Fv to be alls-v-earliest ≤k-cuts in G1 and all v-t-latest ≤k-cuts in G2. Informally speaking, the solutionto the instance (s, t, A1, A2) of the Witness Superset problem picks all sets of arcs thatcover at least one earliest or one latest cut for every vertex. In a post-processing step, wefilter the solution to the Witness Superset problem on the instance (s, t, A1, A2) in orderto extract all the s-t-latest ≤k-cuts. We follow an analogous process to compute all thes-t-earliest ≤k-cuts.Algorithmic framework. We next define a common algorithmic framework for solving thek-bounded All-Pairs Min-Cut problem, as follows. We pick a partition of the vertices V1, V2,such that there is no arc in V2 × V1. Such a partition can be trivially computed from atopological order of the input DAG. Let A1, A2, A1,2 be the sets of arcs in G[V1], in G[V2],and A ∩ (V1 × V2).First, we recursively solve the problem in G[V1] and in G[V2]. The recursion returnswithout doing any work whenever the graph is a singleton vertex.A. Abboud et al. 7:9Second, for each pair of vertices (s, t), such that s ∈ V1 has an outgoing arc from A1,2and t ∈ V2, we solve the instance (s, t, A1,2, A2) of Witness Superset. Notice that theonly non-empty earliest cuts in (V,A1,2) for the pair (x, y) are the arcs (x, y) ∈ A1,2.Finally, for each pair of vertices (s, t), such that s ∈ V1, t ∈ V2, we solve the instance(s, t, A1, A1,2 ∪A2) of Witness Superset.The Witness Superset problem can be solved naively as follows. Let Fv be the setof all s-v-earliest ≤k-cuts and all v-t-latest ≤k-cuts. Assume we have Fv1 ,Fv2 , . . . ,Fvc , forall vertices v1, v2, . . . , vc that are both reachable from s in (V,A1,2) and that reach t in(V,A2). Each of these sets contains 2O(k) cuts. We can identify all sets M of arcs thatcontain at least one cut from each Fi, in time O(k · (2O(k))c). This yields an algorithm withsuper-polynomial running time. However, we speed up this naive procedure by applyingsome judicious pruning, achieving a better running time of O(c · 2O(k2) · poly(k)), which ispolynomial for k = o(√logn). In the following, we sketch the two algorithms that we developfor solving efficiently the k-bounded All-Pairs Min-Cut problem.Iterative division. For the first algorithm, we process the vertices in reverse topologicalorder. When processing a vertex v, we define V1 = {v} and V2 to be the set of vertices thatappear after v in the topological order. Notice that V1 has a trivial structure, and we alreadyknow all s-t-latest ≤k-cuts in G[V2]. In this case, we present an algorithm for solving theinstance (v, t, A1,2, A2) of the Witness Superset problem in time O(2O(k2) · c · poly(k)),where c = |A1,2| is the number of arcs leaving v. We invoke this algorithm for each v-w pair such that w ∈ V2. For k = o(√logn) this gives an algorithm that runs in timeO(outdeg(v) · n1+o(1)) for processing v, and O(mn1+o(1)) in total.Recursive division. For the second algorithm, we recursively partition the set of verticesevenly into sets V1 and V2 at each level of the recursion. We first recursively solve the problemin G[V1] and in G[V2]. Second, we solve the instances (s, t, A1,2, A2) and (s, t, A1, A1,2 ∪A2)of Witness Superset for all pairs of vertices from V1 × V2. Notice that the number ofvertices that are both reachable from s in (V,A1) and reach t in (V,A1,2 ∪A2) can be as highas O(n). This implies that even constructing all Θ(n2) instances of the Witness Supersetproblem, for all s, t, takes Ω(n3) time. To overcome this barrier, we take advantage of thepower of fast matrix multiplications by applying it into suitably defined matrices of binarycodes (codewords). At a very high-level, this approach was used by Fischer and Meyer [16]in their O(nω) time algorithm for transitive closure in DAGs – there the binary codes whereof size 1 indicating whether there exists an arc between two vertices.Algebraic framework. In order to use coordinate-wise boolean matrix multiplication withthe entries of the matrices being codewords we first encode all s-t-earliest and all s-t-latest≤k-cuts using binary codes. The bitwise boolean multiplication of such matrices with binarycodes in its entries allows a serial combination of both s-v cuts and v-t cuts based on ANDoperations, and thus allows us to construct a solution based on the OR operation of pairwiseAND operations. We show that superimposed codes are suitable in our case, i.e., binarycodes where sets are represented as bitwise-OR of codewords of objects, and small sets areguaranteed to be encoded uniquely. Superimposed codes provide a unique representation forsets of k elements from a universe of size poly(n) with codewords of length poly(k logn). Inthis setting, the union of sets translates naturally to bitwise-OR of their codewords.ICALP 20197:10 Faster Algorithms for All-Pairs Bounded Min-CutsTensor product of codes. To achieve our bounds, we compose several identical superim-posed codes into a new binary code, so that encoding set families with it enables us to solvethe corresponding instances of Witness Superset. Our composition has the cost of anexponential increase in the length of the code. Let F = F1, . . . , Fc be the set family that wewish to encode, and let S1, . . . , Sc be their superimposed codes in the form of vectors. Weconstruct a c-dimensional array M where M [i1, . . . , ic] = 1 iff Sj [ij ] = 1, for each 1 ≤ j ≤ c.In other words, the resulting code is the tensor product of all superimposed codes. Thisconstruction creates enough redundancy so that enough information on the structure of theset families is preserved. Furthermore, we can extract the encoded information from thebitwise-OR of several codewords. The resulting code is of length O((k logn)O(K)), where K isthe upperbound on the allowed number of sets in each encoded set family. In our case K ≈ 4k,which results to only a logarithmic dependency on n at the price of a doubly-exponentialdependency on k, thus making the problem tractable for small values of k.From slices to Witness Superset. Finally, we show how the Witness Superset can besolved using tensor product of superimposed codes. Consider the notion of cutting the codeof dimension K with an axis-parallel hyperplane of dimension K − 1. We call this resultingshorter codeword a slice of the original codeword. A slice of a tensor product is a tensorproduct of one dimension less, or an empty set, and a slice of a bitwise-OR of tensor productsis as well a bitwise-OR of tensor products (of one dimension less). Thus, taking a slice ofthe bitwise-OR of the encoding of families of sets is equivalent to removing a particular setfrom some families and to dropping some other families completely and then encoding theseremaining, reduced families. Thus, we can design a non-deterministic algorithm, which ateach step of the recursion picks k slices, one slice for each element of the solution we wantto output, and then recurses on the bitwise-OR of those slices, reducing the dimension byone in the process. This is always possible, since each element that belongs to a particularsolution of Witness Superset satisfies one of the following: it either has a witnessing sliceand thus it is preserved in the solution to the recursive call; or it is dense enough in theinput so that it is a member of each solution and we can detect this situation from scanningthe diagonal of the input codeword. This described nondeterministic approach is then madedeterministic by simply considering every possible choice of k slices at each of the K steps ofthe recursion. This does not increase substantially the complexity of the decoding procedure,since O(((K · poly(k logn))k)K) for K ≈ 4k is still only doubly-exponential in k.4 Reducing 4-Clique to All-Pairs Min-CutIn this section we prove Theorem 1.1 by showing new reductions from the 4-Clique problem tok-bounded All-Pairs Min-Cut with unit vertex capacities. These reductions yield conditionallower bounds that are much higher than previous ones, which are based on SETH, in additionto always producing DAGs. Throughout this section, we will often use the term nodes forvertices.I Definition 4.1 (The 4-Clique Problem). Given a 4-partite graph G, where V (G) = A∪B ∪C ∪D with |A| = |B| = |C| = |D| = n, decide whether there are four nodes a ∈ A, b ∈ B,c ∈ C, d ∈ D that form a clique.This problem is equivalent to the standard formulation of 4-Clique (without the restric-tion to 4-partite graphs). The currently known running times are O(nω+1) using matrixmultiplication [15], and O(n4/ polylogn) combinatorially [35]. The k-Clique ConjectureA. Abboud et al. 7:11[3] hypothesizes that current clique algorithms are optimal. Usually when the k-CliqueConjecture is used, it is enough to assume that the current algorithms are optimal for every kthat is a multiple of 3, where the known running times are O(nωk/3) [30] and O(nk/ polylogn)combinatorially [32], see e.g. [2, 3, 10, 12, 25]. However, we will need the stronger assumptionthat one cannot improve the current algorithms for k = 4 by any polynomial factor. Thisstronger form was previously used by Bringmann, Grønlund, and Larsen [9].4.1 Reduction to the Unbounded CaseWe start with a reduction to the unbounded case (equivalent to k = n), that is, we reduce toAll-Pairs Min-Cut with unit node capacities (abbreviated APMVC, for All-Pairs MinimumVertex-Cut). Later (in Section 4.1) we will enhance the construction in order to bound k.I Lemma 4.2. Suppose APMVC on n-node DAGs with unit node capacities can be solved intime T (n). Then 4-Clique on n-node graphs can be solved in time O(T (n) +MM(n)), whereMM(n) is the time to multiply two matrices from {0, 1}n×n.To illustrate the usage of this lemma, observe that an O(n3.99)-time combinatorialalgorithm for APMVC would imply a combinatorial algorithm with similar running timefor 4-Clique.Proof. Given a 4-partite graph G as input for the 4-Clique problem, the graph H isconstructed as follows. The node set of H is the same as G, and we abuse notationand refer also to V (H) as if it is partitioned into A,B,C, and D. Thinking of A as the set ofsources and D as the set of sinks, the proof will focus on the number of node-disjoint pathsfrom nodes a ∈ A to nodes d ∈ D. The edges of H are defined in a more special way, seealso Figure 4 for illustration.ABCDFigure 4 An illustration of H in the reduction. Solid lines between nodes represent the existenceof an edge in the input graph G, and dashed lines represent the lack thereof.(A to B) For every a ∈ A, b ∈ B such that {a, b} ∈ E(G), add to E(H) a directed edge(a, b).(B to C) For every b ∈ B, c ∈ C such that {b, c} ∈ E(G), add to E(H) a directed edge(b, c).(C to D) For every c ∈ C, d ∈ D such that {c, d} ∈ E(G), add to E(H) a directed edge(c, d).The definition of the edges of H will continue shortly. So far, edges in H correspond toedges in G, and there is a (directed) path a→ b→ c→ d if and only if the three (undirected)edges {a, b}, {b, c}, {c, d} exist in G. In the rest of the construction, our goal is to make this3-hop path contribute to the final a→ d flow if and only if (a, b, c, d) is a 4-clique in G (i.e.,ICALP 20197:12 Faster Algorithms for All-Pairs Bounded Min-Cutsall six edges exist, not only those three). Towards this end, additional edges are introduced,that make this 3-hop path useless in case {a, c} or {b, d} are not also edges in G. This allows“checking” for five of the six edges in the clique, rather than just three. The sixth edge iseasy to “check”.(A to C) For every a ∈ A, c ∈ C such that {a, c} /∈ E(G), add to E(H) a directed edge(a, c).(B to D) For every b ∈ B, d ∈ D such that {b, d} /∈ E(G) in G, add to E(H) a directededge (b, d).This completes the construction of H. Note that these additional edges imply that thereis a path a→ b→ d in H iff {a, b} ∈ E(G) and {b, d} /∈ E(G), and similarly, there is a patha→ c→ d in H iff {a, c} /∈ E(G) and {c, d} ∈ E(G). Let us introduce notations to capturethese paths. For nodes a ∈ A, d ∈ D denote:B′a,d = {b ∈ B | {a, b} ∈ E(G) and {b, d} /∈ E(G) } ,C ′a,d = {c ∈ C | {a, c} /∈ E(G) and {c, d} ∈ E(G) } .We now argue that if an APMVC algorithm is run on H, enough information is receivedto be able to solve 4-Clique on G by spending only an additional post-processing stage ofO(n3) time.B Claim 4.3. Let a ∈ A, d ∈ D be nodes with {a, d} ∈ E(G). If the edge {a, d} does notparticipate in a 4-clique in G, then the node connectivity from a to d in H, denoted NC(a, d),is exactlyNC(a, d) = |B′a,d|+ |C ′a,d|,and otherwise NC(a, d) is strictly larger.Proof of Claim 4.3. We start by observing that all paths from a to d in H have either two orthree hops.Assume now that there is a 4-clique (a, b∗, c∗, d) in G, and let us exhibit a set P ofnode-disjoint paths from a to d of size |B′a,d|+ |C ′a,d|+ 1. For all nodes b ∈ B′a,d, add to Pthe 2-hop path a→ b→ d. For all nodes c ∈ C ′a,d, add to P the 2-hop path a→ c→ d. Sofar, all these paths are clearly node-disjoint. Then, add the 3-hop path a→ b∗ → c∗ → d toP . This path is node-disjoint from the rest because b∗ /∈ B′a,d (because {b∗, d} ∈ E(G)) andc∗ /∈ C ′a,d (because {a, c∗} ∈ E(G)).Next, assume that no nodes b ∈ B, c ∈ C complete a 4-clique with a, d. Then for everyset P of node-disjoint paths from a to d, there is a set P ′ of 2-hop node-disjoint paths from ato d that has the same size. To see this, let a→ b→ c→ d be some 3-hop path in P . Since(a, b, c, d) is not a 4-clique in G and {a, d}, {a, b}, {b, c}, {c, d} are edges in G, we concludethat either {a, c} /∈ E(G) or {b, d} /∈ E(G). If {a, c} /∈ E(G) then a→ c is an edge in H andthe 3-hop path can be replaced with the 2-hop path a→ c→ d (by skipping b) and one isremained with a set of node-disjoint paths of the same size. Similarly, if {b, d} /∈ E(G) thenb→ d is an edge in H and the 3-hop path can be replaced with the 2-hop path a→ b→ d.This can be done for all 3-hop paths and result in P ′. Finally, note that the number of 2-hoppaths from a to d is exactly |B′a,d|+ |C ′a,d|, and this completes the proof of Claim 4.3. CA. Abboud et al. 7:13Computing the estimates. To complete the reduction, observe that the values |B′a,d|+|C ′a,d|can be computed for all pairs a ∈ A, d ∈ D using two matrix multiplications. To computethe |B′a,d| values, multiply the two matrices M,M ′ which have entries from {0, 1}, withMa,b = 1 iff {a, b} ∈ E(G) ∩ A× B and M ′b,d = 1 iff {b, d} /∈ E(G) ∩ B ×D. Observe that|B′a,d| is exactly (M ·M ′)a,d. To compute |C ′a,d|, multiply M,M ′ over {0, 1} where Ma,c = 1iff {a, c} /∈ E(G) ∩A× C and M ′c,d = 1 iff {c, d} ∈ E(G) ∩ C ×D.After having these estimates and computing APMVC on H, it can be decided whether Gcontains a 4-clique in O(n2) time as follows. Go through all edges {a, d} ∈ E(G) ∩ A×Dand decide whether the edge participates in a 4-clique by comparing |B′a,d|+ |C ′a,d| to thenode connectivity NC(a, d) in H. By the above claim, an edge {a, d} with NC(a, d) >|B′a,d| + |C ′a,d| is found if and only if there is a 4-clique in G. The total running time isO(T (n) +MM(n)), which completes the proof of Lemma 4.2. J4.2 Reduction to the k-Bounded CaseNext, we exploit a certain versatility of the reduction and adapt it to ask only about min-cutvalues (aka node connectivities) that are smaller than k. In other words, we will reduce tothe k-bounded version of All-Pairs Min-Cut with unit node capacities (abbreviated kAPMVC,for k-bounded All-Pairs Minimum Vertex-Cut). Our lower bound improves on the Ω(nω)conjectured lower bound for Transitive Closure as long as k = ω(n1/2).I Lemma 4.4. Suppose kAPMVC on n-node DAGs with unit node capacities can be solved intime T (n, k). Then 4-Clique on n-node graphs can be solved in time O(n2k2 ·T (n, k)+MM(n)),where MM(n) is the time to multiply two matrices from {0, 1}n×n.Due to space constraints, the proof appears in the full version.Proof of Theorem 1.1. Assume there is an algorithm that solves kAPMVC in timeO((nω−1k2)1−ε). Then by Lemma 4.4 there is an algorithm that solves 4-Clique in time= O(n2k2 ·(nω−1k2)1−ε+MM(n)) ≤ O(nω+1−ε′), for some ε′ > 0. The bound for combinatorialalgorithms is achieved similarly. JReferences1 A. Abboud and V. V. Williams. Popular Conjectures Imply Strong Lower Bounds for DynamicProblems. In FOCS, pages 434–443, October 2014. doi:10.1109/FOCS.2014.53.2 Amir Abboud, Arturs Backurs, Karl Bringmann, and Marvin Künnemann. Fine-GrainedComplexity of Analyzing Compressed Data: Quantifying Improvements over Decompress-and-Solve. In FOCS, pages 192–203, 2017. doi:10.1109/FOCS.2017.12.3 Amir Abboud, Arturs Backurs, and Virginia Vassilevska Williams. If the Current CliqueAlgorithms are Optimal, So is Valiant’s Parser. In FOCS, pages 98–117, 2015. doi:10.1109/FOCS.2015.16.4 Amir Abboud, Robert Krauthgamer, and Ohad Trabelsi. New Algorithms and Lower Bounds forAll-Pairs Max-Flow in Undirected Graphs. CoRR, abs/1901.01412, 2019. arXiv:1901.01412.5 Amir Abboud, Virginia Vassilevska Williams, and Huacheng Yu. Matching Triangles andBasing Hardness on an Extremely Popular Conjecture. SIAM J. Comput., 47(3):1098–1122,2018. doi:10.1137/15M1050987.6 Srinivasa R Arikati, Shiva Chaudhuri, and Christos D Zaroliagis. All-Pairs Min-Cut in SparseNetworks. Journal of Algorithms, 29(1):82–110, 1998. doi:10.1006/jagm.1998.0961.7 Attila Bernáth and Gyula Pap. Blocking unions of arborescences. CoRR, abs/1507.00868,2015. arXiv:1507.00868.ICALP 20197:14 Faster Algorithms for All-Pairs Bounded Min-Cuts8 Anand Bhalgat, Ramesh Hariharan, Telikepalli Kavitha, and Debmalya Panigrahi. An Õ(mn)Gomory-Hu Tree Construction Algorithm for Unweighted Graphs. In STOC, pages 605–614,2007. doi:10.1145/1250790.1250879.9 Karl Bringmann, Allan Grønlund, and Kasper Green Larsen. A Dichotomy for RegularExpression Membership Testing. In FOCS, pages 307–318, 2017. doi:10.1109/FOCS.2017.36.10 Karl Bringmann and Philip Wellnitz. Clique-Based Lower Bounds for Parsing Tree-AdjoiningGrammars. In CPM, pages 12:1–12:14, 2017. doi:10.4230/LIPIcs.CPM.2017.12.11 Timothy M. Chan. All-Pairs Shortest Paths with Real Weights in O(n3/ log n) Time. Algo-rithmica, 50(2):236–243, 2008. doi:10.1007/s00453-007-9062-1.12 Yi-Jun Chang. Conditional Lower Bound for RNA Folding Problem. CoRR, abs/1511.04731,2015. arXiv:1511.04731.13 Ho Yee Cheung, Lap Chi Lau, and Kai Man Leung. Graph Connectivities, Network Coding,and Expander Graphs. SIAM Journal on Computing, 42(3):733–751, 2013. doi:10.1137/110844970.14 D. Coppersmith and S. Winograd. Matrix multiplication via arithmetic progressions. Journalof Symbolic Computation, 9(3):251–280, 1990. doi:10.1016/S0747-7171(08)80013-2.15 Friedrich Eisenbrand and Fabrizio Grandoni. On the complexity of fixed parameter clique anddominating set. Theor. Comput. Sci., 326(1-3):57–67, 2004. doi:10.1016/j.tcs.2004.05.009.16 M. J. Fischer and A. R. Meyer. Boolean matrix multiplication and transitive closure. InSWAT, pages 129–131. IEEE, 1971. doi:10.1109/SWAT.1971.4.17 Lester Randolph Ford, Jr. and Delbert Ray Fulkerson. Flows in Networks. Princeton UniversityPress, 1962.18 Loukas Georgiadis, Daniel Graf, Giuseppe F. Italiano, Nikos Parotsidis, and PrzemysławUznański. All-Pairs 2-Reachability in O(nω log n) Time. In ICALP, volume 80, pages 74:1–74:14, 2017. doi:10.4230/LIPIcs.ICALP.2017.74.19 R. E. Gomory and T. C. Hu. Multi-Terminal Network Flows. Journal of the Society forIndustrial and Applied Mathematics, 9(4):551–570, 1961. doi:10.1137/0109047.20 Ramesh Hariharan, Telikepalli Kavitha, and Debmalya Panigrahi. Efficient Algorithms forComputing All Low s-t Edge Connectivities and Related Problems. In SODA, pages 127–136,2007. URL: http://dl.acm.org/citation.cfm?id=1283383.1283398.21 Robert Krauthgamer and Ohad Trabelsi. Conditional Lower Bounds for All-Pairs Max-Flow.ACM Trans. Algorithms, 14(4):42:1–42:15, 2018. doi:10.1145/3212510.22 Jakub Łącki, Yahav Nussbaum, Piotr Sankowski, and Christian Wulff-Nilsen. Single Source –All Sinks Max Flows in Planar Digraphs. In FOCS, pages 599–608. IEEE Computer Society,2012. doi:10.1109/FOCS.2012.66.23 F. Le Gall. Powers of Tensors and Fast Matrix Multiplication. In ISSAC, pages 296–303, 2014.doi:10.1145/2608628.2608664.24 Y. T. Lee and A. Sidford. Path Finding Methods for Linear Programming: Solving LinearPrograms in Õ(√rank) Iterations and Faster Algorithms for Maximum Flow. In FOCS, pages424–433, 2014. doi:10.1109/FOCS.2014.52.25 Andrea Lincoln, Virginia Vassilevska Williams, and R. Ryan Williams. Tight Hardness forShortest Cycles and Paths in Sparse Graphs. In SODA, pages 1236–1252, 2018.26 A. Mądry. Computing Maximum Flow with Augmenting Electrical Flows. In FOCS, pages593–602, 2016. doi:10.1109/FOCS.2016.70.27 Dániel Marx. Parameterized graph separation problems. Theoretical Computer Science,351(3):394–406, 2006.28 K. Menger. Zur allgemeinen Kurventheorie. Fundamenta Mathematicae, 10(1):96–115, 1927.29 Hiroshi Nagamochi and Yoko Kamidoi. Minimum cost subpartitions in graphs. Inf. Process.Lett., 102(2-3):79–84, 2007. doi:10.1016/j.ipl.2006.11.011.30 Jaroslav Nešetřil and Svatopluk Poljak. On the complexity of the subgraph problem. Com-mentationes Mathematicae Universitatis Carolinae, 26(2):415–419, 1985.A. Abboud et al. 7:1531 Debmalya Panigrahi. Gomory-Hu trees. In Ming-Yang Kao, editor, Encyclopedia of Algorithms,pages 858–861. Springer, 2016. doi:10.1007/978-1-4939-2864-4.32 Virginia Vassilevska. Efficient algorithms for clique problems. Inf. Process. Lett., 109(4):254–257, 2009. doi:10.1016/j.ipl.2008.10.014.33 V. Vassilevska Williams. Multiplying Matrices Faster Than Coppersmith-Winograd. In STOC,pages 887–898, 2012. doi:10.1145/2213977.2214056.34 Virginia Vassilevska Williams and R. Ryan Williams. Subcubic Equivalences Between Path,Matrix, and Triangle Problems. J. ACM, 65(5):27:1–27:38, 2018. doi:10.1145/3186893.35 Huacheng Yu. An improved combinatorial algorithm for Boolean matrix multiplication. Inf.Comput., 261:240–247, 2018. doi:10.1016/j.ic.2018.02.006.ICALP 2019'}\n",
      "{'_id': ObjectId('6815e61add259dc7e6e1cd2a'), 'authors': 'Chou, Chi-Ning, Golovnev, Alexander, Velusamy, Santhoshini', 'year': '2020', 'title': 'Optimal Streaming Approximations for all Boolean Max-2CSPs and Max-kSAT', 'full_text': 'Optimal Streaming Approximations for all Boolean Max-2CSPs\\nChi-Ning Chou∗ Alexander Golovnev† Santhoshini Velusamy‡\\nAbstract\\nWe prove tight upper and lower bounds on approximation ratios of all Boolean Max-2CSP problems\\nin the streaming model. Specifically, for every type of Max-2CSP problem, we give an explicit con-\\nstant α, s.t. for any ε > 0 (i) there is an (α − ε)-streaming approximation using space O(logn); and\\n(ii) any (α+ ε)-streaming approximation requires space Ω(\\n√\\nn). This generalizes the celebrated work of\\n[Kapralov, Khanna, Sudan SODA 2015; Kapralov, Krachun STOC 2019], who showed that the optimal\\napproximation ratio for Max-CUT was 1/2.\\nPrior to this work, the problem of determining this ratio was open for all other Max-2CSPs. Our results\\nare quite surprising for some specific Max-2CSPs. For the Max-DICUT problem, there was a gap between\\nan upper bound of 1/2 and a lower bound of 2/5 [Guruswami, Velingker, Velusamy APPROX 2017]. We\\nshow that neither of these bounds is tight, and the optimal ratio for Max-DICUT is 4/9. We also establish\\nthat the tight approximation for Max-2SAT is\\n√\\n2/2, and for Exact Max-2SAT it is 3/4. As a byproduct,\\nour result gives a separation between space-efficient approximations for Max-2SAT and Exact Max-2SAT.\\nThis is in sharp contrast to the setting of polynomial-time algorithms with polynomial space, where the\\ntwo problems are known to be equally hard to approximate.\\n∗School of Engineering and Applied Sciences, Harvard University, Cambridge, Massachusetts, USA. Supported by NSF\\nawards CCF 1565264 and CNS 1618026. Email: chiningchou@g.harvard.edu.\\n†School of Engineering and Applied Sciences, Harvard University, Cambridge, Massachusetts, USA. Supported by a Rabin\\nPostdoctoral Fellowship. Email: alexgolovnev@gmail.com.\\n‡School of Engineering and Applied Sciences, Harvard University, Cambridge, Massachusetts, USA. Supported in part by a\\nSimons Investigator Award and NSF Award CCF 1715187. Email: svelusamy@g.harvard.edu.\\nar\\nX\\niv\\n:2\\n00\\n4.\\n11\\n79\\n6v\\n2 \\n [c\\ns.C\\nC]\\n  2\\n9 A\\npr\\n 20\\n20\\n1 Introduction\\nMaximum Boolean Constraint Satisfaction Problems, or Max-CSPs, are a central class of optimization prob-\\nlems, including as special cases problems such as Max-CUT, 3SAT, Graph Coloring, and Vertex Cover [CV08].\\nGiven a set of allowed predicates F , Max-CSP(F) is the optimization problem defined as follows. Every\\ninstance Ψ of the problem consists of a set of Boolean variables X , and a set of constraints applied to\\nthem. Each constraint is a predicate from F applied to the variables from X or their negations. The goal\\nis to compute the maximum number of simultaneously satisfiable constraints. For example, Max-kSAT is\\nMax-CSP (FOR,≤k) where FOR,≤k is the set of OR predicates on at most k variables.\\nSchaefer’s famous dichotomy theorem [Sch78, TZˇ16] states that for any set of allowed predicates F ,\\nsolving Max-CSP(F) exactly is either in P or NP-hard. However, the landscape of approximation algorithms\\nfor Max-CSPs is much more complex (see [MM17] and references therein).\\nThe Max-2CSP problem—Max-CSP where all constraints have length at most 2—is the most studied\\ncase of Max-CSP, and it generalizes many optimization problems on graphs. Starting with the seminal\\nwork of Goemans and Williamson [GW95], a series of works [FG95, Zwi00, LLZ02] developed a 0.87401-\\napproximation algorithm for all Max-2CSPs, while under the P 6= NP and Unique Games conjectures some\\nMax-2CSPs do not admit 0.9001- and 0.87435-approximations, respectively [H˚as01, TSSW00, Aus10].\\nIn this paper, we follow the line of work [KK15, KKS15, KKSV17, GVV17, KK19, GT19] that studies\\nthe unconditional hardness of approximating Max-2CSP through the lens of streaming algorithms. Over the\\nlast decade, there has been a lot of interest in designing algorithms for processing large streams of data\\nusing limited space (see [McG14, Cha15] and references therein). The streaming model was formally defined\\nin [AMS99, HRR98].\\nA streaming algorithm for a Max-2CSP problem makes one pass through the list of constraints and uses\\nspace that is sub-linear (ideally, poly-logarithmic) in the input size.1 Since the algorithm is space bounded,\\nit cannot even store an assignment to the input variables. Thus, a streaming algorithm is required to output\\nan estimate of the maximum number of simultaneously satisfiable constraints. Specifically, for α ∈ [0, 1],\\nan α-approximate streaming algorithm outputs a value v for which the following two conditions hold with\\nprobability 3/4: (i) there exists an assignment σ satisfying at least v constraints, and (ii) v ≥ α · val, where\\nval is the maximum number of simultaneously satisfiable constraints.\\nPrior to this work, the only Max-2CSP for which we knew the optimal streaming approximation factor was\\nMax-CUT. Max-CUT asks us to find a bipartition of the n vertices of an undirected graph that maximizes\\nthe number of edges crossing the partition—called the “cut”. Note that Max-CUT corresponds to the\\nMax-CSP(F) where F contains the binary XOR predicate.2 [Zel11] shows that exact streaming algorithms\\nfor Max-CUT require quadratic space Ω(n2). Since a random partition of a graph with m edges has cut\\nof expected size m/2, a trivial streaming algorithm 1/2-approximates Max-CUT with O(logm) space. It is\\nalso easy to see that for every ε > 0, it suffices to store O˜(n) random edges of the graph to compute a\\n(1− ε)-approximation of Max-CUT. A recent line of work [KKS15, KK15, KKSV17, KK19] shows that these\\ntwo trivial bounds are optimal, i.e., any (1/2 + ε)-approximation algorithm requires linear space Ω(n).\\nHowever, the case for directed graphs is not nearly so well understood. In the Max-DICUT problem\\n(another special case of Max-2CSP), given a directed graph, one needs to compute the maximum number of\\nedges going from the first to the second part of the graph under any bipartition. While [KK19, KKS15]\\nrules out a (1/2+ε)-approximation for Max-DICUT too, the trivial algorithm gives only a 1/4-approximation\\nhere. [GVV17] gives a 2/5-approximation for Max-DICUT, still leaving a gap between the upper and lower\\nbounds.\\nEven the hardness of Max-2SAT is not known in the streaming setting. Recall that in Max-2SAT the\\nonly allowed predicates are variables and pairwise ORs. A random assignment gives a 1/2-approximation,\\nand the classical (\\n√\\n5− 1)/2 ≈ 0.61-approximate algorithm of [LS79] can be implemented in O(log n) space\\nusing `1-sketching [Ind00, KNW10]. No non-trivial upper bounds are known for Max-2SAT.\\n1In this work we focus on randomized streaming algorithms that make one pass over the input in a fixed (adversarial) order,\\nand return the correct answer with probability 3/4.\\n2Although formally Max-CUT is a special case of Max-2XOR where all constraints are of the form xi ⊕ xj = 1, it can be\\nshown that these two problems are equivalent.\\n1\\n1.1 Our contribution\\nIn this work, we resolve a natural question about the approximation guarantees of streaming algorithms\\nfor every Max-2CSP problem.\\nBefore presenting our results, we need a way to classify Boolean functions of two variables. Let\\nf : {0, 1}2 → {0, 1} be a function, then\\n• f is of TR-type, or trivial, if f depends on at most one of its inputs (trivial functions are the two\\nconstant functions, and the four functions which depend on one of the inputs);\\n• f is of OR-type if the truth table of f has exactly one 0 and three 1s;\\n• f is of XOR-type if f depends on both inputs, and the truth table of f has exactly two 0s and two 1s;\\n• f is of AND-type if the truth table of f has exactly three 0s and one 1.\\nIf a set of allowed predicates F contains only constraints of a type Λ ∈ {OR,XOR,AND}, then the\\ncorresponding Max-2CSP problem is called Max-2EΛ (2-Exact-Λ, meaning that all constraints have length\\nexactly 2). If F contains only Λ-type constraints and trivial constraints, then the corresponding Max-2CSP\\nproblem is called Max-2Λ.\\nWe abuse notation by identifying a set of allowed predicates F with the set of types of its predicates.\\nAlso, for a set F = {Λ} containing one element, we write F = Λ. Therefore, a Max-CSP(F) problem is\\ndefined by F ⊆ {TR,OR,XOR,AND}. Note that every Max-2CSP problem corresponds to one such F .\\nFor every Max-CSP(F) problem, we give an explicit constant αF such that (αF − ε)-approximation can\\nbe computed in O(log n) space, while (αF + ε)-approximation requires space Ω(\\n√\\nn), for every ε > 0.\\nTheorem 1.1. Let F ⊆ {TR,OR,XOR,AND} be a set of allowed binary predicates. Let αF = minG⊆F αG,\\nwhere αG is given in Table 1.\\nFor every ε > 0, there exists an (αF − ε)-approximate streaming algorithm for Max-CSP(F) that uses\\nspace O(ε−2 log n). On the other hand, any (αF + ε)-approximate streaming algorithm for Max-CSP(F)\\nrequires space Ω(\\n√\\nn).\\nType G Tightbound\\nPrevious bound\\nαG α\\npr\\nG Reference\\nTR 1 1 Folklore\\nOR 34 [\\n3\\n4 , 1] Folklore\\n{TR,OR}\\n√\\n2\\n2 [\\n√\\n5−1\\n2 , 1] [LS79]\\nXOR 12\\n1\\n2 [KK19]\\nAND 49 [\\n2\\n5 ,\\n1\\n2 ] [GVV17]\\nTable 1: Summary of known and new approximation factors αG for Max-CSP(G). We have suppressed (1±ε)\\nmultiplicative factors.\\nDiscussion. Interestingly, Theorem 1.1 identifies five Max-2CSP problems which completely character-\\nize the hardness of any Max-2CSP problem. Namely, we show that Max-CSP(F) is precisely as hard to\\napproximate as the hardest of the problems from Table 1 expressible by predicates from F .\\nIn particular, Theorem 1.1 closes the gap between 2/5 [GVV17] and 1/2 [KKS15] for the streaming\\napproximation ratio of Max-DICUT. We prove that neither of these bounds is tight, and that the correct\\nbound is 4/9. Similarly, it shows that the (\\n√\\n5− 1)/2-approximate algorithm of [LS79] for Max-2SAT can be\\nimproved further, and that the optimal approximation ratio is\\n√\\n2/2.\\n2\\nMany streaming problems have space-accuracy tradeoffs allowing for better approximations with more\\nspace (e.g., [Cha15, AKL16]). Curiously, Theorem 1.1 shows that every Max-2CSP(F) problem exhibits\\nsharp threshold behavior: it needs only logarithmic space to be approximated up to some constant αF , and\\nit requires polynomial space for every larger approximation factor.\\nIn the classical setting, approximation algorithms for Max-CSPs use space-inefficient techniques including\\nsemidefinite and linear programming, and network flow computations [Yan94, GW94, GW95, H˚as08, Rag08,\\nRS10, MM17]. On the other hand, the best streaming algorithms for Max-CSPs (except for the work [GVV17])\\nused only random assignments to the variables of the instance, including Max-CUT, Max-2SAT, and Unique\\nGames problems. We design streaming algorithms for the Max-2AND and Max-2OR problems (i.e., F =\\n{TR,AND} and F = {TR,OR}) which significantly improve on the approximation ratios guaranteed by\\na random assignment to the variables.\\nAdditionally, Theorem 1.1 reveals a curious difference between streaming approximation of Max-2EOR\\nand Max-2OR (i.e., Exact Max-2SAT and Max-2SAT). The former problem can be 3/4-approximated, while the\\nlatter does not admit better than\\n√\\n2/2-approximations. This shows that adding trivial constraints to Exact\\nMax-2SAT actually makes the problem harder to approximate. This is in sharp contrast to the classical\\nsetting of polynomial-time algorithms with polynomial space, where approximation-preserving reductions\\nbetween the two problems are known [Yan94]. While 3/4-approximation for Exact Max-2SAT is trivial,\\nmany 3/4-approximation algorithms for Max-2SAT use non-efficient (though polynomial) linear programming\\nroutines. This led Williamson to pose a question in 1998 whether there exists an algorithm for Max-2SAT\\nwhich does not use linear programming and at least matches the trivial 3/4-approximation guarantee for\\nExact Max-2SAT [Wil99]. The affirmative answer to this question was given by Poloczek and Schnitger\\nin 2011 [PS11, Pol11, VZ11, PSWVZ17]. Theorem 1.1 complements this result by showing that there\\nis no\\n√\\n2/2 < 3/4-approximation for Max-2SAT in the streaming setting, thus, separating space-efficient\\napproximations for Max-2SAT and Exact Max-2SAT.\\n1.2 Related Work\\nClassical setting. For every Max-2CSP(F) problem, a random assignment satisfies in expectation a con-\\nstant fraction αtrF of the constraints (this algorithm can be easily derandomized via the method of conditional\\nexpectations). In particular, this algorithm gives 1/2- and 1/4-approximations for Max-CUT and Max-2CSP.\\nOn one hand, H˚astad [H˚as01] used the PCP theorem to show that some Max-CSP problems, e.g., MAX-\\nE3SAT, do not admit better than αtrF -approximations unless P = NP. On the other hand, Goemans and\\nWilliamson [GW95] used semidefinite programming (SDP) to significantly improve the bounds for Max-CUT\\nand Max-2CSP to 0.87856 and 0.79607. H˚astad [H˚as08] proved that there is an SDP-based approximation\\nalgorithm with a better than αtrF approximation guarantee for every Max-2CSP. Many of the SDP-based\\napproximation algorithms are optimal under the Unique Games Conjecture [KV05, KKMO07]. We refer the\\nreader to [MM17] for an up-to-date overview of the literature.\\nStreaming setting. While there is a trivial 1/2-approximation for Max-CUT using space O(log n),\\nKapralov et al. [KKS15] showed that for any constant ε > 0, a (1/2+ε)-approximation requires space Ω˜(\\n√\\nn).\\nIndependently, Kogan and Krauthgamer [KK15] showed that (i) (1−ε)-approximation requires space Ω(n1−ε)\\nand (ii) 4/5-approximation requires Ω(nτ ) space for some constant τ > 0. In a subsequent work, [KKSV17]\\nshowed that (1 − ε)-approximation requires Ω(n) space. This line of work culminated in a recent result by\\nKapralov and Krachun [KK19] showing that any (1/2 + ε)-approximation for Max-CUT requires Ω(n) space.\\nRecently Guruswami et al. [GVV17] gave a (2/5 − ε)-approximate algorithm for Max-DICUT for\\nany constant ε > 0, significantly improving on the trivial 1/4-approximation. For k-SAT, Kogan and\\nKrauthgamer [KK15] showed that there is a (1−ε)-approximation using O˜(ε−2kn) space. The hardness side\\nhas been widely open prior to this work and, to the best of our knowledge, the only other hardness result is\\nby Guruswami and Tao [GT19] showing that (1/p+ ε)-approximation for Unique Games with alphabet size\\np requires Ω˜(\\n√\\nn) space for any constant ε > 0.\\n3\\n1.3 Techniques\\nStreaming algorithms. The first step of our proof of Theorem 1.1 is two new algorithms for Max-2OR\\nand Max-2AND that improve on the naive approximations for these problems. For these algorithms, we\\ngeneralize the notion of bias [GVV17] to all Max-2CSP problems, and prove a series of bounds on the value\\nof Max-2CSP w.r.t. the bias (and the numbers of trivial and non-trivial constraints in the instance). This\\nresults in log-space streaming algorithms that sketch the bias (and some additional information about the\\ninstance), and compute good estimates of the value of the instance.\\nIt is not hard to see that Max-2AND is the “hardest” Max-2CSP problem, i.e., an α-approximation\\nfor Max-2AND implies α-approximations for all Max-2CSPs (see Section 6). Therefore, the hardness result\\nof [KK19] for Max-CUT holds for Max-2AND as well, ruling out the possibility of (1/2 + ε)-approximations.\\nOn the other hand, a random assignment for Max-2AND formulas only guarantees a 1/4-approximation.\\nA recent work [GVV17] improves the approximation ratio to (2/5− ε) as follows.\\nLet Ψ be a Max-2EAND instance with m constraints, and val be the maximum number of simultaneously\\nsatisfiable constraints in Ψ. [GVV17] defines the bias of a variable x as the absolute difference between the\\nnumber of positive and negative occurrences of x, and the bias of the instance as the sum of biases of its\\nvariables. It is easy to see that for every instance, val ≤ (m+ bias)/2. [GVV17] proves that the assignment\\nof the input variables according to their biases satisfies at least bias constraints (see Lemma 3.2). Then they\\nconclude that max(bias,m/4) is a 2/5-approximation of val:\\nmax(bias,m/4)\\nval\\n≥ bias/5 + (m/4)(4/5)\\n(m+ bias)/2\\n= 2/5 .\\nThe upper and lower bounds of [GVV17] are shown in red and blue in Figure 1, and the gap between the\\nbounds indeed achieves 2/5 when bias = m/4. While both lower bounds val ≥ max(bias,m/4) are tight as\\nfunctions of bias and m, we show that in the important regime of low bias ∈ [0,m/3], these bounds can be\\nimproved to\\nval ≥ m\\n4\\n+\\nbias2\\n4(m− 2bias) . (1.2)\\nUnlike the lower bound of val ≥ bias from [GVV17], our lower bound cannot be achieved by a greedy\\nassignment to the input variables. Instead, we design a distribution of assignments, whose expected value is\\nat least (1.2). This improved lower bound on val (shown in green in Figure 1) leads to a 4/9-approximation\\nby a sketch for the expression (1.2). Namely, we give a O(log n)-space streaming algorithm that approximates\\nthe green and red bounds in Figure 1, and returns their maximum.\\nPerhaps surprisingly, the optimal approximation ratio for Max-2OR significantly differs from both the\\n3/4-approximation for Max-2EOR, and the trivial 1/2-approximation. The classical algorithm of [LS79] can\\nbe implemented in the streaming setting, but it only provides a (\\n√\\n5−1)/2 ≈ 0.61-approximation. We prove\\nthat the tight bound for Max-2OR is even larger—\\n√\\n2/2. Proofs of these upper and lower bounds are perhaps\\nthe most technical parts of this work. It can be shown that various naive random assignments to the variables\\nused in 1/2- and (\\n√\\n5− 1)/2-approximations cannot lead to better bounds. Instead we construct a family of\\ndistributions of assignments which depend on individual biases of the variables. We use these distributions\\nto prove the existence of assignments of some high value v, and finally we show a way to approximate v\\nin logarithmic space. We remark that it is not always possible to satisfy m\\n√\\n2/2 constraints, thus, we also\\nprove non-trivial upper bounds on val for the case when our estimate v is low v < m\\n√\\n2/2. (See Lemma 3.6\\nand Lemma 3.7 for formal statements of these results.)\\nHardness results. We develop a framework for proving hardness results for various Max-2CSP problems,\\nand use it to establish tight bounds for every Max-2CSP. This framework is based on the communica-\\ntion complexity lower bound of [KKS15] for the Distributional Boolean Hidden Partition problem (DBHP)\\n(which, in turn, extends the results of [GKK+07, VY11] for Boolean Hidden Matching and Boolean Hidden\\nHypermatching). In DBHP, Alice holds a random bipartition of [n], and Bob has a (random) graph G on n\\n4\\n0 0.2m 0.4m 0.6m 0.8m m\\n0\\n0.2m\\n0.4m\\n0.6m\\n0.8m\\nm\\nbias(Ψ)\\nR\\nan\\nge\\nof\\nva\\nl(\\nΨ\\n)\\nFigure 1: Upper and lower bounds on the maximum number val of simultaneously satisfiable constraints of\\nMax-2AND as a function of bias. The blue line is the upper bound m+bias2 , and the red line is the lower bound\\nmax\\n(\\nm\\n4 , bias\\n)\\nfrom [GVV17] (see Lemma 3.2). The green line is the new lower bound m4 +\\nbias2\\n4(m−2bias) from\\nLemma 3.3 in the interval bias ∈ [0,m/3].\\nvertices with some edges marked. Their goal is to use minimal communication to distinguish between the\\nfollowing two cases: in the YES case, the set of Bob’s marked edges is exactly the edges of G that cross\\nAlice’s bipartition; while in the NO case, a random subset of the edges is marked. [KKS15] proved a lower\\nbound of Ω(\\n√\\nn) on the randomized one-way communication complexity of DBHP.\\nFor a set of allowed predicates G, we construct a reduction from DBHP to Max-CSP(G), which naturally\\ninduces distributions DY and DN of Max-CSP(G) instances. Then by a careful analysis we show that the\\ngap between the optimal solutions of instances from DY and DN achieves αG+ε with high probability. This,\\namplified by a series of repetitions, lets us conclude that a space-efficient (αG + ε)-approximate algorithm\\nwould contradict the lower bound on the communication complexity of DBHP.\\nIn our framework, we give separate reductions from DBHP to Max-2EAND and Max-2OR with approxi-\\nmation ratios 4/9 + ε and\\n√\\n2/2 + ε, respectively. For the Max-2EOR problem, we give an efficient streaming\\nreduction from Max-CUT to Max-2EOR which asserts that an α-approximation for Max-2EOR implies an\\nα/(3− 2α)-approximation for Max-CUT. This, equipped with the lower bound from [KK19], proves a linear\\nlower bound Ω(n) on the space complexity of (3/4 + ε)-approximations of Max-2EOR.\\nPutting it all together. Finally, we show that our algorithms for the five problems from Table 1 can\\nbe combined together to handle every Max-2CSP problem. Similarly, we prove that the established lower\\nbounds for these five problems cover all possible Max-2CSPs. This implies that every Max-2CSP problem\\nMax-CSP(F) is precisely as hard to approximate as the hardest problem from Max-TR, Max-2EOR, Max-2OR,\\nMax-2EXOR, Max-2EAND expressible in F , and finishes the proof of Theorem 1.1.\\n1.4 Structure\\nIn Section 2, we review some necessary background knowledge. In Section 3, we provide streaming algorithms\\nwith optimal approximation ratios for all Max-2CSP problems. Sections 4 and 5 are devoted to proving tight\\nbounds on the approximation ratios of streaming algorithms from Section 3. In particular, Section 4 contains\\nthe general framework for our lower bounds, and the reductions from Distributional Boolean Hidden Partition\\nto Max-2CSP problems. Section 5 provides a tight analysis of the approximation ratios resulting from these\\nreductions. Finally, in Section 6, we combine the results of the previous sections to prove Theorem 1.1.\\n5\\n2 Preliminaries\\nLet N = {1, 2, . . . , } be the set of natural numbers, and [n] = {1, 2, . . . , n} for any n ∈ N. We use unionsq for the\\ndisjoint union of two sets. For an 0 < ε < 1, B ∈ (1 ± ε) is shorthand for 1 − ε ≤ B ≤ 1 + ε. For ease of\\nexposition we will abuse notation and associate a vector X ∈ {0, 1}n with the set X ⊆ [n], X = {i : Xi = 1}.\\nAs we explained in Section 1, we will primarily consider Max-CSP(G) where G ∈\\n{TR,OR, {TR,OR},XOR,AND}. In order to get familiar with these problems, we provide several\\nexamples in Table 2.\\ntype G OR {TR,OR} XOR AND\\nproblem name Max-2EOR Max-2OR Max-2EXOR Max-2EAND\\nspecial case Exact Max-2SAT Max-2SAT Max-CUT Max-DICUT\\nTable 2: For each case G ∈ {OR, {TR,OR},XOR,AND}, we give the name of the Max-CSP(G) problem, as\\nwell as one well-studied special case/alternative name of the problem.\\nFor an instance Ψ of a Max-2CSP problem, we denote the number of clauses (constraints) in Ψ by m = |Ψ|.\\nWe denote the set of Boolean variables of Ψ by X = {x1, . . . , xn}. A literal ` is called positive if ` = xi,\\nand negative if ` = ¬xi for some variable xi. A 1-clause is a clause (constraint) which depends only on one\\nvariable. We use pos\\n(1)\\ni (Ψ) and pos\\n(2)\\ni (Ψ) for the number of 1- and 2-clauses where the variable xi appears\\npositively. Similarly, neg\\n(1)\\ni (Ψ) and neg\\n(2)\\ni (Ψ) denote the number of 1- and 2-clauses containing ¬xi.\\nFor an assignment σ : X → {0, 1} of the variables of Ψ, we denote the number of clauses of Ψ satisfied\\nby σ as valΨ(σ). We denote the maximum number of simultaneously satisfiable clauses in Ψ as valΨ:\\nvalΨ = max\\nσ\\nvalΨ(σ) .\\nFor α ∈ [0, 1] and a set of allowed predicates F , an algorithm A is an α-approximation to the Max-CSP(F)\\nproblem if on any input Ψ, A outputs v, such that with probability 3/4, it holds that valΨ ≥ v ≥ α · valΨ .\\nFor example, when α = 1, the algorithm solves Max-CSP(F) exactly (with probability 3/4).\\nWe will use the following definition of the bias of Ψ, which generalizes the definition from [GVV17] to all\\nMax-2CSPs with clauses of length 1 or 2.3\\nDefinition 2.1 (Bias). The bias of a variable xi of an instance Ψ is defined as\\nbiasi(Ψ) =\\n1\\n2\\n· |2pos(1)i Ψ + pos(2)i (Ψ)− 2neg(1)i (Ψ)− neg(2)i (Ψ)| .\\nThe bias vector of Ψ is a vector b ∈ Rn, where bi = biasi(Ψ). Finally, the bias of the formula Ψ is defined\\nas the sum of biases of its variables:\\nbias(Ψ) =\\nn∑\\ni=1\\nbiasi(Ψ) =\\n1\\n2\\nn∑\\ni=1\\n|2pos(1)i Ψ + pos(2)i (Ψ)− 2neg(1)i (Ψ)− neg(2)i (Ψ)| .\\nNote that for every formula Ψ with |Ψ| = m clauses, 0 ≤ bias(Ψ) ≤ m.\\nIn order to approximate the bias of a formula Ψ, we will use a streaming algorithm for approximating\\nthe `1 norm of the bias vector of Ψ.\\nTheorem 2.2 ([Ind00, KNW10]). Given a stream S of poly(n) updates (i, v) ∈ [n] × {1,−1}, let xi =∑\\n(i,v)∈S v for i ∈ [n]. There exists a 1-pass streaming algorithm, which uses O(log n/ε2) bits of memory and\\noutputs a (1± ε)-approximation to the value `1(x) =\\n∑\\ni |xi| with probability 3/4.\\n3For uniformity reasons, our definition of bias differs from the definition in [GVV17] by a multiplicative factor of 2.\\n6\\nWe will need the following concentration inequality [KK19].\\nLemma 2.3 ([KK19, Lemma 2.5]). Let X =\\n∑\\ni∈[N ]Xi where Xi are Bernoulli random variables such that\\nfor any k ∈ [N ], E[Xk|X1, . . . , Xk−1] ≤ p for some p ∈ (0, 1). Let µ = Np. For any ∆ > 0,\\nPr [X ≥ µ+ ∆] ≤ exp\\n(\\n− ∆\\n2\\n2µ+ 2∆\\n)\\n.\\nFinally, we will use the lower bound on the space complexity of streaming algorithms for approximate\\nMax-CUT from [KK19].\\nTheorem 2.4. For any constant ε > 0, any streaming algorithm that (1/2+ε)-approximates Max-CUT with\\nsuccess probability at least 3/4 requires Ω(n) space.\\n2.1 Total variation distance\\nDefinition 2.5 (Total variation distance of discrete random variables). Let Ω be a finite probability space\\nand X,Y be random variables with support Ω. The total variation distance between X and Y is defined as\\nfollows.\\n‖X − Y ‖tvd := 1\\n2\\n∑\\nω∈Ω\\n|Pr[X = ω]− Pr[Y = ω]| .\\nWe will use the two following properties of the total variation distance.\\nProposition 2.6. Let Ω be a finite probability space and X,Y be random variables with support Ω.\\n1. (Triangle inequality) Let W be an arbitrary random variable, then we have ‖X−Y ‖tvd ≥ ‖X−W‖tvd−\\n‖Y −W‖tvd.\\n2. (Data processing inequality) Let W be a random variable that is independent of both X and Y , and f\\nbe a function, then we have ‖f(X,W )− f(Y,W )‖tvd ≤ ‖X − Y ‖tvd.\\nThe triangle inequality for the total variation distance is a standard fact; and the proof of the data\\nprocessing inequality can be found in [KKS15, Claim 6.5].\\n3 Streaming Algorithms\\nIn this section, we present optimal approximation algorithms for Max-2CSPs using O(log n) space. In Theo-\\nrem 1.1 in Section 6 we will prove that it is actually sufficient to design optimal algorithms for Max-CSP(G) in\\nthe following five cases G ∈ {TR,OR, {TR,OR},XOR,AND}. In Section 3.1, we present the trivial algorithm\\nfor Max-2CSPs, this algorithm turns our to be optimal for G ∈ {TR,OR,XOR}. Then we develop and analyze\\noptimal algorithms for the cases G = AND and G = {TR,OR} in Sections 3.2 and 3.3, respectively.\\nFor ease of exposition, we will assume that input instances never contain unsatisfiable and tautological\\nclauses (e.g., (x∧¬x), (x∨¬x)). This assumption is without loss of generality, because a streaming algorithm\\ncan ignore unsatisfiable clauses and have a separate counter for tautological clauses.\\n3.1 Trivial Algorithm\\nFirst we present the trivial algorithm: this algorithm takes a Max-2CSP instance Ψ, counts the number of\\nclauses m = |Ψ| in it, and outputs the expected number of clauses satisfied by a uniform random assignment\\nto the variables of Ψ. In Section 4 we will show that this algorithm gives the best streaming approximation\\nnot only in the case of Max-2XOR (the Max-CUT problem), but also in the case of Max-2EOR.\\n7\\nProposition 3.1 (Folklore). For a function f : {0, 1}2 → {0, 1}, let αf ∈ [0, 1] denote the fraction of 1s\\nin its truth table. Then for a set of allowed predicates F , we define αtrF = minf∈F αf . There exists a\\nstreaming algorithm that uses O(log n) space, and computes αtrF -approximation for Max-CSP(F) with success\\nprobability 1.\\nFor example, for the problem Max-2EOR (i.e., F = {OR}), we have αOR = 3/4, as every clause is satisfied\\nby 3 out of 4 possible assignments to its variables. Since the problem Max-2OR (i.e., F = {TR,OR}) also\\nallows clauses of length 1 (which are satisfied by 1 out of 2 possible assignments to the variable), we have\\nα{TR,OR} = 1/2.\\nProof of Proposition 3.1.\\nAlgorithm 1 αtrF -approximation streaming algorithm for Max-CSP(F)\\nInput: Ψ—an instance of Max-CSP(F).\\n1: Use O(log n) bits to compute m = |Ψ|.\\nOutput: v = αtrF ·m.\\nTo prove that Algorithm 1 computes an αtrF -approximation, we need to show that (i) there exists an\\nassignment σ such that valΨ(σ) ≥ v = αtrF ·m, and (ii) v = αtrF ·m ≥ αtrF · valΨ.\\nNote that since valΨ ≤ |Ψ| = m, (ii) holds trivially. The existence of an assignment σ satisfying (i)\\nis guaranteed by the following bound on the expected number of clauses satisfied by a uniform random\\nassignment σ:\\nE\\nσ\\n[valΨ(σ)] =\\n∑\\nC∈Ψ\\nPr\\nσ\\n[C is satisfied by σ] =\\n∑\\nC∈Ψ\\nαC ≥ αtrF ·m.\\nRemark. For an (αtrF−ε)-approximation, one can reduce the space usage of Algorithm 1 to O\\n(\\nε−2 log log n\\n)\\nbits by using the approximate counting algorithm of Morris [Mor78].\\nRemark. Formally, Algorithm 1 only guarantees a 1/2-approximation for the problem Max-CSP(TR), i.e.,\\nthe problem where all clauses have length 1. In this case, in order to achieve a (1− ε)-approximation using\\nO(log n) space for arbitrary constant ε > 0, one can use an `1-sketch (Theorem 2.2) to approximate the bias\\nvector of the input formula. Indeed, it is easy to see that for an instance Ψ of Max-CSP(TR) with m clauses,\\nvalΨ = (m+ bias(Ψ))/2.\\nWe give αtrG for relevant sets of predicates in Table 3.\\nType G TR OR {TR,OR} XOR AND\\nαtrG 1\\n3\\n4\\n1\\n2\\n1\\n2\\n1\\n4\\nαoptG 1\\n3\\n4\\n√\\n2\\n2\\n1\\n2\\n4\\n9\\nTable 3: For various sets of predicates G, the table presents (i) αtrG—the approximation ratio guaranteed by\\nthe trivial algorithm for Max-CSP(G), and (ii) αoptG —the optimal approximation ratio of streaming algorithms,\\nproven in Sections 3 and 4 for Max-CSP(G). We have suppressed (1 − ε) multiplicative factors for the case\\nG = TR.\\nAs we show in the following sections, this trivial approximation algorithm can be improved for the\\nMax-2AND and Max-2OR problems.\\n8\\n3.2 Algorithm for Max-2AND and Max-2EAND\\nConsider a Max-2AND instance Ψ′ where all clauses are of length 1 or 2. Note that Ψ′ can be written as\\nan equivalent Max-2AND instance Ψ, where 1-clauses of Ψ′ are replaced with 2-clauses containing the same\\nliteral twice.4 In this section, we will consider such representation of every instance of Max-2AND, i.e., we\\nwill assume that all clauses have exactly 2 (not necessarily distinct) literals. Note that in this case, the bias\\n(see Definition 2.1) of Ψ is simply\\nbias(Ψ) =\\n1\\n2\\nn∑\\ni=1\\n|pos(2)i (Ψ)− neg(2)i (Ψ)| ,\\nwhere pos\\n(2)\\ni (Ψ) and neg\\n(2)\\ni (Ψ) are the numbers of occurrences of xi and ¬xi in 2-clauses.\\n[GVV17] gave lower and upper bounds for the maximum number of satisfied clauses valΨ in terms of\\nbias(Ψ) and m (the number of clauses in Ψ). For the sake of being self contained, and to verify that these\\nbounds hold for our slightly more general case where 2-clauses may contain repeated literals, we present the\\nproofs of these bounds in Lemma 3.2 in Section 3.2.1.\\nLemma 3.2 ([GVV17]). Let Ψ by a Max-2AND instance with m clauses. Then\\nbias(Ψ) ≤ valΨ ≤ m+ bias(Ψ)\\n2\\n.\\nWe improve the lower bound of [GVV17] in the important regime of bias(Ψ) ≤ m/3 in the following\\nlemma.\\nLemma 3.3. Let Ψ by a Max-2AND instance with m clauses and bias(Ψ) ≤ m/3. Then\\nvalΨ ≥ m\\n4\\n+\\nbias(Ψ)2\\n4(m− 2bias(Ψ)) ≥\\n2(m+ bias(Ψ))\\n9\\n.\\nThe proof of Lemma 3.3 is based on biased random sampling, and is postponed to Section 3.2.1. For a\\npictorial view of this improvement, see Figure 1.\\nWe are now ready to present a streaming algorithm that (4/9)-approximates Max-2AND and Max-2EAND.\\nTheorem 3.4 ( 49–approximation for Max-2AND and Max-2EAND). For any ε ∈ (0, 0.01), there exists a\\nstreaming algorithm that uses space O(ε−2 log n) and computes\\n(\\n4\\n9 − ε\\n)\\n-approximation for Max-2AND and\\nMax-2EAND with success probability at least 3/4.\\nProof. The algorithm uses the bounds from Lemmas 3.2 and 3.3 to approximate the value of a given instance\\nof Max-2AND.\\nTo prove the correctness of Algorithm 2, we show that (i) valΨ ≥ v and (ii) v ≥\\n(\\n4\\n9 − ε\\n) · valΨ, where v\\nis the output of Algorithm 2.\\n(i) v ≤ valΨ. Since B is an (1± δ)-approximation of the bias, with probability at least 3/4 we have that\\n(1− δ) · bias(Ψ) ≤ B ≤ (1 + δ) · bias(Ψ).\\nFirst, consider the case where B ∈ [0, m3 (1− δ)] :\\nv =\\n2(m+B)\\n9(1 + δ)\\n≤ 2(1 + δ)(m+ bias(Ψ))\\n9(1 + δ)\\n=\\n2(m+ bias(Ψ))\\n9\\n≤ valΨ,\\nwhere the last inequality uses the bound from Lemma 3.3.\\n4We only apply this transformation to Max-2AND instances, because here it plays in our favor. For example, an AND clause\\nwith repeated literals is satisfied by a uniform random assignment with probability 1/2, while an AND clause with distinct\\nvariables is satisfied with probability only 1/4. For the case of OR, a clause with repeated literals would be satisfied only with\\nprobability 1/2, while an OR clause with distinct variables would be satisfied with probability 3/4.\\n9\\nAlgorithm 2\\n(\\n4\\n9 − ε\\n)\\n-approximation streaming algorithm for Max-2AND\\nInput: Ψ—an instance of Max-2AND. Error parameter ε ∈ (0, 0.01).\\n1: Approximate the `1-norm of the bias vector with error δ = ε/2 (Theorem 2.2):\\nCompute B ∈ (1± δ) bias(Ψ).\\n2: Count the number of clauses m = |Ψ|.\\n3: if B ∈ [0, m3 (1− δ)] then\\nOutput: v = 2(m+B)9(1+δ) .\\n4: else\\nOutput: v = B(1+δ) .\\nNow consider the case where B > m3 (1− δ):\\nv =\\nB\\n(1 + δ)\\n≤ bias(Ψ) ≤ valΨ,\\nwhere the last inequality follows from the bound valΨ ≥ bias(Ψ) from Lemma 3.2.\\n(ii) v ≥ (4\\n9\\n− ε) · valΨ. First, consider the case where B ∈ [0, m3 (1− δ)] :,\\nv =\\n2(m+B)\\n9(1 + δ)\\n≥ 2(1− δ)(m+ bias(Ψ))\\n9(1 + δ)\\n≥ 2(1− 2δ)(m+ bias(Ψ))\\n9\\n≥\\n(\\n4\\n9\\n− ε\\n)\\n· valΨ ,\\nwhere the last inequality follows from the bound valΨ ≤ m+bias(Ψ)2 of Lemma 3.2 and δ = ε/2.\\nNow consider the case where B > m3 (1− δ). From Lemma 3.2, valΨ ≤ m+bias(Ψ)2 . Then\\nv\\nvalΨ\\n≥ 2v\\nm+ bias(Ψ)\\n=\\n2B\\n(1 + δ)(m+ bias(Ψ))\\n≥ 2B\\n(1 + δ)(m+ B1−δ )\\n≥ 2B\\n(1 + 3δ)(m+B)\\n≥\\n2m(1−δ)\\n3\\n(1 + 3δ) 4m3\\n=\\n1\\n2\\n· 1− δ\\n1 + 3δ\\n>\\n4\\n9\\nfor every δ < 0.01.\\nWe conclude that Algorithm 2 outputs a (4/9− ε)-approximation for Max-2AND and Max-2EAND.\\n3.2.1 Proofs of Lemma 3.2 and Lemma 3.3\\nLemma 3.2 ([GVV17]). Let Ψ by a Max-2AND instance with m clauses. Then\\nbias(Ψ) ≤ valΨ ≤ m+ bias(Ψ)\\n2\\n.\\nProof. In order to prove the lower bound valΨ ≥ bias(Ψ), we give an assignment σ to the input variables\\nwhich satisfies at least bias(Ψ) clauses. This assignment σ will greedily assign the value of each variable\\naccording to its bias: the variables which appear positively more often than negatively will be assigned 1,\\nand the remaining variables will be assigned 0.\\nRecall that pos\\n(2)\\ni (Ψ) and neg\\n(2)\\ni (Ψ) denote the number of clauses where xi appears positively and nega-\\ntively. For every variable xi with pos\\n(2)\\ni (Ψ) ≥ neg(2)i (Ψ), we set σ(xi) = 1, and we set σ(xi) = 0 otherwise.\\nNote that the number of unsatisfied literals in this case is\\n∑\\ni min\\n{\\npos\\n(2)\\ni (Ψ),neg\\n(2)\\ni (Ψ)\\n}\\n. Thus, the number\\nof unsatisfied clauses is also bounded from above by\\n∑\\ni min\\n{\\npos\\n(2)\\ni (Ψ),neg\\n(2)\\ni (Ψ)\\n}\\n.\\n10\\nFrom\\n2bias(Ψ) =\\n∑\\ni\\nmax\\n{\\npos\\n(2)\\ni (Ψ),neg\\n(2)\\ni (Ψ)\\n}\\n−min\\n{\\npos\\n(2)\\ni (Ψ),neg\\n(2)\\ni (Ψ)\\n}\\n2m =\\n∑\\ni\\nmax\\n{\\npos\\n(2)\\ni (Ψ),neg\\n(2)\\ni (Ψ)\\n}\\n+ min\\n{\\npos\\n(2)\\ni (Ψ),neg\\n(2)\\ni (Ψ)\\n}\\nwe have that ∑\\ni\\nmin\\n{\\npos\\n(2)\\ni (Ψ),neg\\n(2)\\ni (Ψ)\\n}\\n= m− bias(Ψ) . (3.5)\\nThus,\\nvalΨ(σ) ≥ m− (m− bias(Ψ)) = bias(Ψ) .\\nFor the upper bound of valΨ ≤ m2 + bias(Ψ)2 we note that for every assignment σ, the number of unsatisfied\\nliterals is at least\\n∑\\ni min\\n{\\npos\\n(2)\\ni (Ψ),neg\\n(2)\\ni (Ψ)\\n}\\n. (Since xi = 1 produces pos\\n(2)\\ni (Ψ) unsatisfied literals,\\nwhile xi = 0 produces neg\\n(2)\\ni (Ψ) unsatisfied literals.) Thus, the number of unsatisfied clauses is at least\\n1\\n2\\n∑\\ni min\\n{\\npos\\n(2)\\ni (Ψ),neg\\n(2)\\ni (Ψ)\\n}\\n. From (3.5), we have that for every assignment σ,\\nvalΨ(σ) ≤ m− 1\\n2\\n(m− bias(Ψ)) = m+ bias(Ψ)\\n2\\n.\\nLemma 3.3. Let Ψ by a Max-2AND instance with m clauses and bias(Ψ) ≤ m/3. Then\\nvalΨ ≥ m\\n4\\n+\\nbias(Ψ)2\\n4(m− 2bias(Ψ)) ≥\\n2(m+ bias(Ψ))\\n9\\n.\\nProof. First we show that for every Max-2AND instance Ψ with m clauses and bias(Ψ) ≤ m/3, there exists\\nan assignment σ s.t.\\nvalΨ(σ) ≥ 2(m+ bias(Ψ))\\n9\\n.\\nWithout loss of generality we can assume that every variable appears in Ψ positively at least as many\\ntimes as it appears negatively, i.e., pos\\n(2)\\ni (Ψ) ≥ neg(2)i (Ψ) for every i ∈ [n].5 We prove the existence of such\\nan assignment σ by giving a distribution of assignments whose expected number of satisfied clauses is at\\nleast 2(m+bias(Ψ))9 . Let γ ∈ [0, 0.5] be a parameter to be assigned later. For each variable xi, we assign xi = 1\\nwith probability 12 + γ, and xi = 0 with probability\\n1\\n2 − γ.6 Let k0, k1, and k2 denote the number of clauses\\nwith zero, one, and two positive literals. Observe that m = k0 + k1 + k2 and\\n2bias(Ψ) =\\n∑\\ni∈[n]\\n|pos(2)i (Ψ)− neg(2)i (Ψ)| = (2k2 + k1)− (k1 + 2k0) = 2(k2 − k0) .\\nLet us now compute the expected number of satisfied AND clauses under the biased distribution described\\nabove. Note that a clause with two (not necessarily distinct) positive literals is satisfied with probability\\nat least min\\n{(\\n1\\n2 + γ\\n)2\\n, 12 + γ\\n}\\n=\\n(\\n1\\n2 + γ\\n)2\\n. Similarly, a clause with two negative literals is satisfied with\\n5Indeed, given a instance Ψ where pos\\n(2)\\ni (Ψ) < neg\\n(2)\\ni (Ψ), we can consider the instance Ψ\\n′ where every xi is replaced with\\n¬xi, and vice versa. We have that pos(2)i (Ψ′) ≥ neg(2)i (Ψ′), bias(Ψ) = bias(Ψ′), and every assignment for Ψ′ is uniquely mapped\\nto the corresponding assignment for Ψ satisfying the same number of clauses.\\n6Note that if we set γ = 0, the algorithm becomes the trivial random sampling, and if we set γ = 0.5, the algorithm becomes\\nthe greedy algorithm from [GVV17].\\n11\\nprobability at least\\n(\\n1\\n2 − γ\\n)2\\n, and a clause with a positive and negative literals (corresponding to different\\nvariables) is satisfied with probability\\n(\\n1\\n2 − γ\\n) (\\n1\\n2 + γ\\n)\\n.\\nE\\nσ\\n[valΨ(σ)] =\\n2∑\\ni=0\\nki · Pr\\nσ\\n[a clause with i positive literals is satisfied by σ]\\n= k0 ·\\n(\\n1\\n2\\n− γ\\n)2\\n+ k1 ·\\n(\\n1\\n2\\n− γ\\n)(\\n1\\n2\\n+ γ\\n)\\n+ k2 ·\\n(\\n1\\n2\\n+ γ\\n)2\\n=\\nk0 + k1 + k2\\n4\\n+ (k2 − k0) · γ + (k2 − k1 + k0) · γ2\\n=\\nm\\n4\\n+ bias(Ψ) · γ + (2(k2 + k0)−m) · γ2\\n≥ m\\n4\\n+ bias(Ψ) · γ + (2bias(Ψ)−m) · γ2 ,\\nwhere we used that m = k0 + k1 + k2 and 2bias(Ψ) = 2(k2 − k0) ≤ 2(k2 + k0).\\nSince bias(Ψ) ∈ [0,m/3], we can set γ = bias(Ψ)2(m−2bias(Ψ)) ∈ [0, 0.5] and have that\\nE\\nσ\\n[valΨ(σ)] ≥ m\\n4\\n+\\nbias(Ψ)2\\n4(m− 2bias(Ψ)) .\\nFinally, it remains to show that m4 +\\nbias(Ψ)2\\n4(m−2bias(Ψ)) ≥ 2(m+bias(Ψ))9 :\\nm\\n4\\n+\\nbias(Ψ)2\\n4(m− 2bias(Ψ)) =\\n2(m+ bias(Ψ))\\n9\\n+\\nm− 8bias(Ψ)\\n36\\n+\\nbias(Ψ)2\\n4(m− 2bias(Ψ))\\n=\\n2(m+ bias(Ψ))\\n9\\n+\\n(m− 8bias(Ψ))(m− 2bias(Ψ)) + 9bias(Ψ)2\\n36(m− 2bias(Ψ))\\n=\\n2(m+ bias(Ψ))\\n9\\n+\\n(5bias(Ψ)−m)2\\n36(m− 2bias(Ψ))\\n≥ 2(m+ bias(Ψ))\\n9\\n,\\nwhich holds for every bias(Ψ) ∈ [0,m/3].\\n3.3 Algorithm for Max-2OR\\nFor the case of Max-2OR, it is crucial to distinguish 1- and 2-clauses. Therefore, we treat clauses containing\\ntwo identical literals as 1-clauses. We denote the number of 1-clauses of Ψ by m1, and the number of 2-clauses\\nby m2. In particular, the total number of clauses is m = m1 +m2.\\nIn Lemmas 3.6 and 3.7 we give upper and lower bounds on valΨ in terms of m1,m2, and bias(Ψ), we\\npostpone their proofs to Section 3.3.1. In this section we prove that the ratio between the presented lower\\nand upper bounds is bounded by\\n√\\n2\\n2 , and that there is a O(log n)-space algorithm that sketches the lower\\nbounds of Lemma 3.7 on valΨ.\\nWhen the bias of Ψ is large (say, bias(Ψ) = m), it might be possible to satisfy all m clauses of Ψ, so no\\nnon-trivial upper bounds on valΨ can be proven in terms of bias in this case. Even if the bias is low (say,\\nbias(Ψ) = 0), but the formula does not contain 1-clauses, it might still be possible to satisfy all clauses of\\nΨ. (E.g., if all clauses of Ψ contain one positive and one negative literal.) It turns out that for the optimal\\napproximation ratio, we need to bound from above valΨ in the case of low bias and large number of 1-clauses.\\n12\\nLemma 3.6. Let Ψ be a Max-2OR instance with m1 1-clauses, and m2 2-clauses. Then\\nval(Ψ) ≤ min\\n{\\nm1 +m2,\\nm1 + 2m2 + bias(Ψ)\\n2\\n}\\n.\\nThe trivial algorithm guarantees that for every Max-2OR instance Ψ, valΨ ≥ m1/2 + 3m2/4. While this\\nbound is tight in terms of m1 and m2, for instances with high bias > m2/2, we prove a better lower bound\\nof val ≥ (m1 +m2 + bias(Ψ))/2. Clearly, this bound is not sufficient for a better than 1/2-approximation in\\nthe case of low bias(Ψ) = 0. In order to handle this case, we design a distribution of assignments which in\\nexpectation satisfy a large number of clauses in formulas with low bias.\\nLemma 3.7. Let Ψ be a Max-2OR instance with m1 1-clauses, and m2 2-clauses. Then\\n1. valΨ ≥ m1+m2+bias(Ψ)2 ;\\n2. if bias(Ψ) ≤ m2, then\\nvalΨ ≥ m1\\n2\\n+\\n3m2\\n4\\n+\\nbias(Ψ)2\\n4m2\\n.\\nWe will also use the following simple claim.\\nClaim 3.8. For every x ≥ 0, y > 0:\\n2x+ 3y + x2/y\\n4(x+ y)\\n≥\\n√\\n2\\n2\\n.\\nProof. Let z = xy + 1, then\\n2x+ 3y + x2/y\\n4(x+ y)\\n=\\n2xy + 3 +\\nx2\\ny2\\n4\\n(\\nx\\ny + 1\\n) = z2 + 2\\n4z\\n=\\nz\\n4\\n+\\n1\\n2z\\n≥\\n√\\n2\\n2\\n,\\nby the inequality of arithmetic and geometric means.\\nNow we are ready to present an approximation algorithm for the Max-2OR problem.\\nTheorem 3.9 (\\n√\\n2\\n2 –approximation for Max-2OR). For any ε ∈ (0, 0.01), there exists a streaming algorithm\\nthat uses space O(ε−2 log n) and computes\\n(√\\n2\\n2 − ε\\n)\\n-approximation for Max-2OR with success probability at\\nleast 3/4.\\nProof. We prove that Algorithm 3 computes a\\n(√\\n2\\n2 − ε\\n)\\n-approximation by showing that (i) v ≤ valΨ, and\\n(ii) v ≥\\n(√\\n2\\n2 − ε\\n)\\n·valΨ, where v is the output of the algorithm. Recall that by the guarantee of Theorem 2.2,\\nwith probability at least 3/4:\\n(1− δ)bias(Ψ) ≤ B ≤ (1 + δ)bias(Ψ) .\\n(i) v ≤ valΨ. If B ≤ (1− δ)m2, then bias(Ψ) ≤ B/(1− δ) ≤ m2, and, thus, valΨ ≥ m12 + 3m24 + bias(Ψ)\\n2\\n4m2\\nby\\nthe second bound in Lemma 3.7. Then\\nv =\\n(1− δ)2(2m1 + 3m2 +B2/m2)\\n4\\n≤ (2m1 + 3m2 + bias(Ψ)\\n2/m2)\\n4\\n≤ valΨ .\\nIf B > (1− δ)m2, then\\nv =\\n(1− δ)(m1 +m2 +B)\\n2\\n≤ m1 +m2 + bias(Ψ)\\n2\\n≤ valΨ\\nby the first bound in Lemma 3.7.\\n13\\nAlgorithm 3\\n(√\\n2\\n2 − ε\\n)\\n-approximation streaming algorithm for Max-2OR\\nInput: Ψ—an instance of Max-2OR. Error parameter ε ∈ (0, 0.01).\\n1: Approximate the `1-norm of the bias vector with error δ = ε/4 (Theorem 2.2):\\nCompute B ∈ (1± δ) bias(Ψ).\\n2: Count the number of 1- and 2- clauses m1 and m2.\\n3: if B ∈ [0, (1− δ)m2] then\\nOutput: v = (1−δ)\\n2(2m1+3m2+B\\n2/m2)\\n4 .\\n4: else\\nOutput: v = (1−δ)(m1+m2+B)2 .\\n(ii) v ≥\\n(√\\n2\\n2\\n− ε\\n)\\n· valΨ. Let us consider three cases.\\n1. B ≤ (1− δ)m2 and m1 ≤ bias(Ψ).\\nIn this case the output of the algorithm is\\nv =\\n(1− δ)2(2m1 + 3m2 +B2/m2)\\n4\\n≥ (1− δ)\\n4(2m1 + 3m2 + bias(Ψ)\\n2/m2)\\n4\\n≥ (1− 4δ)(2m1 + 3m2 + bias(Ψ)\\n2/m2)\\n4\\n.\\nFrom the upper bound valΨ ≤ m1 +m2 of Lemma 3.6, we have that\\nv\\nvalΨ\\n≥ (1− 4δ) · 2m1 + 3m2 + bias(Ψ)\\n2/m2\\n4(m1 +m2)\\n≥ (1− 4δ) · 2bias(Ψ) + 3m2 + bias(Ψ)\\n2/m2\\n4(bias(Ψ) +m2)\\n≥ (1− 4δ) ·\\n√\\n2\\n2\\n= (1− ε) ·\\n√\\n2\\n2\\n,\\nwhere the second inequality follows from m1 ≤ bias(Ψ), and the last inequality follows from Claim 3.8.\\n2. B ≤ (1− δ)m2 and m1 > bias(Ψ).\\nFrom the upper bound valΨ ≤ m1+2m2+bias(Ψ)2 of Lemma 3.6:\\nv\\nvalΨ\\n≥ (1− 4δ) · 2m1 + 3m2 + bias(Ψ)\\n2/m2\\n2(m1 + 2m2 + bias(Ψ))\\n≥ (1− 4δ) · 2bias(Ψ) + 3m2 + bias(Ψ)\\n2/m2\\n4(bias(Ψ) +m2)\\n≥ (1− 4δ) ·\\n√\\n2\\n2\\n= (1− ε) ·\\n√\\n2\\n2\\n,\\nwhere the second inequality is due to m1 > bias(Ψ), and the last inequality is due to Claim 3.8.\\n3. B > (1− δ)m2.\\n14\\nFrom the bound in Lemma 3.6:\\nvalΨ ≤ min\\n{\\nm1 +m2,\\nm1 + 2m2 + bias(Ψ)\\n2\\n}\\n≤ 1\\n3\\n· (m1 +m2) + 2\\n3\\n· m1 + 2m2 + bias(Ψ)\\n2\\n=\\n2m1 + 3m2 + bias(Ψ)\\n3\\n≤ 2m1 + 3m2 +B\\n3(1− δ) .\\nIn this case, the output of the algorithm is v = (1−δ)(m1+m2+B)2 . Then\\nv\\nvalΨ\\n≥ 3(1− δ)\\n2\\n2\\n· m1 +m2 +B\\n2m1 + 3m2 +B\\n≥ 3(1− δ)\\n2\\n2\\n· m1 +m2(2− δ)\\n2m1 + 4m2\\n≥ 3(1− δ)\\n2(2− δ)\\n8\\n≥\\n√\\n2\\n2\\n,\\nwhere the second inequality is due to B > (1− δ)m2, and the last one holds for every δ < 0.01.\\n3.3.1 Proofs of Lemma 3.6 and Lemma 3.7\\nLemma 3.6. Let Ψ be a Max-2OR instance with m1 1-clauses, and m2 2-clauses. Then\\nval(Ψ) ≤ min\\n{\\nm1 +m2,\\nm1 + 2m2 + bias(Ψ)\\n2\\n}\\n.\\nProof. Since m1 +m2 is the number of clauses in Ψ, the first bound val(Ψ) ≤ m1 +m2 holds trivially.\\nFirst we negate all variables of Ψ with biasi(Ψ) < 0. This transformation does not change\\nbias(Ψ),m1,m2, val(Ψ), and every assignment of the variables of the original instance can be uniquely mapped\\nto a corresponding assignment for the new instance satisfying the same number of clauses. Therefore, without\\nloss of generality, for every i ∈ [n],\\npos\\n(1)\\ni (Ψ) +\\npos\\n(2)\\ni (Ψ)\\n2\\n− neg(1)i (Ψ)−\\nneg\\n(2)\\ni (Ψ)\\n2\\n≥ 0 .\\nConsider an assignment σ to the variables of Ψ. We need to show that valΨ(σ) ≤ m1+2m2+bias(Ψ)2 . Let T\\nbe the set of (indices of) variables of σ assigned the value 1. Then the number of 1 clauses satisfied by σ is\\nS1 =\\n∑\\ni∈T\\npos\\n(1)\\ni (Ψ) +\\n∑\\ni 6∈T\\nneg\\n(1)\\ni (Ψ) .\\nLet S2 denote the number of 2-clauses satisfied by σ. We will show that\\nS2 ≤ min {m2, bias(Ψ) +m1 +m2 − 2S1} . (3.10)\\nFirst we show how (3.10) finishes the proof of the lemma, and then prove (3.10).\\nIndeed, then the number of clauses satisfied by σ is bounded from above by\\nvalΨ(σ) ≤ S1 + S2\\n≤ S1 + min {m2, bias(Ψ) +m1 +m2 − 2S1}\\n≤ S1 + m2\\n2\\n+\\nbias(Ψ) +m1 +m2 − 2S1\\n2\\n=\\nm1 + 2m2 + bias(Ψ)\\n2\\n.\\n15\\nNow we will prove the bound (3.10). The bound S2 ≤ m2 is trivial, since m2 is the total number of\\n2-clauses in the instance. The number of 2-clauses satisfied by variables set to 1 is bounded from above by∑\\ni∈T pos\\n(2)\\ni (Ψ), and the number of 2-clauses satisfied by variables set to 0 is bounded by\\n∑\\ni6∈T neg\\n(2)\\ni (Ψ).\\nTherefore,\\nS2 ≤\\n∑\\ni∈T\\npos\\n(2)\\ni (Ψ) +\\n∑\\ni 6∈T\\nneg\\n(2)\\ni (Ψ) . (3.11)\\nRecall that\\nbias(Ψ) =\\n∑\\ni∈[n]\\npos\\n(1)\\ni (Ψ) +\\npos\\n(2)\\ni (Ψ)\\n2\\n− neg(1)i (Ψ)−\\nneg\\n(2)\\ni (Ψ)\\n2\\n, (3.12)\\nm1 =\\n∑\\ni∈[n]\\npos\\n(1)\\ni (Ψ) + neg\\n(1)\\ni (Ψ) , (3.13)\\nm2 =\\n∑\\ni∈[n]\\npos\\n(2)\\ni (Ψ)\\n2\\n+\\nneg\\n(2)\\ni (Ψ)\\n2\\n, (3.14)\\n−2S1 = −2\\n∑\\ni∈T\\npos\\n(1)\\ni (Ψ)− 2\\n∑\\ni 6∈T\\nneg\\n(1)\\ni (Ψ) , (3.15)\\nand since biasi(Ψ) ≥ 0 for every i:\\n0 ≥ −2\\n∑\\ni 6∈T\\nbiasi(Ψ) = −\\n∑\\ni6∈T\\n2pos\\n(1)\\ni (Ψ) + pos\\n(2)\\ni (Ψ)− 2neg(1)i (Ψ)− neg(2)i (Ψ) . (3.16)\\nSumming (3.12), (3.13), (3.14), (3.15), and (3.16) gives\\nbias(Ψ) +m1 +m2 − 2S1 ≥\\n∑\\ni∈T\\npos\\n(2)\\ni (Ψ) +\\n∑\\ni 6∈T\\nneg\\n(2)\\ni (Ψ) ≥ S2,\\nwhere the last inequality uses (3.11). This finishes the proof of (3.10) and the proof of the lemma.\\nLemma 3.7. Let Ψ be a Max-2OR instance with m1 1-clauses, and m2 2-clauses. Then\\n1. valΨ ≥ m1+m2+bias(Ψ)2 ;\\n2. if bias(Ψ) ≤ m2, then\\nvalΨ ≥ m1\\n2\\n+\\n3m2\\n4\\n+\\nbias(Ψ)2\\n4m2\\n.\\nProof. Without loss of generality, we assume that for every i ∈ [n], biasi(Ψ) ≥ 0. (Again, we can negate all\\nvariables with biasi(Ψ) < 0, and define a bijection between the assignments for the two formulas.) Therefore,\\nfor every i ∈ [n],\\npos\\n(1)\\ni (Ψ) +\\npos\\n(2)\\ni (Ψ)\\n2\\n− neg(1)i (Ψ)−\\nneg\\n(2)\\ni (Ψ)\\n2\\n≥ 0 .\\nLet p1 and n1 be the numbers of 1-clauses with positive and negative literals in Ψ. Let k0, k1, and k2 denote\\nthe numbers of 2-clauses with 0, 1, and 2 positive literals. Then m1 = p1 + n1, and m2 = k0 + k1 + k2.\\nNote that\\nbias(Ψ) =\\n∑\\ni\\npos\\n(1)\\ni (Ψ) +\\npos\\n(2)\\ni (Ψ)\\n2\\n− neg(1)i (Ψ)−\\nneg\\n(2)\\ni (Ψ)\\n2\\n= p1 − n1 + k2 − k0 .\\n16\\nConsider the distribution of assignments to the variables of Ψ, where every variable xi is assigned the\\nvalue 1 independently with probability ( 12 +γ), for a parameter γ ∈ [0, 0.5] to be assigned later. The expected\\nnumber of satisfied 1-clauses under this distribution is\\nS1 =\\n∑\\ni\\n(\\n1\\n2\\n+ γ\\n)\\n· pos(1)i (Ψ) +\\n(\\n1\\n2\\n− γ\\n)\\n· neg(1)i (Ψ) =\\n(\\n1\\n2\\n+ γ\\n)\\np1 +\\n(\\n1\\n2\\n− γ\\n)\\nn1 =\\nm1\\n2\\n+ γ(p1 − n1) .\\nSince every 2-clause contains distinct variables, the expected number of satisfied 2-clauses is\\nS2 = k0 ·\\n(\\n1−\\n(\\n1\\n2\\n+ γ\\n)2)\\n+ k1 ·\\n(\\n1−\\n(\\n1\\n2\\n+ γ\\n)(\\n1\\n2\\n− γ\\n))\\n+ k2 ·\\n(\\n1−\\n(\\n1\\n2\\n− γ\\n)2)\\n=\\n3m2\\n4\\n+ γ · (k2 − k0)− γ2 · (k0 + k2 − k1)\\n≥ 3m2\\n4\\n+ γ · (k2 − k0)−m2γ2 .\\nLet us now compute the expected number of clauses satisfied by an assignment σ from the distribution\\ndefined above.\\nE\\nσ\\n[valΨ(σ)] = S1 + S2 ≥ m1\\n2\\n+ γ(p1 − n1) + 3m2\\n4\\n+ γ · (k2 − k0)− 2m2γ2\\n=\\nm1\\n2\\n+\\n3m2\\n4\\n+ γbias(Ψ)−m2γ2 .\\nFirst, we set γ = 12 and derive the first bound:\\nvalΨ ≥ E\\nσ\\n[valΨ(σ)] ≥ m1 +m2 + bias(Ψ)\\n2\\n.\\nNow, for the case where bias(Ψ) ≤ m2, we set γ = bias(Ψ)2m2 ∈ [0, 0.5], and derive the second bound:\\nvalΨ ≥ E\\nσ\\n[valΨ(σ)] ≥ m1\\n2\\n+\\n3m2\\n4\\n+\\nbias(Ψ)2\\n4m2\\n.\\n4 Space Lower Bounds for Approximating Boolean Max-2CSP\\nIn this section, we establish space lower bounds for streaming approximations for all Max-2CSPs. In The-\\norem 1.1 in Section 6 we will show that it suffices to prove lower bounds for Max-CSP(G) for the following\\nfour cases G ∈ {OR, {TR,OR},XOR,AND}. A linear space lower bound for the case G = XOR is proven by\\nKapralov and Krachun [KK19]. We use this result to prove a linear lower bound for the case F = OR in\\nSection 4.1. We prove the two remaining lower bounds by reductions from the communication complexity\\nproblem DBHP [KKS15]. In Section 4.2, we present a general framework for proving such lower bounds,\\nwhile in Sections 4.3 and 4.4 we give specific reductions for the Max-2AND and Max-2OR problems. Finally,\\nSections 4.5 and 4.6 contain the proofs of some technical results used in the framework in Section 4.2.\\n4.1 From Max-2EXOR to Max-2EOR\\nIn this section, we give a simple streaming reduction from Max-CUT to Max-2EOR, which asserts that a\\nbetter than trivial 3/4-approximation for Max-2EOR would lead to a better then trivial 1/2-approximation\\nfor Max-CUT. Since the latter is known to require linear space [KK19], we get a linear lower bound on the\\nspace complexity of (3/4 + ε)-approximations of Max-2EOR.\\n17\\nLemma 4.1 (Folklore). Let ΨXOR be a Max-2EXOR instance with m clauses. Consider the following reduction\\nfrom ΨXOR to ΨOR, a Max-2EOR instance: For every clause (x ⊕ y) in ΨXOR, we add clauses (x ∨ y) and\\n(¬x ∨ ¬y) to ΨOR. Then\\nvalΨOR = m+ valΨXOR .\\nProof. It suffices to show that for every assignment σ, valΨOR(σ) = m + valΨXOR(σ). Suppose σ satisfies the\\nclause x⊕ y in ΨXOR, then σ(x) 6= σ(y). In this case, σ satisfies both the corresponding clauses, (x ∨ y) and\\n(¬x∨¬y) in ΨOR. On the other hand, if σ does not satisfy x⊕ y in ΨXOR, then σ(x) = σ(y). In this case, σ\\nsatisfies exactly one of the corresponding clauses in ΨOR.\\nCorollary 4.2. For any constant ε > 0, any streaming algorithm that (3/4 + ε)-approximates Max-2EOR\\nwith success probability at least 3/4 requires Ω(n) space.\\nProof. Let ALG be a (3/4 + ε)-approximate algorithm for Max-2EOR. We will show that there exists a\\nstreaming algorithm of the same space complexity as ALG which (1/2 + 4ε/3)-approximates Max-2EXOR.\\nThis, together with the Ω(n) space lower bound for (1/2+ε)-approximations for Max-2EXOR (Theorem 2.4),\\nwill finish the proof.\\nGiven a Max-2EXOR instance ΨXOR with m clauses, we use Lemma 4.1 to convert it into a Max-2EOR\\ninstance ΨOR. Let v be the output of the algorithm ALG on ΨOR, then we output max{m/2, v −m} as an\\napproximation to valΨXOR . It remains to show that\\n(\\n1\\n2 +\\n4ε\\n3\\n) · valΨXOR ≤ max{m/2, v −m} ≤ valΨXOR .\\nFirst, by Lemma 4.1\\nv −m ≤ valΨOR −m = valΨXOR .\\nTogether with the trivial bound valΨXOR ≥ m/2, this establishes that max{m/2, v −m} ≤ valΨXOR . Second,\\nmax\\n{m\\n2\\n, v −m\\n}\\n≥ 1\\n3\\n· m\\n2\\n+\\n2\\n3\\n· (v −m) = m\\n6\\n+\\n2\\n3\\n·\\n(\\n3\\n4\\n+ ε\\n)\\n· valΨOR −\\n2m\\n3\\n=\\n(\\n1\\n2\\n+\\n2ε\\n3\\n)\\n· (valΨXOR +m)−\\nm\\n2\\n=\\n(\\n1\\n2\\n+\\n2ε\\n3\\n)\\n· valΨXOR +\\n2εm\\n3\\n≥\\n(\\n1\\n2\\n+\\n4ε\\n3\\n)\\n· valΨXOR .\\n4.2 Distributional Boolean Hidden Partition (DBHP) Problem\\nWe prove lower bounds for Max-2EAND and Max-2OR in two steps. Recall that the goal of the players in\\nDBHP is to distinguish between two distributions YES and NO. First, we show a reduction from DBHP\\nto Max-CSP(G). This induces a YES and a NO distributions of instances of Max-CSP(G), corresponding\\nto the YES and NO cases of DBHP. Next, we show that with high probability there is a gap between the\\noptimal value of instances from the YES and NO distributions. The ratio α between these optimal values\\nwill be the upper bound on the approximation ratio of space-efficient streaming algorithms. Informally, any\\n(α + ε)-approximate streaming algorithm with space s distinguishes the distributions YES and NO, and,\\ntherefore, can be converted into a communication protocol for DBHP that uses s bits of communication.\\nSince Kapralov, Khanna, and Sudan [KKS15] proved that any communication protocol for DBHP requires at\\nleast Ω(\\n√\\nn) bits of communication, the corresponding space lower bound for streaming algorithms follows.\\nBefore presenting the framework for streaming lower bounds, we will need to define DBHP and slightly\\nadjust it to our setting.\\nFor n ∈ N and p ∈ [0, 1], by G(n, p) we denote the Erdo¨s-Re´nyi distribution of undirected graphs with n\\nvertices, where each edge is chosen independently with probability p.\\n18\\nDefinition 4.3 (DBHP). Let n ∈ N, β ∈ (0, 1/16) be parameters. Let X∗ ∈ {0, 1}n be a uniformly ran-\\ndom vector, and G be a random graph sampled from G(n, 2β/n). Let r be the number of edges in G, and\\nM ∈ {0, 1}r×n be the edge-vertex incidence matrix of G. We will consider the following three distributions\\nof a vector w ∈ {0, 1}r.\\n• (YES distribution) w = MX∗ ∈ {0, 1}r, where the arithmetic is over F2;\\n• (NO distribution) w = 1 + MX∗ ∈ {0, 1}r, where 1 ∈ Fr2 is the all 1s vector, and the arithmetic is\\nover Fr2;\\n• (NO distribution) w be uniformly sampled from {0, 1}r.\\nFor a pair of distinct distributions D 6= D′ ∈ {YES,NO,NO}, we consider the following decisional\\n2-player one-way communication problem DBHPD,D′(n, β). Alice receives X∗ ∈ {0, 1}n, and Bob receives\\n(M,w) as their private inputs, where w is sampled from D or D′ with probability 1/2. A communication\\nprotocol Π for DBHPD,D′(n, β) consists of a message m sent from Alice to Bob. The complexity of the\\nprotocol Π is the length of the message m: |Π| := |m|. The goal of the players is to distinguish between\\nthe distributions D and D′, and the success probability of Π is defined as Pr(M,w)∼D[Bob outputs D]/2 +\\nPr(M,w)∼D′ [Bob outputs D′]/2.\\n[KKS15] showed that for any constant δ > 0, any protocol that solves DBHPYES,NO(n, β) with success\\nprobability (1/2+δ) requires Ω(β3/2\\n√\\nn) bits of communication. The next lemma shows that the same lower\\nbound extends to the DBHPYES,NO problem by an application of the triangle inequality.\\nLemma 4.4 (A modification of [KKS15, Lemma 5.1]). Let β ∈ (n−1/10, 1/16) and s ∈ (n−1/10, 1) be\\nparameters. Any protocol Π for DBHPYES,NO(n, β) that uses s\\n√\\nn bits of communication cannot distinguish\\nbetween the YES and NO distributions with success probability more than 1/2 + c · (β3/2 + s) for some\\nconstant c > 0 and all large enough n.\\nFor completeness, we present a proof of Lemma 4.4 in Section 4.5. For ease of exposition, now we will\\nuse DBHP(n, β) to denote DBHPYES,NO(n, β).\\nFinally, note that the graph G in the definition of DBHP is extremely sparse (in expectation it has\\nr ≈ βn < 0.1n edges), and, thus, it is not immediately useful for designing hard instances of Max-2CSP\\nproblems. In order to overcome this issue, [KKS15] used DBHP where Bob receives a collection of T messages\\nall sampled either from the YES or NO distribution. Now the union of the T sparse graphs received by\\nBob can be used in reductions to Max-2CSPs.\\nDefinition 4.5 (DBHP with T messages). For any β ∈ (0, 1/16) and n, T ∈ N, we define DBHP(n, β, T )\\nas follows. Let X∗ ∈ {0, 1}n be a uniformly random vector, and for 1 ≤ t ≤ T , let Gi be a random graph\\nsampled from G(n, 2β/n), and Mi be the edge-vertex incidence matrix of Gi. Alice receives X\\n∗, and Bob\\nreceives a list (M1, w1), . . . , (MT , wT ), where with probability 1/2 all wt = MtX\\n∗ (YES case), and with\\nprobability 1/2 all wt = 1 + MtX\\n∗ (NO case). The goal of the players is to have a non-trivial advantage\\nover a random guess in distinguishing between the two distributions, while only communication from Alice\\nto Bob is allowed.\\nReduction from DBHP. A reduction from DBHP(n, β, T ) to Max-CSP(G) is defined by a pair of algo-\\nrithms, A and B. Alice receives her input vector X∗ ∈ {0, 1}n, runs A on the input X∗, and outputs a set of\\nMax-CSP(G)-clauses. Bob receives a collection of T pairs (Mt, wt), applies B to each of them, and outputs\\nT sets of Max-CSP(G)-clauses. Finally, the resulting instance of the Max-CSP(G) problem is the union of\\nclauses from A(X∗),B(M1, w1), . . . ,B(MT , wT ).\\nThe reduction above naturally induces two distributions DY (β, T,A,B) and DN (β, T,A,B) of\\nMax-CSP(G) instances, corresponding to the YES and NO distributions of (Mt, wt). Let us pick some\\nvY and vN , such that PrΨ∼DY [valΨ ≥ vY ] > 1 − o(1) and PrΨ∼DN [valΨ ≤ vN ] > 1 − o(1). Note that for\\nany α > vN/vY , an α-approximate streaming algorithm for Max-CSP(G) distinguishes the two distributions\\nDY (β, T,A,B) and DN (β, T,A,B) with high probability. The following theorem states that any streaming\\n19\\nalgorithm that distinguishes these two distributions, requires space Ω(\\n√\\nn). In particular, any streaming\\nα-approximation for Max-CSP(G) requires space at least Ω(√n).\\nTheorem 4.6 (Reduction from DBHP with T messages). Let c > 0 be the constant from Lemma 4.4.\\nFor every T ∈ N, 0 < β ≤ 1/(10cT )2/3, and reduction (A,B) from DBHP to Max-CSP(G), any streaming\\nalgorithm that distinguishes DY (β, T,A,B) and DN (β, T,A,B) with success probability at least 3/4 requires\\nspace at least 140cT ·\\n√\\nn.\\nThe proof of Theorem 4.6 follows the proofs in [KKS15] by using the standard hybrid argument as well\\nas the data processing inequality for total variation. We postpone the details of the proof of Theorem 4.6 to\\nSection 4.6, and first describe reductions from DBHP to Max-2EAND and Max-2OR in Sections 4.3 and 4.4,\\nrespectively.\\n4.3 From DBHP to Max-2EAND\\nNow, we describe the reduction from DBHP to Max-2EAND. In order to describe the reduction, it suffices\\nto specify the parameters β and T , and the algorithms AEAND and BEAND. Recall that we associate a vector\\nX∗ ∈ {0, 1}n with the set of its ones: X ⊆ [n], X = {i : Xi = 1} . Also, recall that the input of Bob, (M,w),\\nconsists of an edge-vertex incidence matrix M ∈ {0, 1}r×n and a vector w ∈ {0, 1}r. In particular, every row\\nof M has exactly two ones.\\nReduction from DBHP to Max-2EAND\\n• Let c > 0 be the constant from Lemma 4.4. For a given error parameter ε ∈ (0, 1), let\\nT = (10000/ε2)3 · (10c)2 and β = 1\\n(10cT )2/3\\nsuch that βT = 10000/ε2.\\n• AEAND(X∗): Sample βnT/4 independent pairs (i, j) ∈ X∗ ×X∗, and for each of them output\\nthe clause (xi ∧ ¬xj).\\n• BEAND(M,w): Let r be the number of rows in M . For each 1 ≤ k ≤ r with wk = 1, let the 1s\\nin the kth row of M be at the ith and jth positions, then output two clauses: (xi ∧ ¬xj) and\\n(¬xi ∧ xj).\\nLemma 4.7. For any ε ∈ (0, 1), let (β, T,AEAND,BEAND) be the parameters described in the above reduction.\\nFor a Max-2EAND instance Ψ, let mΨ denote the number of clauses in Ψ. Then\\nPr\\nΨ∼DY (β,T,AEAND,BEAND)\\n[\\nvalΨ <\\n(\\n3\\n5\\n− ε\\n)\\n·mΨ\\n]\\n= o(1)\\nand\\nPr\\nΨ∼DN (β,T,AEAND,BEAND)\\n[\\nvalΨ >\\n(\\n4\\n15\\n+ ε\\n)\\n·mΨ\\n]\\n= o(1) .\\nWe prove Lemma 4.7 in Section 5.2. An immediate corollary of Theorem 4.6 and Lemma 4.7 is the\\ndesired lower bound for streaming approximation of Max-2EAND.\\nCorollary 4.8. For any constant ε ∈ (0, 1), any streaming algorithm that (4/9+ε)-approximates Max-2EAND\\nwith success probability at least 3/4 requires Ω(\\n√\\nn) space.\\n4.4 From DBHP to Max-2OR\\nNow, we describe the reduction from DBHP to Max-2OR. Again, it suffices to specify the parameters β and\\nT , and the algorithms AOR and BOR.\\n20\\nReduction from DBHP to OR\\n• Let c > 0 be the constant from Lemma 4.4. For a given error parameter ε ∈ (0, 1), let\\nT = (10000/ε2)3 · (10c)2 and β = 1\\n(10cT )2/3\\nsuch that βT = 10000/ε2.\\n• AOR(X∗): Sample\\n√\\n2−1\\n2 · βnT independent copies of i ∈ X∗, and for each of them output the\\n1-clause (xi). Sample another\\n√\\n2−1\\n2 · βnT independent copies of j ∈ X∗, and for each of them\\noutput the 1-clause (¬xj).\\n• BOR(M,w): Let r be the number of rows in M . For each 1 ≤ k ≤ r with wk = 1, let the the\\n1s in the kth row of M be at the ith and jth positions, then output two clauses: (xi ∨ xj) and\\n(¬xi ∨ ¬xj).\\nLemma 4.9. For any ε ∈ (0, 1), let (β, T,AOR,BOR) be the parameters described in the above reduction. For\\na Max-2OR instance Ψ, let mΨ denote the number of clauses in Ψ. Then\\nPr\\nΨ∼DY (β,T,AOR,BOR)\\n[valΨ = mΨ] = 1\\nand\\nPr\\nΨ∼DN (β,T,AOR,BOR)\\n[\\nvalΨ >\\n(√\\n2\\n2\\n+ ε\\n)\\n·mΨ\\n]\\n= o(1) .\\nThe proof of Lemma 4.9 is presented in Section 5.3. Now, the desired lower bound for any streaming\\napproximations for Max-2OR immediately follows from Theorem 4.6 and Lemma 4.9.\\nCorollary 4.10. For any constant ε ∈ (0, 1), any streaming algorithm that (√2/2 + ε)-approximates\\nMax-2OR with success probability at least 3/4 requires Ω(\\n√\\nn) space.\\n4.5 Proof of Lemma 4.4\\nIn this section, we show that the hardness of DBHPYES,NO(n, β) proved in [KKS15] can be easily extended\\nto the hardness of DBHPYES,NO(n, β).\\nLemma 4.4 (A modification of [KKS15, Lemma 5.1]). Let β ∈ (n−1/10, 1/16) and s ∈ (n−1/10, 1) be\\nparameters. Any protocol Π for DBHPYES,NO(n, β) that uses s\\n√\\nn bits of communication cannot distinguish\\nbetween the YES and NO distributions with success probability more than 1/2 + c · (β3/2 + s) for some\\nconstant c > 0 and all large enough n.\\nProof. Let us consider a protocol Π that uses s\\n√\\nn bits of communication to distinguish between the YES\\nand NO distributions. For an Alice’s input X∗, we denote the message that Alice sends to Bob by Π(X∗).\\nFor each D ∈ {YES,NO,NO}, let PD be the distribution of (M,Π(X∗), w) where (X∗,M,w) ∼ D.\\nThe equation (12) in [KKS15] 7 shows that in this case\\n‖PYES − PNO‖tvd = O(β3/2 + s) .\\nObserve that when (X∗,M,w) ∼ NO, both (M,Π(X∗), w) and (M,Π(X∗),1 + w) are distributed ac-\\ncording to PNO. (Indeed, in this case w ∈ {0, 1}r is uniformly random, and independent of the choices of\\nX∗ and M .) Also, from the definitions of the distributions YES and NO, when (X∗,M,w) ∼ YES (and,\\nthus, (M,Π(X∗), w) ∼ PYES), we have that (M,Π(X∗),1 + w) is distributed according to PNO.\\n7In [KKS15], they use D1, D2 to denote PYES and PNO. They also used P instead of Π.\\n21\\nFurther, by the data processing inequality (see Proposition 2.6), adding the constant vector 1 to the\\nvariable w in (M,Π(X∗), w) does not increase the total variation distance. Thus, we have\\n‖PNO − PNO‖tvd ≤ ‖PYES − PNO‖tvd = O(β3/2 + s) .\\nFinally, by the triangle inequality (see Proposition 2.6),\\n‖PYES − PNO‖tvd ≤ ‖PYES − PNO‖tvd + ‖PNO − PNO‖tvd = O(β3/2 + s) .\\nFrom the definition of the total variation distance, we have that the success probability of Bob in distin-\\nguishing YES from NO is at most 1/2 +O(β3/2 + s), which completes the proof.\\n4.6 Proof of Theorem 4.6\\nBefore presenting the proof of Theorem 4.6, we will show that a streaming algorithm ALG for distinguishing\\nthe distributions DY (β, T,A,B) and DN (β, T,A,B) can be turned into a protocol for DBHP(n, β).\\nLemma 4.11. Let n, T, s ∈ N, β ∈ (n−1/10, 1/16), and let (A,B) be a reduction from DBHP to Max-CSP(G).\\nSuppose that a streaming algorithm ALG distinguishes DY (β, T,A,B) and DN (β, T,A,B) using space s\\nwith probability at least 1/2 + ∆, then there is a one-way protocol for DBHP(n, β) using at most s bits of\\ncommunication that succeeds with probability at least 1/2 + ∆/(2T ).\\nProof. First we fix the randomness of the algorithm ALG so that the resulting deterministic algorithm\\nsucceeds with probability at least 1/2 + ∆ (by the averaging argument).\\nNow, for each i = 0, 1, . . . , T , let SYi (resp., S\\nN\\ni ) be the state of ALG after receiving\\nA(X∗),B(M1, w1), . . . ,B(Mi, wi), where the inputs are sampled from the YES (resp., NO) distribution.\\nNote that {SYi } and {SNi } are random variables, and ‖SY0 − SN0 ‖tvd = 0 while the success probability of\\nALG guarantees that ‖SYT − SNT ‖tvd ≥ ∆. By the hybrid argument and the triangle inequality for the total\\nvariation distance (Proposition 2.6), there exists i∗ ∈ [T − 1] such that\\n‖SYi∗+1 − SNi∗+1‖tvd − ‖SYi∗ − SNi∗ ‖tvd ≥\\n∆\\nT\\n. (4.12)\\nThis indicates that the (i∗ + 1)th inputs (i.e., B(Mi, wi)) are sufficient for distinguishing between the\\nYES and the NO cases with non-trivial probability. Specifically, let S˜Y (resp. S˜N ) be the distribution of\\nthe states of ALG, when it starts with a state from SYi∗ and receives one input B(M,w) where (M,w) is\\nsampled from the YES (resp. NO) distributions.\\nClaim 4.13. Let S˜Y and S˜N be the random variables defined above, then ‖S˜Y − S˜N‖tvd ≥ ∆T .\\nProof. First, by the triangle inequality for the total variation distance (see Proposition 2.6), we have\\n‖S˜Y − S˜N‖tvd ≥ ‖S˜Y − SNi∗+1‖tvd − ‖S˜N − SNi∗+1‖tvd .\\nNote that S˜Y = SYi∗+1 by definition, and ‖S˜N −SNi∗+1‖tvd ≤ ‖SYi∗ −SNi∗ ‖tvd by the data processing inequality\\n(Apply item 2 of Proposition 2.6 with X = SYi∗ , Y = S\\nN\\ni∗ , W = B(Mi∗+1, wi∗+1), and f = ALG). This,\\ntogether with (4.12), gives the desired bound\\n‖S˜Y − S˜N‖tvd ≥ ‖SYi∗+1 − SNi∗+1‖tvd − ‖SYi∗ − SNi∗ ‖tvd ≥\\n∆\\nT\\n.\\nFinally, we use ALG to design a protocol for DBHP(n, β). Note that Alice and Bob have the description\\nof the algorithm ALG, and, therefore, know the distributions S˜Y and S˜N . In particular, they both know\\nthe value of i∗. Moreover, since Alice and Bob know i∗, they know the distributions S˜Y and S˜N .\\n22\\nAlgorithm 4 A protocol for DBHP(n, β) using ALG\\nInput: Alice receives input X∗, and Bob receives inputs (M,w).\\nGoal: Distinguish between MX∗ = w (YES case) and MX∗ = 1− w (NO case).\\n1: Alice samples a state SA of ALG from the distribution S\\nY\\ni∗ , conditioned on the first input being A(X∗).\\nAlice sends SA to Bob. Since ALG uses s bits of memory, |SA| ≤ s.\\n2: Bob executes ALG with the initial state SA on the input B(M,w). Let S be the resulting state of ALG.\\n3: Bob outputs YES if Pr[S˜Y = S] ≥ Pr[S˜N = S]; otherwise Bob outputs NO.\\nLet ΩY = {S : Pr[S˜Y = S] ≥ Pr[S˜N = S]} and ΩN = {S : Pr[S˜Y = S] < Pr[S˜N = S]}, then\\n‖S˜Y − S˜N‖tvd = 1\\n2\\n∑\\nS∈ΩY\\nPr[S˜Y = S]− Pr[S˜N = S] + 1\\n2\\n∑\\nS∈ΩN\\nPr[S˜N = S]− Pr[S˜Y = S]\\n=\\n( ∑\\nS∈ΩY\\nPr[S˜Y = S]− Pr[S˜N = S]\\n)\\n− 1\\n2\\n.\\nThis implies that Bob correctly identifies the YES distribution with probability\\nPr\\n(M,w)∼YES\\n[Bob outputs YES] =\\n∑\\nS∈ΩY\\nPr[S˜Y = S] ≥ 1\\n2\\n+ ‖S˜Y − S˜N‖tvd .\\nSimilarly,\\nPr\\n(M,w)∼NO\\n[Bob outputs NO] ≥ 1\\n2\\n+ ‖S˜Y − S˜N‖tvd .\\nThus, the above protocol solves DBHP(n, β) with success probability at least\\n1\\n2\\nPr\\n(M,w)∼YES\\n[Bob outputs YES] +\\n1\\n2\\nPr\\n(M,w)∼NO\\n[Bob outputs NO] ≥ 1\\n2\\n+ ‖S˜Y − S˜N‖tvd ≥ 1\\n2\\n+\\n∆\\nT\\n,\\nwhere the last inequality is due to Claim 4.13.\\nNow we are ready to finish the proof of Theorem 4.6.\\nTheorem 4.6 (Reduction from DBHP with T messages). Let c > 0 be the constant from Lemma 4.4.\\nFor every T ∈ N, 0 < β ≤ 1/(10cT )2/3, and reduction (A,B) from DBHP to Max-CSP(G), any streaming\\nalgorithm that distinguishes DY (β, T,A,B) and DN (β, T,A,B) with success probability at least 3/4 requires\\nspace at least 140cT ·\\n√\\nn.\\nProof. Consider a streaming algorithm that distinguishes between the distributions DY (β, T,A,B) and\\nDN (β, T,A,B) with probability at least 3/4 using space S. Then, by Lemma 4.11, there exists a proto-\\ncol for DBHP(n, β) with at most S bits of communication, and success probability 1/2 + 1/(8T ).\\nOn the other hand, by Lemma 4.4, any protocol for DBHP(n, β) with success probability\\n1\\n2\\n+ c ·\\n(\\nβ3/2 +\\n1\\n40cT\\n)\\nβ≤1/(10cT )2/3\\n≤ 1\\n2\\n+\\n1\\n8T\\nmust use S ≥ 140cT ·\\n√\\nn bits of communication.\\n5 Analysis for the gap of Max-2EAND and Max-2OR instances\\nThe goal of this section is to prove Lemma 4.7 and Lemma 4.9. We analyse the structure of DBHP in\\nSection 5.1 and present an intuitive and graphical view of the reductions. After that, we give the proofs for\\nLemma 4.7 and Lemma 4.9 in Section 5.2 and Section 5.3 respectively.\\n23\\nNotation for an assignment In this section, we interchangeably work with one of the following repre-\\nsentations for σ in order to simplify the presentation. Previously, σ was defined as a function that maps\\n{x1, x2, . . . , xn} to {0, 1}. It can be represented by a vector in {0, 1}n which has σ(xi) as its ith coordinate.\\nIt can also be represented by the set {i ∈ [n] : σ(xi) = 1}.\\n5.1 A graphical view of DBHP\\nHere we introduce a graphical view of DBHP which will provide a more intuitive lens to understand the\\nreductions. Recall that in DBHP, Bob has private inputs M ∈ {0, 1}r and w ∈ {0, 1}r, where M is the\\nedge-incidence matrix of an n-vertex graph G and w is an indicator vector. Specifically, M corresponds to a\\ngraph sampled from G(n, 2β/n) and r denotes the number of edges in this graph. We focus on the subgraph\\nH ⊆ G that contains only those edges from M whose corresponding entry in w is 1. We examine the\\ndistributions of this subgraph H under different input distributions to DBHP. Recall that we are interested\\nin two input distributions to DBHP: YES and NO. In both of these distributions, we first sample a hidden\\npartition X∗ ∈ {0, 1}∗ and then sample T independent graphs from G(n, 2β/n) where the edge-vertex\\nincidence matrices of these graphs are denoted as {Mt}t∈[T ]. In the YES distribution, wt = MtX∗ and in\\nthe NO distribution, wt = 1 −MtX∗. We will abuse notation and call the corresponding distributions of\\nthe subgraph H as YES and NO respectively. We summarize the properties of these distributions in the\\nfollowing lemma.\\nFigure 2: For a random graph on vertex set [n], we partition the edges into two sets: (i) edges that lie across\\nX∗ and X∗ and (ii) edges that lie in X∗ or X∗. In the YES distribution, only the (i) type edges are present\\nin H. In the NO distribution, only the (ii) type edges are present in H.\\nLemma 5.1 (Graphical view of DBHP). For any n ∈ N large enough and ε ∈ (0, 0.25), let\\nT = (10000/ε2)3 · (10c)2 and β = 1\\n(10cT )2/3\\nsuch that βT = 10000/ε2. Let YES and NO be the distri-\\nbutions of the subgraph H induced from DBHP(n, β, T ) as described above, and let mDBHP denote the total\\nnumber of edges in H. For every X∗, σ ∈ {0, 1}n, define mcross(σ) to be the number of edges (i, j) such that\\n(i) σ(xi) 6= σ(xj) and (ii) X∗i = X∗j . We have the following.\\n• (Size of X∗) For each distribution YES,NO and for any constant ε′ ∈ (0, 1) such that ε′ ≥ ε/10, we have\\nPr\\n[∣∣∣|X∗| − n\\n2\\n∣∣∣ > ε′ · n] = o(1) .\\n• (Number of edges) For each distribution YES,NO and for any constant ε′ ∈ (0, 1) such that ε′ ≥ ε/10,\\nwe have\\nPr\\n[∣∣∣∣mDBHP − βnT2\\n∣∣∣∣ > ε′ · βnT] = o(1) .\\n24\\n• (NO distribution) For any constant ε′ ∈ (0, 1) such that ε′ ≥ ε/10, we have\\nPr\\nNO\\n[\\n∃σ ∈ {0, 1}, mcross(σ) >\\n( |σ ∩X∗| · |σ ∩X∗|+ |σ ∩X∗| · |σ ∩X∗|\\nn2\\n)\\n· 2βnT + ε′ · βnT\\n]\\n= o(1) .\\nProof.\\n• (Size of X∗) For any ε′ ∈ (0, 1), we have\\nPr\\n[∣∣∣|X∗| − n\\n2\\n∣∣∣ > ε′ · n] ≤ 2 ·∑dn/2−ε′·nei=0 (ni)\\n2n\\n= 2−Ωε′ (n) = o(1) .\\n• (Number of edges) Here we only prove the case for YES distribution while the other case can be\\nproved similarly. Also, we only show the upper bound for mDBHP − βnT/2 while the lower bound can\\nbe proved symmetrically.\\nLet ε′′ ∈ (0, 1) be error parameters that will be chosen in the end according to ε′. First, by Lemma 2.3,\\nwe have for any constant ε′′ ∈ (0, 1),\\nPr\\nYES\\n[∣∣∣|X∗| − n\\n2\\n∣∣∣ > ε′′ · n] = o(1) . (5.2)\\nDenote the event where −ε′′n ≤ |X∗|−n/2 ≤ ε′′n as GOOD. Now, for each t ∈ [T ], let mt = ‖MtX∗‖1,\\ni.e., the number of edges in Mt that crosses X\\n∗ and X∗, we have\\nE\\nYES\\n[mt | GOOD] ≤\\n(\\n1\\n2\\n+ ε′′\\n)\\n· βn .\\nFurther, note that when conditioning on X∗, mt are independent, thus by Chernoff bound, when n is\\nlarge enough, we have\\nPr\\nYES\\n\\uf8ee\\uf8f0∑\\nt∈[T ]\\nmt −\\n(\\n1\\n2\\n+ ε′′\\n)\\n· βnT > ε′′ · βnT\\n2\\n∣∣∣ GOOD\\n\\uf8f9\\uf8fb = o(1) .\\nAs mDBHP =\\n∑\\nt∈[T ]mt, by choosing ε\\n′′ = ε′/3, we conclude that\\nPr\\n[∣∣∣∣mDBHP − βnT2\\n∣∣∣∣ > ε′ · βnT2\\n]\\n= o(1) .\\n• (NO distribution) First, for each t ∈ [T ], let m(t)cross(σ) denote the number of cross edges (i.e., σ(xi) 6=\\nσ(xj) and X\\n∗\\ni = X\\n∗\\nj ) from Mt. Note that mcross =\\n∑\\nt∈[T ]m\\n(t)\\ncross(σ). Next, observe that for each\\nt ∈ [T ], m(t)cross(σ) is a sum of |σ ∩X∗| · |σ ∩X∗| + |σ ∩X∗| · |σ ∩X∗| independent Bernoulli random\\nvariables with expectation 2β/n. Also, random variables m\\n(1)\\ncross(σ), . . . ,m\\n(T )\\ncross(σ) are independent to\\neach other because we have fixed X∗ and σ. Thus, by Chernoff bound (i.e., Lemma 2.3), we have\\nPr\\n[\\nmcross(σ) >\\n(|σ ∩X∗| · |σ ∩X∗|+ |σ ∩X∗| · |σ ∩X∗|) · 2β\\nn\\n· T + ε′βnT\\n]\\n< exp\\n(\\n−ε\\n′2β2n2T 2\\n2βnT\\n)\\n= exp\\n(\\n−ε\\n′2βnT\\n2\\n)\\n< 2−10n\\nwhere the last inequality is due to the choice of T = 10000/ε2 and ε′ ≥ ε/10. Finally, by union bound,\\nwe have\\nPr\\n[\\n∃σ ∈ {0, 1}n, mcross(σ) >\\n( |σ ∩X∗| · |σ ∩X∗|+ |σ ∩X∗| · |σ ∩X∗|\\nn2\\n)\\n· 2βnT + ε′βnT\\n]\\n= o(1) .\\n25\\n5.2 The gap of Max-2EAND instances\\nIn this subsection, we complete the proof of the following lemma using the graphical view of DBHP.\\nLemma 4.7. For any ε ∈ (0, 1), let (β, T,AEAND,BEAND) be the parameters described in the above reduction.\\nFor a Max-2EAND instance Ψ, let mΨ denote the number of clauses in Ψ. Then\\nPr\\nΨ∼DY (β,T,AEAND,BEAND)\\n[\\nvalΨ <\\n(\\n3\\n5\\n− ε\\n)\\n·mΨ\\n]\\n= o(1)\\nand\\nPr\\nΨ∼DN (β,T,AEAND,BEAND)\\n[\\nvalΨ >\\n(\\n4\\n15\\n+ ε\\n)\\n·mΨ\\n]\\n= o(1) .\\nProof. Recall that AEAND(X∗) uses X∗ to sample βnT/4 independent copies of (i, j) ∈ X∗×X∗ and outputs\\n(xi ∧ ¬xj). On the other hand, for each row of M with the ith and jth entry being 1, if the corresponding\\nentry of w is 1, BEAND(M,w) outputs (xi ∧ ¬xj) and (¬xi ∧ xj).\\n• (YES distribution) Consider the following assignment σ:\\nσ(xi) =\\n{\\n1, i ∈ X∗ ;\\n0, otherwise .\\nUnder this assignment, each clause (xi ∧ ¬xj) generated by AEAND is satisfied since (i, j) ∈ X∗ ×X∗.\\nFor every pair of clauses (xi ∧¬xj) and (¬xi ∧xj) generated by BEAND, exactly one of them is satisfied\\nby σ. Therefore, σ satisfies βnT/4 + mDBHP clauses while the total number of clauses is mΨ =\\nβnT/4 + 2mDBHP.\\nFrom Lemma 5.1, we know that mDBHP ∈ (1± ε/10) · βnT/2 with probability at least 1− o(1). Thus,\\nwe have mΨ ∈ (5/4 ± ε/10) · βnT and σ satisfies at least (1 − ε/10) · βnT clauses with probability\\n1− o(1). Therefore, val(σ) ≥ (3/5− ε) ·mΨ with probability at least 1− o(1).\\n• (NO distribution) Consider any fixed assignment σ ∈ {0, 1}n: σ satisfies a clauses (xi∧¬xj) generated\\nby AEAND if and only if σ(xi) = 1 and σ(xj) = 0. Let a(σ) be the random variable that denotes the\\nnumber of clauses generated by AEAND which are satisfied by σ. Observe that a(σ) is the sum of βnT/4\\nindependent Bernoulli random variables with mean |σ∩X∗| · |σ∩X∗|/(|X∗| · |X∗|).By Chernoff bound\\n(i.e., Lemma 2.3), we have\\nPr\\nNO\\n[\\na(σ) >\\n|σ ∩X∗| · |σ ∩X∗|\\n|X∗| · |X∗| ·\\nβnT\\n4\\n+\\nεβnT\\n10\\n]\\n< 2−10n .\\nApplying the union bound, we have\\nPr\\nNO\\n[\\n∃σ ∈ {0, 1}n, a(σ) > |σ ∩X\\n∗| · |σ ∩X∗|\\n|X∗| · |X∗| ·\\nβnT\\n4\\n+\\nεβnT\\n10\\n]\\n= o(1) . (5.3)\\nNow, let us consider the clauses generated by BEAND: an edge (i, j) in Mt is selected by wt if and only\\nif X∗ contains both i and j, or contains neither (i.e., X∗i = X\\n∗\\nj ). Observe that exactly one of (xi∧¬xj)\\nand (¬xi ∧xj) is satisfied by σ if and only if σ(xi) 6= σ(xj); otherwise, both are unsatisfied. Therefore,\\nthe number of clauses satisfied by σ is exactly mcross(σ), i.e., the number of edges (i, j) such that (i)\\nX∗i = X\\n∗\\nj and (ii) σ(xi) 6= σ(xj). Therefore, the total number of satisfied clauses is given by\\nvalΨ(σ) = a(σ) +mcross(σ) . (5.4)\\n26\\nBy Lemma 5.1, we have\\nPr\\nNO\\n[\\n∃σ ∈ {0, 1}n, mcross(σ) >\\n( |σ ∩X∗| · |σ ∩X∗|+ |σ ∩X∗| · |σ ∩X∗|\\nn2\\n)\\n· 2βnT + εβnT\\n10\\n]\\n= o(1) .\\nSince |X∗| · |X∗| ≤ n24 ,\\nPr\\nNO\\n[\\n∃σ ∈ {0, 1}n, mcross(σ) >\\n( |σ ∩X∗| · |σ ∩X∗|+ |σ ∩X∗| · |σ ∩X∗|\\n|X∗| · |X∗|\\n)\\n· βnT\\n2\\n+\\nεβnT\\n10\\n]\\n= o(1)\\n(5.5)\\nLet p = |σ ∩X∗|/|X∗| ∈ [0, 1] and q = |σ ∩X∗|/|X∗| ∈ [0, 1].\\nCombining (5.3), (5.4), and (5.5), we get\\nPr\\nΨ∼DN (β,T,AEAND),BEAND\\n[\\n∃σ{0, 1}n, valΨ(σ) >\\n(\\npq + 2p(1− p) + 2q(1− q)\\n4\\n)\\n· βnT + εβnT\\n5\\n]\\n= o(1) .\\nWe have pq+2p(1−p)+2q(1−q)4 =\\n8−(3p−2)2−(3q−2)2−3(p−q)2\\n24 ≤ 1/3. Since mΨ ∈ (5/4 ± ε/10) · βnT with\\nprobability 1− o(1), we conclude that\\nPr\\nΨ∼DN (β,T,AEAND),BEAND\\n[\\n∃σ{0, 1}n, valΨ(σ) >\\n(\\n4\\n15\\n+ ε\\n)\\n·mΨ\\n]\\n= o(1) .\\n5.3 The gap of Max-2OR instances\\nIn this subsection, we complete the proof of the following lemma using the graphical view of DBHP.\\nLemma 4.9. For any ε ∈ (0, 1), let (β, T,AOR,BOR) be the parameters described in the above reduction. For\\na Max-2OR instance Ψ, let mΨ denote the number of clauses in Ψ. Then\\nPr\\nΨ∼DY (β,T,AOR,BOR)\\n[valΨ = mΨ] = 1\\nand\\nPr\\nΨ∼DN (β,T,AOR,BOR)\\n[\\nvalΨ >\\n(√\\n2\\n2\\n+ ε\\n)\\n·mΨ\\n]\\n= o(1) .\\nProof. Recall that AOR(X∗) uses X∗ to sample\\n√\\n2−1\\n2 · βnT independent copies of i ∈ X∗ and another√\\n2−1\\n2 · βnT independent copies of j ∈ X∗ and output (xi) as well as (¬xj). On the other hand, for each\\nrow of M with the ith and jth entry being 1, if the corresponding entry of w is 1, BOR outputs (xi ∨ xj) and\\n(¬xi ∨ ¬xj).\\n• (YES distribution) Consider the following assignment σ:\\nσ(xi) =\\n{\\n1, i ∈ X∗ ;\\n0, otherwise .\\n. Under this assignment, every clause of the form (xi) or of the form (¬xj) generated by AOR is satisfied\\nbecause i ∈ X∗ and j ∈ X∗. Similarly, every pair of clauses (xi ∨ xj) and (¬xi ∨ ¬xj) generated by\\nBOR are also satisfied since in the YES distribution, (i, j) ∈ X∗ ×X∗. Thus, valΨ = mΨ as desired.\\n27\\n• (NO distribution) Consider any fixed assignment σ ∈ {0, 1}n: Let a(σ) be the random variable that\\ndenotes the number of clauses generated by AEAND which are satisfied by σ. Observe that a(σ) is\\nthe sum of\\n√\\n2−1\\n2 βnT ≥ 100n independent Bernoulli random variables with mean |σ ∩ X∗|/|X∗| and√\\n2−1\\n2 βnT ≥ 100n independent Bernoulli random variables with mean |σ ∩ X∗|/|X∗|. By Chernoff\\nbound (i.e., Lemma 2.3), we have\\nPr\\nNO\\n[\\na(σ) >\\n( |σ ∩X∗|\\n|X∗| +\\n|σ ∩X∗|\\n|X∗|\\n)\\n·\\n√\\n2− 1\\n2\\nβnT +\\nεβnT\\n15\\n]\\n< 2−10n .\\nApplying the union bound, we have\\nPr\\nNO\\n[\\n∃σ ∈ {0, 1}n, a(σ) >\\n( |σ ∩X∗|\\n|X∗| +\\n|σ ∩X∗|\\n|X∗|\\n)\\n·\\n√\\n2− 1\\n2\\nβnT +\\nεβnT\\n15\\n]\\n= o(1) . (5.6)\\nNow, consider the clauses generated by BOR: an edge (i, j) in Mt is selected by wt if and only if\\nX∗i = X\\n∗\\nj . Observe that both the clauses (xi ∨ xj) and (¬xi ∨ ¬xj) are satisfied if and only if\\nσ(xi) 6= σ(xj); otherwise, exactly one of them is satisfied. Therefore, the number of satisfied clauses\\namong the clauses generated by BOR is mDBHP + mcross(σ). Therefore, the total number of satisfied\\nclauses is given by\\nvalΨ(σ) = a(σ) +mDBHP +mcross(σ) . (5.7)\\nBy Lemma 5.1, we have\\nPr\\nNO\\n[\\n∃σ ∈ {0, 1}n, mcross(σ) >\\n( |σ ∩X∗| · |σ ∩X∗|+ |σ ∩X∗| · |σ ∩X∗|\\nn2\\n)\\n· 2βnT + εβnT\\n15\\n]\\n= o(1) .\\nand\\nPr\\nNO\\n[\\nmDBHP >\\nβnT\\n2\\n+ ε · βnT\\n15\\n]\\n= o(1). (5.8)\\nSince |X∗| · |X∗| ≤ n24 ,\\nPr\\nNO\\n[\\n∃σ ∈ {0, 1}n, mcross(σ) >\\n( |σ ∩X∗| · |σ ∩X∗|+ |σ ∩X∗| · |σ ∩X∗|\\n|X∗| · |X∗|\\n)\\n· βnT\\n2\\n+\\nεβnT\\n15\\n]\\n= o(1)\\n(5.9)\\nLet p = |σ ∩X∗|/|X∗| ∈ [0, 1] and q = |σ ∩X∗|/|X∗| ∈ [0, 1]. Combining (5.6), (5.7), (5.9) and (5.8),\\nwe get\\nPr\\nΨ∼DN (β,T,AOR),BOR\\n[\\n∃σ{0, 1}n, valΨ(σ) >\\n(\\n(p+ q)(\\n√\\n2− 1)\\n2\\n+\\n1\\n2\\n+\\np(1− p) + q(1− q)\\n2\\n)\\n· βnT + εβnT\\n5\\n]\\n= o(1) .\\nWe have p+q2 ·\\n(√\\n2− 1)+ 12 + p(1−p)+q(1−q)2 = 1− (p−√2/2)2+(q−√2/2)22 ≤ 1. Since mΨ ∈ (√2±ε/10)·βnT\\nwith probability 1− o(1), we conclude that\\nPr\\nΨ∼DN (β,T,AOR),BOR\\n[\\n∃σ{0, 1}n, valΨ(σ) >\\n(√\\n2\\n2\\n+ ε\\n)\\n·mΨ\\n]\\n= o(1) .\\n28\\n6 Proof of Theorem 1.1\\nTheorem 1.1. Let F ⊆ {TR,OR,XOR,AND} be a set of allowed binary predicates. Let αF = minG⊆F αG,\\nwhere αG is given in Table 1.\\nFor every ε > 0, there exists an (αF − ε)-approximate streaming algorithm for Max-CSP(F) that uses\\nspace O(ε−2 log n). On the other hand, any (αF + ε)-approximate streaming algorithm for Max-CSP(F)\\nrequires space Ω(\\n√\\nn).\\nType G Tightbound\\nPrevious bound\\nαG α\\npr\\nG Reference\\nTR 1 1 Folklore\\nOR 34 [\\n3\\n4 , 1] Folklore\\n{TR,OR}\\n√\\n2\\n2 [\\n√\\n5−1\\n2 , 1] [LS79]\\nXOR 12\\n1\\n2 [KK19]\\nAND 49 [\\n2\\n5 ,\\n1\\n2 ] [GVV17]\\nTable 1: Summary of known and new approximation factors αG for Max-CSP(G). We have suppressed (1±ε)\\nmultiplicative factors.\\nProof. Note that for G listed in Table 1, the space lower bounds for Max-CSPG are proven in Corol-\\nlary 4.2, Corollary 4.10, Theorem 2.4, and Corollary 4.8, respectively. Then the space lower bound for\\nany Max-CSP(F) directly follows from the fact that for G ⊆ F , any hard instance for Max-CSP(G) is also a\\nhard instance of Max-CSP(F).\\nWe provide a case-by-case analysis to prove the upper bounds.\\nCase I – arg minG⊆F αG ∈ {TR,OR, {TR,OR}}: In this case, F = {OR}, F = {TR}, or F = {OR,TR},\\nand each of these cases is covered in Table 1. The corresponding upper bounds for these cases are proven in\\nProposition 3.1 and Theorem 3.9.\\nCase II – arg minG⊆F αG = XOR: In this case, F ⊆ {OR,XOR,TR}. Consider any instance Ψ of\\nMax-CSP(F). A random assignment satisfies every constraint in Ψ with probability at least 1/2. There-\\nfore, the trivial streaming algorithm that counts the number of clauses, m and outputs m/2 achieves 1/2\\napproximation (see Proposition 3.1).\\nCase III – arg minG⊆F αG = AND: In this case, F ⊆ {AND,OR,XOR,TR}. Any Boolean constraint f(x)\\nof length at most 2 can be expressed as the disjunction of (at most 4) AND constraints {fi(x)} such that for\\nany assignment σ, the number of satisfied constraints among {fi(σ)} is exactly 1 if f(σ) = 1; otherwise, it is\\n0. Therefore, any instance Ψ of Max-CSP(F) can be reduced to Ψ′, an instance of Max-2AND with the same\\noptimal value. Now the approximation algorithm for Max-2AND from Theorem 3.4 finishes the proof.\\nOpen Questions\\nOur work gives optimal approximation ratios for all Boolean maximum constraint satisfaction problems\\nwith constraints of length at most two. It would be interesting to understand the complexity of constraint\\nlanguages with arity greater than two, and larger alphabet sizes.\\n29\\nIn terms of lower bounds, we show that better than 49 - and\\n√\\n2\\n2 -approximations for Max-2-AND and\\nMax-2-OR require space Ω(\\n√\\nn). Can we improve these space lower bounds to Ω(n), matching the space\\nrequirements of standard algorithms that give 1− ε approximation?\\nAcknowledgement\\nWe thank Madhu Sudan for helpful discussions and for pointing out a mistake in an old proof.\\nReferences\\n[AKL16] Sepehr Assadi, Sanjeev Khanna, and Yang Li. Tight bounds for single-pass streaming complex-\\nity of the set cover problem. In STOC 2016, pages 698–711, 2016.\\n[AMS99] Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the\\nfrequency moments. J. Comput. Syst. Sci., 58(1):137–147, 1999.\\n[Aus10] Per Austrin. Towards sharp inapproximability for any 2-CSP. SIAM J. Comput., 39(6):2430–\\n2463, 2010.\\n[Cha15] Amit Chakrabarti. Data stream algorithms. Lecture notes, page 79, 2015.\\n[CV08] Nadia Creignou and Heribert Vollmer. Boolean constraint satisfaction problems: When does\\nPost’s lattice help? In Complexity of Constraints, pages 3–37. Springer, 2008.\\n[FG95] Uriel Feige and Michel Goemans. Approximating the value of two prover proof systems, with\\napplications to MAX 2SAT and MAX DICUT. In ISTCS 1995, pages 182–189. IEEE, 1995.\\n[GKK+07] Dmitry Gavinsky, Julia Kempe, Iordanis Kerenidis, Ran Raz, and Ronald De Wolf. Exponential\\nseparations for one-way quantum communication complexity, with applications to cryptography.\\nIn STOC 2007, pages 516–525, 2007.\\n[GT19] Venkatesan Guruswami and Runzhou Tao. Streaming hardness of unique games. In APPROX\\n2019, pages 5:1–5:12. LIPIcs, 2019.\\n[GVV17] Venkatesan Guruswami, Ameya Velingker, and Santhoshini Velusamy. Streaming complexity\\nof approximating Max 2CSP and Max Acyclic Subgraph. In APPROX 2017. LIPIcs, 2017.\\n[GW94] Michel X Goemans and David P Williamson. New 3/4-approximation algorithms for the max-\\nimum satisfiability problem. SIAM J. Discrete Math., 7(4):656–666, 1994.\\n[GW95] Michel X. Goemans and David P. Williamson. Improved approximation algorithms for maxi-\\nmum cut and satisfiability problems using semidefinite programming. J. ACM, 42(6):1115–1145,\\n1995.\\n[H˚as01] Johan H˚astad. Some optimal inapproximability results. J. ACM, 48(4):798–859, 2001.\\n[H˚as08] Johan H˚astad. Every 2-CSP allows nontrivial approximation. Comput. Complex., 17(4):549–\\n566, 2008.\\n[HRR98] Monika Rauch Henzinger, Prabhakar Raghavan, and Sridhar Rajagopalan. Computing on data\\nstreams. External memory algorithms, 50:107–118, 1998.\\n[Ind00] Piotr Indyk. Stable distributions, pseudorandom generators, embeddings and data stream\\ncomputation. In FOCS 2000, pages 189–197. IEEE, 2000.\\n[KK15] Dmitry Kogan and Robert Krauthgamer. Sketching cuts in graphs and hypergraphs. In ITCS\\n2015, pages 367–376. ACM, 2015.\\n30\\n[KK19] Michael Kapralov and Dmitry Krachun. An optimal space lower bound for approximating\\nMAX-CUT. In STOC 2019, pages 277–288. ACM, 2019.\\n[KKMO07] Subhash Khot, Guy Kindler, Elchanan Mossel, and Ryan O’Donnell. Optimal inapproximability\\nresults for MAX-CUT and other 2-variable CSPs? SIAM J. Comput., 37(1):319–357, 2007.\\n[KKS15] Michael Kapralov, Sanjeev Khanna, and Madhu Sudan. Streaming lower bounds for approxi-\\nmating MAX-CUT. In SODA 2015, pages 1263–1282. SIAM, 2015.\\n[KKSV17] Michael Kapralov, Sanjeev Khanna, Madhu Sudan, and Ameya Velingker. (1 + ω(1))-\\napproximation to MAX-CUT requires linear space. In SODA 2017, pages 1703–1722. SIAM,\\n2017.\\n[KNW10] Daniel M. Kane, Jelani Nelson, and David P. Woodruff. On the exact space complexity of\\nsketching and streaming small norms. In SODA 2010, pages 1161–1178. SIAM, 2010.\\n[KV05] Subhash Khot and Nisheeth K. Vishnoi. On the unique games conjecture. In FOCS 2005,\\nvolume 5, page 3, 2005.\\n[LLZ02] Michael Lewin, Dror Livnat, and Uri Zwick. Improved rounding techniques for the MAX 2-SAT\\nand MAX DI-CUT problems. In IPCO 2002, pages 67–82. Springer, 2002.\\n[LS79] Karl Lieberherr and Ernst Specker. Complexity of partial satisfaction. In SFCS 1979, pages\\n132–139. IEEE, 1979.\\n[McG14] Andrew McGregor. Graph stream algorithms: a survey. SIGMOD Record, 43(1):9–20, 2014.\\n[MM17] Konstantin Makarychev and Yury Makarychev. Approximation algorithms for CSPs. In The\\nConstraint Satisfaction Problem: Complexity and Approximability, volume 7, pages 287–325.\\nLIPIcs, 2017.\\n[Mor78] Robert Morris. Counting large numbers of events in small registers. Commun. ACM, 21(10):840–\\n842, 1978.\\n[Pol11] Matthias Poloczek. Bounds on greedy algorithms for MAX SAT. In ESA 2011, pages 37–48.\\nSpringer, 2011.\\n[PS11] Matthias Poloczek and Georg Schnitger. Randomized variants of Johnson’s algorithm for MAX\\nSAT. In SODA 2011, pages 656–663. SIAM, 2011.\\n[PSWVZ17] Matthias Poloczek, Georg Schnitger, David P. Williamson, and Anke Van Zuylen. Greedy\\nalgorithms for the maximum satisfiability problem: Simple algorithms and inapproximability\\nbounds. SIAM J. Comput., 46(3):1029–1061, 2017.\\n[Rag08] Prasad Raghavendra. Optimal algorithms and inapproximability results for every CSP? In\\nSTOC 2008, pages 245–254, 2008.\\n[RS10] Prasad Raghavendra and David Steurer. Graph expansion and the unique games conjecture.\\nIn STOC 2010, pages 755–764, 2010.\\n[Sch78] Thomas J. Schaefer. The complexity of satisfiability problems. In STOC 1978, pages 216–226.\\nACM, 1978.\\n[TSSW00] Luca Trevisan, Gregory B. Sorkin, Madhu Sudan, and David P. Williamson. Gadgets, approx-\\nimation, and linear programming. SIAM J. Comput., 29(6):2074–2097, 2000.\\n[TZˇ16] Johan Thapper and Stanislav Zˇivny`. The complexity of finite-valued CSPs. J. ACM, 63(4):1–33,\\n2016.\\n31\\n[VY11] Elad Verbin and Wei Yu. The streaming complexity of cycle counting, sorting by reversals, and\\nother problems. In SODA 2011, pages 11–25. SIAM, 2011.\\n[VZ11] Anke Van Zuylen. Simpler 3/4-approximation algorithms for MAX SAT. In WAOA 2011, pages\\n188–197. Springer, 2011.\\n[Wil99] David P Williamson. Lecture notes on approximation algorithms. Technical report, Technical\\nReport RC–21409, IBM, 1999.\\n[Yan94] Mihalis Yannakakis. On the approximation of maximum satisfiability. J. Algorithms, 17(3):475–\\n502, 1994.\\n[Zel11] Mariano Zelke. Intractability of Min- and Max-Cut in streaming graphs. Inf. Process. Lett.,\\n111(3):145–150, 2011.\\n[Zwi00] Uri Zwick. Analyzing the MAX 2-SAT and MAX DI-CUT approximation algorithms of Feige\\nand Goemans. Unpublished manuscript, 2000.\\n32\\n'}\n",
      "{'_id': ObjectId('6815e61add259dc7e6e1cd2b'), 'authors': 'Li, Yi, Wang, Ruosong, Woodruff, David P.', 'year': '2019', 'title': 'Tight Bounds for the Subspace Sketch Problem with Applications', 'full_text': 'ar\\nX\\niv\\n:1\\n90\\n4.\\n05\\n54\\n3v\\n3 \\n [c\\ns.D\\nS]\\n  1\\n2 O\\nct \\n20\\n19\\nTight Bounds for the Subspace Sketch Problem with Applications\\nYi Li\\nNanyang Technological University\\nyili@ntu.edu.sg\\nRuosong Wang\\nCarnegie Mellon University\\nruosongw@andrew.cmu.edu\\nDavid P. Woodruff\\nCarnegie Mellon University\\ndwoodruf@cs.cmu.edu\\nAbstract\\nIn the subspace sketch problem one is given an n× d matrix A with O(log(nd)) bit entries,\\nand would like to compress it in an arbitrary way to build a small space data structure Qp, so\\nthat for any given x ∈ Rd, with probability at least 2/3, one has Qp(x) = (1 ± ε)‖Ax‖p, where\\np ≥ 0 and the randomness is over the construction of Qp. The central question is:\\nHow many bits are necessary to store Qp?\\nThis problem has applications to the communication of approximating the number of non-\\nzeros in a matrix product, the size of coresets in projective clustering, the memory of streaming\\nalgorithms for regression in the row-update model, and embedding subspaces of Lp in functional\\nanalysis. A major open question is the dependence on the approximation factor ε.\\nWe show if p ≥ 0 is not a positive even integer and d = Ω(log(1/ε)), then Ω˜(ε−2 · d) bits\\nare necessary. On the other hand, if p is a positive even integer, then there is an upper bound\\nof O(dp log(nd)) bits independent of ε. Our results are optimal up to logarithmic factors, and\\nshow in particular that one cannot compress A to O(d) “directions” v1, . . . , vO(d), such that for\\nany x, ‖Ax‖1 can be well-approximated from 〈v1, x〉, . . . , 〈vO(d), x〉. Our lower bound rules out\\narbitrary functions of these inner products (and in fact arbitrary data structures built from A),\\nand thus rules out the possibility of a singular value decomposition for ℓ1 in a very strong sense.\\nIndeed, as ε→ 0, for p = 1 the space complexity becomes arbitrarily large, while for p = 2 it is\\nat most O(d2 log(nd)). As corollaries of our main lower bound, we obtain new lower bounds for\\na wide range of applications, including the above, which in many cases are optimal.\\nYi Li was supported in part by a Singapore Ministry of Education (AcRF) Tier 2 grant MOE2018-T2-1-013.\\nRuosong Wang and David P. Woodruff were supported in part by an Office of Naval Research (ONR) grant N00014-\\n18-1-2562, as well as the Simons Institute for the Theory of Computing where part of this work was done.\\nContents\\n1 Introduction 2\\n1.1 Our Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.2 Connection with Banach Space Theory . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n1.3 Comparison with Prior Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n1.3.1 Comparison with Previous Results in Functional Analysis . . . . . . . . . . . 8\\n1.3.2 Comparison with Previous Results for Graph Sparsifiers . . . . . . . . . . . . 8\\n1.4 Our Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2 Preliminaries 13\\n3 An Ω˜\\n(\\nε−2\\n)\\nLower Bound 15\\n3.1 Spectrum of Matrices M . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n3.2 Orthogonalizing Rows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n3.3 Space Lower Bound on Qp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n4 Lower Bounds for p > 2 23\\n4.1 Lower Bounds for the Subspace Sketch Problem for p > 2 . . . . . . . . . . . . . . . 23\\n4.2 Lower Bounds for the For-All Version . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n4.2.1 Lower Bound for p ≥ 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n4.2.2 Lower Bound for 1 ≤ p ≤ 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n5 Linear Embeddings 27\\n6 Sampling-based Embeddings 28\\n7 Oblivious Sketches 29\\n8 Lower Bounds for M-estimators 31\\n8.1 Proof of Lemma 8.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n9 Lower Bounds on Coresets for Projective Clustering 34\\n10 Upper Bounds for the Tukey Loss p-Norm 36\\n10.1 Mollification of Tukey Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n10.2 Estimation Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n11 An Upper Bound for ℓ1 Subspace Sketches in Two Dimensions 40\\n1\\n1 Introduction\\nThe explosive growth of available data has necessitated new models for processing such data. A\\nparticularly powerful tool for analyzing such data is sketching, which has found applications to\\ncommunication complexity, data stream algorithms, functional analysis, machine learning, numer-\\nical linear algebra, sparse recovery, and many other areas. Here one is given a large object, such as\\na graph, a matrix, or a vector, and one seeks to compress it while still preserving useful information\\nabout the object. One of the main goals of a sketch is to use as little memory as possible in order\\nto compute functions of interest. Typically, to obtain non-trivial space bounds, such sketches need\\nto be both randomized and approximate. By now there are nearly-optimal bounds on the memory\\nrequired of sketching many fundamental problems, such as graph sparsification, norms of vectors,\\nand problems in linear algebra such as low-rank approximation and regression. We refer the reader\\nto the surveys [30, 40] as well as the compilation of lecture notes [2].\\nIn this paper we consider the subspace sketch problem.\\nDefinition 1.1. Given an n × d matrix A with entries specified by O(log(nd)) bits, an accuracy\\nparameter ε > 0, and a function Φ : Rn → R≥0, design a data structure QΦ so that for any x ∈ Rd,\\nwith probability at least 0.9, QΦ(x) = (1± ε)Φ(Ax).\\nThe subspace sketch problem captures many important problems as special cases. We will\\nshow how to use this problem to bound the communication of approximating statistics of a matrix\\nproduct, the size of coresets in projective clustering, the memory of streaming algorithms for\\nregression in the row-update model, and the embedding dimension in functional analysis. We will\\ndescribe these applications in more detail below.\\nThe goal in this work is to determine the memory, i.e., the size of QΦ, required for solving\\nthe subspace sketch problem for different functions Φ. We first consider the classical ℓp-norms\\nΦ(x) =\\n∑n\\ni=1 |xi|p, in which case the problem is referred to as the ℓp subspace sketch problem1.\\nWe later extend our techniques to their robust counterparts Φ(x) =\\n∑n\\ni=1 φ(xi), where φ(t) = |t|p\\nif |t| ≤ τ and φ(t) = τp otherwise. Here Φ is a so-called M -estimator and known as the Tukey loss\\np-norm. It is less sensitive to “outliers” since it truncates large coordinate valus at τ . We let Qp\\ndenote QΦ when Φ(x) =\\n∑\\ni |xi|p, and use Qp,τ when Φ is the Tukey loss p-norm.\\nIt is known that for p ∈ (0, 2] and r = O(ε−2), if one chooses a matrix S ∈ Rr×n of i.i.d. p-stable\\nrandom variables, then for any fixed y ∈ Rn, from the sketch S · y one can output a number z\\nfor which (1 − ε)‖y‖p ≤ z ≤ (1 + ε)‖y‖p with probability at least 0.9 [17]. We say z is a (1 ± ε)-\\napproximation of ‖y‖p. For p = 1, the output is just med(Sy), where med(·) denotes the median\\nof the absolute values of the coordinates in a vector. A sketch S with r = O(ε−2 log n) rows is also\\nknown for p = 0 [22]. For p > 2, there is a distribution on S ∈ Rr×n with r = O(n1−2/p log n/ε2) for\\nwhich one can output a (1 ± ε)-approximation of ‖y‖p given Sy with probability at least 0.9 [15].\\nBy appropriately discretizing the entries, one can solve the ℓp subspace sketch problem by storing\\nS · A for an appropriate sketching matrix S, and estimating ‖Ax‖p using S · A · x. In this way,\\none obtains a sketch of size O˜(ε−2d)2 bits for p ∈ [0, 2], and a sketch of size O˜(n1−2/p/ε2 · d) bits\\nfor p > 2. Note, however, that this is only one particular approach, based on choosing a random\\n1Note we are technically considering the p-th power of the ℓp-norms, but for the purposes of (1+ε)-approximation,\\nthey are the same for constant p. Also, when p < 1, ℓp is not a norm, though it is still a well-defined quantity. Finally,\\nℓ0 denotes the number of non-zero entries of x.\\n2Throughout we use O˜, Ω˜, and Θ˜ to hide factors that are polynomial in log(nd/ε). We note that our lower bounds\\nare actually independent of n.\\n2\\nmatrix S, and better approaches may be possible. Indeed, note that for p = 2, one can simply store\\nATA and output Q2(x) = x\\nTATAx. This is exact (i.e., holds for ε = 0) and only uses O(d2 log(nd))\\nbits of space, which is significantly smaller than O˜(ε−2d) for small enough ε. We note that the ε−2\\nterm may be extremely prohibitive in applications. For example, if one wants high accuracy such\\nas ε = 0.1%, the ε−2 factor is a severe drawback of existing algorithms.\\nA natural question is what makes it possible for p = 2 to obtain O˜(d2) bits of space, and whether\\nit is also possible to achieve O˜(d2) space for p = 1. One thing that makes this possible for p = 2\\nis the singular value decomposition (SVD), namely, that A = UΣV T for matrices U ∈ Rn×d and\\nV ∈ Rd×d with orthonormal columns, and Σ is a non-negative diagonal matrix. Then ‖Ax‖22 =\\n‖ΣV Tx‖22 since U has orthonormal columns. Consequently, it suffices to maintain the d inner\\nproducts 〈Σ1,1v1, x〉, . . . , 〈Σd,dvd, x〉, where the vi’s are the rows of V T . Thus one can “compress”\\nA to d “directions” Σi,ivi. A natural question is whether for p = 1 it is also possible find O(d)\\ndirections v1, . . . , vO(d), such that for any x, ‖Ax‖1 can be well-approximated from some function\\nof 〈v1, x〉, . . . , 〈vO(d), x〉. Indeed, this would be the analogue of the SVD for p = 1, for which little\\nis known.\\nThe central question of our work is:\\nHow much memory is needed to solve the subspace sketch problem as a function of Φ?\\n1.1 Our Contributions\\nUp to polylogarithmic factors, we resolve the above question for ℓp-norms and Tukey loss p-norms\\nfor any p ∈ [0, 2). For p ≥ 2 we also obtain a surprising separation for even integers p from other\\nvalues of p.\\nOur main theorem is the following. We denote by Z+ the set of positive integers.\\nTheorem 1.1 (Informal). Let p ∈ [0,∞) \\\\ 2Z+ be a constant. For any d = Ω(log(1/ε)) and\\nn = Ω˜(ε−2 · d), we have that Ω˜(ε−2 · d) bits are necessary to solve the ℓp subspace sketch problem.\\nWhen p ∈ 2Z+, there is an upper bound of O(dp log(nd)) bits, independent of ε (see Re-\\nmark 3.15). This gives a surprising separation between positive even integers and other values of\\np; in particular for positive even integers p it is possible to obtain ε = 0 with at most O(dp log(nd))\\nbits of space, whereas for other values of p the space becomes arbitrarily large as ε→ 0. This also\\nshows it is not possible, for p = 1 for example, to find O(d) representative directions for ε = 0\\nanalogous to the SVD for p = 2. Note that the lower bound in Theorem 1.1 is much stronger than\\nthis, showing that there is no data structure whatsoever which uses fewer than Ω˜(ε−2 · d) bits, and\\nso as ε gets smaller, the space complexity becomes arbitrarily large.\\nIn addition to the ℓp-norm, in the subspace sketch problem we also consider a more general\\nentry-decomposable Φ, that is, Φ(v) =\\n∑\\ni φ(vi) for v ∈ Rn and some φ : R → R≥0. We show the\\nsame Ω˜(ε−2 · d) lower bounds for a number of M -estimators φ.\\nTheorem 1.2. The subspace sketch problem requires Ω˜(ε−2d) bits when d = Ω(log(1/ε)) and\\nn = Ω˜(ε−2 · d) for the following functions φ:\\n• (L1-L2 estimator) φ(t) = 2(\\n√\\n1 + t2/2− 1);\\n• (Huber estimator) φ(t) = t2/(2τ) · 1{|t|≤τ} + (|t| − τ/2) · 1{|t|>τ};\\n• (Fair estimator) φ(t) = τ2(|x|/τ − ln(1 + |t|/τ));\\n• (Cauchy estimator) φ(t) = (τ2/2) ln(1 + (t/τ)2);\\n3\\n• (Tukey loss p-norm) φ(t) = |t|p · 1{|t|≤τ} + τp · 1{|t|>τ}.\\nWe also consider a mollified version of the 1-Tukey loss function, for which the lower bound of\\nΩ˜(ε−2d) bits still holds. Furthermore, the lower bound is tight up to logarithmic factors, since we\\ndesign a new algorithm which approximates Φ(x) using O˜(ε−2) bits, which implies an upper bound\\nof O˜(ε−2d) for the subspace sketch problem. See Section 10 for details.\\nWhile Theorem 1.1 gives a tight lower bound for p ∈ [0, 2), matching the simple sketching\\nupper bounds described earlier, and also gives a separation from the O(dp log(nd)) bit bound for\\neven integers p ≥ 2, one may ask what exactly the space required is for even integers p ≥ 2 and\\narbitrarily small ε. For p = 2, the O(d2 log(nd)) upper bound is tight up to logarithmic factors\\nsince the previous work [3, Theorem 2.2] implies an Ω˜(d2) lower bound once ε = O(1/\\n√\\nd). For\\np > 2, we show the following: for a constant ε ∈ (0, 1), there is an upper bound of O˜(dp/2) bits (see\\nRemark 4.4), which is nearly tight in light of the following lower bound, which holds for constant\\nε.\\nTheorem 1.3 (Informal). Let p ≥ 2 and ε ∈ (0, 1) be constants. Suppose that n = Ω˜(dp/2), then\\nΩ˜(dp/2) bits are necessary to solve the ℓp subspace sketch problem.\\nNote that Theorem 1.3 holds even if p is not an even integer, and shows that a lower bound of\\ndΩ(p) holds for every p ≥ 2.\\nWe next turn to concrete applications of Theorems 1.1 and 1.2.\\nStatistics of a Matrix Product. In [41], an algorithm was given for estimating ‖A · B‖p for\\ninteger matrices A and B with O(log n) bit integer entries (see Algorithm 1 in [41] for the general\\nalgorithm). When p = 0, this estimates the number of non-zero entries of A · B, which may be\\nuseful since there are faster algorithms for matrix product when the output is sparse, see [31] and\\nthe references therein. More generally, norms of the product A · B can be used to determine how\\ncorrelated the rows of A are with the columns of B. The bit complexity of this problem was\\nstudied in [38, 41]. In [38] a lower bound of Ω(ε−2 · n) bits was shown for estimating ‖AB‖0 for\\nn × n matrices A,B up to a (1 + ε) factor, assuming n ≥ 1/ε2 (this lower bound holds already\\nfor binary matrices A and B). This lower bound implies an ℓ0-subspace sketch lower bound of\\nΩ(ε−2 · d) assuming that d ≥ 1/ε2. Our lower bound in Theorem 1.1 considerably strengthens this\\nresult by showing the same lower bound (up to polylog(d/ε) factors) for a much smaller value of\\nd = Ω(log(1/ε)). For any p ∈ [0, 2], there is a matching upper bound up to polylogarithmic factors\\n(such an upper bound is given implicitly in the description of Algorithm 1 of [41], where the ε there\\nis instantiated with\\n√\\nε, and also follows from the random sketching matrices S discussed above).\\nProjective Clustering. In the task of projective clustering, we are given a set X ⊂ Rd of n\\npoints, a positive integer k, and a non-negative integer j ≤ d. A center C is a k-tuple (V1, V2, . . . , Vk),\\nwhere each Vi is a j-dimensional affine subspace in R\\nd. Given a function φ : R→ R≥0, the objective\\nis to find a center C that minimizes the projective cost, defined to be\\ncost(X, C) =\\n∑\\nx∈X\\nφ(dist(x, C)),\\nwhere dist(x, C) = mini dist(x, Vi), the Euclidean distance from a point p to its nearest subspace\\nVi in C = (V1, V2, . . . , Vk). The coreset problem for projective clustering asks to design a data\\n4\\nstructure Qφ such that for any center C, with probability at least 0.9, Qφ(C) = (1 ± ε) cost(X, C).\\nNote that in this and other computational geometry problems, the dimension d may be small (e.g.,\\nd = log(1/ε)), though one may want a high accuracy solution. Although possibly far from optimal,\\nsurprisingly our lower bound below is the first non-trivial lower bound on the size of coresets for\\nprojective clustering.\\nTheorem 1.4 (Informal). Suppose that φ(t) = |t|p for p ∈ [0,∞)\\\\2Z+ or φ is one of the functions\\nin Theorem 1.2. For k ≥ 1 and j = Ω(log(k/ε)), any coreset for projective clustering requires\\nΩ˜(ε−2kj) bits.\\nLinear Regression. In the linear regression problem, there is an n×d data matrix A and a vector\\nb ∈ Rn. The goal is to find a vector x ∈ Rd so as to minimize Φ(Ax− b), where Φ(v) =∑i φ(vi) for\\nv ∈ Rn and some φ : R→ R≥0. Here we consider streaming coresets for linear regression in the row-\\nupdate model. In the row-update model, the streaming coreset is updated online during one pass\\nover the n rows of\\n(\\nA b\\n)\\n, and outputs a (1±ε)-approximation to the optimal value minxΦ(Ax−b)\\nat the end. By a simple reduction, our lower bound for the subspace sketch problem implies lower\\nbounds on the size of streaming coresets for linear regression in the row-update model. To see this,\\nwe note that by taking sufficiently large λ,\\nmin\\ny\\n(Φ(Ay) + λΦ(x− y)) = Φ(Ax).\\nThus, a streaming coreset for linear regression can solve the subspace sketch problem, which we\\nformalize in the following corollary.\\nCorollary 1.5. Suppose that φ(t) = |t|p for p ∈ [0,∞) \\\\ 2Z+ or φ is one of the functions in\\nTheorem 1.2. Any streaming coreset for linear regression in the row-update model requires Ω˜\\n(\\nε−2d\\n)\\nbits when d = Ω(log(1/ε)).\\nSubspace Embeddings. Let p ≥ 1. Given A ∈ Rn×d, the ℓp subspace embedding problem asks\\nto find a linear map T : Rn → Rr such that for all x ∈ Rd,\\n(1− ε)‖Ax‖p ≤ ‖TAx‖p ≤ (1 + ε)‖Ax‖p. (1)\\nThe smallest r which admits a T for every A is denoted by Np(d, ε), which is of great interest in\\nfunctional analysis. When T is allowed to be random, we require (1) to hold with probability at\\nleast 0.9. This problem can be seen as a special case of the “for-all” version of the subspace sketch\\nproblem in Definition 1.1. In the for-all version of the subspace sketch problem, the data structure\\nQp is required to, with probability at least 0.9, satisfy Qp(x) = (1± ε)‖Ax‖p simultaneously for all\\nx ∈ Rd. In this case, the same lower bound of Ω˜(ε−2 · d) bits holds for p ∈ [1,∞) \\\\ 2Z.\\nSince the data structure can store T if it exists, we can turn our bit lower bound into a dimension\\nlower bound on Np(d, ε). Doing so will incur a loss of an O˜(d) factor (Theorem 5.1). We give an\\nΩ˜(ε−2) lower bound, which is the first such lower bound giving a dependence on ε for general p.\\nCorollary 1.6. Suppose that p ∈ [1,∞)\\\\2Z and d = Ω(log(1/ε)). It holds that Np(d, ε) = Ω˜(ε−2).\\nThe dependence on ε in this lower bound is tight, up to polylog(1/ε) factors, for all values of\\np ∈ [1,∞) \\\\ 2Z [34]. When p ∈ 2Z, no lower bound with a dependence on ε should exist, since a\\n5\\nd-dimensional subspace of ℓnp always embeds into ℓ\\nr\\np isometrically with r =\\n(d+p−1\\np\\n) − 1 [23]. See\\nmore discussion below in Section 1.2 on functional analysis. We also prove a bit complexity lower\\nbound for the aforementioned for-all version of the subspace sketch problem. We refer the reader\\nto Section 4.2 for details.\\nTheorem 1.7. Let p ≥ 1 be a constant. Suppose that ε > 0 is a constant. The for-all version of\\nthe subspace sketch problem requires Ω(dmax{p/2,1}+1) bits.\\nThis lower bound immediately implies a dimension lower bound of Np(d, ε) = Ω˜(d\\nmax{p/2,1}) for\\nthe subspace embedding problem for constant ε, recovering existing lower bounds (up to logarithmic\\nfactors), which are known to be tight.\\nSampling by Lewis Weights. While it is immediate that Np(d, ε) ≥ d, our lower bound above\\nthus far has not precluded the possibility that Np(d, ε) = O˜(d + 1/ε\\n2). However, the next corol-\\nlary, which lower bounds the target dimension for sampling-based embeddings, indicates this is\\nimpossible to achieve using a prevailing existing technique.\\nCorollary 1.8. Let p ≥ 1 and p /∈ 2Z. Suppose that Qp(x) = ‖TAx‖pp solves the ℓp subspace sketch\\nproblem for some T ∈ Rr×n for which each row of T contains exactly one non-zero element. Then\\nr = Ω˜(ε−2d), provided that d = Ω(log(1/ε)).\\nThe same lower bound holds for the for-all version of the ℓp subspace sketch problem. As a\\nconsequence, since the upper bounds of Np(d, ε) in (2) for 1 ≤ p < 2 are based on subsampling\\nwith the “change of density” technique (also known as sampling by Lewis weights [13]), they are,\\nwithin the framework of this classical technique, best possible up to polylog(d/ε) factors.\\nOblivious Sketches. For the for-all version of the ℓp subspace sketch problem, we note that\\nthere exist general sketches such as the Cauchy sketch [12] which are beyond the reach of the\\ncorollary above. Note that the Cauchy sketch is an oblivious sketch, which means the distribution\\nis independent of A. We also prove a dimension lower bound of Ω˜(ε−2 · d) on the target dimension\\nfor oblivious sketches (see Section 7), which is tight up to logarithmic factors since the Cauchy\\nsketch has a target dimension of O(ε−2d log(d/ε)).\\nTheorem 1.9 (Informal). Let p ∈ [1, 2) be a constant. Any oblivious sketch that solves the for-all\\nversion of the ℓp subspace sketch problem has a target dimension of Ω˜(ε\\n−2d).\\nTherefore, it is natural to ask in general whether Np(d, ε) = Ω˜(d/ε\\n2). A proof using the\\nframework of this paper would require an Ω˜(d2/ε2) lower bound for the for-all version of the ℓp\\nsubspace sketch problem. We conjecture it is true; however, our current methods, giving almost-\\ntight lower bounds (in the for-each sense), do not extend to give this result and so we leave it as a\\nmain open problem.\\n1.2 Connection with Banach Space Theory\\nIn the language of functional analysis, the ℓp subspace embedding problem is a classical problem in\\nthe theory of Lp spaces with a rich history. For two Banach spaces X and Y , we say X K-embeds\\ninto Y , if there exists an injective homomorphism T : X → Y satisfying ‖x‖X ≤ ‖Tx‖Y ≤ K‖x‖X\\nfor all x ∈ X. Such a T is called an isomorphic embedding. A classical problem in the theory of\\n6\\nBanach spaces is to consider the isomorphic embedding of finite-dimensional subspaces of Lp =\\nLp(0, 1) into ℓnp = (R\\nn, ‖ · ‖p), where p ≥ 1 is a constant. Specifically, the problem asks what is the\\nminimum value of n, denoted by Np(d, ε), for which all d-dimensional subspaces of Lp (1+ε)-embed\\ninto ℓnp . A comprehensive survey of this problem can be found in [20].\\nThe case of p = 2 is immediate, in which case one can take n = d and ε = 0, obtaining an\\nisometric embedding, and thus we assume p 6= 2. We remark that, when p is an even integer, it is\\nalso possible to attain an isometric embedding into ℓnp with n =\\n(d+p−1\\np\\n) − 1 [23]. In general, the\\nbest3 known upper bounds on Np(d, ε) are as follows.\\nNp(d, ε) ≤\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\nCε−2d log d p = 1\\nCε−2d(log ε−2d)(log log ε−2d+ log(1/ε))2 p ∈ (1, 2)\\nCpε\\n−2dp/2 log2 d log(d/ε) p ∈ (2,∞) \\\\ 2Z\\nCε−2(10d/p)p/2 p ∈ 2Z,\\n(2)\\nwhere C > 0 is an absolute constant and Cp > 0 is a constant that depends only on p. The cases\\nof p = 1 and p ∈ (1, 2) are due to Talagrand [36, 37]. The case of non-even integers p > 2 is taken\\nfrom [24, Theorem 15.13], based on the earlier work of Bourgain et al. [7]. The case of even integers\\np is due to Schechtman [35].\\nThe upper bounds in (2) are established by subsampling with a technique called “change of\\ndensity” [20]. First observe that it suffices to consider embeddings from ℓNp to ℓ\\nn\\np since any d-\\ndimensional subspace of Lp (1 + ε)-embeds into ℓ\\nN\\np for some large N . Now suppose that E is a\\nd-dimensional subspace of ℓNp . One can show that randomly subsampling coordinates induces a\\nlow-distortion isomorphism between E and E restricted onto the sampled coordinates, provided\\nthat each element of E is “spread out” among the coordinates, which is achieved by first applying\\nthe technique of change of density to E.\\nRegarding lower bounds, a quick lower bound follows from the tightness of Dvoretzky’s Theorem\\nfor ℓp spaces (see, e.g. [28, p21]), which states that if ℓ\\nd\\n2 2-embeds into ℓ\\nn\\np , then n ≥ cd for 1 ≤ p < 2\\nand n ≥ (cd/p)p/2 for p ≥ 2, where c > 0 is an absolute constant. Since ℓd2 embeds into Lp\\nisometrically for all p ≥ 1 [19, p16], identical lower bounds for Np(d, ε) follow. Hence the upper\\nbounds in (2) are, in terms of d, tight for p ∈ 2Z and tight up to logarithmic factors for other values\\nof p. However, the right dependence on ε is a long-standing open problem and little is known.\\nSee [20, p845] for a discussion on this topic. It is known that N1(d, ε) ≥ c(d)ε−2(d−1)/(d+2) [7],\\nwhose proof critically relies upon the fact that the unit ball of a finite-dimensional space of ℓ1 is the\\npolar of a zonotope (a linear image of cube [−1, 1]d) and the ℓ1-norm for vectors in the subspace\\nthus admits a nice representation [5]. However, a lower bound for general p is unknown. Our\\nCorollary 1.6 shows that n ≥ cε−2/poly(log(1/ε)) for all p ≥ 1 and p 6∈ 2Z, which is the first lower\\nbound on the dependence of ε for general p, and is optimal up to logarithmic factors. We would\\nlike to stress that except for the very special case of ℓ1, no lower bound on the dependence on ε\\nwhatsoever was known for p 6∈ 2Z. We consider this to be significant evidence of the generality\\nand novelty of our techniques. Moreover, even our lower bound for p = 1 is considerably wider in\\nscope, as discussed more below.\\n3A few upper bounds for the case p ∈ (2,∞) \\\\ 2Z are known, none of which dominates the rest. Here we choose\\nthe one having the best dependence on both d and ε, up to polylog(d/ε) factors.\\n7\\n1.3 Comparison with Prior Work\\n1.3.1 Comparison with Previous Results in Functional Analysis\\nAs discussed, the mentioned lower bounds on Np(d, ε) come from the tightness of Dvoretzky’s\\nTheorem, which shows the impossibility of embedding ℓd2 into a Banach space with low distortion.\\nHere the hardness comes from the geometry of the target space. In contrast, we emphasize that\\nthe hardness in our ℓp subspace sketch problem comes from the source space, since the target space\\nis unconstrained and the output function Qp(·) does not necessarily correspond to an embedding.\\nThe lower bound via tightness of Dvoretzky’s Theorem cannot show that ℓdp does not (1+ε)-embed\\ninto ℓnq for d = Θ(log(1/ε)) and n = O(1/ε\\n1.99), where q 6∈ 2Z.\\nWhen the target space is not ℓp, lower bounds via functional analysis are more difficult to obtain\\nsince they require understanding the geometry of the target space. Since our data structure prob-\\nlem has no constraints on Qp(·), the target space does not even need to be normed. In theoretical\\ncomputer science and machine learning applications, the usual “sketch and solve” paradigm typi-\\ncally just requires the target space to admit an efficient algorithm for the optimization problem at\\nhand4. Our lower bounds are thus much wider in scope than those in geometric functional analysis.\\n1.3.2 Comparison with Previous Results for Graph Sparsifiers\\nRecently, the bit complexity of cut sparsifiers was studied in [3, 10]. Given an undirected graph\\nG = (V,E), |V | = d, a function f : 2V → R is a (1 + ε)-cut sketch, if for any vertex set S ⊆ V ,\\n(1− ε)C(S, V \\\\ S) ≤ f(S) ≤ (1 + ε)C(S, V \\\\ S),\\nwhere C(S, V \\\\ S) denotes the capacity of the cut between S and V \\\\ S. The main result of these\\nworks is that any (1+ ε)-cut sketch requires Ω(ε−2d log d) bits to store. Note that a cut sketch can\\nbe constructed using a for-all version of the ℓp subspace sketch for any p, by just taking the matrix\\nA to be the edge-vertex incidence matrix of the graph G and querying all vectors x ∈ {0, 1}d. Thus,\\none may naturally ask if the lower bounds in [3, 10] imply any lower bounds for the subspace sketch\\nproblem.\\nWe note that both works [3, 10] have explicit constraints on the value of ε. In [3], in order\\nto prove the Ω(ε−2d) lower bound, it is required that ε = Ω(1/\\n√\\nd). In [10] the lower bound of\\nΩ(d log d/ε2) requires ε = ω(1/d1/4). Thus, the strongest lower bound that can be proved using\\nsuch an approach is Ω˜(d2). This is natural, since one can always store the entire adjacency matrix\\nof the graph in O˜(d2) bits. Our lower bound, in contrast, becomes arbitrarily large as ε→ 0.\\n1.4 Our Techniques\\nWe use the case of p = 1 to illustrate our ideas behind the Ω˜\\n(\\nε−2\\n)\\nlower bound for the ℓp subspace\\nsketch problem, when d = Θ(log(1/ε)). We then extend this to an Ω˜\\n(\\nε−2d\\n)\\nlower bound for general\\nd via a simple padding argument. We first show how to prove a weaker Ω˜\\n(\\nε−1\\n)\\nlower bound for\\nthe for-all version of the problem, and then show how to strengthen the argument to obtain both\\na stronger Ω˜\\n(\\nε−2\\n)\\nlower bound and in the weaker original version of the problem (the “for-each”\\nmodel, where we only need to be correct on a fixed query x with constant probability).\\n4For example, consider the space Rn endowed with a premetric d(x, y) =\\n∑\\ni f(xi− yi), where f(x) = τx1{x≥0}+\\n(τ − 1)x1{x≤0} (τ ∈ (0, 1)), which is not even symmetric when τ 6= 12 . See [42] for an embedding into this space.\\n8\\nNote that the condition that d = Θ(log(1/ε)) is crucial for our proof. As shown in Section 11,\\nwhen d = 2, there is actually an O˜(ε−1) upper bound, and thus our Ω˜(ε−2) lower bound does not\\nhold universally for all values of d. It is thus crucial that we look at a larger value of d, and we\\nshow that d = Θ(log(1/ε)) suffices.\\nTo prove our bit lower bounds for the ℓ1 subspace sketch problem, we shall encode random bits\\nin the matrix A such that having a (1 + ε)-approximation to ‖Ax‖1 will allow us to recover, in the\\nfor-each case, some specific random bit, and in the for-all case, all the random bits using different\\nchoices of x. A standard information-theoretic argument then implies that the lower bound for the\\nsubspace sketch problem is proportional to the number of random bits we can recover.\\nWarmup: An Ω˜\\n(\\nε−1\\n)\\nLower Bound for the For-All Version. In our hard instance, we let\\nd = Θ(log(1/ε)) be such that n = 2d = Θ˜(1/ε). Form a matrix A ∈ Rn×d by including all vectors\\ni ∈ {−1, 1}d as its rows and then scaling the i-th row by a nonnegative scalar ri ≤ poly(d). We can\\nthink of r as a vector in Rn with ‖r‖∞ ≤ poly(d). Now, we query Q1(i) for all vectors i ∈ {−1, 1}d.\\nFor an appropriate choice of d = Θ(log(1/ε)), for all i ∈ {−1, 1}d, we have\\n‖Ai‖1 =\\n∑\\nj∈{−1,1}d\\nrj · |〈i, j〉| ≤ 2d · poly(d) < 1\\nε\\n. (3)\\nSince Q1(i) is a (1± ε)-approximation to ‖Ai‖1, and ‖Ai‖1 is always an integer, we can recover the\\nexact value of ‖Ai‖1 using Q1(i), for all i ∈ {−1, 1}d.\\nNow we define a matrix M ∈ Rn×n, whereMi,j = |〈i, j〉|, where i, j are interpreted as vectors in\\n{−1, 1}d. A simple yet crucial observation is that, ‖Ai‖1 is exactly the i-th coordinate ofMr. Notice\\nthat this critically relies on the assumption that r has nonnegative coordinates. Thus, the problem\\ncan be equivalently viewed as designing a vector r ∈ Rn with ‖r‖∞ ≤ poly(d) and recovering r\\nfrom the vector Mr. At this point, a natural idea is to show that the matrix M has a sufficiently\\nlarge rank, say, rank(M) = Ω˜(ε−1), and carefully design r to show an Ω(rank(M)) = Ω˜(ε−1) lower\\nbound.\\nFourier analysis on the hypercube shows that the eigenvectors of M are the rows of the normal-\\nized Hadamard matrix, while the eigenvalues of M are the Fourier coefficients associated with the\\nfunction g(s) = |d− 2wH(s)|, where wH(s) is the Hamming weight of a vector s ∈ Fd2. Considering\\nall vectors of Hamming weight d/2 in Fd2 and their associated Fourier coefficients, we arrive at the\\nconclusion that there are at least\\n( d\\nd/2\\n)\\neigenvalues of M with absolute value∣∣∣∣∣∣∣\\n∑\\n0≤i≤d\\ni is even\\n(−1)i/2\\n(\\nd/2\\ni/2\\n)\\n|d− 2i|\\n∣∣∣∣∣∣∣ ,\\nwhich can be shown to be at least Ω(2d/2/poly(d)). The formal argument is given in Section 3.1.\\nHence rank(M) ≥ ( dd/2) = Ω(2d/poly(d)). Without loss of generality we assume the rank(M) ×\\nrank(M) upper-left block of A is non-singular.\\nNow an Ω˜(1/ε) lower bound follows readily. Set r so that\\nri =\\n{\\nsi, i ≤ rank(M);\\n0, i > rank(M),\\n9\\nwhere {si}rank(M)i=1 is a set of i.i.d. Bernoulli random variables. Since the exact value ofMr is known\\nand the rank(M) × rank(M) upper-left block of A is non-singular, one can recover the values of\\n{si}rank(M)i=1 by solving a linear system, which implies an Ω(rank(M)) = Ω˜(ε−1) lower bound.\\nBefore proceeding, let us first review why our argument fails for p = 2. For the ℓp-norm, the\\nFourier coefficients associated with the vectors of Hamming weight d/2 on the Boolean cube are∣∣∣∣∣∣∣\\n∑\\n0≤i≤d\\ni is even\\n(−1)i/2\\n(\\nd/2\\ni/2\\n)\\n|d− 2i|p\\n∣∣∣∣∣∣∣ = Θ\\n(\\n2d/2√\\nd\\n∣∣∣sin pπ\\n2\\n∣∣∣) .\\nTherefore this sum vanishes if and only if p is an even integer, in which case rank(A) will no longer\\nbe Ω(2d/poly(d)) and the lower bound argument will fail.\\nAn Ω˜\\n(\\nε−2\\n)\\nLower Bound for the For-Each Version. To strengthen this to an Ω˜(ε−2) lower\\nbound, it is tempting to increase d so that n = 2d = Ω˜(ε−2). In this case, however, we can no\\nlonger recover the exact value of Mr, since each entry of Mr now has magnitude Θ˜(ε−2) and the\\nfunction Q1(·) only gives a (1± ε)-approximation. We still obtain a noisy version of Mr, but with\\na Θ˜(1/ε) additive error on each entry. One peculiarity of the model here is that if some entries of\\nr are negative, then ‖Ai‖1 = (M |r|)i (cf. (3)), where |r| denotes the vector formed by taking the\\nabsolute value of each coordinate of r, i.e., ‖Ai‖1 depends only on the absolute values of entries\\nof r, which suggests that the constraint that each entry of Mr has magnitude Θ˜(1/ε2) with an\\nadditive error of Θ˜(1/ε) is somehow intrinsic.\\nTo illustrate our idea for overcoming the issue of large additive error, for the time being let us\\nforget the actual form ofM previously defined in the argument for our Ω˜\\n(\\nε−1\\n)\\nlower bound and con-\\nsider instead a general M ∈ Rn×n with orthogonal rows, each row having ℓ2 norm Ω(2d/2/poly(d)).\\nFor now we also allow r to contain negative entries such that ‖r‖∞ ≤ poly(d), and pretend that\\nthe noisy version of Mr has an Θ˜(1/ε) additive error on each entry. Now, let\\nr =\\nn∑\\ni=1\\nsi · Mi‖Mi‖2 ,\\nwhere {si}ni=1 is a set of i.i.d. Rademacher random variables. By a standard concentration inequality,\\n‖r‖∞ ≤ poly(d) holds with high probability (recall that n = 2d). Consider the vector Mr. Due to\\nthe orthogonality of the rows of M , the i-th coordinate of Mr will be\\n〈Mi, r〉 = si · ‖Mi‖2.\\nProvided that ‖Mi‖2 is larger than the additive error Θ˜(1/ε), we can still recover si by just looking\\nat the sign of 〈Mi, r〉. Thus, for an appropriate choice of d such that 2d/2/poly(d) = Ω˜(1/ε), we\\ncan obtain an Ω(2d) = Ω˜(1/ε2) lower bound.\\nNow we return to the original M withMi,j = |〈i, j〉|, whose rows are not necessarily orthogonal.\\nThe previous argument still goes through so long as we can identify a subset R ⊆ [n] = [2d] of size\\n|R| ≥ Ω(2d/poly(d)) such that the rows {Mi}i∈R are nearly orthogonal, meaning that the ℓ2 norm\\nof the orthogonal projection of Mi onto the subspace spanned by other rows {Mj}j∈R\\\\{i} is much\\nsmaller than ‖Mi‖2.\\n10\\nTo achieve this goal, we study the spectrum of M , and as far as we are aware, this is the first\\nsuch study of spectral properties of this matrix. The Fourier argument mentioned above implies\\nthat at least Ω(2d/poly(d)) eigenvalues of A have the same absolute value Ω(2d/2/poly(d)). If\\nall other eigenvalues of A were zero, then we could identify a set of |R| ≥ Ω(2d/poly(d)) nearly\\northogonal rows using rows of A each with ℓ2 norm Ω(2\\nd/2/poly(d)), using a procedure similar to\\nthe standard Gram-Schmidt process. The full details can be found in Section 3.2. Although the\\nother eigenvalues of M are not all zero, we can simply ignore the associated eigenvectors since they\\nare orthogonal to the set of nearly orthogonal rows we obtain above.\\nLastly, recall that what we truly obtain is M |r| rather than Mr unless r ≥ 0. To fix this, note\\nthat ‖r‖∞ ≤ poly(d) with high probability, and so we can just shift each coordinate of r by a fixed\\namount of poly(d) to ensure that all entries of r are positive. We can still obtain 〈Mi, r〉 with an\\nadditive error Θ˜(1/ε), since the amount of the shift is fixed and bounded by poly(d).\\nNotice that the above argument in fact holds even for the for-each version of the subspace sketch\\nproblem. By querying the i-th vector on the Boolean cube for some i ∈ R, we are able to recover\\nthe sign of si with constant probability. Given this, a standard information-theoretic argument\\nshows that our lower bound holds for the for-each version of the problem.\\nThe formal analysis given in Section 3.3 is a careful combination of all the ideas mentioned\\nabove.\\nApplications: M-estimators and Projective Clustering Coresets. Our general strategy\\nfor proving lower bounds forM -estimators is to relate oneM -estimator, for which we want to prove\\na lower bound, to another M -estimator for which a lower bound is easy to derive. For the L1-L2\\nestimator, the Huber estimator and the Fair estimator, when |t| is sufficiently large, φ(t) = (1±ε)|t|\\n(up to rescaling of t and the function value), and thus the lower bounds follow from those for the\\nℓ1 subspace sketch problem.\\nFor the Cauchy estimator, we relate it to another estimator φaux(t) = ln |x| ·1{|x|≥1}. In Section\\n8, we show that our Fourier analytic arguments also work for φaux(t). Since for sufficiently large t,\\nthe Cauchy estimator satisfies φ(t) = (1± ε)φaux(t) (up to rescaling of t and the function value), a\\nlower bound for the Cauchy estimator follows.\\nTo prove lower bounds on coresets for projective clustering, the main observation is that when\\nk = 1 and j = d− 1, by choosing the query subspace to be the orthogonal complement of a vector\\nz, the projection cost is just\\n∑\\nx∈X φ(〈x, z〉), and thus we can invoke our lower bounds for the\\nsubspace sketch problem. We use a coding argument to handle general k. In Lemma 9.1, we show\\nthere exists a set S = {(s1, t1), (s2, t2), . . . , (sk, tk)}, where si, ti ∈ RO(log k), 〈si, ti〉 = 0 and 〈si, tj〉 is\\narbitrarily large when i 6= j. Now for k copies of the hard instance of the subspace sketch problem,\\nwe add si as a prefix to all data points in the i-th hard instance, and set the query subspace to be\\nthe orthogonal complement of a vector z, to which we add ti as a prefix. Now, the data points in\\nthe i-th hard instance will always choose the i-th center in the optimal solution, since otherwise an\\narbitrarily large cost will incur. Thus, we can solve k independent copies of the subspace sketch\\nproblem, and the desired lower bound follows.\\nIn the rest of the section, we shall illustrate our techniques for proving lower bounds that depend\\non p for the ℓp subspace sketch problem. These lower bounds hold even when ε is a constant. We\\nagain resort to information theory, trying to recover, using Qp queries, the entire matrix A among\\na collection S of matrices. The lower bound is then Ω(log |S|) bits.\\n11\\nAn Ω˜(dp/2) Lower Bound for the For-Each Version. Our approach for proving the Ω˜(dp/2)\\nlower bound is based on the following crucial observation: consider a uniformly random matrix\\nA ∈ {−1, 1}Θ(dp/2)×d and a uniformly random vector x ∈ {−1, 1}d. Then E ‖Ax‖pp = O(dp),\\nwhereas for each row Ai ∈ Rd of A, interpreted as a column vector, ‖AAi‖pp ≥ dp. Intuitively, the\\nlower bound comes from the fact that one can recover the whole matrix A by querying all Boolean\\nvectors x ∈ {−1, 1}d using the function Qp(·), since if x is a row of A, then ‖Ax‖pp would be slightly\\nlarger than its typical value, by adjusting constants.\\nTo implement this idea, one can generate a set of almost orthogonal vectors S ⊆ Rd and require\\nthat all rows of A come from S. A simple probabilistic argument shows that one can construct a\\nset of |S| = dp vectors such that for any distinct s, t ∈ S, |〈s, t〉| ≤ O(√d log d)5. If we form the\\nmatrix A using n = Ω˜(dp/2) vectors from S as its rows, then for any vector t that is not a row of A,\\n‖At‖pp ≤ n · (d log d)p/2 ≪ dp\\nfor some appropriate choice of n. Thus, by querying Qp(s) for all vectors s ∈ S, one can recover\\nthe whole matrix A, even when ε is a constant. By a standard information-theoretic argument,\\nthis leads to a lower bound of Ω\\n(\\nlog\\n( dp\\nΩ˜(dp/2)\\n))\\n= Ω˜(dp/2) bits. Furthermore, one only needs to\\nquery |S| = dp vectors, which means the lower bound in fact holds for the for-each version of the ℓp\\nsubspace sketch problem, by a standard repetition argument and losing a log d factor in the lower\\nbound.\\nAn Ω(dmax{p/2,1}+1) Lower Bound for the For-All Version. In order to obtain the nearly\\noptimal Ω(dmax{p/2,1}+1) lower bound for the for-all version, we must abandon the constraint that\\nall rows of the A matrix come from a set S of poly(d) vectors. Our plan is still to construct a\\nlarge set of matrices S ⊆ {+1,−1}Θ(dp/2)×d, and show that for any distinct matrices S, T ∈ S, it\\nis possible to distinguish them using the function Qp(·), thus proving an Ω(log |S|) lower bound.\\nThe new observation is that, to distinguish two matrices S, T ∈ S, it suffices to have a single row\\nof T , say Ti, such that ‖STi‖pp ≪ dp. Again using the probabilistic method, we show the existence\\nof such a set S with size exp (Ω(dp/2+1)), which implies an Ω(log |S|) = Ω(dp/2+1) lower bound.\\nOur main technical tool is Talagrand’s concentration inequality, which shows that for any p ≥ 2\\nand vector x ∈ {−1, 1}d, for a matrix A ∈ RΘ(dp/2)×d with i.i.d. Rademacher entries, ‖Ax‖p = Θ(d)\\nwith probability 1−exp(−Ω(d)). This implies that for two random matrices S, T ∈ RΘ(dp/2)×d with\\ni.i.d. Rademacher entries, the probability that there exists some row Ti of T such that ‖STi‖pp ≪ dp\\nis at least 1 − exp (Ω(dp/2+1)), since the Θ(dp/2) rows of T are independent. By a probabilistic\\nargument, the existence of the set S follows. The formal analysis is given in Section 4.2.1.\\nThe above argument fails to give an Ω(d2) lower bound when p < 2. However, for any p < 2,\\nsince ℓn2 embeds into ℓ\\nm\\np with m = Op(n) and a constant distortion, we can directly reduce the case\\nof p < 2 to the case of p = 2. The formal analysis can be found in Section 4.2.2. Combining these\\ntwo results yields the Ω(dmax{p/2,1}+1) lower bound.\\n5The O(\\n√\\nlog d) factor can be removed using more sophisticated constructions based on coding theory (see Lemma\\n4.1).\\n12\\n2 Preliminaries\\nFor a vector x ∈ Rn, we use ‖x‖p to denote its ℓp-norm, i.e., ‖x‖p = (\\n∑n\\ni=1 |xi|p)1/p. When p < 1,\\nit is not a norm but it is still a well-defined quantity and we call it the ℓp-norm for convenience.\\nWhen p = 0, ‖x‖0 is defined to be the number of nonzero coordinates of x.\\nFor two vectors x, y ∈ Rn, we use projy x ∈ Rn to denote the orthogonal projection of x onto y.\\nFor a matrix A ∈ Rn×d, we use Ai ∈ Rd to denote its i-th row, treated as a column vector. We use\\n‖A‖2 to denote its spectral norm, i.e., ‖A‖2 = sup‖x‖2=1 ‖Ax‖2, and ‖A‖F to denote its Frobenius\\nnorm, i.e., ‖A‖F =\\n(∑n\\ni=1\\n∑d\\nj=1A\\n2\\nij\\n)1/2\\n.\\nSuppose that A ∈ Rm×n has singular values σ1 ≥ σ2 ≥ · · · ≥ σr ≥ 0, where r = min{m,n}. It\\nholds that σ1 = ‖A‖2 ≤ ‖A‖F =\\n(∑r\\ni=1 σ\\n2\\ni\\n)1/2\\n. The condition number of A is defined to be\\nκ(A) =\\nsup‖x‖2=1 ‖Ax‖2\\ninf‖x‖2=1 ‖Ax‖2\\n.\\nTheorem 2.1 (Eckart–Young–Mirsky Theorem). Suppose that A ∈ Rm×n has singular values\\nσ1 ≥ σ2 ≥ · · · ≥ σr > 0, where rank(A) = r ≤ min{m,n}. For any matrix B ∈ Rm×n such that\\nrank(B) ≤ k ≤ r, it holds that\\n‖A−B‖2F ≥\\nr∑\\ni=k+1\\nσ2i .\\nBelow we list a handful of concentration inequalities which will be useful in our arguments.\\nLemma 2.2 (Hoeffding’s inequality, [6, p34]). Let s1, . . . , sn be i.i.d. Rademacher random variables\\nand a1, . . . , an be real numbers. Then\\nPr\\n{∑\\ni\\nsiai > t\\n}\\n≤ exp\\n(\\n− t\\n2\\n2\\n∑\\ni a\\n2\\ni\\n)\\n.\\nLemma 2.3 (Khintchine’s inequality, [6, p145]). Let s1, . . . , sn be i.i.d. Rademacher random vari-\\nables and a1, . . . , an be real numbers. There exist absolute constants A,B > 0 such that\\nA\\n(∑\\ni\\na2i\\n)1/2\\n≤\\n(\\nE |\\n∑\\ni\\nsiai|p\\n)1/p\\n≤ B√p\\n(∑\\ni\\na2i\\n)1/2\\n.\\nLemma 2.4 (Talagrand’s inequality, [6, p204]). Let X = (X1, . . . ,Xn) be a random vector with\\nindependent coordinates taking values in [−1, 1]. Let f : [−1, 1]n → R be a convex 1-Lipschitz\\nfunction. It holds for all t ≥ 0 that\\nPr {f(X)− E f(X) ≥ t} ≤ e−t2/8.\\nLemma 2.5 (Gaussian concentration, [39, p105]). Let p ≥ 1 be a constant. Consider a random\\nvector X ∼ N(0, In) and a non-negative 1-Lipschitz function f : (Rn, ‖ · ‖2)→ R, then\\nPr\\n{\\n|f(x)− (E(f(x))p)1/p| ≥ t\\n}\\n≤ 2e−ct2 ,\\nwhere c = c(p) > 0 is a constant that depends only on p.\\n13\\nLemma 2.6 (Extreme singular values, [39, p91]). Let A be an N×n matrix with i.i.d. Rademacher\\nentries. Let σmin(A) and σmax(A) be the smallest and largest singular values of A. Then for every\\nt ≥ 0, with probability at least 1− 2 exp(ct2), it holds that\\n√\\nN − C√n− t ≤ σmin(A) ≤ σmax(A) ≤\\n√\\nN +C\\n√\\nn+ t,\\nwhere C, c > 0 are absolute constants.\\nLemma 2.7. Let U1, . . . , Uk ∈ Rn be orthonormal vectors and s1, . . . , sk be independent Rademacher\\nrandom variables. It holds that\\nPr\\n{∥∥∥∥∥\\nk∑\\ni=1\\nsi · Ui\\n∥∥∥∥∥\\n∞\\n≤ 3\\n√\\nln k\\n}\\n≥ 1− 1\\nk1.3\\n.\\nProof. Let Z =\\n∑k\\ni=1 siUi, then\\nZj =\\nk∑\\ni=1\\nsiUi,j.\\nSince {Ui} is a set of orthonormal vectors, we have that\\nk∑\\ni=1\\nU2i,j ≤ 1.\\nIt follows from Hoeffding’s inequality (Lemma 2.2) that for each j ∈ [k],\\nPr{|Zj | ≥ 3\\n√\\nln k} ≤ exp(−2 ln k).\\nThe claimed inequality follows by taking a union bound over all j ∈ [k].\\nWe also need a result concerning uniform approximation of smooth functions by polynomials.\\nLet Pn denote the space of polynomials of degree at most n. For a given function f ∈ C[a, b], the\\nbest degree-n approximation error En(f ; [a, b]) is defined to be\\nEn(f ; [a, b]) = inf\\np∈Pn\\n‖f − p‖∞,\\nwhere the ‖·‖∞ norm is taken over [a, b]. The following bound on approximation error is a classical\\nresult.\\nLemma 2.8 ([33, p23]). Let f(x) have a k-th derivative on [−1, 1]. If n > k,\\nEn(f ; [−1, 1]) ≤ 6\\nk+1ek\\n(k + 1)nk\\nωk\\n(\\n1\\nn− k\\n)\\n,\\nwhere ωk is the modulus of continuity of f\\n(k), defined as\\nωk(δ) = sup\\nx,y∈[−1,1]\\n|x−y|≤δ\\n∣∣∣f (k)(x)− f (k)(y)∣∣∣ .\\n14\\n3 An Ω˜\\n(\\nε−2\\n)\\nLower Bound\\nTo prove the space lower bound of the data structure Qp, we appeal to information theory. We\\nshall encode random bits in A such that if for each x, Qp(x) approximates ‖Ax‖pp (or ‖Ax‖p when\\np = 0) up to a 1±ε factor with probability at least 0.9, we can recover from Qp(x) some random bit\\n(depending on x) with at least constant probability. A standard information-theoretic argument\\nimplies a lower bound on the size of Qp which is proportional to the number of random bits we can\\nrecover.\\nFor each p ≥ 0, we define a family of matrices M (p) = {M (d,p)}∞d=1, where M (d,p) is a 2d × 2d\\nmatrix with entries defined as\\nM\\n(d,p)\\ni,j = |〈i, j〉|p,\\nwhere i and j are interpreted as vectors on the Boolean cube {−1, 1}d. We assume 00 = 0 through-\\nout the paper.\\n3.1 Spectrum of Matrices M\\nLemma 3.1. For any d ≥ 1, M (d,p) can be rewritten as H(d)Λ(d,p)(H(d))T in its spectral decompo-\\nsition form, where Λ(d,p) is a 2d × 2d diagonal matrix, and H(d) is a 2d × 2d normalized Hadamard\\nmatrix.\\nProof. Let T be the natural isomorphism from the multiplicative group {−1, 1}d to the additive\\ngroup Fd2. Then |〈i, j〉|p = g(T i + Tj) for some function g defined on Fd2. It can be computed\\n(see [25, Lemma 5]) that the singular values of a matrix with entries g(T i + Tj) are the absolute\\nvalues of the Fourier coefficients of g. In our particular case, the singular values of M (d,p) are\\n|gˆ(s)| =\\n∣∣∣∣∣∣\\n∑\\nx∈Fd2\\n(−1)〈s,x〉g(x)\\n∣∣∣∣∣∣ , s ∈ Fd2.\\nFurthermore, the proof of that lemma shows that H(d) in the spectral decomposition is given by\\n(H(d)s )z =\\n1\\n2d/2\\n(−1)〈s,z〉, s, z ∈ Fd2,\\nwhich implies that H(d) is a normalized Hadamard matrix.\\nLemma 3.2. When d is even, there are at least\\n( d\\nd/2\\n)\\nentries in Λ(d,p) with absolute value∣∣∣∣∣∣∣\\n∑\\n0≤i≤d\\ni is even\\n(−1)i/2\\n(\\nd/2\\ni/2\\n)\\n|d− 2i|p\\n∣∣∣∣∣∣∣ , Λ(d,p)0 ≥ 0.\\nProof. We shall use the notation in the proof of Lemma 3.1. Consider the Fourier coefficients gˆ(s)\\nfor s ∈ Fd2 with Hamming weight d/2, which is the same for all\\n( d\\nd/2\\n)\\nsuch s’s. Note that\\ngˆ(s) =\\nd∑\\ni=0\\ni∑\\nj=0\\n(−1)j\\n(\\nd/2\\nj\\n)(\\nd/2\\ni− j\\n)\\ng(i).\\n15\\nBy comparing the coefficients of xi on both sides of the identity (1 + x)d/2(1− x)d/2 = (1− x2)d/2,\\nwe see that\\ni∑\\nj=0\\n(−1)j\\n(\\nd/2\\nj\\n)(\\nd/2\\ni− j\\n)\\n=\\n{\\n(−1)i/2(d/2\\ni/2\\n)\\n, i is even;\\n0, i is odd.\\nHence\\ngˆ(s) =\\n∑\\neven i\\n(−1)i/2\\n(\\nd/2\\ni/2\\n)\\ng(i).\\nFinally, observe that, for x, y ∈ {+1,−1}d, we have (d − 〈x, y〉)/2 = dH(x, y) = wH(Tx + Ty),\\nwhere dH(x, y) denotes the Hamming distance between x and y and wH(s) denotes the Hamming\\nweight of s ∈ Fd2. Hence g(i) = |d− 2i|p and the conclusion follows.\\nLet N (d) be the multiplicity of the singular value Λ\\n(d,p)\\n0 of M\\n(d,p). We know from the preceding\\nlemma that N (d) ≥ ( dd/2). By permuting the columns of H(d), we may assume the absolute value\\nof the first N (d) diagonal entries of Λ(d,p) are all equal to Λ\\n(d,p)\\n0 , i.e.,∣∣∣Λ(d,p)1 ∣∣∣ = ∣∣∣Λ(d,p)2 ∣∣∣ = · · · = ∣∣∣Λ(d,p)N(d)∣∣∣ = Λ(d,p)0 .\\nThe following lemma is critical in lower bounding Λ\\n(d,p)\\n0 . We found the result in a post on\\nmath.stackexchange.com [1] but could not find it in any published literature and so we reproduce\\nthe proof in full from [1], with small corrections regarding convergence of integrals.\\nLemma 3.3. It holds for all complex p satisfying 0 < Re p < 2n that\\nn∑\\nk=1\\n(−1)k+1\\n(\\n2n\\nn+ k\\n)\\nkp = 22n−p\\nΓ(p + 1)\\nπ\\n(\\nsin\\nπp\\n2\\n) ∫ ∞\\n0\\nsin2n t\\ntp+1\\ndt. (4)\\nProof. By the binomial theorem,\\n(z − 1)2n =\\n2n∑\\nk=0\\n(\\n2n\\nk\\n)\\n(−z)k =\\nn∑\\nk=−n\\n(\\n2n\\nk + n\\n)\\n(−z)k+n.\\nSplitting the sum at k = −1 and k = 1, we have\\nn∑\\nk=1\\n(−1)k\\n(\\n2n\\nn+ k\\n)(\\nzk + z−k\\n)\\n= (−1)n(z − 1)2n z−n −\\n(\\n2n\\nn\\n)\\n.\\nPlugging in z = exp (2it) yields\\n(2 sin t)2n = 2\\nn∑\\nk=1\\n(−1)k\\n(\\n2n\\nn+ k\\n)\\ncos(2kt) +\\n(\\n2n\\nn\\n)\\n. (5)\\nPlug (5) into the integral on the right-hand side of (4) and introduce a regularizer exp(−st) (s > 0)\\nunder the integral sign:∫ ∞\\n0\\ne−st\\n(2 sin t)2n\\ntp+1\\ndt = 2\\nn∑\\nk=1\\n(−1)k\\n(\\n2n\\nn+ k\\n)∫ ∞\\n0\\ne−st cos(2kt)\\ntp+1\\ndt+\\n(\\n2n\\nn\\n)\\nΓ(−p)sp, −1 < Re p < 0.\\n16\\nOne can compute that∫ ∞\\n0\\ne−st cos(2kt)\\ntp+1\\ndt =\\n1\\n2\\n∫ ∞\\n0\\ne−st(ei2kt + e−i2kt)\\ntp+1\\ndt =\\n(s− 2ki)p + (s+ 2ki)p\\n2\\nΓ(−p)\\n= (4k2 + s2)\\np\\n2 cos\\n(\\np arctan\\n2k\\ns\\n)\\nΓ(−p), −1 < Re p < 0.\\nIt follows that∫ ∞\\n0\\ne−st\\n(2 sin t)2n\\ntp+1\\ndt\\n= 2\\nn∑\\nk=1\\n(−1)k\\n(\\n2n\\nn+ k\\n)\\n(4k2 + s2)\\np\\n2 cos\\n(\\np arctan\\n2k\\ns\\n)\\nΓ(−p) +\\n(\\n2n\\nn\\n)\\nΓ(−p)sp, −1 < Re p < 0.\\nIt is easy to verify that the integral on the left-hand side is analytic whenever the integral converges.\\nAnalytic continuation permits p to be extended to {p : −1 < Re p < 2n} \\\\ Z. Now, for p such that\\n0 < Re p < 2n and p /∈ Z, let s → 0+ on both sides. It is also easy to verify that we can take the\\nlimit s→ 0+ under the integral sign, hence∫ ∞\\n0\\n(2 sin t)2n\\ntp+1\\ndt = 2\\nn∑\\nk=1\\n(−1)k\\n(\\n2n\\nn+ k\\n)\\n(2k)p cos\\n(πp\\n2\\n)\\nΓ(−p), 0 < Re p < 2n, p 6∈ Z. (6)\\nInvoking the reflection identity (see, e.g. [4, p9])\\nΓ(−p)Γ(1 + p) = − π\\nsin(pπ)\\n, p 6∈ Z (7)\\nwe obtain that\\nn∑\\nk=1\\n(−1)k\\n(\\n2n\\nn+ k\\n)\\nkp = −2\\n2n−pΓ(p + 1)\\nπ\\n(\\nsin\\npπ\\n2\\n) ∫ ∞\\n0\\nsin2n t\\ntp+1\\ndt, 0 < Re p < 2n, p 6∈ Z.\\nFinally, analytic continuation extends p to the integers in (0, 2n).\\nAs an immediate corollary of Lemma 3.3, we have:\\nCorollary 3.4. Suppose that d ∈ 8Z. There exists an absolute constant c > 0 such that\\nΛ\\n(d,p)\\n0 ≥ c\\n2d/2√\\nd\\n∣∣∣sin pπ\\n2\\n∣∣∣ .\\nProof. Letting 2n = d/2 and k = n− i/2, the summation in Lemma 3.2 becomes\\n22p+1\\n∣∣∣∣∣\\nn∑\\nk=1\\n(−1)k\\n(\\n2n\\nn+ k\\n)\\nkp\\n∣∣∣∣∣ = 2d/22p+1Γ(p+ 1)π ∣∣∣sin pπ2 ∣∣∣\\n∫ ∞\\n0\\nsind t\\ntp+1\\ndt.\\nSince (cf. [14, p511]) ∫ π\\n0\\nsind x dx =\\n√\\nπΓ(d+12 )\\nΓ(d2 )\\n≥ C√\\nd\\n,\\n17\\nwhere C > 0 is an absolute constant, we have that∫ ∞\\n0\\nsind t\\ntp+1\\ndt ≥\\n∞∑\\nn=0\\n1\\n((n + 1)π)p+1\\n∫ (n+1)π\\nnπ\\nsind x dx ≥ C√\\nd\\n· ζ(p+ 1)\\nπp+1\\n.\\nNotice that h(p) = Γ(p + 1)ζ(p + 1)/(π/2)p+1 is a positive continuous function on (0,∞) and\\nh(p)→∞ as p→∞ and p→ 0+, it must hold that infp>0 h(p) > 0. The conclusion follows.\\n3.2 Orthogonalizing Rows\\nSuppose we are given a matrix Π ∈ Rn×n in its spectral decomposition form Π = HΣHT , where\\nΣi,i =\\n{\\n±σ, i ≤ r;\\n0, r < i ≤ n,\\nand H is the normalized Hadamard matrix. The goal of this section is to identify a set of orthogonal\\nvectors, using rows of Π.\\nLemma 3.5. Each row of Π has the same ℓ2 norm ‖Πi‖2 = σ\\n√\\nr/n.\\nProof.\\n‖Πi‖2 = ‖HiΣHT ‖2 = ‖HiΣ‖2.\\nThe lemma follows since all entries in H have absolute value 1/\\n√\\nn, and the r non-zero entries on\\nthe diagonal of Σ have absolute value σ.\\nTo identify a set of orthogonal vectors using the rows of Π, we run a procedure similar to the\\nstandard Gram-Schmidt process.\\nLemma 3.6. There is a set R ⊆ [n] with size |R| = r/100 such that for each i ∈ R, Πi can be\\nwritten as\\nΠi = Ri + Pi, (8)\\nwhere {Ri}i∈R is a set of orthogonal vectors, and Pi is the orthogonal projection of Πi onto the\\nsubspace spanned by {Rj}j∈R\\\\{i}. Furthermore, for each i ∈ R, ‖Ri‖22 ≥ 99/100‖Πi‖22 = 99/100 ·\\nσ2r/n.\\nProof. We show how to construct such a set R. Suppose that we have found a set R with size\\nstrictly less than r/100 with Πi = Ri + Pi satisfying the stated constraints. We shall show how to\\nincrease the size of R by one.\\nLet Π = S + Q. Here, for each i ∈ [n] we have Πi = Si + Qi, where Qi is the orthogonal\\nprojection of Πi onto the subspace spanned by {Rj}j∈R and Si = Πi−Qi. Notice that for all j ∈ R\\nwe have Qj = Πj and Sj = 0. Since ‖Π‖2F = rσ2 and rank(Q) ≤ |R|, by Theorem 2.1 we have∑\\ni∈[n]\\n‖Si‖22 = ‖S‖2F = ‖Π−Q‖2F ≥ ‖Π‖2F − |R| · σ2 >\\n99\\n100\\n‖Π‖2F =\\n99\\n100\\n∑\\ni∈[n]\\n‖Πi‖22.\\nThus, by averaging, there exists i /∈ R such that ‖Si‖22 > 99/100‖Πi‖22. We add i into R and set\\nRi in (8) to be Si and Pi to be Qi. It is easy to verify that the stated constraints still hold. We\\ncontinue this process inductively until |R| = r/100.\\n18\\nLemma 3.7. Suppose that e ∈ Rn satisfies ‖e‖∞ ≤ 0.1σ\\n√\\nr/n. Let x ∈ Rn be a random vector\\ndefined as\\nx =\\n∑\\ni∈R\\nsi · Ri‖Ri‖2 ,\\nwhere {si}i∈R is a set of i.i.d. Rademacher random variables. Here the set R and the orthogonal\\nvectors {Ri}i∈R are as defined in Lemma 3.6. For each i ∈ R, it holds that\\nPr\\nx\\n{sign ((Πx+ e)i) = sign(si)} ≥ 4\\n5\\n.\\nProof. For each i ∈ R, we have\\n〈Πi, x〉 = 〈Ri, x〉+ 〈Pi, x〉 = si · ‖Ri‖2 +\\n∑\\nj∈R\\\\{i}\\nsj · ‖projRj Πi‖2.\\nWe first analyze the second term.\\nE |〈Pi, x〉| ≤\\n\\uf8eb\\uf8ed ∑\\nj∈R\\\\{i}\\n‖projRj Πi‖22\\n\\uf8f6\\uf8f81/2 = ‖Pi‖2 ≤ 1\\n10\\n‖Πi‖2.\\nBy Markov’s inequality, with probability at least 4/5, we have |〈Pi, x〉| ≤ ‖Πi‖2/2.\\nRecall that ‖Ri‖2 ≥ 99/100‖Πi‖2 (Lemma 3.6) and ‖Πi‖2 = σ\\n√\\nr/n (Lemma 3.5). It happens\\nwith probability at least 4/5 that |ei|+ |〈Pi, x〉| < |〈Ri, x〉|, in which case we have sign ((Πx+ e)i) =\\nsign(si).\\n3.3 Space Lower Bound on Qp\\nIn this section, we describe a reduction from the subspace sketch problem to the INDEX problem,\\na classical problem in communication complexity. We shall rephrase the problem in the context\\nof a data structure. The INDEX data structure stores an input string s ∈ {−1, 1}n and supports\\na query function, which receives an input i ∈ [n] and outputs si ∈ {−1, 1} which is the i-th bit\\nof the underlying string. To prove the lower bound for the subspace sketch problem, we need the\\nfollowing lower bound for the distributional INDEX problem.\\nLemma 3.8 ([29]). In the INDEX problem, suppose that the underlying string s is drawn uniformly\\nfrom {−1, 1}n and the input i of the query function is drawn uniformly from [n]. Any (randomized)\\ndata structure for INDEX that succeeds with probability at least 2/3 requires Ω(n) bits of space,\\nwhere the randomness is taken over both the randomness in the data structure and the randomness\\nof s and i.\\nThroughout the reduction, d is a fixed parameter with value to be determined later. For the\\nmatrix M (d,p), we consider its spectrum-truncated version\\nM˜ (d,p) , H(d)diag(Λ\\n(d,p)\\n1 ,Λ\\n(d,p)\\n2 . . . . ,Λ\\n(d,p)\\nN(d)\\n, 0, 0, . . . , 0)(H(d))T .\\nLemma 3.9. Each row of M˜ (d,p) is orthogonal to all eigenvectors associated with eigenvalues other\\nthan Λ\\n(d,p)\\n1 ,Λ\\n(d,p)\\n2 , . . . ,Λ\\n(d,p)\\nN(d)\\n.\\n19\\nProof. Let v1, . . . , v2d be the columns of H\\n(d). Then M˜ (d,p) =\\n∑N(d)\\ni=1 Λ\\n(d,p)\\ni viv\\nT\\ni . Let w be an\\neigenvector corresponding to another eigenvalue. Then\\nM˜ (d,p)w =\\nN(d)∑\\ni=1\\nΛ\\n(d,p)\\ni vi(v\\nT\\ni w) = 0,\\nsince vi and w are orthogonal as they are associated with distinct eigenvalues.\\nNow we invoke Lemma 3.6 on the matrix M˜ (d,p) and obtain a setR ⊆ [2d] and a set of orthogonal\\nvectors {Ri}i∈R. We shall encode |R| random bits in A and show how to recover them.\\nLet\\nx =\\n∑\\ni∈R\\nsi · Ri‖Ri‖2 .\\nBy Lemma 2.7, with probability 1 − exp(−Ω(d)), it holds that ‖x‖∞ ≤ 3\\n√\\nd. We condition on\\n‖x‖∞ ≤ 3\\n√\\nd in the rest the proof, since we can include the alternative case ‖x‖∞ > 3\\n√\\nd in the\\noverall failure probability.\\nNext we define a vector y ∈ R2d to be yi = (xi +∆(d))1/p, where ∆(d) = 5\\n√\\nd is a constant that\\ndepends only on d. Clearly, it holds for all i ∈ [2d] that 2√d ≤ ypi ≤ 8\\n√\\nd. Round each entry of y\\nto its nearest integer multiple of δ = 1/(p(8\\n√\\nd)1−1/p2d), obtaining y˜. A simple calculation using\\nthe mean-value theorem shows that for all i ∈ [2d],\\n|y˜pi − (xi +∆(d))| = |y˜pi − ypi | ≤ p(8\\n√\\nd)\\np−1\\np δ ≤ 2−d. (9)\\nFinally we construct the matrix A ∈ R2d×d to be used in the ℓp subspace sketch problem. The\\nj-th row of A is the j-th vector of {−1, 1}d, scaled by y˜j.\\nLemma 3.10. The matrix A constructed above for the ℓp subspace sketch problem satisfies κ(A) ≤\\nC for some constant C that depends on p only.\\nProof. Let B be the 2d × d matrix whose rows are all vectors in {−1, 1}d. Then,\\n‖Bx‖22 = 2d E\\n∣∣∣∣∣\\nd∑\\ni=1\\nsixi\\n∣∣∣∣∣\\n2\\n,\\nwhere s1, . . . , sd is a Rademacher sequence. It follows from Khintchine’s inequality that\\nC12\\nd/2‖x‖2 ≤ ‖Bx‖2 ≤ C22d/2‖x‖2 (10)\\nfor some constants C1, C2. Notice that the rows of A are rescaled rows of B with the scaling factors\\nin [(2\\n√\\nd)1/p, (8\\n√\\nd)1/p]. Hence κ(A) ≤ C for some constant C that depends on p only.\\nThe recovery algorithm is simple. The vector to be used for querying the data structure is the\\ni-th vector on the Boolean cube {−1, 1}d, where i ∈ R. Given Qp(i), we guess the sign of si to be\\njust the sign of Qp(i)− 〈M (d,p)i ,∆(d) · 1〉. Next we prove the correctness of the recovery algorithm.\\nThe guarantee of the subspace sketch problem states that, with probability at least 0.9, it holds\\nsimultaneously for all i ∈ {−1, 1}d that\\n‖Ai‖pp ≤ Qp(i) ≤ (1 + ε)‖Ai‖pp. (11)\\n20\\nWe condition on this event in the remaining part of the analysis.\\nFirst we notice that for any i ∈ {−1, 1}d,\\n‖Ai‖pp =\\n2d∑\\nj=1\\n|〈Aj , i〉|p =\\n∑\\nj∈{−1,1}d\\n|〈i, y˜j · j〉|p =\\n∑\\nj∈{−1,1}d\\ny˜pj |〈i, j〉|p. (12)\\nNext we give an upper bound on the value of ‖Ai‖pp, for all i ∈ {−1, 1}d.\\nLemma 3.11. For each i ∈ {−1, 1}d, the matrix A constructed for the ℓp subspace sketch problem\\nsatisfies\\n‖Ai‖pp ≤ 2d · (8d1.5)p.\\nProof. Each term in the summation (12) is upper bounded by (8d1.5)p, which implies the stated\\nlemma.\\nCombining the preceding lemma with the query guarantee (11), the preceding lemma implies\\nthat, it holds for all i ∈ R2d that\\n|Qp(i) − ‖Ai‖pp| ≤ ε · 2d · (8d1.5)p.\\nOn the other hand, by (9) and (12),∣∣∣‖Ai‖pp − 〈M (d,p)i , (x+ 1 ·∆(d))〉∣∣∣ ≤ ∑\\nj∈{−1,1}d\\n|〈i, j〉|p · |y˜pi − (xi +∆(d))| ≤ dp.\\nThus by the triangle inequality,∣∣∣(Qp(i) − 〈M (d,p)i ,∆(d) · 1〉)− (〈M (d,p)i , x〉)∣∣∣ ≤ ε · 2d · (8d1.5)p + dp.\\nNotice that x is a linear combination of rows of M˜ (d,p). By Lemma 3.9,\\nM (d,p)x = M˜ (d,p)x.\\nBy Lemma 3.7, if\\nε · 2d · (8d1.5)p + dp ≤ 0.1Λ(d,p)0\\n√\\nN (d)/2d/2, (13)\\nthen with probability 4/5, (Qp(i) − 〈M (d,p)i ,∆(d) · 1〉) has the same sign as\\n(\\nM˜ (d,p)x\\n)\\ni\\n, in which\\ncase we recover the correct sign. By Lemma 3.8, the size of Qp is lower bounded by Ω(|R|).\\nNow for each ε > 0 and p ∈ (0,∞) \\\\ 2Z, by Lemma 3.2, (13) can be satisfied by setting\\n2d/2 ≥ sin(pπ/2)\\nε · polylog(1/ε) ,\\nwhich implies a space complexity lower bound of\\nΩ(|R|) = Ω(N (d)) = Ω(2d/\\n√\\nd) = Ω\\n(\\n1\\nε2 · polylog(1/ε)\\n)\\nbits.\\nFormally, we have proved the following theorem.\\n21\\nTheorem 3.12. Let p ∈ (0,∞) \\\\ 2Z. There exist constants C ∈ (0, 1] and ε0 > 0 that depend\\nonly on p such that the following holds. Let d0 = 2 log2(C/(εpolylog(1/ε)). For any ε ∈ (0, ε0),\\nd ≥ d0 and n ≥ 2d0 , any data structure for the ℓp subspace sketch requires Ω\\n(\\n1\\nε2·polylog(1/ε)\\n)\\nbits.\\nThe lower bound holds even when κ(A) ≤ K for some constant K that only depends on p.\\nWe note that the polylog(1/ε) factors in the definition of d0 and the bit lower bound may not\\nhave the same exponent.\\nNext we strengthen the lower bound to Ω˜(d/ε2) bits.\\nCorollary 3.13. Under the assumptions of C, ε0, d in Theorem 3.12 and the assumption that n =\\nΩ\\n(\\nd\\nε2·polylog(1/ε)\\n)\\n, any data structure for the ℓp subspace sketch problem requires Ω\\n(\\nd\\nε2·polylog(1/ε)\\n)\\nbits. The polylog(1/ε) factors in the two Ω-notations may not have the same exponent.\\nProof. Let A′ ∈ Rn′×d′ be the hard instance matrix for Theorem 3.12, where d′ = 2 log2(C/(εpolylog(1/ε))\\nand n′ = 2d\\n′\\n. We construct a block diagonal matrix A with b = d/d′ blocks, each being an inde-\\npendent copy of A’, so that A has d columns. The number of rows in A is bn′ = Ω\\n(\\nd\\nε2 polylog(1/ε)\\n)\\n.\\nIn this case, the ℓp sketch problem on A\\n′ requires a data structure of Ω˜(b/ε2) = Ω\\n(\\nd\\nε2·polylog(1/ε)\\n)\\nbits, since we are now solving the INDEX problem with Ω˜(b · 1/ε2) random bits.\\nThe corollary above is also true for p = 0.\\nCorollary 3.14. Under the assumptions of C, ε0, d and n in Corollary 3.13, any data structure\\nfor the ℓ0 subspace sketch problem requires Ω\\n(\\nd\\nε2·polylog(1/ε)\\n)\\nbits.\\nProof. The matrix M (d,0) is defined as (M (d,0))i,j = 1{〈i,j〉6=0}. Note that each row of M\\n(d,0) has\\nthe same number of 1s; let Wd denote this number. Observe that Lemma 3.4 continues to hold\\nbecause we have by symmetry\\nn∑\\nk=1\\n(−1)k+1\\n(\\n2n\\nn+ k\\n)\\n=\\n1\\n2\\n(\\n2n\\nn\\n)\\n≥ c2\\n2n\\n√\\nn\\nfor some absolute constant c > 0. Let yj = xi +∆\\n(d), where xi and ∆\\n(d) are as defined before. In\\nthe construction of A, replicate y˜j times (rounded to an integer multiple of δ = 2\\n−d) the j-th vector\\nof {−1, 1}d. Our guess of the sign si is then the sign of δQ0(i)−∆(d)Wd. Similar to the procedure\\nabove, we have that\\nδ |Q0(i) − ‖Ai‖0| ≤ δε‖Ai‖0 ≤ ε · 8\\n√\\nd · 2d\\nand ∣∣∣δ‖Ai‖0 − 〈M (d,0)i , (x+ 1 ·∆(d))〉∣∣∣ ≤∑\\nj\\n|yˆj − xi −∆(d)|1{〈i,j〉6=0} ≤ δ2d = 1.\\nAnd therefore it suffices to have\\nε · 8\\n√\\nd · 2d + 1 ≤ 0.1Λ(d,p)0\\n√\\nN (d)/2d/2,\\nwhich holds when 2d/2 = 1/(ε/polylog(1/ε)) as before. Therefore the analogue of Theorem 3.12\\nholds and so does the analogue of Corollary 3.13.\\n22\\nRemark 3.15. The condition that p /∈ 2Z+ is necessary for the lower bound. When p ∈ 2Z+,\\nit is possible to achieve ε = 0 with O(dp log(nd)) words. Recall that a d-dimensional subspace of\\nℓp space can be isometrically embedded into ℓ\\nr\\np with r =\\n(d+p−1\\np\\n) − 1 [23]. In general the data\\nstructure does not necessarily correspond to a linear map and can be of any form. Indeed, there\\nis a much simpler data structure as follows, based on ideas in [35]. For each x ∈ Rd, let yx ∈ Rd\\nbe defined as (yx)i = ((Ax)i)\\np/2, then ‖yx‖22 = ‖Ax‖pp. Observe that each coordinate (yx)i is a\\npolynomial of dp/2 terms in x1, . . . , xd. Form an n × dp/2 matrix B, where the i-th row consists\\nof the coefficients in the polynomial corresponding to (yx)i. The data structure stores B\\nTB. To\\nanswer the query Qp(x), one first calculates from x a d\\np/2-dimensional vector x′ whose coordinates\\nare all possible monomials of total degree p/2. Note that Bx′ = yx. Hence one can just answer\\nQp(x) = (x\\n′)TBTBx′ = ‖Bx′‖22 = ‖Ax‖pp without error. This Qp does not give an isometric\\nembedding but is much simpler than known isometric embeddings, and the space complexity is\\nO(dp log(nd)) bits.\\n4 Lower Bounds for p > 2\\n4.1 Lower Bounds for the Subspace Sketch Problem for p > 2\\nIn this section, we prove a lower bound on the ℓp subspace sketch problem, in the case that ε is a\\nconstant and p ≥ 2. We need the following result from coding theory.\\nLemma 4.1 ([32]). For any p ≥ 1 and d = 2k−1 for some integer k, there exist a set S ⊂ {−1, 1}d\\nand a constant Cp depending only on p which satisfy\\n(i) |S| = dp;\\n(ii) For any s, t ∈ S such that s 6= t, |〈s, t〉| ≤ Cp\\n√\\nd.\\nLemma 4.2. For any p ≥ 1, C ≥ 1 and d = 2k − 1 for an integer k, there exist a set S ⊂ {−1, 1}d\\nwith size |S| = dp, a set M ⊂ RR×d for some R and a constant Cp depending only on p which\\nsatisfy\\n(i) For any M1,M2 ∈ M such that M1 6= M2, there exists x ∈ S such that ‖Mx‖p < d/C and\\n‖Mx‖p ≥ d.\\n(ii) |M| ≥ exp (dp/2/(CpCp)).\\nProof. Set R = dp/2/(CpC\\np). Then R ≤ dp/e. We set M to be the set of R × d matrices whose\\nrows are all possible combinations of R distinct vectors in S, where S is the set constructed in\\nLemma 4.1. Clearly, |M| = (dpR) ≥ eR. Furthermore, consider two different M1,M2 ∈ M . There\\nexists an x ∈ S which is a row of M1 but not a row of M2. Thus, ‖M1x‖p ≥ d and\\n‖M2x‖p ≤ Cp\\n√\\nd · R1/p < d/C.\\nTheorem 4.3. Solving the ℓp subspace sketch problem requires Ω˜(d\\np/2) bits when 0 < ε < 1 and\\np ≥ 2 are constants and n = Ω(dp/2).\\nProof. We first prove a lower bound for randomized data structures for the ℓp subspace sketch\\nproblem with failure probability d−p/100. Let M ⊂ RR×d and S ⊂ {−1, 1}d be as constructed in\\n23\\nLemma 4.2. Choose a matrix M from M uniformly at random. Since for each x ∈ {−1, 1}d, with\\nprobability at least 1− d−p/100,\\n‖Mx‖pp ≤ Qp(x) ≤ (1 +O(ε))‖Mx‖pp, (14)\\nby a union bound, with probability at least 0.99, (14) holds simultaneously for all x ∈ S. It follows\\nfrom Lemma 4.2(i) that by querying ‖Mx‖p for all x ∈ S, one can distinguish all different M ∈ M.\\nA standard information-theoretic argument leads to a lower bound of Ω(log |M|) = Ω(dp/2).\\nFor randomized data structures for the ℓp subspace sketch problem with constant failure proba-\\nbility, a standard repetition argument implies that the failure probability can be reduced to d−p/100\\nusing O(log d) independent repetitions. Therefore a lower bound of Ω˜(dp/2) bits follows.\\nRemark 4.4. The lower bound in Theorem 4.3 is nearly optimal. To obtain an ℓp subspace sketch\\nwith constant ε and O˜(dp/2) bits, one can first apply Lewis weights sampling [13] to reduce the size\\nof A to O˜(dp/2)× d, and then apply the embedding in [15] to further reduce the number of rows of\\nA to O˜(d(p/2)·(1−2/p)) = O˜(dp/2−1). Therefore the data structure takes O˜(dp/2) bits to store.\\n4.2 Lower Bounds for the For-All Version\\nIn this section, we prove a lower bound on the for-all version of the ℓp subspace sketch problem\\nfor the case of p ≥ 2 and constant ε. In the for-all version of the ℓp subspace sketch problem,\\nthe data structure Qp is required to, with probability at least 0.9, satisfy Qp(x) = (1 ± ε)‖Ax‖p\\nsimultaneously for all x ∈ Rd.\\n4.2.1 Lower Bound for p ≥ 2\\nThroughout this section we assume that p ≥ 2 is a constant.\\nLet N = cpd\\np/2 in this section, where cp > 0 is a constant that depends only on p. Denote the\\nunit ball in ℓnp by B\\nn\\np . For each x ∈ Bn2 , we define a function fx : RN×d → R by\\nfx(A) = ‖Ax‖p.\\nLemma 4.5. The function fx(·) satisfies the following properties:\\n(i) E[fx(A)] ≤ Cc1/pp √p\\n√\\nd, where entries of A are i.i.d. Rademacher random variables and C is\\na constant that depends only on cp;\\n(ii) fx(·) is 1-Lipschitz with respect to the Frobenius norm;\\n(iii) fx(·) is a convex function.\\nProof. By Khintchine’s inequality, (E |(Ax)i|p)1/p ≤ C√p‖x‖2 = C√p, where C is an absolute con-\\nstant. It follows that E ‖Ax‖pp ≤ N(C√p)p and by Jensen’s inequality, E ‖Ax‖p ≤ (E ‖Ax‖pp)1/p ≤\\nN1/pC\\n√\\np = Cc\\n1/p\\np\\n√\\np\\n√\\nd, which implies (i). To prove (ii), note that\\nfx(A−B) = ‖Ax−Bx‖p ≤ ‖Ax−Bx‖2 ≤ ‖A−B‖2 ≤ ‖A−B‖F .\\n(iii) is a simple consequence of the convexity of the ℓp norm.\\nThe following lemma is a direct application of Talagrand’s concentration inequality (Lemma 2.4)\\nwith Lemma 4.5.\\n24\\nLemma 4.6. Let A ∈ RN×d and x ∈ Rd have i.i.d. Rademacher random variables. It holds that\\nPr\\nA,x\\n{\\nfx(A) ≥ Cc1/pp\\n√\\npd\\n}\\n≤ e−cd,\\nwhere C is an absolute constant and cp is a constant depending only on p.\\nProof. Let xˆ = x/\\n√\\nd. We have ‖xˆ‖2 = 1. By Lemma 4.5 and Lemma 2.4, we have\\nPr\\nA\\n{\\nfxˆ(A) ≥ Cc1/pp\\n√\\np\\n√\\nd\\n}\\n≤ e−cd.\\nSince fx(A) =\\n√\\ndfxˆ(A), we have\\nPr\\nA\\n{\\nfx(A) ≥ Cc1/pp\\n√\\npd\\n}\\n≤ e−cd,\\nwhich implies the stated lemma.\\nLemma 4.7. There exists a multiset S ⊆ {+1,−1}N×d such that\\n(i) |S| ≥ exp(c1Nd);\\n(ii) For any S, T ∈ S such that S 6= T , there exists i ∈ [N ], such that ‖STi‖p ≤ Cc1/pp √pd;\\n(iii) When p > 2, for any S ∈ S, κ(S) ≤ 2.\\nProof. We first define a set of bad matrices Bad ⊆ {+1,−1}N×d to be\\nBad =\\n{\\nA ∈ {+1,−1}N×d : Pr\\nx\\n{\\n‖Ax‖p ≥ Cc1/pp\\n√\\npd\\n}\\n≥ 3e−cd\\n}\\n,\\nwhere x ∈ {+1,−1}d is an i.i.d. Rademacher vector and Cp, c are the same constants in Lemma 4.6.\\nIt follows from Lemma 4.6 that\\nPr\\nA\\n{A ∈ Bad} ≤ 1\\n3\\n,\\nsince otherwise\\nPr\\nA,x\\n{\\nfx(A) ≥ Cc1/pp\\n√\\npd\\n}\\n≥ Pr\\nA\\n{A ∈ Bad} · Pr\\nx\\n{\\nfx(A) ≥ Cc1/pp\\n√\\npd | A ∈ Bad\\n}\\n> e−cd.\\nLet the multiset T ⊆ {+1,−1}N×d of size |T | = exp(c2Nd) consist of independent uniform samples\\nof matrices in {+1,−1}N×d. We define three events as follows.\\n• E1: |T \\\\ Bad| ≥ |T |/3;\\n• E2: For each S ∈ T \\\\ Bad and each T ∈ T \\\\ {S}, there exists some i ∈ [N ] such that\\n‖STi‖p ≤ Cpd.\\n• E3: There are at least (5/6)|T | matrices T ∈ T such that κ(T ) ≤ 2.\\nWe analyze the probability of each event below.\\nFirst, notice that E |T ∩ Bad| ≤ |T |/3. Thus, by Markov’s inequality we have Pr(|T ∩ Bad| ≥\\n2|T |/3) ≤ 1/2, which implies Pr(Ec1) ≤ 1/2.\\n25\\nNext, consider a fixed matrix S ∈ {+1,−1}N×d \\\\ Bad. For a random matrix T ∈ {+1,−1}N×d\\nwhose entries are i.i.d. Rademacher random variables, for each row Ti of T , by definition of Bad,\\nwe have\\nPr\\n{\\n‖STi‖p ≥ Cc1/pp\\n√\\npd\\n}\\n≤ 3e−cd.\\nSince the rows of T are independent,\\nPr\\n{\\n‖STi‖p ≥ Cc1/pp\\n√\\npd, ∀i ∈ [N ]\\n}\\n≤ 3Ne−cNd ≤ e−c′Nd.\\nChoosing appropriate constants for C and c (and thus c′) allows for a union bound over all pairs\\nS ∈ T \\\\ Bad and T ∈ T \\\\ {S}, and we have Pr(Ec2) ≤ 1/3.\\nLast, for the condition number, recall the classical result that for a random matrix T of i.i.d.\\nRademacher entries, it holds with probability ≥ 1 − exp(−c3d) that smin(T ) ≥\\n√\\nN − c4\\n√\\nd and\\nsmin(T ) ≥\\n√\\nN + c4\\n√\\nd, which implies that κ(T ) ≤ (√N + c4\\n√\\nd)/(\\n√\\nN − c4\\n√\\nd) ≤ 2 when d is\\nsufficiently large. Letting T1 = {T ∈ |T | : κ(T ) > 2}, we have E |T1| ≤ e−c3d|T |. Thus by a Markov\\nbound, Pr{|T1| ≥ 6e−c3d|T |} ≤ 1/10, and thus Pr(Ec3) ≤ 1/10.\\nSince Pr(Ec1) +Pr(Ec2) + Pr(Ec3) < 1, there exists a set T for which all E1, E2, E3 hold. Taking S\\nto be the well-conditioned matrices in T \\\\ Bad, we see that S satisfies conditions (i)–(iii).\\nTheorem 4.8. The for-all version of the ℓp subspace sketch problem requires Ω((d/p)\\np/2 · d) bits\\nto solve when p ≥ 2 and ε < 1 are constants and n = Ω(dp/2). The lower bound holds even when\\nκ(A) ≤ 2 if p > 2, and all entries in A are in {+1,−1}.\\nProof. Choose a matrix A uniformly at random from the set S in Lemma 4.7. Suppose that\\nQp : R\\nd → R satisfies\\n‖Ax‖pp ≤ Qp(x) ≤ (1 +O(ε))‖Ax‖pp, x ∈ Rd.\\nFor any row Ai ∈ Rd of A, interpreted as a column vector, ‖AAi‖p ≥ d, whereas for any B ∈ S\\\\{A},\\nthere exists a row Ai of A such that ‖BAi‖p ≤ Cc1/pp √pd < d/3, provided that cp (and thus C)\\nis small enough. Thus, by appropriate choice of the constants in Lemma 4.7, we can use Qp to\\ndetermine which matrix A ∈ S has been chosen. By Property (ii) of the set S, it must hold\\nthat all elements of S are distinct from each other. It then follows from a standard information-\\ntheoretic argument that the size of the data structure for the ℓp sketch problem is lower bounded\\nby Ω(log |S|) = Ω(Nd) = Ω((d/p)p/2 · d).\\n4.2.2 Lower Bound for 1 ≤ p ≤ 2\\nThe lower bound for 1 ≤ p < 2 follows from the lower bound for p = 2 by embedding ℓp into ℓ2. It\\nis known that ℓn2 K-embeds into ℓ\\nm\\np for some m ≤ cn, where c = c(p) and K = K(p) are constants\\nthat depend only on p. Furthermore, the embedding T : ℓn2 → ℓmp can be realized using a rescaled\\nmatrix of i.i.d. Rademacher entries (with high probability). See [27, Section 2.5] for a proof for\\np = 1, which can be generalized easily to a general p. Thus, one can reduce the for-all version of\\nthe ℓ2 subspace sketch problem to the for-all version of the ℓp subspace sketch with 1 ≤ p ≤ 2.\\nThus the lower bound of Ω(d2) also holds when 1 ≤ p ≤ 2.\\n26\\n5 Linear Embeddings\\nIn this section, our goal is to show that isomorphic embeddings into low-dimensional spaces induce\\nsolutions to the subspace sketch problem. Therefore a lower bound on the subspace sketch problem\\nimplies a lower bound on the embedding dimension.\\nTheorem 5.1. Let p, q ≥ 1, ε > 0 and A ∈ RN×d with full column rank. Let E ⊆ ℓNp be the column\\nspace of A and suppose that T : E → ℓnq is a (1 + ε)-isomorphic embedding. Then there exists a\\ndata structure for the for-all version of the ℓp subspace sketch problem on A with approximation\\nratio 1± 6pε and O(nd log(N |1/p−1/2|dnκ(A)/ε)) bits.\\nProof. Without loss of generality, we may assume that 1κ(A)‖x‖2 ≤ ‖Ax‖2 ≤ ‖x‖2. Let B ∈ Rn×d\\nbe such that B = TA. Then ‖Ax‖pp ≤ ‖Bx‖pq ≤ (1 + ε)p‖Ax‖pp. Round each entry of B to an\\ninteger multiple of δ = ε/(D1n\\n1/qd1/2κ(A)), where D1 = max{1, N1/2−1/p}, obtaining B˜. First we\\nclaim that the rounding causes only a minor loss,\\n(1− ε)p‖Bx‖pq ≤ ‖B˜x‖pq ≤ (1 + ε)p‖Bx‖pq . (15)\\nIndeed, write B = B˜ +∆B, where each entry of ∆B is bounded by δ. Then\\n‖(∆B)x‖q ≤ n1/qd1/2δ‖x‖2 ≤ n1/qd1/2δ‖x‖2 ≤ n1/qd1/2δ · κ(A) · ‖Ax‖2 ≤ ε‖Ax‖p ≤ ε‖Bx‖q.\\nThis proves (15) and so\\n(1− ε)p‖Ax‖pp ≤ ‖B˜x‖pq ≤ (1 + ε)2p‖Ax‖pp,\\nwhich implies that the matrix B˜ can be used to solve the ℓp subspace sketch problem on A with\\napproximation ratio 1± 6pε. Since\\n‖Bx‖q ≤ (1 + ε)p‖Ax‖p ≤ (1 + ε)pD2‖Ax‖2 ≤ (1 + ε)pD2‖x‖2,\\nwhere D2 = max{1, N1/p−1/2}, each entry of B is at most eD2. Hence, after rounding, each entry\\nof B˜ can be described in O(log(D2/δ)) = O(log(dnDκ(A)/ε)) bits, where D = D1D2 = N\\n|1/2−1/p|.\\nThe matrix B˜ can be described in O(nd log(D2/δ)) bits. The value of δ can be described in\\nO(log(1/δ)) bits, which is dominated by the complexity for describing B˜. Therefore the size of the\\ndata structure is at most O(nd log(D2/δ)) = O(nd log(Ddnκ(A)/ε)) bits.\\nThe dimension lower bound for linear embeddings now follows as a corollary from combining the\\npreceding theorem with Theorem 3.12, where we choose d = C log(1/ε) and note that N = O(1/ε2)\\nand κ(A) = O(1) in our hard instance.\\nCorollary 5.2. Let p ∈ [1,∞) \\\\ 2Z and suppose that d ≥ C log(1/ε). It holds that\\nNp(d, ε) ≥ cp · 1/(ε2 · polylog(1/ε)),\\nwhere cp > 0 is a constant that depends only on p and C > 0 is an absolute constant.\\n27\\nRemark 5.3. It is not clear how much the assumption d ≥ C log 1ε can be weakened. The best\\nknown results for p = 1 are as follows [23].\\nN1(d, ε) ≤\\n\\uf8f1\\uf8f4\\uf8f2\\uf8f4\\uf8f3\\nc2ε\\n−1/2, d = 2;\\nc(d)\\n(\\n1\\nε2\\nlog 1ε\\n)(d−1)/(d+2)\\n, d = 3, 4;\\nc(d)\\n(\\n1\\nε2\\n)(d−1)/(d+2)\\n, d ≥ 5,\\nwhich is substantially better than 1/(ε2 polylog(1/ε)) for constant d. In a similar lower bound [7]\\nN1(d, ε) ≥ c(d)ε−2(d−1)/(d+2) ,\\nthe constant c(d) ≈ e−c′d lnd, so the lower bound is nontrivial for d up to O (log 1ε/ log log 1ε). Since\\nN1(d, ε) is increasing, optimizing d w.r.t. ε yields thatN1(d, ε) = Ω\\n(\\nε−2 exp(−c′′\\n√\\nln(1/ε) ln ln(1/ε))\\n)\\nfor all d = Ω\\n(√\\nln(1/ε)\\n)\\n. Our result improves the lower bound to ε−2/polylog(1/ε) for larger d\\nand, more importantly, works for general p ≥ 1 that is not an even integer.\\nRemark 5.4. In the case of p > 2, it is an immediate corollary from Theorem 4.8 that Np(d, ε) =\\nΩ(dp/2/ log d), which recovers the known (and nearly tight) lower bound up to a logarithmic factor.\\n6 Sampling-based Embeddings\\nOur goal in this section is to prove the following lower bound.\\nTheorem 6.1. Let p ≥ 1 and p /∈ 2Z. Suppose that Qp(x) = ‖TAx‖pp solves the for-all version of\\nthe ℓp subspace sketch problem on A and ε for some T ∈ Rm×n such that each row of T contains\\nexactly one non-zero element. Then it must hold that m ≥ cpd/(ε2 polylog(1/ε)), provided that\\nd ≥ C log(1/ε), where cp > 0 is a constant that depends only on p and C > 0 is an absolute\\nconstant.\\nProof. Let A be the hard instance matrix for the Ω˜(1/ε2) lower bound in Corollary 5.2. Recall\\nthat A is a diagonal matrix with k = Θ(d/ log(1/ε)) diagonal blocks. Each block has dimension\\n2s × s with 2s/2 = 1/ε1−o(1). Furthermore, each block can be written as DB, where D is a 2s × 2s\\ndiagonal matrix and B is the 2s × s matrix whose rows are all vectors in {−1, 1}s, and each entry\\nin D has magnitude in [(2\\n√\\ns)1/p, (8\\n√\\ns)1/p]. The matrix B can be described using O(s2s) bits and\\nthe matrix D using O(2s log s) bits. Without loss of generality we may assume that each nonzero\\nentry of T is an integer multiple of ε1/p, since the loss of rounding, by the triangle inequality, is at\\nmost ε‖Ax‖pp. Next, we shall bound the number of bits needed to describe T .\\nLetting x = ej be a canonical basis vector,\\n(1 + ε)‖Ax‖pp ≥ ‖TAx‖pp =\\nm∑\\ni=1\\n|tiAi,j|p ≥ 2\\n√\\ns\\nm∑\\ni=1\\n|ti|p.\\nOn the other hand,\\n‖Ax‖pp ≤ k · 8\\n√\\ns‖Bx‖pp ≤ 8k\\n√\\ns(2s/2‖Bx‖2)p ≤ C ′k\\n√\\ns2sp,\\n28\\nwhere we used (10) for the last inequality. It follows from the AM-GM inequality that\\n∑\\ni\\nlog\\n|ti|\\nε1/p\\n= log\\n∏\\ni\\n|ti|\\nε1/p\\n≤ m\\np\\nlog\\n(\\n1\\nm\\n∑\\ni\\n|ti|p\\nε\\n)\\n≤ C ′′m\\n(\\ns+ log\\n1\\nε\\n+ log\\nk\\nm\\n)\\n≤ C ′′′ms,\\nthat is, T can be described using O(ms) bits, provided that m ≥ k. Therefore TA can be described\\nin O(s2s + 2s log s + ms) = O((m + 1/ε2) log(1/ε)) bits. Combining with the lower bound of\\nΩ(d/(ε2 poly log(1/ε)) bits, we see that m = Ω(d/(ε2 polylog(1/ε))).\\nA similar argument shows that when m < k, the matrix T can be described in O(d) bits, which\\nleads to a contradiction to the lower bound. Hence it must hold that m ≥ k and, as we proved\\nabove, m = Ω(d/(ε2 polylog(1/ε))).\\nThe lower bound for the (for-each) ℓp subspace sketch problem loses further a factor of log d.\\nCorollary 6.2. Let p ≥ 1 and p /∈ 2Z. Suppose that Qp(x) = ‖TAx‖pp solves the (for-each) version\\nof the ℓp subspace sketch problem on A and ε for some T ∈ Rm×n such that each row of T contains\\nexactly one non-zero element. Then it must hold that m ≥ cpd/(ε2 · log d · polylog(1/ε)), provided\\nthat d ≥ C log(1/ε), where cp > 0 is a constant that depends only on p and C > 0 is an absolute\\nconstant.\\nProof. Observe that we used the approximation to ‖Aei‖pp for each canonical basis vector ei in the\\nproof of Theorem 6.1, which holds with a constant probability if we make O(log d) independent\\ncopies of the (randomized) data structure. This incurs a further loss of a log d factor in the lower\\nbound.\\n7 Oblivious Sketches\\nAn oblivious subspace embedding for d-dimensional subspaces E in ℓnp is a distribution on linear\\nmaps T : ℓnp → ℓmp such that it holds for any d-dimensional subspace E ⊆ ℓnp that\\nPr\\nT\\n{(1− ε)‖x‖p ≤ ‖Tx‖p ≤ (1 + ε)‖x‖p, ∀x ∈ E} ≥ 0.99.\\nMore generally, an oblivious sketch is a distribution on linear maps T : ℓnp → Rm, accompanied\\nby a recovery algorithm A, such that it holds for any d-dimensional subspace E ⊆ ℓnp that\\nPr\\nT\\n{(1− ε)‖x‖p ≤ A(Tx) ≤ (1 + ε)‖x‖p, ∀x ∈ E} ≥ 0.99.\\nIt is clear that an oblivious embedding is a special case of an oblivious sketch, whereA(Tx) = ‖Tx‖p.\\nIn this section we shall show that when 1 ≤ p < 2, any oblivious sketch requires m = Ω˜(d/ε2).\\nBefore proving the lower bound, let us prepare some concentration results. We use Sn−1 to\\ndenote the unit sphere in (Rn, ‖ · ‖2). First, observe that the norm function x 7→ ‖x‖p is a Lips-\\nchitz function of Lipschitz constant max{1, n1/p−1/2}. Also note that (Eg∼N(0,In) ‖g‖pp)1/p = βpn1/p,\\nwhere βp = (Eg∼N(0,1) |g|p)1/p. Standard Gaussian concentration (Lemma 2.5) leads to the follow-\\ning:\\nLemma 7.1. Let p ≥ 1 be a constant and g ∼ N(0, In). It holds with probability at least 1 −\\nexp(−cε2nmin{1,2/p}) that (1 − ε)βpn1/p ≤ ‖g‖p ≤ (1 + ε)βpn1/p, where c = c(p) > 0 is a constant\\nthat depends only on p.\\n29\\nSuppose that G is an n × d Gaussian random matrix of i.i.d. N(0, 1) entries. Observe that for\\na fixed x ∈ Sd−1, Gx ∼ N(0, In). A typical ε-net argument on Sd−1 allows us to conclude the\\nfollowing lemma. We remark that this gives Dvoretzky’s Theorem for ℓp spaces.\\nLemma 7.2. Let 1 ≤ p < 2 be a constant and G be an n × d Gaussian random matrix. There\\nexist constants C = C(p) > 0 and c = c(p) > 0 such that whenever n ≥ Cd log(1/ε)/ε2, it holds\\nPr\\n{\\n(1− ε)βpn1/p ≤ ‖Gx‖p ≤ (1 + ε)βpn1/p, ∀x ∈ Sd−1\\n} ≥ 1− 2 exp(−cε2n).\\nNow, consider two distributions on n × d matrices, where n = Θ(dε−2 log(1/ε)). The first\\ndistribution L1 is just the distribution of a Gaussian random matrix G of i.i.d. N(0, 1) entries, and\\nthe second distribution L2 is the distribution of G+σuvT , where G is the Gaussian random matrix\\nof i.i.d. N(0, 1) entries, u ∼ N(0, In) and v ∼ N(0, Id) and σ = α\\n√\\nε/d for some constant α to be\\ndetermined later, and G, u and v are independent.\\nTheorem 7.3. Let 1 ≤ p < 2 be a constant. Suppose that S ∈ Rm×n is an oblivious sketch for\\nd-dimensional subspaces in ℓnp , where n = Θ(dε\\n−2 log(1/ε)). It must hold that m ≥ cd/ε2, where\\nc = c(p) > 0 is a constant depending only on p.\\nProof. It follows from the preceding lemma that, ifA ∼ L1, we have supx∈Sd−1 ‖Ax‖p ≤ (1+ε)βpn1/p\\nwith probability at least 0.999 with an appropriate choice of constant in the Θ-notation of n. Next\\nwe consider the supremum of ‖Ax‖p when A ∼ L2. Observe that\\nsup\\nx∈Sd−1\\n∥∥(G+ σuvT )x∥∥\\np\\n≥\\n∥∥∥∥(G+ σuvT ) v‖v‖2\\n∥∥∥∥\\np\\n=\\n∥∥∥∥G v‖v‖2 + σu‖v‖2\\n∥∥∥∥\\np\\n.\\nSince v ∼ N(0, Id), the direction v/‖v‖2 ∼ Unif(Sd−1) and the magnitude ‖v‖2 are independent,\\nand by rotational invariance of the Gaussian distribution, Gx ∼ N(0, Id) for any x ∈ Sd−1. Hence∥∥∥∥G v‖v‖2 + σu‖v‖2\\n∥∥∥∥\\np\\ndist\\n== ‖u1 + σtu2‖p dist==\\n√\\n1 + σ2t2‖u‖p,\\nwhere t follows the distribution of ‖v‖2 and u1, u2 are independent N(0, In) vectors. Applying\\nthe preceding two lemmata, we see that with probability at least 0.998, it holds that t ≥ 0.99√d\\nand ‖u‖p ≥ (1 − ε)βpn1/p. Therefore, when A ∼ L2, with probability at least 0.998, we have\\nsupx∈Sd−1 ‖Ax‖p ≥\\n√\\n1 + 0.992α2ε(1− ε)βpn1/p ≥ (1 + 4ε)βpn1/p, for an appropriate choice of α.\\nTherefore with the corresponding recovery algorithm A,\\nPr\\nA∼L1,S\\n{\\nsup\\nx∈Sn−1\\nA(SAx) ≤ (1 + ε)2βpn1/p\\n}\\n≥ 0.9,\\nPr\\nA∼L2,S\\n{\\nsup\\nx∈Sn−1\\nA(SAx) ≥ (1 + 4ε)(1 − ε)βpn1/p\\n}\\n≥ 0.9,\\nwhich implies that the linear sketch S can be used to distinguish L1 from L2 by evaluating\\nsupx∈Sd−1 A(SAx). It then follows from [26, Theorem 4] that the size of the sketch md ≥ c/σ4 =\\nc′d2/ε2 for some absolute constants c, c′ > 0, and thus m ≥ c′d/ε2.\\n30\\n8 Lower Bounds for M-estimators\\nThe main theorem of this section is the following.\\nTheorem 8.1. Suppose there exist α, λ > 0 and p ∈ (0,∞)\\\\2Z such that φ(t/λ) ∼ α|t|p as t→∞\\nor t→ 0. Then the subspace sketch problem for Φ(x) =∑ni=1 φ(xi) requires Ω(d/(ε2 polylog(1/ε)))\\nbits when d ≥ C1 log(1/ε) and n ≥ C2d/(ε2 polylog(1/ε)) for some absolute constants C1, C2 > 0.\\nProof. We reduce the problem to the ℓp subspace sketch problem. We prove the statement in the\\ncase of t→∞ below. The proof for the case of t→ 0 is similar.\\nFor a given ε > 0, there exists M such that (1− ε)α|t|p ≤ φ(t/λ) ≤ (1 + ε)α|t|p for all |t| ≥M .\\nLet A be our hard instance for the ℓp subspace sketch problem in Theorem 3.12. Then each row of\\nA is a {−1, 1}-vector scaled by a factor of y˜i ≥ ∆ for some ∆ = Ω\\n(\\nlog1/(2p)(1/ε)\\n)\\n. One can recover\\na random sign used in the construction of A by querying Ax for a {−1, 1}-vector x. Therefore, if\\n(Ax)i 6= 0, it must hold that |(Ax)i| ≥ ∆. This implies that there exists a scaling factor β =M/∆\\nsuch that (1−ε)α‖βAx‖pp ≤ Φ(λ−1βAx) ≤ (1+ε)α‖βAx‖pp , that is, α−1β−pΦ(λ−1βAx) is a (1±ε)-\\napproximation to ‖Ax‖pp for {−1, 1}-vectors x. The conclusion follows from Theorem 3.13 (which\\nplants independent copies of hard instance A in diagonal blocks) and a rescaling of ε.\\nWe have the following immediate corollary.\\nCorollary 8.2. The subspace sketch problem for Φ requires Ω(d/(ε2 polylog(1/ε))) bits when d ≥\\nC1 log(1/ε) and n ≥ C2d/(ε2 polylog(1/ε)) for some absolute constants C1, C2 > 0 for the following\\nfunctions φ:\\n• (L1-L2 estimator) φ(t) = 2(\\n√\\n1 + t2/2− 1);\\n• (Huber estimator) φ(t) = t2/(2τ) · 1{|t|≤τ} + (|t| − τ/2) · 1{|t|>τ};\\n• (Fair estimator) φ(t) = τ2(|x|/τ − ln(1 + |t|/τ));\\n• (Tukey loss p-norm) φ(t) = |t|p · 1{|t|≤τ} + τp · 1{|t|>τ}.\\nNow we prove the Ω(d/(ε2 polylog(1/ε))) lower bound for the subspace sketch problem for the\\nCauchy estimator φ(t) = (τ2/2) ln(1 + (t/τ)2). First consider an auxiliary function φaux(t) =\\nln |x| · 1{|x|≥1}, for which we shall have also an Ω(d/(ε2 polylog(1/ε)) lower bound by following the\\napproach in Section 3 with some changes we highlight below. Instead of M\\n(d,p)\\ni,j = |〈i, j〉|p, we shall\\ndefine M\\n(d,p)\\ni,j = φaux(〈i, j〉), and we proceed to define N (d) and Λ(d,p)0 in the same manner. The\\nfollowing lemma is similar to Corollary 3.4, showing that this new matrix M (d,p) also has large\\nsingular values. The proof is postponed to Section 8.1.\\nLemma 8.3. Suppose that d ∈ 8Z. Then Λ(d,p)0 ≥ c2d/2/\\n√\\nd for some absolute constant c > 0.\\nTherefore, the entire lower bound argument in Corollary 3.14 goes through. We can then con-\\nclude that the subspace sketch problem for Φaux(x) =\\n∑n\\ni=1 φaux(xi) requires Ω(d/(ε\\n2 polylog(1/ε))\\nbits. Now, for the Cauchy estimator φ(t) = (τ2/2) ln(1 + (t/τ)2), note that (1 − ε)τ2φaux(t) ≤\\nφ(τ · t) ≤ (1+ ε)τ2φaux(t) for all sufficiently large t. It follows from a similar argument to the proof\\nof Theorem 8.1 that the same lower bound continues to hold for the subspace sketch problem for\\nthe Cauchy estimator.\\nCorollary 8.4. The subspace sketch problem for Φ requires Ω(d/(ε2 polylog(1/ε))) bits for the\\nCauchy estimator φ(t) = (τ2/2) ln(1+(t/τ)2), when d ≥ C1 log(1/ε) and n ≥ C2d/(ε2 polylog(1/ε))\\nfor some absolute constants C1, C2 > 0.\\n31\\n8.1 Proof of Lemma 8.3\\nDifferentiate both sides of (6) w.r.t p,\\n−\\n∫ ∞\\n0\\n(2 sin t)2n ln t\\ntp+1\\ndt = 2\\nn∑\\nk=1\\n(−1)k\\n(\\n2n\\nn+ k\\n)\\n(2k)p ln(2k) cos\\n(πp\\n2\\n)\\nΓ(−p)\\n− 2\\nn∑\\nk=1\\n(−1)k\\n(\\n2n\\nn+ k\\n)\\n(2k)p\\nπ\\n2\\nsin\\n(πp\\n2\\n)\\nΓ(−p)\\n− 2\\nn∑\\nk=1\\n(−1)k\\n(\\n2n\\nn+ k\\n)\\n(2k)p cos\\n(πp\\n2\\n)\\nΓ′(−p).\\nThus using the reflection identity (7),\\nΓ(p+ 1)\\nπ\\nsin\\n(πp\\n2\\n)∫ ∞\\n0\\n(2 sin t)2n ln t\\ntp+1\\ndt =\\nn∑\\nk=1\\n(−1)k\\n(\\n2n\\nn+ k\\n)\\n(2k)p ln(2k)\\n− 2\\nn∑\\nk=1\\n(−1)k\\n(\\n2n\\nn+ k\\n)\\n(2k)p\\nπ\\n2\\nsin2\\n(πp\\n2\\n)\\nsin(pπ)\\n−\\nn∑\\nk=1\\n(−1)k\\n(\\n2n\\nn+ k\\n)\\n(2k)p\\nΓ′(−p)\\nΓ(−p) .\\nLetting p→ 0+, we see that the middle term on the right-hand side vanishes, which implies\\n22n\\nπ\\nlim\\np→0+\\nsin\\n(πp\\n2\\n)∫ ∞\\n0\\n(sin t)2n ln t\\ntp+1\\ndt =\\nn∑\\nk=1\\n(−1)k\\n(\\n2n\\nn+ k\\n)\\nln k\\n− lim\\np→0+\\nn∑\\nk=1\\n(−1)k\\n(\\n2n\\nn+ k\\n)(\\nΓ′(−p)\\nΓ(−p) − ln 2\\n)\\n.\\nInvoking Lemma 3.3, we obtain that\\nn∑\\nk=1\\n(−1)k\\n(\\n2n\\nn+ k\\n)\\nln k =\\n22n\\nπ\\nlim\\np→0+\\nsin\\n(πp\\n2\\n) ∫ ∞\\n0\\n(\\n(sin2n t) ln t\\ntp+1\\n−\\n(\\nΓ′(−p)\\nΓ(−p) − ln 2\\n)\\nsin2n t\\ntp+1\\n)\\ndt.\\n(16)\\nWe claim that the limit on the right-hand side at least c/\\n√\\nn for some absolute constant c > 0.\\nNote that letting p→ 0+ in Lemma 3.3 leads to\\nn∑\\nk=1\\n(−1)k+1\\n(\\n2n\\nn+ k\\n)\\n=\\n22n\\nπ\\nlim\\np→0+\\nsin\\n(πp\\n2\\n)∫ π\\n0\\nsin2n t\\ntp+1\\ndt,\\nand the left-hand side is\\nn∑\\nk=1\\n(−1)k+1\\n(\\n2n\\nn+ k\\n)\\n=\\n1\\n2\\n(\\n2n\\nn\\n)\\n∼ 2\\n3\\n· 2\\n2n\\n√\\nn\\n,\\n32\\nthus\\nlim\\np→0+\\np\\n∫ π\\n0\\nsin2n t\\ntp+1\\ndt ∼ 4\\n3\\n√\\nn\\n.\\nNote the fact that Γ′(x)/Γ(x) = −1/x− γ + o(1) as x→ 0+ (e.g., plugging n = 1 into Eq. (1.2.15)\\nin [4, p13]), where γ = 0.577 · · · is the Euler gamma constant. Thus the limit on the right-hand\\nside of (16) is the same as\\nlim\\np→0+\\nsin\\n(πp\\n2\\n) ∫ ∞\\n0\\n(\\n(sin2n t) ln t\\ntp+1\\n−\\n(\\n1\\np\\n− γ − ln 2\\n)\\nsin2n t\\ntp+1\\n)\\ndt.\\nand it suffices to show that\\nlim\\np→0+\\np\\n∫ ∞\\n0\\n(\\n(sin2n t) ln t\\ntp+1\\n− 1\\np\\nsin2n t\\ntp+1\\n)\\ndt > − c√\\nn\\n(17)\\nfor some constant c ∈ (0, 43(γ + ln 2)). Since the limit above must exist, we can pick a sequence\\npk → 0 such that e1/pk is a multiple of π. Hence we assume that N = e1/p/π is an integer below.\\nNow, split the integral into [0, π] and [π,∞). Observe that\\nlim\\np→0+\\nsin\\n(πp\\n2\\n) ∫ π\\n0\\n(sin2n t) ln t\\ntp+1\\ndt = 0, lim\\np→0+\\nsin\\n(πp\\n2\\n)∫ π\\n0\\nsin2n t\\ntp+1\\ndt = 0\\nbecause the integrands, viewed as functions of (p, t), are bounded on [0, 1]× [0, π], since sin2n t ∼ t2n\\nnear t = 0 and so t = 0 is not a singularity. Furthermore,\\nlim\\np→0+\\nsin\\n(πp\\n2\\n) ∫ π\\n0\\n1\\np\\n· sin\\n2n t\\ntp+1\\ndt =\\nπ\\n2\\n∫ π\\n0\\nsin2n t\\nt\\ndt\\nbecause the integrand is uniformly continuous on [0, 1]× [0, π] and we can take the limit under the\\nintegral sign.\\nNow we deal with the integral on [π,∞). Following our approach in Corollary 3.4, we have that∫ ∞\\nπ\\n(p ln t− 1)(sin\\n2n t)\\ntp+1\\ndt = −\\n∫ Nπ\\nπ\\n(1− p ln t)(sin\\n2n t)\\ntp+1\\ndt+\\n∫ ∞\\nNπ\\n(p ln t− 1)(sin\\n2n t)\\ntp+1\\ndt\\n≥\\n(\\n−\\nN−1∑\\nk=1\\n1− p ln(kπ)\\n(πk)p+1\\n+\\n∞∑\\nk=N+1\\np ln((k − 1)π) − 1\\n(πk)p+1\\n)\\nIn, (18)\\nwhere\\nIn =\\n∫ π\\n0\\nsin2n tdt ∼\\n√\\nπ\\nn\\n,\\nand we used the fact that (1− p ln t)/tp+1 is nonnegative and decreasing when ln t ≤ 1/p.\\nThe bracketed term on the rightmost side of (18) is\\n−\\nN−1∑\\nk=1\\n1− p ln(kπ)\\n(πk)p+1\\n+\\n∞∑\\nk=N+1\\np ln((k − 1)π)− 1\\n(πk)p+1\\n=\\nN−1∑\\nk=1\\np ln(kπ)− 1\\n(πk)p+1\\n+\\n∞∑\\nk=N+1\\np ln((k − 1)π)− 1\\n(πk)p+1\\n=\\n∞∑\\nk=1\\np ln(kπ)− 1\\n(πk)p+1\\n+\\n∞∑\\nk=N+1\\np ln(1− 1k )\\n(πk)p+1\\n.\\n33\\nThe second term clearly tends to 0 as p→ 0+, while the first term is equal to\\n−ζ(1 + p) + (p lnπ)ζ(1 + p)− pζ ′(1 + p)\\nπ1+p\\n→ lnπ − γ\\nπ\\n, p→ 0+,\\nwhere ζ(p) =\\n∑∞\\nn=1 1/n\\np is the Riemann zeta function and we used the fact that ζ(1 + p) =\\n1\\np + γ + f(p) for an analytic function f on C with f(0) = 0 (see, e.g. [4, p15]).\\nTherefore we conclude that the limit on the left-hand side in (17) is at least\\nlnπ − γ\\nπ\\nIn − π\\n2\\n∫ π\\n0\\nsin2n t\\nt\\ndt\\nand it suffices to show that ∫ π\\n0\\nsin2n t\\nt\\ndt ≤ c1√\\nn\\n+ o\\n(\\n1√\\nn\\n)\\nfor some\\nc1 <\\n2\\nπ\\n(\\nlnπ − γ√\\nπ\\n+\\n4\\n3\\n(γ + ln 2)\\n)\\n= 1.282 · · · .\\nFirst observe that ∫ π\\nπ/2\\nsin2n t\\nt\\ndt ≤ 2\\nπ\\n∫ π\\nπ/2\\nsin2n tdt =\\n2\\nπ\\n· 1\\n2\\nIn ∼ 1√\\nπn\\n,\\nand ∫ 1\\n0\\nsin2n t\\nt\\ndt ≤\\n∫ 1\\n0\\nt2n−1dt =\\n1\\n2n\\n.\\nLetting δ =\\n√\\n(ln n)/n, then\\n√\\nn sin2n t→ 0 as n→∞ on t ∈ [1, π/2 − δ], and thus∫ pi\\n2\\n−δ\\n1\\nsin2n t\\nt\\ndt = o\\n(\\n1√\\nn\\n)\\nand we can now bound∫ pi\\n2\\npi\\n2\\n−δ\\nsin2n t\\nt\\ndt ≤ 1π\\n2 − δ\\n∫ pi\\n2\\npi\\n2\\n−δ\\nsin2n tdt ≤ 1π\\n2 − δ\\n· 1\\n2\\nIn ∼ 1√\\nπn\\n.\\nTherefore we conclude that we can choose c1 to be\\nc1 =\\n2√\\nπ\\n= 1.128 · · ·\\nas desired.\\n9 Lower Bounds on Coresets for Projective Clustering\\nWe shall prove a lower bound of Ω˜(kj/ε2) bits for coresets for projective clustering. First we need\\na lemma which provides codewords to encode the clustering information.\\nLemma 9.1. For any given integer L ≥ 1 and even integer D ≥ 2, there exists a set S =\\n{(s1, t1), (s2, t2), . . . , (sm, tm)} of size m ≥ c2D/\\n√\\nD, where si, ti ∈ RD and c > 0 is an absolute\\nconstant, such that\\n34\\n• 〈si, ti〉 = 0;\\n• 〈si, tj〉 ≥ L2 for i 6= j;\\n• all entries of si and ti are in {0, L}.\\nProof. We first consider the case L = 1. Let {si} be the set of all binary vectors with Hamming\\nweight D/2, and ti = 1\\nD − si, i.e., ti is the complement of si. Thus, 〈si, ti〉 = 0 by construction.\\nFor any i 6= j, since si 6= sj, and both si and sj have Hamming weight D/2, we have 〈si, tj〉 ≥ 1.\\nFor a general L, we replace all entries of value 1 in the construction above with L.\\nIn the rest of the section, we also use an n× d matrix to represent a point set of size n in Rd,\\nwhere each row represents a point in Rd.\\nBelow we set up the framework of the hard instance for the projective subspace clustering\\nproblem. For a given k, choosing D = O(log k), we can obtain a set S of size k as guaranteed by\\nLemma 9.1. Suppose that j ≥ D + 1 and d ≥ j + 1. Without loss of generality we may assume\\nthat d = j + 1, otherwise we just embed our hard instance in Rj+1 into Rd by appending zero\\ncoordinates.\\nFor a set A consisting of k matrices A(1), A(2), . . . , A(k) ∈ Rn×(j+1−D), we form a point set\\nX = X(A) ∈ Rnk×d, whose rows are indexed by (i, j) ∈ [k]× [n] and defined as\\nXi,j =\\n(\\nsTi A\\n(i)\\nj\\n)\\n,\\nwhere A\\n(i)\\nj denotes the j-th row of A\\n(i).\\nSuppose that y ∈ Rj+1−D. For each i ∈ [k], let Vi,Wi ⊆ Rj+1 be j-dimensional subspaces that\\nsatisfy\\nVi ⊥ vi, vi =\\n(\\nti 0\\nj+1−D\\n)\\n,\\nWi ⊥ wi, wi =\\n(\\nti y\\n)\\n,\\nwhere, for notational simplicity, we write vertical concatenation in a row. Last, for each ℓ ∈ [k],\\ndefine a center\\nCℓ = (V1, . . . , Vℓ−1,Wℓ, Vℓ+1, . . . , Vk).\\nLemma 9.2. When ‖y‖2 = 1 and L2 ≥ maxi ‖A(ℓ)i ‖2, it holds that cost(X, Cℓ) = Φ(A(ℓ)y/‖wℓ‖2).\\nProof. One can readily verify, using Lemma 9.1, that Pij ⊥ vi whenever i 6= ℓ, and thus Pij ∈ V ′i\\nand dist(Pij ,Xℓ) = 0 for i 6= ℓ.\\nOn the other hand, for i 6= ℓ,\\ndist(Pℓj , Vi) =\\n|〈Pℓj , vi〉|\\n‖vi‖2 =\\n|〈Pℓj , vi〉|\\nL ·\\n√\\nD/2\\n≥ L√\\nD/2\\n.\\nand\\ndist(Pℓj ,Wℓ) =\\n|〈Pℓj , wℓ〉|\\n‖wℓ‖2 =\\n|〈A(ℓ)i , y〉|\\n‖wℓ‖2 ≤\\n‖A(ℓ)i ‖2‖y‖2√\\nD\\n2 L\\n2 + ‖y‖22\\n.\\nHence when L2 ≥ ‖y‖2maxi ‖A(ℓ)i ‖2, it must hold that Wℓ is the subspace in Xℓ that is the closest\\nto Pℓj for all j, and therefore\\ncost(X, Cℓ) =\\nn∑\\nj=1\\nφ(dist(Pℓj ,Wℓ)) = Φ\\n(\\nA(ℓ)y\\n‖wℓ‖2\\n)\\n.\\n35\\nTheorem 9.3. Suppose that there exists a function Φ and absolute constants C0 and ε0 such that\\nfor any d ≥ C0 log(k/ε) and ε ∈ (0, ε0), solving the subspace sketch problem for Φ requires M bits.\\nThen there exists an absolute constant C1 such that for any k ≥ 1 and j ≥ C1 log(k/ε), any coreset\\nfor projective clustering for Φ requires kM bits.\\nProof. We prove this theorem by a reduction from the subspace sketch problem for Φ to coresets\\nfor projective clustering for Φ.\\nChoose D = O(log k) and d′ := j + 1 − D = C0 log(1/ε). Let A(1), . . . , A(k) ∈ Rn×d′ be k\\nindependent hard instances for the subspace sketch problem for Φ. Let X be as constructed before\\nLemma 9.2. If one can compute a projective clustering coreset for X so that one can approximate\\ncost(X, Cℓ) up to a (1±ε)-factor, it follows from Lemma 9.2 that one can approximate Φ(A(ℓ)y/‖w‖2)\\nup to a (1 ± ε)-factor for every ℓ ∈ [k] and every unit vector y ∈ Rd′ . Solving the subspace sketch\\nproblem for Φ for each A(ℓ) requires M bits. Therefore, solving k independent instances requires\\nkM bits.\\nWe have the following immediate corollary.\\nCorollary 9.4. Under the assumptions of Theorem 9.3, any coreset for projective clustering re-\\nquires Ω(jM/ log(k/ε)) bits.\\nProof. Let b = j/(C0 log(k/ε)). Let X\\n′ be a block diagonal matrix of b blocks, each diagonal block\\nis an independent copy of the hard instance X in Theorem 9.3. It then follows from Theorem 9.3\\nthat the lower bound is Ω(bM) bits.\\nA lower bound of Ω(jk/(ε2 log k · polylog(1/ε)) follows immediately for Φ(x) = ‖x‖pp (Theo-\\nrem 3.12) for p ∈ [0,+∞) \\\\ 2Z+, and the M -estimators in Corollary 8.2 and Corollary 8.4.\\n10 Upper Bounds for the Tukey Loss p-Norm\\nWe shall prove in this section an O˜(1/ε2) upper bound for estimating a mollified version of the\\nTukey loss 1-norm Φ(x) for a vector x ∈ Rn.\\n10.1 Mollification of Tukey Loss Function\\nConsider the classic “bump” function ψ and the standard scaled version ψt, which are defined as\\nψ(x) =\\n{\\nCψ exp(− 11−x2 ), |x| < 1;\\n0, otherwise,\\nψt(x) =\\n1\\nt\\nψ\\n(x\\nt\\n)\\n,\\nwhere Cψ is the normalization constant such that\\n∫ 1\\n−1 ψ(x)dx = 1. The following is a result on\\nthe decay of its Fourier transform. We define the Fourier transform of a function f ∈ L1(R) to be\\nfˆ(t) =\\n∫\\nR\\neitxf(x)dx, and thus f(x) = (2π)−1\\n∫\\nR\\ne−itxfˆ(t)dt if fˆ ∈ L1(R).\\nLemma 10.1 ([18]). There exists an absolute constant C > 0 such that |ψ̂(t)| ≤ C|t|−3/4e−\\n√\\n|t|.\\nThis enables us to upper bound the derivatives of ψ. This is probably a classical result but we\\ndo not know an appropriate reference and so we reproduce the proof here.\\n36\\nLemma 10.2. There exist absolute constants C1, C2 > 0 such that\\n∥∥ψ(k)∥∥\\n∞\\n≤ C1(C2k log k)2k+2.\\nProof. First note that∥∥∥ψ(k)∥∥∥\\n∞\\n≤ 1\\n2π\\n∥∥∥ψ̂(k)∥∥∥\\n1\\n=\\n1\\n2π\\n∥∥∥(it)kψ̂(t)∥∥∥\\n1\\n=\\n1\\n2π\\n∫\\nR\\n|t|k\\n∣∣∣ψ̂(t)∣∣∣ dt.\\nOn the one hand, taking T = C1(k log k)\\n2, it follows from Lemma 10.1 that\\n|t|k|ψ̂(t)| ≤ C2t−5/4, |t| > T,\\nwhere C1, C2 > 0 are absolute constants. On the other hand, |ψˆ(t)| ≤ ‖ψ‖1 = 1 for all t. Therefore∫\\nR\\n|t|k|ψ̂(t)|dt ≤\\n∫\\n|t|>T\\nC2\\nt5/4\\ndt+\\n∫ T\\n−T\\n|t|kdt\\n≤ C3 + 2T k+1\\n≤ C4(\\n√\\nC1k log k)\\n2k+2.\\nThe result then follows.\\nRecall that the Tukey 1-loss function is\\nφ(x) =\\n{\\n|x|, |x| ≤ τ ;\\nτ, |x| > τ.\\nIt is easy to verify that (φ ∗ ψτ/4)(x) = φ(x) when τ/4 ≤ |x| ≤ 3τ/4, and thus if we define\\nφ˜(x) =\\n{\\nφ(x), |x| ≤ 3τ/4;\\n(φ ∗ ψτ/4)(x), |x| > 3τ/4,\\nit would be clear that φ˜ is infinitely times differentiable on (0,∞). Also observe that φ˜(x) = φ(x) =\\nτ when |x| ≥ 5τ/4, we see that φ˜ just mollifies φ on [3τ/4, 5τ/4]. We shall take φ˜ to be our mollified\\nversion of the Tukey 1-loss function.\\nNext we bound the derivatives of φ˜.\\nLemma 10.3. There exist absolute constants C1, C2 > 0 such that\\n∣∣∣φ˜(k)(x)∣∣∣ ≤ C1τ(C2k2 log2 k/τ)k+1\\nfor x ∈ [3τ/4, 5τ/4].\\nProof. Observe that∣∣∣φ˜(k)(x)∣∣∣ ≤ max\\nx∈[ 3\\n4\\nτ, 5\\n4\\nτ ]\\nφ(x) ·\\n∥∥∥ψ(k)τ/4∥∥∥∞ ≤ τ ∥∥∥ψ(k)τ/4∥∥∥∞ = τ( τ4 )k+1\\n∥∥∥ψ(k)∥∥∥\\n∞\\n.\\nThe result follows from Lemma 10.2.\\nAs a corollary of the preceding proposition, we have\\nLemma 10.4. Let a ∈ (0, 34) and b > 1 be constants. There exists a polynomial p(x) of degree\\nO( b−aτ log\\n2 1\\nεaτ log\\n2 log 1εaτ ) such that |p(x)− φ˜(x)| ≤ εφ˜(x) on [aτ, bτ ].\\n37\\nProof. Since φ˜(x) ≥ aτ on [aτ, bτ ], it is sufficient to consider the uniform approximation |p(x) −\\nφ˜(x)| ≤ ε(aτ) on [aτ, bτ ]. It follows from Lemma 2.8 that when n > k,\\nEn(φ˜; [aτ, bτ ]) ≤ 6\\nk+1ek\\n(k + 1)nk\\n·\\n(\\n(b− a)τ\\n2\\n)k\\n‖φ˜(k)‖∞ · 1\\nn− k .\\nInvoking Lemma 10.3, we obtain for n ≥ 2k that\\nEn(φ˜; [aτ, bτ ]) ≤ C1\\n(k + 1)(b− a)\\n(\\nC2(b− a)k2 log2 k\\nτn\\n)k+1\\n,\\nwhere C1, C2 > 0 are absolute constants. It is now clear that we can take k = O(log\\n1\\nεaτ ) and\\nn = O( b−aτ k\\n2 log2 k) so that En(φ˜; [aτ, bτ ]) ≤ ε · aτ .\\n10.2 Estimation Algorithm\\nSince φ˜(x) agrees with |x| for small |x|, it follows from Theorem 8.1 that solving the subspace sketch\\nproblem for Φ˜(x) requires Ω˜(d/ε2) bits. In this subsection we show that this lower bound is tight\\nup to polylogarithmic factors. Specifically we have the following theorem.\\nTheorem 10.5. Let Φ˜(x) be the mollified Tukey loss 1-norm of x ∈ Rn. There exists a randomized\\nalgorithm which returns an estimate Z to Φ˜(x) such that (1 − ε)Φ˜(x) ≤ Z ≤ (1 + ε)Φ˜(x) with\\nprobability at least 0.9. The algorithm uses O˜(1/ε2) bits of space.\\nThis theorem implies an O˜(d/ε2) upper bound for the corresponding subspace sketch problem.\\nThe remaining of the section is devoted to the proof of this theorem.\\nWe shall first sample rows of A with sampling rate Θ\\n(\\nτ\\nΦ˜(x)ε2\\n)\\n. However, we do not know\\nΦ˜(x) in advance. To implement this, we sample rows of A using O(log n) different sampling rates\\n1, (1.1)−1, (1.1)−2, . . . , 1.1−O(log n), and in parallel, estimate Φ˜(x) using a separate data structure of\\nO(polylog(n)·d) space [9, 8], which gives an estimate F satisfying 0.9Φ˜(x) ≤ F ≤ 1.1Φ˜(x). Then we\\nchoose a sampling rate r = 1.1−s for some integer s that is closest to τ\\nFε2\\n. Thus r ∈\\n[\\nτ\\n2Φ(x)ε2\\n, 2τ\\nΦ˜(x)ε2\\n]\\nwhen Φ˜(x) > τ\\n2ε2\\n, and r = 1 otherwise.\\nNow we show that for the chosen sampling rate r, the sampled entries give an accurate estimation\\nto Φ˜(x). This is definitely true when r = 1, in which case there is no sampling at all. Otherwise,\\nlet Xi = Φ˜(xi) if item i is sampled and Xi = 0 otherwise. Let X =\\n∑\\niXi and Z = (1/r)X. It is\\nclear that E[Z] = Φ˜(x). We calculate the variance below.\\nVar(Z) =\\n1\\nr2\\nVar(X) =\\n1\\nr2\\n∑\\ni\\nVar(Xi)\\n2 =\\n1\\nr2\\n∑\\ni\\n(r − r2)(Φ˜(xi))2\\n≤ 1\\nr\\n∑\\ni\\n(Φ˜(xi))\\n2\\n= O\\n(\\nΦ˜(x)ε2\\nτ\\n·\\n∑\\ni\\nφ˜(xi) · τ\\n)\\n= O(ε2) · (Φ˜(x))2.\\n38\\nIt follows from Chebyshev’s inequality that with constant probability,\\nZ =\\n1\\nr\\n∑\\nXi = (1±O(ε))Φ˜(x).\\nWe condition on this event in the rest of the proof. Thus, it suffices to estimate the summation of\\nΦ˜(xi) for those xi that are sampled. In the rest of this section, we use L to denote the indices of\\nentries that are sampled at the sampling rate r.\\nFor each i ∈ L with |xi| ≥ τ , we claim that\\n|xi| ≥ Ω(ε2) · ‖(xL)−O(1/ε2)‖1, (19)\\nwhere xL denotes the vector x restricted to the indices in L and v−k denotes the vector v after\\nzeroing out the largest k entries in magnitude.\\nWe first show that Φ˜(xL) = O\\n(\\nτ\\nε2\\n)\\n, which is clearly true when r = 1, since in this case,\\nΦ˜(xL) = Φ˜(x) = O\\n(\\nτ\\nε2\\n)\\n. When r < 1,\\n∑\\ni∈L Φ˜(xi) = (1±O(ε)) · r · Φ˜(x) = O\\n(\\nτ\\nε2\\n)\\n.\\nLet L′ = {i ∈ L : |xi| ≥ τ/2}. It follows that |L′| ≤ Φ˜(xL)/(τ/2) = O(1/ε2). Hence\\n‖xL\\\\L′‖1 =\\n∑\\ni∈L\\\\L′\\n|xi| = Φ˜(xL\\\\L′) ≤ Φ˜(xL) = O\\n( τ\\nε2\\n)\\n,\\nestablishing (19).\\nTherefore, to find all i ∈ L with |xi| ≥ τ , we use an ℓp-heavy hitter data structure, which can be\\nrealized by a Count-Sketch structure [11] which hashes xL into O(1/β) buckets and finds β-heavy\\nhitters relative to ‖(xL)−1/β‖1. Set β = Θ(ε2). In the end we obtain a list H ⊆ L such that every\\ni ∈ H is a β/2-heavy hitter relative to ‖(xL)−1/β‖1, and all β-heavy hitters are in H. Furthermore,\\nfor each i ∈ H the data structure also returns an estimate xˆi such that |xi|/2 ≤ |xˆi| ≤ 2|xi|\\nwhenever |xi| ≥ τ/2. The data structure has space complexity O˜(1/ε2).\\nFor each xi ∈ L with |xi| ≥ τ , it must hold that i ∈ H. Let H1 = {i ∈ H : |xˆi| ≥ 5τ/4} and\\nH2 = {i ∈ H : 3τ/8 ≤ |xˆi| ≤ 5τ/2}.\\nFor each i ∈ H1, by the estimation guarantee it must hold that |xi| ≥ 5τ/4. Hence S1 = τ |H1| =\\nΦ˜(xH1).\\nFor each i ∈ H2 it must hold that |xi| ∈ [ 316τ, 5τ ], and thus\\n‖x[n]\\\\H1‖1 ≤ 5Φ˜(x[n]\\\\H1).\\nLet p(x) be a polynomial such that |p(x)− φ˜(x)| ≤ εφ˜(x) on [ 316τ, 5τ ]. By Lemma 10.4, it is possible\\nto achieve deg p = O(log3(1/(ετ))). We now use an estimation algorithm analogous to theHighEnd\\nstructure in [21], which uses the same space O˜(1/ε2). Using the same BasicHighEnd structure\\nin [21], with constant probability, for each xi ∈ H we have T estimates xˆi,1, . . . , xˆi,T ∈ C such that\\nxˆi,t = xi + δi,t, where each δi,t ∈ C satisfies |δi,t| ≤ |xi|/2 and E(δi,t)k = 0 for k = 1, . . . , 3 deg p.\\nThe estimator is\\nS2 = Re\\n∑\\ni∈H2\\nΦ˜\\n(\\n1\\nT\\nT∑\\ni=1\\np(xˆi,t)\\n)\\n.\\nIt follows from the analysis in [21] that (x can be replaced with x[n]\\\\H1 in the analysis of the\\nvariance) that the algorithm will output, with a constant probability,\\nS2 = Φ˜(xH2)± ε‖x[n]\\\\H1‖1 = (1± 5ε)Φ˜(xH2).\\n39\\nFor each i ∈ [n] \\\\ (H1 ∪ H2), it must hold that |xi| ≤ τ/2 and thus we can use an ℓ1 sketch\\nalgorithm as in [21], and obtain S3 = (1± ε)Φ˜(x[n]\\\\(H1∪H2)).\\nFinally, the algorithm returns S1+S2+S3, which is a (1±10ε)-approximation to Φ˜(x). Rescaling\\nε proves the correctness of the estimate.\\nFor the part of evaluating S2 and S3, the space complexity is the same as the HighEnd and ℓp\\nsketch algorithm in [21], which are both O˜(1/ε2) bits.\\n11 An Upper Bound for ℓ1 Subspace Sketches in Two Dimensions\\nIn this section, we prove an O(polylog(n)/ε) upper bound for the ℓ1 subspace sketch problem when\\nd = 2. Our plan is to reduce the ℓ1 subspace sketch problem with d = 2 to coresets for the weighted\\n1-median problem with d = 1. For the latter problem, an O(polylog(n)/ε) upper bound is known\\n[16].\\nFor the special case where the first column of the A matrix is all ones, the ℓ1 subspace sketch\\nproblem with d = 2 is equivalent to coresets for 1-median with d = 1. To see this, by homogeneity,\\nwe may assume x2 = 1 for the query vector x ∈ R2. Thus, ‖Ax‖1 =\\n∑n\\ni=1 |x1 +Ai,2|, which is the\\n1-median cost of using x2 as the center on {−A1,2,−A2,2, . . . ,−An,2}. When entries of the first\\ncolumn of A are positive but not necessarily all ones, we have\\n‖Ax‖1 =\\nn∑\\ni=1\\nAi,1\\n∣∣∣∣x1 + Ai,2Ai,1\\n∣∣∣∣ ,\\nwhich is the weighted 1-median cost of using x1 as the center on {−A1,2/A1,1,−A2,2/A2,1, . . . ,−An,2/An,1},\\nwith weights {Ai,1, Ai,2, . . . , An,2}. It has been shown in [16, Theorem 2.8] that there exists a coreset\\nof size O(polylog(n)/ε) for the weighted 1-median problem when d = 1.\\nFor general A, we divide the rows of A into three separate matrices A+, A− and A0. Here, all\\nentries in the first column of A+ are positive, all entries in the first column of A− are negative,\\nand all entries in the first column of A0 are zeroes. Since ‖Ax‖1 = ‖A+x‖1 + ‖A−x‖1 + ‖A0x‖1,\\nwe can design subspace sketches separately for A+, A− and A0. Our reduction above implies an\\nO(polylog(n)/ε) upper bound for A+ and A−. For A0, since all entries in the first column are all\\nzero, we have\\n‖A0x‖1 = |x2|\\n∑\\ni\\n|A0i,2|.\\nThus, it suffices to store\\n∑\\ni |A0i,2| for A0.\\nTheorem 11.1. The ℓ1 subspace sketch problem can be solved using O˜(1/ε) bits when d = 2.\\nReferences\\n[1] Alternate proof for weighted alternating shifted central binomial sum relation.\\nhttps://math.stackexchange.com/questions/2827591/. Accessed: 20 Oct 2018.\\n[2] Sketching algorithms for big data. https://www.sketchingbigdata.org/.\\n40\\n[3] Alexandr Andoni, Jiecao Chen, Robert Krauthgamer, Bo Qin, David P. Woodruff, and Qin\\nZhang. On sketching quadratic forms. In Proceedings of the 2016 ACM Conference on Innova-\\ntions in Theoretical Computer Science, ITCS ’16, pages 311–319, New York, NY, USA, 2016.\\nACM.\\n[4] George E. Andrews, Richard Askey, and Ranjan Roy. Special Functions. Encyclopedia of\\nMathematics and its Applications. Cambridge University Press, 1999.\\n[5] Ethan D. Bolker. A class of convex bodies. Transactions of the American Mathematical Society,\\n145:323–345, 1969.\\n[6] Ste´phane Boucheron, Ga´bor Lugosi, and Olivier Bousquet. Concentration Inequalities: A\\nNonasymptotic theory of Independence. Oxford University Press, 2013.\\n[7] J. Bourgain, J. Lindenstrauss, and V. Milman. Approximation of zonoids by zonotopes. Acta\\nMath., 162:73–141, 1989.\\n[8] Vladimir Braverman, Stephen R Chestnut, David P Woodruff, and Lin F Yang. Streaming\\nspace complexity of nearly all functions of one variable on frequency vectors. In Proceedings\\nof the 35th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,\\npages 261–276. ACM, 2016.\\n[9] Vladimir Braverman and Rafail Ostrovsky. Zero-one frequency laws. In Proceedings of the\\nforty-second ACM symposium on Theory of computing, pages 281–290. ACM, 2010.\\n[10] Charles Carlson, Alexandra Kolla, Nikhil Srivastava, and Luca Trevisan. Optimal lower bounds\\nfor sketching graph cuts. arXiv preprint arXiv:1712.10261, 2017.\\n[11] Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data\\nstreams. Theoretical Computer Science, 312(1):3–15, 2004.\\n[12] Kenneth L. Clarkson, Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, Xiangrui\\nMeng, and David P. Woodruff. The fast Cauchy transform and faster robust linear regression.\\nSIAM Journal on Computing, 45(3):763–810, 2016.\\n[13] Michael B. Cohen and Richard Peng. lp row sampling by lewis weights. In Proceedings of the\\nForty-seventh Annual ACM Symposium on Theory of Computing, STOC ’15, pages 183–192,\\nNew York, NY, USA, 2015. ACM.\\n[14] R. Courant and F. John. Introduction to Calculus and Analysis II/1. Classics in Mathematics.\\nSpringer Berlin Heidelberg, 1999.\\n[15] Sumit Ganguly and David P. Woodruff. High probability frequency moment sketches. In 45th\\nInternational Colloquium on Automata, Languages, and Programming, ICALP 2018, July 9-\\n13, 2018, Prague, Czech Republic, pages 58:1–58:15, 2018.\\n[16] Sariel Har-Peled and Akash Kushal. Smaller coresets for k-median and k-means clustering.\\nDiscrete & Computational Geometry, 37(1):3–19, 2007.\\n[17] Piotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream\\ncomputation. J. ACM, 53(3):307–323, 2006.\\n41\\n[18] Steven G. Johnson. Saddle-point integration of C∞ “bump” functions. arXiv:1508.04376\\n[math.CV], 2015.\\n[19] William B. Johnson and Joram Lindenstrauss. Chapter 1 – Basic concepts in the geometry of\\nBanach spaces. In W. B. Johnson and J. Lindenstrauss, editors, Handbook of the Geometry of\\nBanach Spaces, volume 1 of Handbook of the Geometry of Banach Spaces, pages 1–84. Elsevier\\nScience B.V., 2001.\\n[20] William B. Johnson and Gideon Schechtman. Chapter 19 – Finite dimensional subspaces of\\nLp. In W. B. Johnson and J. Lindenstrauss, editors, Handbook of the Geometry of Banach\\nSpaces, volume 1 of Handbook of the Geometry of Banach Spaces, pages 837–870. Elsevier\\nScience B.V., 2001.\\n[21] Daniel M. Kane, Jelani Nelson, Ely Porat, and David P. Woodruff. Fast moment estimation\\nin data streams in optimal space. In Proceedings of the Forty-third Annual ACM Symposium\\non Theory of Computing, STOC ’11, pages 745–754, New York, NY, USA, 2011. ACM.\\n[22] Daniel M. Kane, Jelani Nelson, and David P. Woodruff. An optimal algorithm for the distinct\\nelements problem. In Proceedings of the Twenty-Ninth ACM SIGMOD-SIGACT-SIGART\\nSymposium on Principles of Database Systems, PODS 2010, June 6-11, 2010, Indianapolis,\\nIndiana, USA, pages 41–52, 2010.\\n[23] Alexander Koldobsky and Hermann Ko¨nig. Chapter 21 – Aspects of the isometric theory of\\nBanach spaces. In W. B. Johnson and J. Lindenstrauss, editors, Handbook of the Geometry\\nof Banach Spaces, volume 1 of Handbook of the Geometry of Banach Spaces, pages 837–870.\\nElsevier Science B.V., 2001.\\n[24] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: Isoperimetry and Pro-\\ncesses. Springer, 1991.\\n[25] Yi Li, Xiaoming Sun, Chengu Wang, and David P. Woodruff. On the communication com-\\nplexity of linear algebraic problems in the message passing model. In Fabian Kuhn, editor,\\nDistributed Computing, pages 499–513, Berlin, Heidelberg, 2014. Springer Berlin Heidelberg.\\n[26] Yi Li and David P. Woodruff. Tight Bounds for Sketching the Operator Norm, Schatten\\nNorms, and Subspace Embeddings. In Klaus Jansen, Claire Mathieu, Jose´ D. P. Rolim, and\\nChris Umans, editors, Approximation, Randomization, and Combinatorial Optimization. Al-\\ngorithms and Techniques (APPROX/RANDOM 2016), volume 60 of Leibniz International\\nProceedings in Informatics (LIPIcs), pages 39:1–39:11, Dagstuhl, Germany, 2016. Schloss\\nDagstuhl–Leibniz-Zentrum fuer Informatik.\\n[27] Jiˇr´ı Matousˇek. Lecture notes on metric embeddings.\\nhttps://kam.mff.cuni.cz/~matousek/ba-a4.pdf. Accessed: 26 Oct 2018.\\n[28] Vitali D. Milman and Gideon Schechtman. Asymptotic Theory of Finite Dimensional Normed\\nSpaces. Lecture Notes in Mathematics Vol. 1200. Springer-Verlag Berlin Heidelberg, 1986.\\n[29] Peter Bro Miltersen, Noam Nisan, Shmuel Safra, and Avi Wigderson. On data structures and\\nasymmetric communication complexity. Journal of Computer and System Sciences, 57(1):37–\\n49, 1998.\\n42\\n[30] S. Muthukrishnan. Data streams: Algorithms and applications. Foundations and Trends in\\nTheoretical Computer Science, 1(2), 2005.\\n[31] Rasmus Pagh. Compressed matrix multiplication. ACM Trans. Comput. Theory, 5(3):9:1–9:17,\\nAugust 2013.\\n[32] Udaya Parampalli, Xiaohu Tang, and Serdar Boztas. On the construction of binary sequence\\nfamilies with low correlation and large sizes. IEEE Transactions on Information Theory,\\n59(2):1082–1089, 2013.\\n[33] Theodore J. Rivlin. An introduction to the approximation of functions. Blaisdell Book in\\nNumerical Analysis and Computer Science. Blaisdell Publishing Company, 1969.\\n[34] Gideon Schechtman. More on embedding subspaces of Lp in l\\nn\\nr . Compositio Mathematica,\\n61(2):159–169, 1987.\\n[35] Gideon Schechtman. Tight embedding of subspaces of Lp in ℓ\\nn\\np for even p. Proceedings of the\\nAmerican Mathematical Society, 139(12):4419–4421, 2011.\\n[36] Michel Talagrand. Embedding subspaces of L1 into ℓ\\nN\\n1 . Proceedings of the American Mathe-\\nmatical Society, 108(2):363–369, 1990.\\n[37] Michel Talagrand. Embedding subspaces of Lp in ℓ\\nN\\np . In J. Lindenstrauss and V. Milman,\\neditors, Geometric Aspects of Functional Analysis, pages 311–326, Basel, 1995. Birkha¨user\\nBasel.\\n[38] Dirk Van Gucht, Ryan Williams, David P. Woodruff, and Qin Zhang. The communication\\ncomplexity of distributed set-joins with applications to matrix multiplication. In Proceedings\\nof the 34th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,\\nPODS ’15, pages 199–212, New York, NY, USA, 2015. ACM.\\n[39] Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data\\nScience. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University\\nPress, 2018.\\n[40] David P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends\\nin Theoretical Computer Science, 10(1-2):1–157, 2014.\\n[41] David P. Woodruff and Qin Zhang. Distributed statistical estimation of matrix products\\nwith applications. In Proceedings of the 37th ACM SIGMOD-SIGACT-SIGAI Symposium on\\nPrinciples of Database Systems, SIGMOD/PODS ’18, pages 383–394, New York, NY, USA,\\n2018. ACM.\\n[42] J. Yang, X. Meng, and M. Mahoney. Quantile regression for large-scale applications. SIAM\\nJournal on Scientific Computing, 36(5):S78–S110, 2014.\\n43\\n'}\n",
      "{'_id': ObjectId('6815e61add259dc7e6e1cd2c'), 'authors': 'Chuzhoy, Julia, Gao, Yu, Li, Jason, Nanongkai, Danupon, Peng, Richard, Saranurak, Thatchaphol', 'year': '2020', 'title': 'A Deterministic Algorithm for Balanced Cut with Applications to Dynamic\\n  Connectivity, Flows, and Beyond', 'full_text': 'A Deterministic Algorithm for Balanced Cut\\nwith Applications to Dynamic Connectivity, Flows, and Beyond\\nJulia Chuzhoy\\nTTIC\\nYu Gao\\nGeorgia Tech∗\\nJason Li\\nCMU\\nDanupon Nanongkai\\nKTH\\nRichard Peng\\nGeorgia Tech∗\\nThatchaphol Saranurak\\nTTIC\\nOctober 18, 2019\\nWe consider the classical Minimum Balanced Cut problem: given a graph G, compute a partition of\\nits vertices into two subsets of roughly equal volume, while minimizing the number of edges connecting\\nthe subsets. We present the first deterministic, almost-linear time approximation algorithm for this\\nproblem. Our algorithm in fact provides a stronger guarantee: it either returns a balanced cut whose\\nvalue is close to a given target value, or it certifies that such a cut does not exist by exhibiting a large\\nsubgraph of G that has high conductance. We use this algorithm to obtain deterministic algorithms for\\ndynamic connectivity and minimum spanning forest, whose worse-case update time on an n-vertex graph\\nis no(1), thus resolving a major open problem in the area of dynamic graph algorithms. Our work also\\nimplies deterministic algorithms for a host of additional problems, whose time complexities match, up\\nto subpolynomial in n factors, those of known randomized algorithms. The implications include almost-\\nlinear time deterministic algorithms for solving Laplacian systems and for approximating maximum flows\\nin undirected graphs.\\nThese results were obtained independently by Chuzhoy, and the group consisting of Gao, Li, Nanongkai, Peng, and\\nSaranurak. Chronologically, Gao et al. obtained their result in July 2019, while Chuzhoy’s result was obtained in September\\n2019, but there was no communication between the groups until early October 2019.\\n∗part of this work was done while visiting MSR Redmond\\nar\\nX\\niv\\n:1\\n91\\n0.\\n08\\n02\\n5v\\n1 \\n [c\\ns.D\\nS]\\n  1\\n7 O\\nct \\n20\\n19\\nContents\\n1 Introduction 1\\n1.1 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\\n1.2 Application: Dynamic Connectivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.3 Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n1.4 Paper Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2 Preliminaries 9\\n2.1 Conductance and Sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2 Reduction to Constant-Degree Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n3 Matching Player’s Problem and Algorithms 12\\n3.1 An Algorithm for k ≥ 1 (Proof of Theorem 3.2) . . . . . . . . . . . . . . . . . . . . . . . . 12\\n3.2 An Improved Algorithm for k = 1 (Proof of Theorem 3.3) . . . . . . . . . . . . . . . . . . 17\\n4 Cut Player’s Problem Variants and Their Reductions 18\\n5 A Recursive Algorithm for CutAugment 21\\n5.1 Key Tool 1: Simultaneous Small Expanders Embedding . . . . . . . . . . . . . . . . . . . 21\\n5.2 Key Tool 2: Modified KKOV’s Cut-Matching Game . . . . . . . . . . . . . . . . . . . . . 23\\n5.3 Algorithm and Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n6 Combining Everything: A Fast Algorithm for BalCutPrune 26\\n6.1 One level of recursion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n6.2 High Conductance Regime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n6.3 General Regime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n7 Applications of BalCutPrune 31\\n7.1 Expander Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n7.2 Dynamic Connectivity and Minimum Spanning Forests . . . . . . . . . . . . . . . . . . . . 32\\n7.3 Spectral Sparsifiers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n7.4 Laplacian Solvers and Laplacian-based Graph Algorithms . . . . . . . . . . . . . . . . . . 34\\n7.5 Approximate Max-Flows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n7.6 Low Vertex Expansion Cut . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n7.7 Vertex Max Flow, Min Cut, and Lowest Vertex Expansion Cut . . . . . . . . . . . . . . . 36\\n8 Faster BalCutPrune in Low Conductance Regime 37\\n8.1 CutMatch and CutAugment via Max Flow . . . . . . . . . . . . . . . . . . . . . . . . 37\\n8.2 BalCutPrune without calling Prune . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n8.3 Applications in Low Conductance Regime . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\\n9 Open Problems 42\\nA Omitted Proofs 50\\nA.1 Proof of Lemma 2.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\nA.2 Proof of Lemma 4.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\nA.3 Proof of Lemma 4.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\nA.4 Proof of Theorem 5.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\\nA.5 Proof of Lemma 5.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\n1 Introduction\\nIn the classical Minimum Balanced Cut problem, the input is an n-vertex graph G = (V,E), and the\\ngoal is to compute a partition of V into two subsets A and B with volG(A), volG(B) ≥ volG/3, while\\nminimizing the number of edges connecting the two subsets |EG(A,B)|; here, EG(A,B) denotes the\\nset of edges in G with one endpoint in A and another in B, and volG(S) of a subset S of vertices is\\nthe volume of S – the sum of the degrees of the vertices of S in G, and volG = volG(V ) is the total\\nvolume of the graph. The Minimum Balanced Cut problem is closely related to the Minimum Conductance\\nCut problem, where the goal is to compute a subset S of vertices of minimum conductance, defined as\\n|EG(S, V \\\\S)|/min{volG(S), volG(V \\\\S)}, and to the Sparsest Cut problem, where the goal is to compute\\na subset S of vertices of minimum sparsity : |EG(S, V \\\\ S)|/min{|S|, |V \\\\ S|}. While all three problems\\nare known to be NP-hard, approximation algorithms for them are among the most central and widely\\nused tools in algorithm design, especially due to their natural connections to the hierarchical divide-and-\\nconquer paradigm [Ra¨c02, ST04, Tre05, AHK10, RST14, KT19, NSW17]. We note that approximation\\nalgorithms for Minimum Balanced Cut often consider a relaxed (or a bi-criteria) version, where we only\\nrequire that the solution (A,B) returned by the algorithm satisfies volG(A), volG(B) ≥ volG/4, but the\\nsolution value is compared to that of optimal balanced cut.\\nThe first approximation algorithm for Minimum Balanced Cut, whose running time is near-linear in\\nthe graph size, was developed in the seminal work of Spielman and Teng [ST04]. This algorithm was used\\nin [ST04] in order to decompose a given graph into a collection of “near-expanders”, which are then ex-\\nploited in order to construct spectral sparsifiers, eventually leading to an algorithm for solving systems of\\nlinear equations in near-linear time. Algorithms for Minimum Balanced Cut also served as crucial building\\nblocks in the more recent breakthrough results that designed near- and almost-linear time1 approxima-\\ntion algorithms for a large class of flow and regression problems [She13, KLOS14, Pen16, KPSW19] and\\nfaster exact algorithms for maximum flow, shortest paths with negative weights, and minimum-cost flow\\n[CMSV17, Mad16]. Spielman and Teng’s expander decomposition was later strengthened by Nanongkai,\\nSaranurak and Wulff-Nilsen [NSW17, Wul17, NS17], who used it to obtain algorithms for the dynamic\\nminimum spanning forest problem with improved worst-case update time. The fastest current algorithm\\nfor computing expander decompositions is due to Saranurak and Wang [SW19]; a similar decomposition\\nwas recently used by Chuzhoy and Khanna [CK19] in their algorithm for the decremental single-source\\nshortest paths problem, that in turn led to a faster algorithm for approximate vertex-capacitated maxi-\\nmum flow.\\nUnfortunately, all algorithms mentioned above are randomized. This is mainly because all existing\\nalmost- and near-linear time algorithms for Minimum Balanced Cut are randomized [ST04, KRV09]. A\\nfundamental question in this area that remains open is then: is there a deterministic algorithm for\\nMinimum Balanced Cut with similar performance guarantees? Resolving this questions seems a key step\\nto obtaining fast deterministic algorithms for all aforementioned problems, and to resolving one of the\\nmost prominent open problems in the area of dynamic graph algorithms, namely, whether there is a\\ndeterministic algorithm for Dynamic Connectivity, whose worst-case update time is smaller than the classic\\nO(\\n√\\nn) bound by Frederickson [Fre85, EGIN97], by a polynomial in n factor.\\nThe best previous published bound on the running time of a determinsitic algorithm for Minimum\\nBalanced Cut is O(mn) [ACL07]. A recent unpublished manuscript by a subset of the authors, together\\nwith Yingchareonthawornchai [GLN+], obtains a running time of min(nω+o(1),m1.5+o(1)), where ω <\\n2.372 is the matrix multiplication exponent, and n and m are the number of nodes and edges of the\\ninput graph, respectively. This algorithm is used in [GLN+] to obtain faster deterministic algorithms for\\nthe vertex connectivity problem. However, the running time of the algorithm of [GLN+] for Minimum\\nBalanced Cut is somewhat slow, and it just falls short of breaking the O(\\n√\\nn) worst-case update time\\nbound for Dynamic Connectivity.\\n1We informally say that an algorithm runs in near-linear time, if its running time is O(m · poly logn), where m and n\\nare the number of edges and vertices in the input graph, respectively. We say that the running time is almost-linear, if it\\nis bounded by m1+o(1).\\n1\\n1.1 Our Results\\nWe present a deterministic (bi-criteria) algorithm for Minimum Balanced Cut that achieves a factor\\n2O((logn)\\n2/3(log logn)1/3))-approximation in time m1+o(1). In fact our algorithm provides somewhat stronger\\nguarantees: it either computes an almost-balanced cut whose value is within a 2O((logn)\\n2/3(log logn)1/3))\\nfactor of a given target value z; or it certifies that every balanced cut in G has value Ω(z), by producing\\na large sub-graph of G that has a large conductance. This algorithm implies fast deterministic algo-\\nrithms for all the above mentioned problems, including, in particular, improved worst-case update time\\nguarantees for (undirected) Dynamic Connectivity and Minimum Spanning Forest.\\nIn order to provide more details on our results and techniques, we need to introduce some notation.\\nThroughout, we assume that we are given an m-edge, n-node undirected graph, denoted by G = (V,E).\\nA cut in G is a partition (A,B) of V into two non-empty subsets; abusing the notation, we will also refer\\nto subsets S of vertices with S 6= ∅, V as cuts, meaning the partition (S, V \\\\ S) of V . The conductance\\nof a cut S in G, which was already mentioned above, is defined as:\\nΦG (S) :=\\n|EG (S, V \\\\ S) |\\nmin (volG (S) , volG (V \\\\ S)) ,\\nand the conductance of a graph G, that we denote by Φ(G), is the smallest conductance of any cut S of\\nG:\\nΦG := min\\nS(V,S 6=∅\\nΦG(S).\\nWe say that a cut S is balanced if volG(S), volG(V \\\\ S) ≥ volG/3. The main tool that we use in our\\napproximation algorithm for the Minimum Balanced Cut problem is the BalCutPrune problem, that\\nis defined next. Informally, the problem seeks to either find a low-conductance balanced cut in a given\\ngraph, or to produce a certificate that every balanced cut has a high conductance, by exhibiting a large\\nsub-graph of G that has a high conductance. For a graph G and a subset S of its vertices, we denote by\\nG− S the graph obtained from G by deleting all vertices of S from it.\\nDefinition 1.1 (BalCutPrune problem). The input to theBalCutPrune problem is a triple (G,φU , φL),\\nwhere G = (V,E) is an m-edge graph, and φU , φL ∈ (0, 1] are parameters with φL ≤ φU . The goal is to\\ncompute a cut S in G, with volG(S) ≤ volG/2, such that one of the following hold: either\\n1. (Cut) volG(S) ≥ volG/3 and ΦG(S) ≤ φU ; or\\n2. (Prune) |EG(S, V \\\\ S)| ≤ φU · volG and ΦG−S ≥ φL.\\nOur main technical result is the following.\\nTheorem 1.2 (Main Result). There is a deterministic algorithm, that, given a graph G with n vertices\\nand m edges, and parameters φU , φL ∈ (0, 1], such that φUφL ≥ 2Ω(log\\n2/3 n·(log logn)1/3), solves the resulting\\ninstance (G,φU , φL) of BalCutPrune in time m\\n1+o(1).\\nThe algorithm from Theorem 1.2 immediately implies a deterministic bi-criteria factor-2O(log\\n2/3 n·(log logn)1/3)-\\napproximation algorithm for Minimum Balanced Cut, with running time m1+o(1). Indeed, given a graph\\nG = (V,E), we can perform a binary search over values φU ∈ (0, 1/100]. Given any such value φU , we\\ncan set φL = φU/2\\nc log2/3 n·(log logn)1/3 for an appropriately chosen constant c, and run the algorithm from\\nTheorem 1.2 on the resulting instance (G,φU , φL) of BalCutPrune. If the outcome of the algorithm\\nis a cut S with volG/3 ≤ volG(S) ≤ volG/2 and ΦG(S) ≤ φU , (the Cut outcome), then we obtain a\\nbalanced cut (A,B) with A = S, B = V \\\\ S, and |EG(A,B)| = O(φU · volG). If the outcome is a cut S\\nwith volG(S) ≤ volG/2 and |EG(S, V \\\\S)| ≤ φU · volG (the Prune outcome), but volG(S) ≥ volG/4, then\\nwe again obtain an (almost) balanced cut (A,B) with |EG(A,B)| = O(φU · volG), as before. Lastly,\\nif neither of these cases happen, then the algorithm’s outcome is a cut S with volG(S) ≤ volG/4,\\n|EG(S, V \\\\ S)| ≤ φU · volG, and ΦG[V \\\\S] ≥ φL. We claim that in this case, for any balanced cut\\n2\\n(A,B) in G, |EG(A,B)| ≥ Ω(φL · volG) holds. This is because any such partition (A,B) of V defines\\na partition (A′, B′) of V \\\\ S, with volG(A′), volG(B′) ≥ Ω(volG), and, since ΦG[V \\\\S] ≥ φL, we get that\\n|EG(A,B)| ≥ Ω(φL · volG). Therefore, we obtain the following corollary:\\nCorollary 1.3. There is an algorithm that, given an n-vertex m-edge graph G, and a target value\\nz, either returns a partition (A,B) of V (G) with volG(A), volG(B) ≥ volG/4 and |EG(A,B)| ≤ z ·\\n2O(log\\n2/3 n·(log logn)1/3), or it certifies that for any partition (A,B) of V (G) with volG(A), volG(B) ≥\\nvolG/3, |EG(A,B)| > z must hold. The running time of the algorithm is m1+o(1).\\nInformally, an algorithm for the BalCutPrune problem is required to either (1) output a cut S\\nthat is balanced (volG(S) ≥ volG/3) and has a low conductance (ΦG(S) ≤ φU ), or (2) certify that,\\nonce we remove, or “prune”, a relatively small set S of vertices from G, we obtain a graph of relatively\\nhigh conductance (ΦG−S ≥ φL). Moreover, in the latter case, there are relatively few edges connecting\\nS to the high-conductance graph (EG(S, V \\\\ S) ≤ φUvolG). Therefore, in a sense, an algorithm for\\nthe BalCutPrune problem provides stronger guarantees than those needed for solving the Minimum\\nBalanced Cut problem: if the value of the Minimum Balanced Cut is higher than the given threshold, then\\nthe certificate that we provide, in the form of a large high-conductance subgraph of G, can be exploited\\ndirectly in various algorithms.\\nWe note that algorithms for Minimum Balanced Cut often differ in the type of certificate that they\\nprovide when the value of the Minimum Balanced Cut is greater than the given threshold (that corresponds\\nto the “prune” case in Definition 1.1). The original near-linear time algorithm of Spielman and Teng\\n[ST04] outputs a set S of nodes of small volume, with the guarantee that for some S′ ⊆ S, the graph\\nG − S′ has high conductance. This guarantee, however, is not sufficient for several applications. A\\nversion that was found to be more useful in several recent applications, such as e.g. Dynamic Connectivity\\n[SW19, NSW17, Wul17, NS17], is somewhat similar to that in the definition of BalCutPrune, but with\\na somewhat stronger guarantee in the (Prune) case2.\\nThe ratio φUφL is sometimes referred to as the approximation factor, and for most applications\\nφU\\nφL\\n=\\npoly( 1φU , log n) and φL = 1/n\\no(1) suffice. The approximation factor and the time complexity of Spielman\\nand Teng’s algorithm [ST04] depend on both φU and φL, and the guarantees that they provide are\\nsufficiently low for most applications, including all applications discussed in this paper. Many subsequent\\npapers have improved their approximation factor or the time complexity, e.g. [KRV09, ACL07, OV11,\\nOSV12, Mad10b]; we do not discuss these results here since they are not directly related to this work.\\nApplications. As already mentioned earlier, our results imply faster deterministic algorithms for a\\nnumber of problems; the performance of our algorithms matches that of the best current randomized\\nalgorithms, to within factor no(1). We summarize these bounds in Table 1 and Table 2; see Section 7\\nfor a more detailed discussion. We now turn to discuss the implications of our results to the Dynamic\\nConnectivity problem, which was the original motivation of this work.\\n1.2 Application: Dynamic Connectivity\\nIn its most basic form, in the Dynamic Connectivity problem, we are given a graph G that undergoes\\nedge deletions and insertions, and the goal is to maintain the information of whether G is connected.\\nThe Dynamic Connectivity problem and its generalizations – dynamic Spanning Forest (SF) and dynamic\\nMinimum Spanning Forest (MSF) – have played a central role in the development of the area of dynamic\\ngraph algorithms for more than three decades (see, e.g., [NS17, NSW17] for further discussions).\\nAn important measure of an algorithm’s performance is its update time – the amount of time that is\\nneeded in order to process each update (an insertion or a deletion of an edge). We distinguish between\\n2To be precise, that version requires that |EG(S, V \\\\S)| ≤ φU ·volG(S), which is somewhat stronger than our requirement\\nthat |EG(S, V \\\\ S)| ≤ φU · volG. But for all applications we consider, our guarantee still suffices, possibly because the two\\nguarantees are essentially the same when the cut S is balanced.\\n3 Note: To simplify the discussion, we only state best bounds for sparse graphs.\\n3\\nProblem Best previous running\\ntime: deterministic\\nBest previous running\\ntime: randomized\\nOur results:\\ndeterministic\\nMax flow – exact O˜(mmin{m1/2, n2/3})\\n[GR98]\\nO˜(m3/2) [DS08] &\\nO˜(m\\n√\\nn) [LS14]3\\nÔ(m3/2): see\\ndiscussion after\\nCorollary 7.7\\nMax flow:\\n(1 + \\x0f)-approximate\\nO˜(mmin{m1/2, n2/3})\\n[GR98] (Note: exact)\\nO˜(m\\x0f−1) [She17]\\n(See also\\n[She13, KLOS14, Pen16])\\nÔ(m\\x0f−1)\\nCorollary 7.8\\nVertex-capacitated max\\nflow and min cut:\\n(1 + \\x0f)-approximation\\nO˜(mmin{m1/2, n2/3})\\n[GR98] (Note: exact)\\nÔ(n2/poly(\\x0f)) [CK19] Ô(n2/poly(\\x0f))\\nCorollary 7.11\\nMinimum-cost\\nunit-capacity max flow\\nO˜(mmin{m1/2, n2/3})\\n[GT89]\\nO˜(m10/7) [Mad13,\\nMad16, CMSV17]3\\nÔ(m10/7)\\nsee discussion after\\nCorollary 7.7\\nMinimum-cost max flow O˜(mn) [AGOT92, GT87] O˜(m3/2) [DS08] &\\nO˜(m\\n√\\nn) [LS14]3\\nÔ(m3/2) see\\ndiscussion after\\nCorollary 7.7\\nMinimum-cost bipartite\\nperfect matching\\nO˜(m\\n√\\nn) [GT89] O˜(m10/7) [CMSV17]\\n(Note: We only state best\\nbound for sparse graphs)\\nÔ(m10/7) see\\ndiscussion after\\nCorollary 7.7\\nConductance:\\nno(1)-approximation\\nÔ(m1.5)\\nimplicit in [GLN+]\\nO˜(m) implicit in\\n[KRV09, Pen16]\\nÔ(m)\\nCorollary 8.11\\nSparsest cut with vertex\\ncapacities:\\nno(1)-approximation\\nÔ(m1.5) [GLN+] Ô(n2) [CK19]\\nO(m\\n√\\nn) implicit in\\n[GLN+]\\nÔ(n2) Corollary 7.12\\nÔ(min{m√n,mφ−1})\\nCorollary 7.9\\nφ is vertex expansion.\\n(\\x0f, \\x0f/no(1))-expander\\ndecomposition\\nÔ(m1.5)\\nimplicit in [GLN+]\\nO˜(m\\x0f−1) [SW19]\\nÔ(m) [NS17]\\n(See also [Wul17])\\nÔ(m)\\nCorollaries 7.1\\nand 8.12\\nCongestion\\napproximator and\\noblivious routing with\\nno(1)-quality\\nΩ(m2) O˜(m) [RST14] Ô(m)\\nby derandomizing\\n[Mad10a]\\n(1 + \\x0f)-\\napproximate spectral\\nsparsifiers\\nO(mn3\\x0f−2)\\n[BSS12, dCSHS16]\\nO˜(m\\x0f−2) [ST11, LS17] Ô(m)\\nCorollary 7.3\\nNote: no(1)-approx.\\nLaplacian solvers O˜(m1.31 log 1\\n\\x0f\\n) [ST03] O˜(m log 1\\n\\x0f\\n)\\n[ST14, CKM+14, KS16]\\nÔ(m log 1\\n\\x0f\\n)\\nCorollary 7.7\\nSingle-source shortest\\npaths with negative\\nweights\\nO˜(m\\n√\\nn) [GT89, Gol95] O˜(m10/7) [CMSV17]3 Ô(m10/7) following\\nCorollary 7.7\\nTable 1: Applications of our results to static graph problems. As usual, n and m denote the number of\\nnodes and edges of the input graph, respectively. We use O˜ and Ô notation to hide polylogn and no(1)\\nfactors respectively. For readability, we assume that the weights and the capacities of edges/nodes are\\npolynomial in the problem size.\\n4\\nProblem Best previous worst-case\\nupdate time:\\ndeterministic\\nBest previous worst-case\\nupdate time: randomized\\nOur results:\\ndeterministic\\nDynamic connectivity O(\\n√\\nn) [Fre85, EGIN97]\\nO(\\n√\\nn · log logn√\\nlogn\\n)\\n[KKPT16]\\nO(log4 n)\\n[KKM13, GKKT15]\\nÔ(1)\\nCorollary 7.2\\nDynamic Minimum\\nSpanning Forest\\nO(\\n√\\nn) [Fre85, EGIN97] Ô(1) [NSW17] Ô(1)\\nCorollary 7.2\\nTable 2: Applications of our results to dynamic graph problems. As before, n and m denote the number\\nof nodes and edges of the input graph, respectively. We use O˜ and Ô notation to hide polylogn and no(1)\\nfactors respectively. For readability, we assume that the weights and the capacities of edges/nodes are\\npolynomial in problem size.\\namortized update time, that upper-bounds the average time that the algorithm spends on each update,\\nand worst-case update time, that upper-bounds the largest amount of time that the algorithm ever spends\\non a single update.\\nThe first non-trivial bound for Dynamic Connectivity problem dates back to Frederickson’s work from\\n1985 [Fre85], that provided a deterministic algorithm with O(\\n√\\nm) worst-case update time. Combining\\nthis algorithm with the sparsification technique of Eppstein et al. [EGIN97] provides a deterministic\\nalgorithm for Dynamic Connectivity with O(\\n√\\nn) worst-case update time. Improving and refining this\\nbound has been an active research direction in the past three decades, but unfortunately, practically all\\nfollow-up results require either randomization or amortization:\\n1. (Amortized & Randomized) In their 1995 breakthrough, Henzinger and King [HK99] greatly\\nimprove the O(\\n√\\nn) worst-case update bound with a randomized Las Vegas algorithm, whose ex-\\npected amortized update is poly log(n). This result has been subsequently improved, and current\\nbest randomized algorithms have amortized update time that almost matches existing lower bounds,\\nto within O((log log n)2) factors; see, e.g., [HHKP17, Tho00, HT97, PD06].\\n2. (Amortized & Deterministic) Henzinger and King’s 1997 deterministic algorithm [HK97] achieves\\nan amortized update time of O(n1/3 log n). This was later substantially improved to O(log2 n) amor-\\ntized update time by the deterministic algorithm of Holm, de Lichtenberg, and Thorup [HdLT01],\\nand later to O(log2(n)/ log log n) by Wulff-Nilsen [Wul13].\\n3. (Worst-Case & Randomized) The first improvement over the O(\\n√\\nn) worst-case update bound\\nwas due to Kapron, King and Mountjoy [KKM13], who provided a randomized Monte Carlo algo-\\nrithm with worst-case update time O(log5 n). This bound was later improved to O(log4 n) by Gibb\\net al. [GKKT15]. Subsequently, Nanongkai, Saranurak, and Wulff-Nilsen [NSW17, Wul17, NS17]\\npresented a Las Vegas algorithm for the more general dynamic MSF problem with no(1) worst-case\\nupdate time.\\nA major open problem that was raised repeatedly (see, e.g., [KKM13, PT07, KKPT16, Kin16,\\nKin08, HdLT01, Wul17]) is: Can we achieve an O(n1/2−\\x0f) worst-case update time with a determinis-\\ntic algorithm? The only progress so far on this question is the deterministic algorithm of Kejlberg-\\nRasmussen et al. [KKPT16], that slightly improves the O(\\n√\\nn) bound to O(\\n√\\nn(log log n)2/ log n) using\\nword-parallelism. In this paper, we resolve this question, and provide a somewhat stronger result, that\\nholds for the more general dynamic MSF problem:\\nTheorem 1.4. There are deterministic algorithms for Dynamic Connectivity and dynamic MSF, with\\nno(1) worst-case update time.\\nThis result is obtained by replacing one component of the algorithm of Nanongkai, Saranurak, and\\nWulff-Nilsen [NSW17] for dynamic MSF with a new algorithm: the algorithm in [NSW17] is randomized\\n5\\nonly because it needs to quickly compute an expander decomposition. In [NSW17], this was done via a\\nrandomized algorithm. Since our results allow us to do this deterministically, we achieve the same no(1)\\nworst-case update time as in [NSW17] via a deterministic algorithm.\\n1.3 Techniques\\nOur algorithm is based on the cut-matching game framework introduced by Khandekar, Rao and Vazirani\\n[KRV09], that has been used in numerous algorithms for computing sparse cuts [NS17, SW19, GLN+]\\nand beyond (e.g. [CC13, RST14, CC16, CL16]). Intuitively, the cut-matching game consists of two\\nalgorithms: one algorithm, called the cut player, needs to compute balanced cuts of a given graph with\\nsome useful properties, and the second algorithm, called the matching player, needs to solve a single-\\ncommodity maximum flow / minimum cut problem. A combination of these two algorithms is then used\\nin order to compute a sparse cut in the input graph, or to certify that no such cut exists. Unfortunately,\\nall current algorithms for the cut player are randomized. In order to obtain a deterministic algorithm,\\nwe use a recursive strategy. The key idea of our algorithm is to play many small cut-matching games\\nsimultaneously, which results in the matching player having to solve a somewhat harder problem, similar\\nto multi-commodity flow. We implement the cut player algorithm by recursively solving the balanced\\nsparse cut problem on smaller and smaller graphs. We now provide more details on the cut-matching\\ngame and of our implementation of it. We start by describing an algorithm for a somewhat weaker variant\\nof Theorem 1.2, where φU , φL ≥ 1/no(1). We discuss its extension to the full range of parameters φU , φL\\nlater.\\nOverview of the Cut-Matching Game. We start by a high-level overview of a variant of the cut-\\nmatching game, due to Khandekar et al. [KKOV07]. We say that a graph W is an expander if it has no\\ncut of conductance less than 1/no(1). Given a graph G = (V,E), the goal of the cut-matching game is to\\neither find a balanced and sparse cut in G, or to embed an expander W = (V,E′) (called a witness) into\\nG. Note that W and G are defined over the same vertex set. The embedding of W into G needs to map\\nevery edge e of W to a path Pe in G connecting the endpoints of e. The congestion of this embedding\\nis the maximum number of paths in {Pe | e ∈ E(W )} that share a single edge of G. We require that\\nthe congestion of the resulting embedding is low. (In fact the embedding that we use maps all but a few\\nedges of W to paths in G; we omit this detail to simplify the discussion.) Such an embedding serves as a\\ncertificate that there is no balanced sparse cut in G. This follows from the fact that, if W is an expander\\ngraph, and it has a low-congestion embedding into another graph G, then G itself is also an expander\\n(with a possibly weaker conductance that depends on the conductance of W and on the congestion of\\nthe embedding).\\nThe algorithm proceeds via an interaction between two algorithms, the cut player, and the matching\\nplayer, over O(log n) rounds.\\nAt the beginning of every round, we are given a graph W whose vertex set is V , and its embedding\\ninto G; at the beginning of the first round, W contains the set V of vertices and no edges. In every round,\\nthe cut player either\\n(C1) “cuts W”, by finding a balanced sparse cut S in W ; or\\n(C2) “certifies W” by announcing that W is an expander.\\nIf W is certified (Item (C2)), then we have constructed the desired embedding of an expander into G, so\\nwe can terminate the algorithm and certify that G has no balanced sparse cut. If a cut S is found in W\\n(Item (C1)), then we invoke the matching player, who either\\n(M1) “matches W”, by adding to W a large matching M ⊆ S× (V \\\\S) that can be embedded into G; or\\n(M2) “cuts G”, by finding a balanced sparse cut T in G (the cut T is intuitively what prevents the\\nmatching player from embedding any large matching M ⊆ S × (V \\\\ S) into G).\\n6\\nIf a sparse balanced cut T is found in graph G (Item (M2)), then we return this cut and terminate\\nthe algorithm. Otherwise, the game continues to the next round. It was shown in [KKOV07] that the\\nalgorithm must terminate after Θ(log n) rounds.\\nIn the original cut-matching game by Khandekar, Rao and Vazirani [KRV09], the matching player was\\nimplemented by an algorithm that computes a single-commodity maximum flow / minimum cut. The\\nalgorithm for the cut player was defined somewhat differently, in that in the case of Item (C1), the cut\\nthat it produced was not necessarily sparse, but it still had some useful properties that guaranteed that\\nthe algorithm terminates after O(log2 n) iterations. In order to implement the cut player, the algorithm\\n(implicitly) considers n vectors of dimension n each, that represent the probability distributions of random\\nwalks on the witness graph, starting from different vertices of G, and then uses a random projection of\\nthese vectors in order to construct the partition. The algorithm exploits the properties of the witness\\ngraph in order to compute these projections efficiently, without explicitly constructing these vectors,\\nwhich would be too time consuming. Previous work (see, e.g., [SW19, CK19]) implies that one can use\\nalgorithms for computing maximal flows instead of maximum flows in order to implement the matching\\nplayer in near-linear time deterministically, if the target parameters φU , φL ≥ 1/no(1). This still left open\\nthe question: Can we implement the cut player deterministically and efficiently?\\nA natural strategy for derandomizing the algorithm of [KRV09] for the cut player is to avoid the\\nrandom projection of the vectors. In a previous work of a subset of the authors with Yingchareontha-\\nwornchai [GLN+], this idea was used to develop a fast PageRank-based algorithm for the cut player, that\\ncan be viewed as a derandomization of the algorithm of Andersen, Chung and Lang for balanced sparse\\ncut [ACL07]. Unfortunately, it appears that this bound cannot lead to an algorithm whose running time\\nis below Θ(n2): if we cannot use random projections, we need to deal with n vectors of dimension n each\\nwhen implementing the cut player, and so the running time of Ω(n2) seems inevitable. In this paper, we\\nimplement the cut player in a completely different way from the previously used approaches, by solving\\nthe balanced sparse cut problem recursively.\\nWe start by observing that, in order to implement the cut player via the approach of [KKOV07], it is\\nsufficient to provide an algorithm for computing a balanced sparse cut on the witness graph W ; in fact,\\nit is not hard to see that it is sufficient to solve this problem approximately. However, this leads us to\\na chicken-and-egg situation, where, in order to solve the Minimum Balanced Cut problem on the input\\ngraph G, we need to solve the Minimum Balanced Cut problem on the witness graph W . While graph W\\nis guaranteed to be quite sparse, it is not clear that solving the Minimum Balanced Cut problem on this\\ngraph is much easier.\\nThis motivates our recursive approach, in which, in order to solve the Minimum Balanced Cut problem\\non the input graph G, we run a large number of cut-matching games in it simultaneously, each of which\\nhas a witness graph containing significantly fewer vertices. It is then sufficient to solve the Minimum\\nBalanced Cut problem on each of the resulting, much smaller, witness graphs. The key to the analysis\\nis to control the sizes of all graphs that we obtain at every level of the recursion, in order to bound the\\ntotal running time.\\nThis general recursive approach is not new and was used before, e.g., in Madry’s construction of\\nj-trees [Mad10a], and in the recursive construction of short cycle decompositions [CGP+18, LSY19].\\nIn fact, [GLN+] use Madry’s j-trees to solve Minimum Balanced Cut by running cut-matching games\\non graphs containing fewer and fewer nodes, obtaining an (m1.5+o(1))-time algorithm. Unfortunately,\\nimproving this bound further does not seem viable via this approach, since the total number of edges\\ncontained in the graphs that belong to higher recursive levels is very large. Specifically, assume that we\\nare given an n-node graph G with m edges, together with a parameter k ≥ 1. We can then use the j-trees\\nin order to reduce the problem of computing Minimum Balanced Cut on G to the problem of computing\\nMinimum Balanced Cut on k graphs, each of which contains roughly n/k nodes. Unfortunately, each of\\nthese graphs may have Ω(m) edges. Therefore, the total number of edges in all resulting graphs may\\nbe as large as Ω(mk), which is one of the major obstacles to obtaining faster algorithms for Minimum\\nBalanced Cut using this approach.\\n7\\nNew Recursive Strategy. First, we partition the nodes of the input graphG into k subsets V1, V2, . . . , Vk\\nof roughly equal cardinality, for a large enough parameter k (we use k = no(1)). Intuitively, instead of\\nembedding an expander W into the input graph G as in the original cut-matching game, we rely on a\\nstructural lemma, that roughly states that G is an expander if and only if\\n(A) we can embed a graph W into G, where W is a union of node-disjoint expanders Wi = (Vi, Ei) for\\n1 ≤ i ≤ k; and\\n(B) the graph G′, obtained by contracting, for all 1 ≤ i ≤ k, all nodes of Vi into a single node vi, is an\\nexpander.\\n(see Section 5.1 for a more precise and detailed statement.) Since graph G′ only contains k nodes, we\\ncan efficiently verify whether Item (B) holds, or obtain a sparse balanced cut of G′ and hence of G. In\\norder to efficiently compute an embedding of the expanders Wi into G, as required by Item (A), we use\\nthe following modification of the cut-matching game. Each round of our cut-matching game starts with\\na collection {W1, . . . ,Wk} of witness graphs, that are embedded into G via a low-congestion embedding.\\nInitially, each graph Wi contains the set Vi of vertices and no edges. We apply the algorithm of the cut\\nplayer to each of the graphs Wi. As before, for each such witness graph Wi, the cut player either:\\n(C1′) “cuts Wi” by finding a balanced sparse cut Si in Wi, or\\n(C2′) “certifies Wi” by announcing that Wi is an expander (in this case we return Si = ∅).\\nIf every witness graph Wi is certified, then we can terminate the algorithm, and obtain the embeddings\\nrequired by Item (A). Otherwise, we invoke a new algorithm, that we call a multi-matching player, who\\neither\\n(M1′) “matches the witness graphs” by computing, for each 1 ≤ i ≤ k, a matching Mi ⊆ (Si × (V \\\\ Si))\\nthat is added to Wi, such that all edges in M =\\n⋃\\niMi that can be embedded into G with low\\ncongestion, and |M | is large; or\\n(M2′) “cuts G” by finding a balanced sparse cut T in G.\\nNote that the multi-matching player differs from the standard matching player in that in Item (M1′) we\\nneed to compute k different matchings between k different pre-specified pairs of vertex subsets. Observe\\nthat the original cut-matching game is a special case of our cut-matching game where k = 1. Just like in\\nthe original cut matching game, if a cut T in G is found as in Item (M2′), then we return it as a balanced\\nsparse cut in G and terminate the algorithm. Otherwise, the game continues to the next round. Like\\nin the original cut-matching game, after Θ(log n) rounds the algorithm is guaranteed to terminate with\\neither a balanced cut in G, or with the cut player certifying every witness graph Wi.\\nCut Player. Our algorithm for the cut player simply runs the same procedure recursively on each of\\nthe witness graphs Wi. Therefore, for each such graph Wi, at the next recursive level, we will construct k\\nnew witness graphs Wi,1, . . .Wi,k, each of which only contains O(n/k\\n2) vertices. While the total number\\nof edges in all witness graphs grows by a factor of poly log(n) at each recursive level, by setting k to be\\nlarge enough (k = no(1) suffices), we can ensure that the recursion terminates after relatively few levels,\\nand that the total number of edges in all resulting witness graphs is suitably bounded. This ensures that\\nwe can implement the algorithm for the cut player in almost-linear time.\\nMulti-Matching Player. Readers who are familiar with the previous implementation of the matching\\nplayer based on computing maximum single-commodity flows may wonder whether we need to compute\\nk-commodity flows here. This would hurt the running time since we are not aware of an almost-linear\\ntime deterministic algorithm for this problem. Fortunately, Chuzhoy and Khanna [CK19] recently showed\\nan algorithm that implements the matching player (for the case where k = 1) by repeatedly computing\\na maximal (as oppose to maximum) set of short edge-disjoint paths in G, which can be done quickly via\\nEven-Shiloach’s algorithm for decremental single-source shortest paths [ES81]. Their approach can be\\nseamlessly extended to implement our multi-matching player efficiently.\\n8\\nSummary. To conclude, our algorithm (i) partitions the nodes in G into k sets {Vi}1≤i≤k, (ii) im-\\nplements the cut player by creating k witness graphs and then computing the Minimum Balanced Cut\\nproblem recursively in each of these witness graphs, and (iii) implements the multi-matching player fol-\\nlowing the ideas from [CK19]. Additionally, it needs to embed another witness graph into the graph G′\\nthat is obtained from G by contracting each set Vi of vertices into a single node; for this part we exploit\\nthe fact that G′ has very few vertices. The algorithm that we have described so far only proves a weaker\\nversion of Theorem 1.2, where the parameters φU , φL are relatively large (say at least 1/n\\no(1)). This is\\nbecause our algorithm for the multi-matching player only works in this regime. However, this is already\\nsufficient for all our applications, including for an algorithm for approximate maximum flow. The latter\\nalgorithm can then in turn be used in order to implement the multi-matching player for the full range of\\nthe parameters φU , φL required in Theorem 1.2, completing the proof of Theorem 1.2.\\n1.4 Paper Organization\\nWe start with preliminaries in Section 2, where, besides providing basic definitions, we prove a lemma\\nthat allows us to assume that the input graph has constant-degree. In Section 3, we define the problem\\nto be solved by the multi-matching player, and provide an algorithm for solving it. We also provide an\\nimproved algorithm the case where k = 1 (that is, the problem of the standard matching player), which\\nwe exploit later. In Section 4 and Section 5 we define several variants of the BalCutPrune problem\\nand provide reductions between them. In Section 6, we combine all these reductions together to obtain\\na recursive algorithm for Theorem 1.2, for the case where the parameters φL, φU are sufficiently large.\\nIn Section 7, we use our result from Section 6 to obtain algorithms for all our applications. Finally, in\\nSection 8 we prove Theorem 1.2 for the full range of parameters φL, φU , by building on the deterministic\\nalgorithm for approximate maximum flow presented in Section 7. We conclude with open problems in\\nSection 9.\\n2 Preliminaries\\nIn this paper, all graphs are undirected unweighted multi-graphs that may have self loops. Let G = (V,E)\\nbe a graph. Let V (G) = V and E(G) = E be a set of nodes and edges of G respectively. We emphasize\\nthat each self loop at a node u contributes 1 to its degree, denoted by degG u. For any A,B ⊆ V , let\\nEG(A,B) denote the set of edges between A and B. For any set S ⊆ V , the volume of S is the sum of\\ndegrees of nodes inside S and is denoted by volG(S) =\\n∑\\nu∈S degG u. We write volG := volG(V ) = 2|E| .\\nLet G[S] denote the subgraph of G induced by the set S. We also write G−S = G[V −S]. We define\\nG{S} to be the induced subgraph with degree preserved, i.e. we add self loops to each node in G{S} so\\nthat degG{S} u = degG u for all u ∈ S. Let G/S be the (multi)-graph G after contracting S where we\\nkeep self-loops.\\nWe write S = V − S. We call a partition (S, S) of V where ∅ 6= S ⊂ V a cut. We sometimes refer to\\nthe set S itself as a cut. The size of a cut S is denoted by δG(S) = |EG(S, S)|. A cut S is β-balanced\\nwhere β ≤ 1/2 if βvolG ≤ volG(S) ≤ (1− β)volG and it is β-vertex balanced if β|V | ≤ |S| ≤ (1− β)|V |.\\nWe will also use TAlgo(·) to denote the running times of procedure Algo with certain input param-\\neters. In particular, the running time of CutPrune defined above in Definition 1.1 can be written as\\nTBalCutPrune(m,φU , φL). For this paper, all running times we state are worst-case and deterministic. In\\nparticular, our main result as stated in Theorem 1.2 is essentially:\\nTBalCutPrune(m,φL, φU ) ≤ m1+o(1) for φUφL ≥ 2O(log\\n2/3 n·(log logn)1/3).\\n2.1 Conductance and Sparsity\\nThe two important notions about cuts in this paper are conductance and sparsity both of which measure\\nhow much a cut “expands”. The conductance of a cut S is denoted by ΦG(S) = δG(S)/min{volG(S), volG(S)}\\n9\\nwhich is a portion of edges incident to S that has endpoints outside S. The conductance of a graph G is the\\nminimum conductance of cuts in G and is denoted by ΦG = min∅6=S(V ΦG(S) . Similarly, the sparsity of a\\ncut S is denoted by σG(S) = δG(S)/min{|S|, |S|} and the sparsity of a graph G is σG = min∅6=S(V σG(S).\\nInformally, we usually say that cuts with high conductance or sparsity are expanding and cuts with\\nlow conductance/sparsity are sparse. Graphs with no sparse cuts are expanders. Below, we state a basic\\nrelation between conductance and sparsity.\\nProposition 2.1 (Conductance vs. sparsity). Let G = (V,E) be a connected graph with maximum degree\\n∆. For any set S ⊂ V ,\\nσG(S)/∆ ≤ ΦG(S) ≤ σG(S).\\nProof. This follows from the fact that |S| ≤ volG(S) ≤ ∆|S| for any S ⊆ V .\\nThroughout the paper, we always work with graphs whose degrees are almost constant as will be\\njustified in Section 2.2. So, by Proposition 2.1, the values of conductance and sparsity are almost the\\nsame. We suggest that readers should interpret them as similar values. The reason we work with both\\nnotions is that conductance is more natural than sparsity in some technical arguments, and vice versa.\\nThe statement below roughly says that suppose we find a sparse cut S, then remove S from the graph,\\nand find another sparse cut T in the remaining graph. Then, the union S ∪ T is also a sparse cut.\\nProposition 2.2 (Union of sparse cuts is sparse). We have the following:\\n1. Let S ⊆ V where δG(S)volG(S) ≤ φ. Let T ⊆ V − S where\\nδG{V−S}(T )\\nvolG{V−S}(T )\\n≤ φ. Then δG(S∪T )volG(S∪T ) ≤ φ. In\\nparticular, if volG(S ∪ T ) ≤ volG(V )/2, then ΦG(S ∪ T ) ≤ φ .\\n2. Let S ⊆ V where δG(S)|S| ≤ φ. Let T ⊆ V − S where\\nδG{V−S}(T )\\n|T | ≤ φ. Then δG(S∪T )|S∪T | ≤ φ. In\\nparticular, if |S ∪ T | ≤ |V |/2, then σG(S ∪ T ) ≤ φ.\\nProof. We have volG(S ∪ T ) = volG(S) + volG(T ) = volG(S) + volG{V−S}(T ). Also, δG(S ∪ T ) ≤\\nδG(S) + δG{V−S}(T ). So\\nδG(S∪T )\\nvolG(S∪T ) ≤ max{\\nδG(S)\\nvolG(S)\\n,\\nδG{V−S}(T )\\nvolG{V−S}(T )\\n} ≤ φ. The proof for the other statement\\nis analogous.\\n2.2 Reduction to Constant-Degree Graphs\\nIn this section, we show that graphs with bounded degrees are in fact universal for the problem of finding\\na balanced sparse cut. Specifically, we show that high degree vertices can be replaced by constant degree\\nexpanders in a way that preserves the solutions.\\nLemma 2.3 (Suffice to find balanced cuts on constant-degree graphs). There is an algorithm that, given\\nan m-edge graph G = (V,E), in time O(m) outputs a graph G′ = (V ′, E′) with the following properties.\\n1. G′ has O(m) vertices with maximum degree 20.\\n2. ΦG′ = Θ(ΦG).\\nMoreover, given a β-balanced cut S in G′ where ΦG′(S) ≤ \\x0f for some small enough constant \\x0f < 1, then\\nwe can obtain in O(m) time a Ω(β)-balanced cut T in G where ΦG(T ) = O(ΦG′(S)).\\nTo prove Lemma 2.3, we use the notions of inner and outer conductance w.r.t. a partition of vertices\\ndefined below. These notions will be used in the analysis of our main algorithm as well. Let V =\\n{V1, . . . Vk} be a partition of V . We say that a cut S respects V if for each i, either Vi ⊆ S or Vi ∩ S = ∅\\n(i.e. no overlapping). Let ΦoutG,V = minS respects V ΦG(S) be the outer conductance of G w.r.t. V. Let\\nΦinG,V = mini ΦG[Vi] be the inner conductance of G w.r.t. V. We say that Vi is a clump if, for each u ∈ Vi,\\ndegG[Vi](u) ≥ degG(u)/10.4 In particular, for every S ⊂ Vi, we have volG[Vi](S) = Θ(volG(S)).\\n4The constant 10 is arbitrary.\\n10\\nLemma 2.4. Suppose that V = {V1, . . . Vk} is a partition of V where each Vi is a clump. We have\\n1. ΦG = Ω(Φ\\nout\\nG,V · ΦinG,V);\\nand\\n2. Given a β-balanced cut S where ΦG(S) ≤ \\x0fΦinG,V\\nfor some small enough constant \\x0f < 1, then we can obtain in O(m) time a Ω(β)-balanced cut T\\nrespecting V where ΦG(T ) = O(ΦG(S)/ΦinG,V).\\nThe idea of the proof of Item 2 of Lemma 2.4 is simple. Roughly, for each part Vi, if S overlaps with Vi\\nmore than half of volume of Vi, then we include Vi into S, otherwise we remove Vi from S. The resulting\\ncut T then respects V, and we lose at most ΦinG,V factor in the conductance because each ΦG[Vi] ≥ ΦinG,V\\n. Applying this argument to each cut gives us Item 1 of Lemma 2.4. To make this idea formal, the proof\\nis a bit tedious and so we defer it to Appendix A.1.\\nThe rest of this section is for proving Lemma 2.3. We first define the notion of expander split.\\nDefinition 2.5 (Expander Split). Let G = (V,E) be a graph. The expander split graph G′ of G is\\nobtained from G by the following operations.\\n• For each node u ∈ V , we replace u by a constant-degree expander Xu with deg(u) nodes. We call\\nXu a super-node in G\\n′.\\n• Let Eu = {eu,1, . . . , eu,deg(u)} denote the set of edges in G incident to u. For each e = (u, v), if\\ne = eu,i = ev,j , we add an edge between the i-th node of Xu and the j-th node of Xv.\\nWe note that an expander split of any graph can be efficiently computed because we can explicitly\\nconstruct an expander fast:\\nFact 2.6 (Fast explicit expanders). Given any number n, there is a deterministic algorithm with running\\ntime O(n) that constructs a graph Hn with n vertices such that each vertex has degree at most 16, and\\nthe conductance ΦHn = Ω(1).\\nProof. We assume that n ≥ 10, otherwise Hn can be constructed in constant time. The expander\\nconstruction by Margulis, Gabber and Galil is as follows. For any number k, the graph H ′k2 is defined\\nover a vertex set Zk ×Zk where Zk = Z/kZ. For each vertex (x, y) ∈ Zk ×Zk, its eight adjacent vertices\\nare (x± 2y, y), (x± (2y + 1), y), (x, y ± 2x), (x, y ± (2x+ 1)). In [GG81], it is shown that ΦH′\\nk2\\n= Ω(1).\\nLet k be such that (k − 1)2 < n ≤ k2. As n ≥ 10, so k ≥ 4, and so (k − 1)2 ≥ k2/2. So we can\\ncontract disjoint pairs of vertices in H ′k2 and obtain a graph Hn with n nodes where each node has degree\\nbetween 8 and 16. Note that ΦHn ≥ ΦH′\\nk2\\n. It is clear that the construction takes O(n) time.\\nFinally, we are ready to prove Lemma 2.3.\\nProof of Lemma 2.3. For 1), this follows immediately from the definition of expander split. For 2),\\nΦG′ ≤ ΦG simply because we can convert any cut S in G to a cut S′ in G′ with the same conductance\\n(i.e. for every u ∈ S, add nodes in Xu to S′). To show that ΦG′ = Ω(ΦG) , let V = {Xu}u∈V be the\\npartition of nodes in G′, where Xu is defined as in Definition 2.5. For any cut S′ in G′ respecting V, there is\\na corresponding cut S in G. Note that δG′(S\\n′) = δG(S) and volG′(S′) = Θ(volG(S)). So ΦoutG′,V = Θ(ΦG).\\nBy Fact 2.6, ΦinG′,V = Θ(1). Note that each node u in G\\n′ is such that degG′(u) = Θ(degXu(u)). In\\nparticular, each super-node Xu is a clump in G\\n′. By Lemma 2.4 we have ΦG′ = Ω(ΦoutG′,VΦ\\nin\\nG,V) = Ω(ΦG).\\nThe running time follows from Fact 2.6. The “moreover” part follows from Item 2 of Lemma 2.4.\\n11\\n3 Matching Player’s Problem and Algorithms\\nIn this section, we give the definition of the CutMatch problem that the matching player in our (variant\\nof) cut-matching game needs to solve. The matching player in previous works that are based on the cut-\\nmatching game [KRV09, KKOV07, OSVV08, SW19] also implicitly solves this problem but only when\\nk = 1:\\nDefinition 3.1 (CutMatch). In theCutMatch problem, the input is (G,φ, cong, β, {A1, B1, . . . , Ak, Bk})\\nwhere G = (V,E) is an n-node graph with maximum degree at most 20, φ ∈ [0, 1] is a conductance pa-\\nrameter, cong ≥ 1 is a congestion parameter, β ∈ [0, 1] is a balance parameter, and A1, B1, . . . , Ak, Bk\\nare disjoint subsets of V where |Ai| = |Bi| for each i, and T = A1 ∪ B1 ∪ · · · ∪ Ak ∪ Bk is called the set\\nof terminals, and returns either\\n1. (Cut): a cut S where βvolG/160 ≤ volG(S) ≤ volG/2 and ΦG(S) ≤ φ , or\\n2. (Match): a collection P of paths connecting u ∈ Ai to v ∈ Bi such that each terminal is an\\nendpoint of exactly one path, except at most βn many terminals which are not an endpoint of any\\npath, and every edge is contained in at most cong paths.\\nFor 1 ≤ i ≤ k, let Mi = {(u, v) | u ∈ Ai and v ∈ Bi are the two endpoint of a path in P}. Note that Mi\\nis a matching (which might not be a subgraph of G) and |T − ∪ki=1V (Mi)| ≤ βn. When a CutMatch\\nalgorithm returns Item 2, it is required to also return M1, . . . ,Mk. Let TCutMatch(n, k, φ, cong, β) denote\\nthe worst-case time required for deterministically solving CutMatch with the above parameters.\\nIt will be clear later that our algorithms for CutMatch can be extended to work with general\\ngraphs, instead of only constant degree graphs. However, this does not fit quite well with the rest of our\\nframework. Therefore, Definition 3.1 is stated only for constant degree graphs. Also note that cong can\\nbe thought of as a congestion guarantee that is different for each CutMatch algorithm. We treat it as\\nan input here, since this makes results more convenient to state and use.\\nThe main goal of this section is to show two algorithms for solving the CutMatch problem. The\\nfirst one works for any parameter k ≥ 1 which is crucial in our main recursive algorithm in Section 5.\\nTheorem 3.2 (ES-CutMatch Algorithm). Let cmatch be a large enough constant. There is an algorithm\\ncalled ES-CutMatch for solving the CutMatch problem that, for any (n, k, φ, cong, β) where\\ncong ≥ cmatchφ−2 log2 n,\\nhas running time TCutMatch(n, k, φ, cong, β) = O(nk(log\\n3 n)/φ3).\\nWhen k = 1, we also show an algorithm which is faster and has less restricted lower bound on cong.\\nTo obtain our main result in Theorem 6.1, using the following Push-CutMatch algorithm instead of\\nES-CutMatch improves both the approximation and running time.\\nTheorem 3.3 (Push-CutMatch Algorithm). There is an algorithm called Push-CutMatch for solv-\\ning the CutMatch problem that, for any (n, k, φ, cong, β) where k = 1 and\\ncong ≥ 4φ−1\\nhas running time TCutMatch(n, 1, φ, cong, β) = O(nφ\\n−1 log n).\\n3.1 An Algorithm for k ≥ 1 (Proof of Theorem 3.2)\\nIn section, we prove Theorem 3.2. The idea of the proof is based on using Even-Shiloach trees (ES-trees)\\n[ES81] for either finding a sparse cut or listing many disjoint paths connecting the sets Ai and Bi of\\nterminals. This idea was used first by Chuzhoy and Khanna [CK19]. We slightly extend it to work with\\nk pairs of terminal sets {(A1, B1), . . . (Ak, Bk)} by working with k many ES-trees.\\n12\\nBefore describing the ES-CutMatch algorithm itself, we start with the following simple algorithm\\nfor finding sparse cuts that separates two sets A and B that are far away. For any two sets A,B ⊂ V in\\nG = (V,E), we denote by distG(A,B) the minimum distance between any two nodes u ∈ A and v ∈ B.\\nLemma 3.4 (SingleABCut Algorithm). There is an algorithm called SingleABCut that, given\\n(G, (A,B)) where G = (V,E) is an m-edge graph and A,B ⊂ V are two disjoint subsets, returns in\\nO(m) time a cut S where ΦG(S) ≤ 10 logmdistG(A,B) , volG(S) ≤ volG/2, and S separates A from B (i.e. either\\n(A ⊆ S and B ∩ S = ∅) or (B ⊆ S and A ∩ S = ∅)).\\nProof. For any set X ⊂ V and a number r, let Ball(X, r) denote the set of all nodes within distance\\nat most r from X. Let L = distG(A,B). We can assume that L ≥ 10 logm; otherwise the lemma holds\\ntrivially. Observe that Ball(A,L/3) and Ball(B,L/3) are disjoint. The algorithm is to simply perform\\nbreadth-first search from A and B and compute Ball(A, r) and Ball(B, r) for all r ≤ L/3. This can\\nbe done in O(m) time.\\nWe assume w.l.o.g. that volG(Ball(A,L/3)) ≤ volG/2. It suffices to prove that there is some i ≤ L/3\\nsuch that ΦG(Ball(A, i)) ≤ 10 logmL . Suppose otherwise for a contradiction. Then, for every i ≤ L/3,\\nvolG(Ball(A, i+ 1)) ≥ (1 + 10 logm\\nL\\n)volG(Ball(A, i)).\\nThis is because the conductance of Ball(A, i) is volG(Ball(A,i+1))−volG(Ball(A,i))volG(Ball(A,i)) , which is assumed to be\\nat least 10 logmL . So\\nvolG(Ball(A,L/3)) ≥ (1 + 10 logm\\nL\\n)L/3 > volG\\nwhich is a contradiction (since L ≥ 10 logm; note that the logarithmic is to base two).\\nNext, we slightly extend SingleABCut to work with k pairs of (A1, B1), . . . , (Ak, Bk). This algorithm\\nreturns a sparse cut if the distance between Ai and Bi is large for every i.\\nLemma 3.5 (ManyABCut Algorithm). There is an algorithm called ManyABCut that, given an\\nm-edge graph G = (V,E) and disjoint sets A1, B1, . . . , Ak, Bk ⊂ V where L = mini{distG(Ai, Bi)}\\nand ν =\\n∑k\\ni=1 min{volG(Ai), volG(Bi)}, returns in O(km) time a cut S where ΦG(S) ≤ 30 logmL and\\nν/2 ≤ volG(S) ≤ volG/2.\\nProof. See Algorithm 1 for the description of the algorithm. We assume that L ≥ 30 logm; otherwise the\\nlemma trivially holds. Obviously,\\nvolG(S) ≤ volG/2\\nsince we always return the set of smaller volume among Sˆ and V − Sˆ. Observe that, before the cut S\\nis returned, we have that Sˆ = ∪k′i=1Si where k′ ≤ k. By Lemma 3.4,\\nδGˆi\\n(Si)\\nvolGˆi\\n(Si)\\n≤ 10 logmL where Gˆi is the\\ngraph Gˆ before removing Si. By Proposition 2.2, we have that\\nδG(Sˆ)\\nvolG(Sˆ)\\n≤ 10 logm\\nL\\n.\\nCase 1. If we return S = Sˆ as in Item 3 of Algorithm 1, then we have volG(Sˆ) ≤ volG/2. So\\nΦG(S) =\\nδG(Sˆ)\\nvolG(Sˆ)\\n≤ 10 logmL . Observe we have separated Ai and Bi by removing the set Si returned by\\nLemma 3.4 for all i. So, in the final graph Gˆ, either V (Gˆ) ∩ Ai = ∅ or V (Gˆ) ∩ Bi = ∅ for every i. In\\nother words, either S ⊇ Ai or S ⊇ Bi, for every i. So volG(S) ≥ ν ≥ ν/2.\\nCase 2. If we return S = V − Sˆ as in Item 2d of Algorithm 1, then let S′ = ∪k′−1i=1 Si. We have\\nvolG(S\\n′) ≤ volG/2 and volGˆ(Sk′) ≤ volGˆ/2 = volG(V − S′)/2. So\\nvolG(Sˆ) = volG(S\\n′) + volGˆ(Sk′) ≤ 3volG/4.\\n13\\nAlgorithm 1 ManyABCut(G, {(A1, B1), . . . , (Ak, Bk)}) // For Lemma 3.5\\nInput: An m-edge graph G = (V,E) and disjoint sets A1, B1, . . . , Ak, Bk ⊂ V .\\nOutput: A cut S such that ν/2 ≤ volG(S) ≤ volG/2 and ΦG(S) ≤ 30 logmL where L = mini{distG(Ai, Bi)}\\nand ν =\\n∑k\\ni=1 min{volG(Ai), volG(Bi)}.\\nAlgorithm:\\n1. Initialize Sˆ ← ∅, i← 1 and Gˆ← G.\\n2. For i = 1, . . . , k:\\n(a) Si ← SingleABCut(Gˆ, (V (Gˆ) ∩Ai, V (Gˆ) ∩Bi) (by Lemma 3.4).\\n(b) Gˆ← Gˆ{V − Si}.\\n(c) Sˆ ← Sˆ ∪ Si.\\n(d) If volG(S) ≥ volG/2, return S = V − Sˆ.\\n3. Return S = Sˆ.\\n(The above use the fact that modifying G to G{V − Sˆ} does not change nodes’ degrees.) It follows that\\nvolG(S) ≥ volG/4 ≥ ν/2, and volG(Sˆ) ≤ 3volG(S). The latter implies that\\nΦG(S) ≤ 3 · δG(S)\\nvolG(S)\\n≤ 30 logm\\nL\\n.\\nFinally, we describe the ES-CutMatch algorithm for CutMatch in Algorithm 2. We show the\\ncorrectness and analyze the running time below.\\nCorrectness of ES-CutMatch. First, observe that there are at most\\nr = O\\n(\\nL log n\\nφ\\n)\\n= O\\n(\\nlog2 n\\nφ2\\n)\\niterations in the repeat loop of Algorithm 2. This is because as long as the condition on Item 3c in\\nAlgorithm 2 is not true, we remove at least Ω(φ/L) fraction of nodes from Tˆ in each iteration. Now, we\\nprove the output satisfies the conditions of the problem.\\nCase 1 Suppose that a collection P of paths is returned (from Item 4). It is clear that each terminal\\nis an endpoint of at most one path P , because when P is added into P we immediately remove the\\nendpoints of P from Tˆ . As we have |Tˆ | < βn, this just means there are at most βn many terminals\\nwhich are not an endpoint of any path. Also every edge is contained in at most r = O( log\\n2 n\\nφ2 ) ≤ cong\\npaths, because all paths added to P in the same iterations are disjoint and cong is required to be at least\\ncmatch\\nlog2 n\\nφ2 for a large enough constant cmatch. Thus, P satisfies the output conditions.\\nCase 2 Suppose that a cut S is returned (from Item 3(c)ii). Let Gˆ be the graph before ManyABCut\\nis called and Sˆ be the cut returned. We will be done after showing that\\nβvolG/160 ≤ volG(S) ≤ volG/2 and ΦG(S) ≤ φ. (1)\\nFirst inequality of (1) By Lemma 3.5 we have\\nvolGˆ/2 ≥ volGˆ(Sˆ) ≥ ν/2, (2)\\nwhere ν =\\n∑k\\ni=1 min{volGˆ(Aˆi), volGˆ(Bˆi)}. It follows from the first inequality in Equation (2) that\\nvolGˆ(Sˆ) ≤ volGˆ/2 ≤ volGˆ(V − Sˆ). (3)\\n14\\nAlgorithm 2 ES-CutMatch(G,φ, cong, β, {A1, B1, . . . , Ak, Bk}) //For Definition 3.1 and Theorem 3.2\\nThe input and output are as defined in Definition 3.1, namely:\\nInput: An n-node graph G with maximum degree 20, a conductance parameter φ ∈ [0, 1], a congestion\\nparameter cong ≥ 1, a balance parameter β ∈ [0, 1], and disjoint subsets A1, B1, . . . , Ak, Bk of V .\\nInput Conditions: (i) cong ≥ cmatchφ−2 log2 n for large enough constant cmatch. (ii) ∀i : |Ai| = |Bi|.\\nOutput: Let T = A1 ∪B1 ∪ · · · ∪Ak ∪Bk be the set of terminals. The algorithm returns either\\n1. a cut S where βvolG/160 ≤ volG(S) ≤ volG/2 and ΦG(S) ≤ φ, or\\n2. a collection P of paths connecting u ∈ Ai to v ∈ Bi such that each terminal is an endpoint of\\nexactly one path, except at most βn many terminals which are not an endpoint of any path, and\\nevery edge is contained in at most cong paths.\\nAlgorithm:\\n1. Set the length parameter L← 120φ−1 logm.\\n2. Initialize Aˆi ← Ai, Bˆi ← Bi for all i. Below, we maintain Tˆ = ∪ki=1(Aˆi ∪ Bˆi) as the “current”\\nterminal set.\\n3. Repeat until |Tˆ | < βn:\\n(a) Set Gˆ← G.\\n(b) While there is a path P of length at most L connecting nodes between Aˆi and Bˆi for some i:\\ni. Add P to P.\\nii. Remove the two endpoints of P from Aˆi and Bˆi.\\niii. Delete edges in P from Gˆ.\\n(c) If fewer than φ|Tˆ |/24L paths are added to P in this iteration:\\ni. Sˆ ←ManyABCut(Gˆ, {(Aˆ1, Bˆ1), (Aˆ2, Bˆ2), . . . (Aˆk, Bˆk)}} (by Lemma 3.5).\\nii. Return S = Sˆ if volG(Sˆ) ≤ volG/2, otherwise return S = V − Sˆ.\\n4. Return P.\\n15\\nAdditionally, we can bound ν as follows.\\nν ≥\\nk∑\\ni=1\\nmin{|Aˆi|, |Bˆi|} =\\nk∑\\ni=1\\n|Aˆi| (|Aˆi| = |Bˆi| is maintained)\\n= |Tˆ |/2 ≥ βn/4 (|Tˆ | ≥ βn− φ|Tˆ |/(24L) so |Tˆ | ≥ βn/2) (4)\\n≥ βvolG/80 (G has maximum degree 20). (5)\\nIf the algorithm returns S = Sˆ in Item 3(c)ii, we have\\nvolG(S) = volG(Sˆ) ≥ volGˆ(Sˆ)\\n(2)\\n≥ ν/2\\n(5)\\n≥ βvolG/160.\\nThus, the first inequality of Equation (1) holds. Similarly, if the algorithm returns S = V − Sˆ, we have\\nvolG(S) = volG(V − Sˆ) ≥ volGˆ(V − Sˆ)\\n(3)\\n≥ volGˆ(Sˆ)\\n(2)\\n≥ ν/2\\n(5)\\n≥ βvolG/160.\\nSecond inequality of (1) As we set S = Sˆ if volG(Sˆ) ≤ volG/2 and otherwise set S = V − Sˆ, we have\\nvolG(S) ≤ volG/2.\\nThird inequality of (1) Let P ′ be the set of paths added into P in this iteration. Let E′ = ∪P∈P′P be\\na set of edges in such paths. As the condition in Item 3c in Algorithm 2 holds, we have |P ′| ≤ φ|Tˆ |/(24L);\\nthus\\n|E′| ≤ L · |P ′| < φ|Tˆ |/24\\n(4)\\n≤ φν/12\\n(2)\\n≤ φvolGˆ(Sˆ)/6. (6)\\nNote that\\nΦGˆ(Sˆ) ≤\\n30 logm\\nL\\n(since mini{distGˆ(Aˆi, Bˆi)} > L and by Lemma 3.5)\\n= φ/4. (7)\\nThus\\nδG(Sˆ) ≤ |E′|+ δGˆ(Sˆ)\\n≤ φvolGˆ(Sˆ)/6 + φvolGˆ(Sˆ)/4 (by Equations (6) and (7))\\n≤ φvolGˆ(Sˆ) (8)\\n≤ φvolGˆ(V − Sˆ) (by Equation (3)) (9)\\nThe last two inequality implies that\\nδG(Sˆ)\\n(8),(9)\\n≤ φ ·min{volGˆ(Sˆ), volGˆ(V − Sˆ)} ≤ φ ·min{volG(Sˆ), volG(V − Sˆ)}.\\nSo, ΦG(S) =\\nδG(Sˆ)\\nmin{volG(Sˆ),volG(V−Sˆ)} ≤ φ. This completes the correctness.\\nRunning time of ES-CutMatch. We start with the following key lemma for the running time.\\nLemma 3.6. Item 3b of Algorithm 2 can be implemented in O(kmL) time.\\nProof. For each 1 ≤ i ≤ k, we create an auxiliary graph Gˆi from Gˆ as follows. We add a source node si\\nand edges (si, a) for all a ∈ Aˆi and we add a sink node ti into Gˆ and edges (b, ti) for all b ∈ Bˆi. Whenever\\nGˆ, Aˆi, Bˆi are updated, we will update Gˆi accordingly. Observe that Gˆi will undergo only edge deletions.\\nNow, we initialize the following Even-Shiloach Tree (ES-tree) data structure.\\n16\\nLemma 3.7 ([ES81]). There is a deterministic data structure called ES-tree that, given an unweighted\\nundirected graph G undergoing edge deletions, a root node s, and a depth parameter D, maintains, for\\nevery node v a value δ(s, v) such that δ(s, v) = distG(s, v) if distG(s, v) ≤ D and δ(s, v) =∞ otherwise.\\nIt has a total update time of O(mD + n). During the updates, it can also report a shortest sv-path P in\\nO(|P |) time.\\nNote that there are many algorithms that have faster total update time then that of the ES-tree\\n(e.g. [HKN18, BC16, CK19, HKN16, HKN14a, BR11, HKN14b, Ber17]), but we cannot use any of them\\nbecause they are either randomized or can return only distance (not the path). Moreover, since we are\\nonly interested in short length paths (small D), a total update time improvement would not benefit us\\nmuch anyway.\\nWe use the ES-tree to maintain a BSF tree Ti rooted at si of depth L + 2 in Gˆi undergoing edge\\ndeletions. It takes O(mL) total update time. Therefore, the total time for maintaining all Ti is O(kmL).\\nThe ES-tree data structure can also report whenever the depth of ti in Ti is more than L+ 2. Note that\\ndistGˆ(Aˆi, Bˆi) ≤ L iff the depth of ti in Ti is at most L + 2. So as long as distGˆ(Aˆi, Bˆi) ≤ L, we can\\nobtain a path P of length at most L from Aˆi to Bˆi in time O(|P |) by cutting the path from si to ti in\\nTi. Therefore, the total time for returning all such paths is O(m).\\nAs there are O( log\\n2 n\\nφ2 ) iterations, the total running time on Item 3b is O(kmL) · O( log\\n2 n\\nφ2 ) = O(km ·\\nlog3 n\\nφ3 ). The running time spent on ManyABCut is O(km) by Lemma 3.5. So the total running time of\\nAlgorithm 2 is O(km · log3 nφ3 ) = O˜(kn/φ3). (Recall that the maximum degree of G is 20.)\\n3.2 An Improved Algorithm for k = 1 (Proof of Theorem 3.3)\\nTheorem 3.3 is an easy application of either the bounded height variant of the push-relabel-based max\\nflow algorithm by Henzinger Rao and Wang [HRW17] or the bounded height variant of the blocking-\\nflow-based algorithms by Orrecchia and Zhu [OA14].5 Note that this algorithm helps in improving the\\napproximation ratio of our main result. Most of our applications (e.g. dynamic connectivity) still hold\\nwithout using this algorithm.\\nHigh-Level Ideas. Both push-relabel and blocking flow algorithms try to route flow from sources to\\nsinks as much as possible while maintaining a level `(u) ≥ 0 on each node u. In the standard variants\\n[GT88, Din70], levels can be as large as O(n), the running time is O(mn), and the algorithm either\\nreturns a flow satisfying the demand or return a cut which certifies that there is no feasible flow (this\\ncorresponds to returning the exact max flow and min cut).\\nIn the bounded height variant, the levels are bounded to be at most h where h is a parameter. This\\nimmediately speeds up the running time to O(mh). However, even if there exists a flow satisfying the\\ndemand, the algorithm might not return it because it stops early. Fortunately, even when the algorithm\\ndoes not return a feasible flow, using a ball growing argument, one can show that there must exist a level\\ncut Si = {u | `(u) ≥ i} for some i such that the conductance ΦG(Si) ≤ O(logm/h). To summarize, we\\ncan speed up the algorithm significantly if the goal is to either return a flow satisfying the demand or\\nreturn a sparse cut.\\nTo take care of balancedness of a cut, we can extend the argument above and show that, given a\\nparameter x, the algorithm either flow satisfying the demand except for x many units or return a sparse\\ncut volume at least ∼ x. To prove Theorem 3.3, we just set each node in A as a source of 1 unit and each\\nnode in B as a sink of 1 unit, and set h = O(logm/φ) and x = Θ(βn) and invoke either the algorithms\\nby [HRW17] or [OA14].\\n5Both algorithms are designed to have local running time, i.e. they do not need to read the whole graph. However, we\\ndo not need this important property to prove Theorem 3.3.\\n17\\nPreliminaries. Now, we introduce definitions so that we can formally invoke the known algorithms\\nabove which is formally stated in Lemma 3.8. We follow the notations from [SW19]. Given an unweighted\\nundirected graph G = (V,E), let ∆ : V → Z≥0 denote a source function and T : V → Z≥0 denote a\\nsink function. We use mass to refer to the substance being routed. For a node v, ∆(v) specifies the\\namount of mass initially placed on v, and T (v) specifies the capacity of v as a sink. We require that∑\\nu ∆(u) ≤\\n∑\\nu T (u), i.e. the total amount of mass to be routed is at most the total capacity of sinks.\\nA routing f : V × V → R satisfies f(u, v) = −f(v, u) and f(u, v) = 0 for (u, v) /∈ E. f(u, v) > 0\\nmeans that mass is routed in the direction from u to v. A congestion of f is maxe∈E |f(e)|. We say that\\nf is a preflow if |f(e)| ≤ 1 for each e ∈ E and ∑u f(v, u) ≤ ∆(v) for each v (i.e. the net amount of mass\\nrouted away from a node can be at most the amount of its initial mass). We also treat f as a function on\\nvertices, where f(v) = ∆(v)+\\n∑\\nu f(u, v) is the amount of mass ending at v after the routing f . We define\\nthe absorbed mass on a node v as abf (v) = min(f(v), T (v)) and the excess on v is exf (v) = f(v)−abf (v)\\nwhich measure the amount of flow on v which cannot be absorbed. Note that if exf (v) = 0 for all v, then\\nall the mass are successfully routed to sinks. Let |exf (·)| =\\n∑\\nv exf (v) denote the total amount of mass\\nwhich is not successfully routed.\\nBelow, we formulate a lemma that is easy to use for us, which can be derived from Theorem 3.3 in\\n[NSW17] (or Theorem 3.1 in [HRW17]) by substituting some parameters.\\nLemma 3.8. There is a deterministic algorithm called BoundedPushRelabel that takes as an input a\\ntuple (G,∆, T, φ) where G = (V,E) is an m-edge graph, ∆ is a source function such that ∆(u) ≤ degG u\\n∀u ∈ V , T is a sink function where T (u) ≤ degG u ∀u ∈ V and\\n∑\\nu ∆(u) ≤\\n∑\\nu T (u), and φ ≥ 0\\nis a conductance parameter. In time O(mφ−1 logm) the algorithm returns an integral preflow f with\\ncongestion at most 4/φ and the excess exf (·) of f . Moreover, if |exf (·)| > 0, then the algorithm also\\nreturns a cut (S, S) where ΦG(S) < φ and min{volG(S), volG(S)} ≥ |exf (·)|.\\nProof of Theorem 3.3. Now, we describe the Push-CutMatch algorithm and prove Theorem 3.3.\\nGiven (G,φ, cong, β, {A,B}), we define a source function ∆(u) = 1 for u ∈ A otherwise ∆(u) = 0, and a\\nsink function T (u) = 1 for u ∈ A otherwise T (u) = 0. Then, we invokeBoundedPushRelabel(G,∆, T, φ).\\nLet f be the returned preflow with congestion at most 4/φ ≤ cong, where the inequality is by the condition\\non cong in Theorem 3.3.\\nConsider when |exf (·)| ≥ βn/4 ≥ βvolG/160 (the last inequality is because the maximum degree is\\nat most 20). In this case we return the smaller side of the cut (S, S) which satisfies the condition of\\nItem 1 of Definition 3.1. Otherwise, we have that |exf (·)| < βn/4. By definition, all the mass from A is\\nsuccessfully routed by the preflow f to B except |exf (·)| units. As the congestion of F is at most 4/φ,\\nwe can decompose the preflow f into a collection P of paths in time O(m/φ) using, for example, the\\nlink-cut tree [ST83] or depth-first search.6 By the definitions of ∆ and T , each path starts from A and\\nends at B. Moreover, each terminal node in A ∪ B is an endpoint of exactly one path, except at most\\n2|exf (·)| < βn many terminals which are not an endpoint of any path. This satisfies the condition of\\nItem 2 of Definition 3.1. The total running time is O(mφ−1 logm) = O(nφ−1 log n).\\n4 Cut Player’s Problem Variants and Their Reductions\\nIn this section, we formally define the main problem in this paper called BalCutPrune and related\\nproblems. In Sections 6 and 8, we will show fast algorithms for BalCutPrune and many applications\\nthat follow from them in Section 7. At a high level, the goal in BalCutPrune is to either cut a balanced\\nsparse cut, or prune (i.e. delete) a small part of the input graph so that the rest forms an expander.\\n6Algorithm sketch: First, we remove the excess by finding a path from the node v with exf (v) > 0 in a depth-first search\\nmanner. It can can be argue that we will eventually reach a source node. Reduce the flow on every traversed edge by one.\\nRepeat this until there is no excess left. Secondly, we decompose the resulting flow into paths in a similar way, as follows.\\nStart from a source s s.t. f(s) > 0. Follow edges (u, v) with f(u, v) > 0 in a depth-first search manner. It can be argued\\nthat we will eventually reach a sink t with f(t) < 0. Report the traversed path and reduce the flow on every edge in this\\npath by one. Repeat the process until no such source s can be found.\\n18\\nDefinition 4.1 (CutPrune and BalCutPrune). In the CutPrune problem, an algorithm is given\\n(G,φU , φL, β) where G = (V,E) is an m-edge graph, φU , φL ∈ [0, 1] where φL ≤ φU and β ≤ 1/3, and\\nreturns either\\n1. (Cut): a cut S ⊂ V such that βvolG ≤ volG(S) ≤ volG/2 and ΦG(S) ≤ φU , or\\n2. (Prune): a cut S ⊂ V such that volG(S) ≤ volG/2, δG(S) ≤ φUvolG and ΦG−S ≥ φL.\\nLet TCutPrune(m,φU , φL, β) denote the worst-case time required for deterministically solving CutPrune\\nwith the above parameters. Let BalCutPrune be a problem defined in the same way as CutPrune\\nwhere β = 1/3 is fixed, and let TBalCutPrune(m,φU , φL) be defined similarly.\\nBelow, we also state the related problems including vBalCutPrune and CutAugment. These\\nproblems will help us highlighting the key idea as follows. We show in this section the following rather\\nstraightforward reductions. (Lemma numbers indicate the corresponding lemmas stated below.)\\nvBalCutPrune\\nLem 4.7−−−−−→ BalCutPrune Lem 4.6−−−−−→ CutPrune Lem 4.5−−−−−→ CutAugment. (10)\\nThat is, an algorithm forCutAugment implies the ones forCutPrune, BalCutPrune, and vBalCutPrune\\nrespectively. Then, in Section 5 we will show the follwing main technical reduction that runs on graphs\\nwith fewer nodes. (We use “⇒” to emphasize that the reduction produces graphs with fewer nodes.)\\nCutAugment\\nLem 5.1\\n=====⇒ vBalCutPrune\\nThis will imply recursive algorithms for solving all these problems, including the main oneBalCutPrune.\\nThe fact that we only need to show a reduction from CutAugment to vBalCutPrune helps us pre-\\nsenting the key idea in a cleaner way.\\nNow, we define vBalCutPrune which is almost the same as BalCutPrune except that it concerns\\nthe sparsity and vertex-balance of a cut instead of its conductance (edge)-balance.\\nDefinition 4.2 (vBalCutPrune). In the vBalCutPrune problem, an algorithm is given (G,φU , φL)\\nwhere G = (V,E) is an n-node connected graph with maximum degree ∆, and φU , φL ∈ [0, 1] where\\nφL ≤ φU , and returns either\\n1. (Cut): a cut S ⊂ V such that |V |/6 ≤ |S| ≤ |V |/2 and σG(S) ≤ φU , or\\n2. (Prune): a cut S ⊂ V such that |S| ≤ |V |/2, δG(S) ≤ φU |V | and σG−S ≥ φL.\\nLet TvBalCutPrune(n,∆, φU , φL) denote the worst-case time required for deterministically solving vBalCutPrune\\nwith the above parameters.\\nNext, we define CutAugment. At a high level, the goal in CutAugment is to either cut a balanced\\nsparse cut, or augment (i.e. insert) a number of edges so that the input graph becomes an expander.\\nDefinition 4.3 (CutAugment). In theCutAugment problem, an algorithm is given (G,φU , φL, βcut, βaug)\\nwhere G = (V,E) is an m-edge graph, φU , φL ∈ [0, 1] where φL ≤ φU and βcut, βaug ∈ [0, 1], and returns\\neither\\n1. (Cut): a set S ⊂ V such that βcutvolG ≤ volG(S) ≤ volG/2 and ΦG(S) ≤ φU , or\\n2. (Augment): a set F of edges where |F | ≤ βaugvolG and ΦG∪F ≥ φL.\\nLet TCutAugment(m,φU , φL, βcut, βaug) denote the worst-case time required for deterministically solving\\nCutAugment with the above parameters.\\n19\\nReductions as in (10)\\nBelow we provide details of the chain of simple reductions presented in (10). To get the rightmost\\nreduction from CutPrune to CutAugment, we use an algorithm developed in [SW19, Wul17, NS17]\\ncalled Prune. At a high level, suppose we are given an expander G′ = (V,E′) and we need to delete a\\nset of edges D ⊂ E(G′). So, G := G′ −D might not be an expander anymore. Prune can “repair” this\\nexpander by pruning a small set P of nodes from G so that G−P is an expander. The formal statement\\nbelow follows from Theorem 1.3 in the paper by Saranurak and Wang [SW19].\\nTheorem 4.4 (Prune [SW19]). Let cPrune be a large enough constant. There is an algorithm called\\nPrune that take as an input a tuple (G,D, φ) where G = (V,E) is an m-edge graph, D is a set of edges\\nof size |D| = d, and φ ∈ [0, 1] is such that G′ = (V,E ∪D) has conductance ΦG′ ≥ φ. The algorithm then\\noutputs a set P ⊂ V in time\\nTPrune(m, d, φ) = O(d logm/φ\\n2)\\nsuch that ΦG−P ≥ φ/6, volG(P ) ≤ cPruned/φ, and δG(P ) ≤ cPruned.7\\nGiven an algorithm for CutAugment, we combine it with Prune and immediately obtain an algo-\\nrithm for CutPrune:\\nLemma 4.5 (CutPrune→ CutAugment). Let cPrune be the constant from Theorem 4.4. There is an\\nalgorithm for solving CutPrune with 6φL ≤ φU such that\\nTCutPrune (m,φU , φL, β) ≤ TCutAugment\\n(\\nm,φU , 6φL, β,\\nφL\\ncPrune\\n)\\n+ TPrune\\n(\\nm,\\n⌊\\n2mφL\\ncPrune\\n⌋\\n, 6φL\\n)\\n= TCutAugment\\n(\\nm,φU , 6φL, β,\\nφL\\ncPrune\\n)\\n+O ((m logm) /φL) .\\nProof. Given an input tuple (G,φU , φL, β) for CutPrune, we invoke CutAugment(G,φU , 6φL, β,\\nφL\\ncPrune\\n)\\nin time TCutAugment(m,φU , 6φL, β,\\nφL\\ncPrune\\n). If CutAugment returns a cut S, then it is clear that S sat-\\nisfies the conditions of Item 1 from Definition 4.1 for CutPrune. Otherwise, a set F is returned,\\nwhere |F | ≤ ( φLcPrune )volG ≤\\n2mφL\\ncPrune\\nand ΦG∪F ≥ 6φL. Then, we invoke Prune(G ∪ F, F, 6φL). In\\ntime TPrune(m, b 2mφLcPrune c, 6φL) the algorithm returns a set P where (i) ΦG−P ≥ (6φL)/6, (ii) volG(P ) ≤\\ncPrune|F |\\n6φL\\n≤ volG/6 and (iii) δG(P ) ≤ cPrune|F | ≤ φLvolG ≤ φUvolG. Hence, outputting S = P gives S\\nthat satisfies the conditions of Item 2 from Definition 4.1. (Note that since 6φL ≤ φU ≤ 1, we satisfy the\\ninput conditions in Definition 4.3 and Theorem 4.4 for CutAugment and Prune, respectively.)\\nGiven an algorithm for solving CutPrune, it is easy to obtain the ones for BalCutPrune and\\nvBalCutPrune, respectively. The idea is straightforward: We simply repeatedly cut the graph until\\nthe union of all cuts is balanced (in terms of the volume or the number of nodes). The union is a sparse\\ncut by Proposition 2.2. To make this idea formal, there are several cases in the analysis which are quite\\ntedious. So, we defer the proofs to Appendix.\\nLemma 4.6 (BalCutPrune→ CutPrune). There is an algorithm for BalCutPrune with 3φL ≤ φU\\nsuch that\\nTBalCutPrune(m,φU , φL) ≤ O(1/β) · TCutPrune(m,φU/3, φL, β).\\nLemma 4.7 (vBalCutPrune → BalCutPrune). There is an algorithm for vBalCutPrune with\\n60∆φL ≤ φU such that\\nTvBalCutPrune(n,∆, φU , φL) ≤ O(∆) · TBalCutPrune(n∆, φU/(60∆), φL).\\n7In [SW19] it is guaranteed that ΦG{V−P} ≥ φ/6 instead of ΦG−P ≥ φ/6. Observe that the former implies the latter.\\nWe only need the latter here. (Recall that both are measures on induced subgraphs on V − P , but we add self-loops in\\nG{V − P} to preserve nodes’ degrees.)\\n20\\n5 A Recursive Algorithm for CutAugment\\nThe main technical lemma is presented in this section. At a high level, we show that solvingCutAugment\\ncan be reduced to solving O˜(k) instances of vBalCutPrune in graphs whose numbers of edges are\\nsmaller by a factor of roughly k and also solving O˜(1) instances of CutMatch. As CutMatch admits\\nfast algorithms from Section 3 and vBalCutPrune can in turn be reduced to CutAugment as shown\\nin Section 4, this completes the cycle of reductions and gives us a recursive algorithm for CutAugment.\\nFrom this, we will derive a fast algorithm for BalCutPrune in Section 6 as our main result. The formal\\nstatement is as follows. (Below we use a constant ckkov from Theorem 5.5 which will be stated and proved\\nlater in this section.)\\nLemma 5.1 (CutAugment⇒ vBalCutPrune). Let ckkov and cmatch be constants from Theorem 5.5\\nand Theorem 3.2 respectively. There is a small enough constant c0 such that the following holds.\\n1. For any k ≥ 1, there is an algorithm for the CutAugment problem that, for any input tuple\\n(m,φU , φL, βcut, βaug) and R such that\\nR ≥ ckkov log(2m/k), cong ≥ cmatch log\\n2 2m\\nφ2U\\n, α ≤ 1\\n4\\n, φL ≤ c0αφU\\nR6cong logm\\nand βcut ≤ βaug/(330R),\\nthe algorithm has running time\\nTCutAugment (m,φU , φL, βcut, βaug) ≤ kR · TvBalCutPrune (2m/k,R, 1/4, α)\\n+R · TCutMatch (2m, k, φU , cong, 160βcut) +O\\n(\\nm+ k5\\n)\\n.\\n2. There is an an algorithm for the CutAugment problem that, for any input tuple (m,φU , φL, βcut, βaug)\\nand R such that\\nR ≥ ckkov log (2m) , cong ≥ cmatch log 2m\\nφU\\n, α ≤ 1\\n4\\n, φL ≤ c0α\\nR5cong\\nand βcut ≤ βaug/(330R),\\nthe algorithm has running time\\nTCutAugment (m,φU , φL, βcut, βaug) ≤ R · TvBalCutPrune (2m,R, 1/4, α)\\n+R · TCutMatch (2m, 1, φU , cong, 160βcut) +O (m) .\\nRoughly the first result in Lemma 5.1 gives a reduction from CutAugment on an m-edge graph to\\nvBalCutPrune on kR graphs of 2m/k edges by solving CutMatch with parameter k for R times. The\\nsecond result requires milder conditions. It gives a reduction from CutAugment to vBalCutPrune\\non R graphs of 2m edges (i.e. the graphs are not smaller) by solving CutMatch with k = 1 for R times.\\nBehind these results is an algorithm that tries to embed many small expanders simultaneously into the\\ninput graph. The notion of embedding is discussed in Section 5.1. The framework which allows us to\\nembed expanders is based on the variant of the cut-matching game by Khandekar, Khot, Orecchia, and\\nVishnoi [KKOV07] which is discussed in Section 5.2. Finally, we describe our algorithm and its analysis\\nin Section 5.3.\\n5.1 Key Tool 1: Simultaneous Small Expanders Embedding\\nIn this section, we define a notion of path embedding (or simply embedding) and show a key observation\\nin Lemma 5.4 on how to certify that a given graph is expander by only embedding small expanders into\\nit.\\n21\\nDefinition 5.2 (Embedding). An embedding (H,P) into a graph G = (V,E) consists of a graph H =\\n(V,E′) and a collection of paths P in G where each edge e′ = (u, v) ∈ E′ represents a path in P connecting\\nu and v. (G′,P) is called an c-congestion embedding if every edge in G is used by at most c paths in P.\\nWe also say that H can be embedded into G with congestion c.8\\nObserve that if we can embed an expander H into a graph G with small congestion, then G must also\\nbe an expander:\\nProposition 5.3. If H can be embedded into G with congestion c, then σG ≥ σH/c.\\nProof. Let (H,P) be an embedding into G with congestion c. Consider any set T where |T | ≤ |V |/2. It\\nsuffices to prove that δH(T ) ≤ c · δG(G). To see this, for each edge e′ ∈ EH(T, V − T ), the corresponding\\npath Pe′ ∈ P must contains an edge from EG(T, V − T ). However, each edge in G can be used by at\\nmost c many paths. So δH(T ) ≤ c · δG(G).\\nProposition 5.3 has been the key observation in all previous flow-based algorithms (e.g. [LR99, ARV09,\\nKRV09, KKOV07, AHK10, She09]) for certifying that a given graph is an expander. However, given a\\ngraph G, we do not know how to embed an expander H of the same size into G deterministically in\\nalmost-linear time.\\nThe following is the key observation of our algorithm. Suppose we partition nodes of G into k parts\\nV1, . . . , Vk of equal size, and let G\\n′ be the graph after contracting each Vi. Let Hi be an expander whose\\nnode set is V (Hi) = Vi. So, the size of Hi is smaller than G by roughly a factor of k. If we can embed all\\nHi into G simultaneously with low congestion and the contracted graph G\\n′ is an expander, then G must\\nalso be an expander. Roughly speaking, to certify an expander, the task reduces to embedding small\\ndisjoint expanders simultaneously instead of embedding one big expander. The statement below makes\\nthis observation precise:\\nLemma 5.4. Let G = (V,E) be a graph with maximum degree ∆. Let V = {V1, . . . , Vk} be a partition\\nof V . For each i, let Hi = (Vi, Ei) be a graph on vertex set Vi with maximum degree ∆. Let G\\n′ be\\nobtained from G by contracting each Vi. If ∪ki=1Hi can be embedded into G with congestion c, then\\nΦG ≥ Ω(ΦG′ ·(mini ΦHi )∆3c ).\\nProof. Consider Gˆ = G ∪ (∪ki=1Hi) (in other words, Gˆ = (V,E ∪ (∪ki=1Ei))). Observe that Gˆ can be\\nembedded into G with congestion c+ 1: we can embed edges ∪ki=1Ei with congestion c and the rest edges\\n(u, v) in Gˆ can be embedded to edge (u, v) in G. Thus, by Proposition 5.3,\\nσG ≥ σGˆ/(c+ 1) (11)\\nNext, consider the inner conductance Φin\\nGˆ,V and the outer conductance Φ\\nout\\nGˆ,V of Gˆ w.r.t. V (see the\\ndefinitions in Section 2.2). As G and each Hi have maximum degree ∆,\\nΦin\\nGˆ,V ≥ mini ΦHi/∆ and Φ\\nout\\nGˆ,V ≥ ΦG′/∆.\\nBy Lemma 2.4, we have that ΦGˆ = Ω(Φ\\nout\\nGˆ,V · ΦinGˆ,V) = Ω(ΦG′ · (mini ΦHi)/∆2).\\nWe can now conclude that\\nΦG ≥ σG/∆ (Proposition 2.1)\\n≥ σGˆ/(∆(c+ 1)) (Equation (11))\\n= ΦGˆ/(∆(c+ 1)) (Proposition 2.1)\\n= Ω\\n(\\nΦG′ · (mini ΦHi)\\n∆3c\\n)\\n(Lemma 2.4).\\n8More generally, to embed W into G usually means to find a multi-commodity flow where each commodity corresponds\\nto the endpoints of an edge in W . In this paper, we can be more specific by finding a collection of (integral) paths P.\\n22\\n5.2 Key Tool 2: Modified KKOV’s Cut-Matching Game\\nThe Cut-matching Game. The cut-matching game is introduced by Khandekar, Rao, and Vazirani\\n[KRV09] and can be viewed as a process for “forcing” an adversary to construct an expander for us.\\nSuppose that we want to construct an expander and an adversary wants to slow us down. The cut-\\nmatching game between the adversary and us proceeds in rounds as follows. Initially, we start with an\\nempty graph W . In each round t, we first choose a cut (St, St) cleverly. Then, an adversary chooses an\\narbitrary matching Mt of size min{|St|, |St|} crossing the cut (St, St) and set W ← W ∪Mt. The key\\ncontribution in [KRV09] is to show that even in this adversarial process, there is a simple and efficient\\nalgorithm for choosing a cut in each round so that, within a polylogarithmic number of rounds, W is\\nguaranteed to be an expander (to be more precise, the sparsity σW is high).\\nThe crucial property of the cut-matching game is that, given a graph G, the process allows us to easily\\nembed W into G. This is because embedding a matching crossing a cut (St, St) exactly corresponds to\\nfinding a flow where St contains source nodes and St contains sink nodes (and decomposing the flow\\ninto paths). As W is a union of such matchings, it suffices to solve polylogarithmic instances of the flow\\nproblem. Since the resulting W must be an expander, we can arguing about the conductance of G.\\nThe algorithm for choosing cuts by [KRV09] is inherently randomized and we do not know how to\\nuse it. Fortunately, Khandekar, Khot, Orecchia, and Vishnoi (KKOV) [KKOV07] show another way\\nfor choosing a cut in the cut-matching game. They show that, by choosing a most-vertex-balanced\\nsparsest cut in each round, the number of rounds is at most logarithmic. Although this can be found\\ndeterministically, finding such cut is NP-hard.\\nIn this section, we slightly modify KKOV’s framework and show that the number of rounds is still small\\neven when we allow an approximate result – in particular, when a cut is returned by vBalCutPrune.\\nThis idea was also used in vertex connectivity algorithms [GLN+].\\nTheorem 5.5. Given a number n, a parameter φ ∈ [0, 1/4], Algorithm 3 calls a vBalCutPrune\\nalgorithm for R ≥ ckkov log n many times where ckkov is some constant, and then returns an n-node\\ngraph W with maximum degree R and σW ≥ φ/9. Hence, ΦW ≥ φ/9R.\\nThe proof of Theorem 5.5 is almost the same as the one in [KKOV07] but we slightly adjust small\\ndetails so that it fits well with other parts of our algorithms. Therefore, we only include the proof in the\\nappendix for completeness.\\nAlgorithm 3 The modified cut-matching game framework of [KKOV07].\\nInput: A number n and a parameter φ ∈ [0, 1/4]. Let R ≥ ckkov log n where ckkov is some constant.\\nOutput: An n-node graph W with maximum degree R and σW ≥ φ/9 (so ΦW ≥ φ/9R ).\\n1. Let W0 be an n-node empty graph.\\n2. For t = 1, . . . , R:\\n(a) At ← vBalCutPrune(Wt, R, 1/4, φ).\\n(b) Let Bt be an arbitrary subset of V −At where |Bt| = |At|.\\n(c) Wt ←Wt−1 ∪Mt where Mt is an arbitrary matching between At and Bt of size |At|.\\n3. Return W = WR.\\n5.3 Algorithm and Analysis\\nHigh-level description. The algorithm for Lemma 5.1 is described in Algorithm 4. At a high level,\\nAlgorithm 4 works as follows when k > 1. We first apply Lemma 2.3 and assume that the given graph\\nG = (V,E) has constant degree. Then, we partition V into k parts equally, V1, . . . , Vk. As suggested\\n23\\nby Lemma 5.4, we will try to embed k expanders simultaneously into these k parts. To do this, we\\nimplement the cut-matching game from Section 5.2 on each part. That is, we first initialize Wi as an\\nempty graph on vertex set Vi. For each round of the cut-matching game, we invoke vBalCutPrune\\nto find a vertex-balanced sparse cut in each Wi (independently), which defines the sets Ai, Bi ⊂ Vi.\\nThe goal now is to embed a matching Mi between nodes from Ai and Bi simultaneously for all i. The\\nES-CutMatch algorithm from Theorem 3.2 is exactly designed for this. If ES-CutMatch returns a\\nbalance sparse cut, then we return such cut and terminate. Otherwise, it returns a collection P of paths\\nwhich corresponds k matchings M1, . . . ,Mk and ∪iMi can be embedded into G with small congestion.\\nSo, we set Wi ← Wi ∪Mi. Therefore, after O(logm) rounds, we conclude using Theorem 5.5 that each\\nWi is an expander. Moreover, ∪iWi can be embedded into G with small congestion as desired.\\nThere is a small issue in the above argument because ES-CutMatch does not guarantee that all\\nnodes from Ai and Bi are matched in Mi as required by Theorem 5.5. Fortunately, only a small fraction\\nof nodes from ∪i(Ai ∪ Bi) is not matched. So, we can fix this simply by augmenting Mi with a set of\\nedges Fi in each round so that every node Ai ∪Bi is matched. We then set Wi ←Wi ∪ (Mi ∪Fi) in each\\nround, and so Wi is an expander after O(logm) rounds. We let F\\nin denote all the “fake” matching edges\\naugmented into all Mi in all rounds. Although ∪iWi might not be embeddable into G because of F in,\\nwe still have that ∪iWi can be embedded into G ∪ F in = (V,E ∪ F in with small congestion.\\nNext, we let G′ be the graph after contracting each Vi. As G′ only has k nodes and we are willing to\\nspend poly(k) time, we can invoke any polynomial-time algorithm on G′. If there is a balanced sparse\\ncut S′ in G′, then we return the corresponding cut S in G and terminate. Otherwise, we can find F ′\\nwhere G′ ∪ F ′ is an expander. We let F out be obtained from F ′ after “uncontraction” (defined formally\\nin Item 7a of Algorithm 4). By Lemma 5.4, we can conclude that G ∪ F in ∪ F out is an expander. So\\nwe return F in ∪ F out as an output for the CutAugment problem. See Algorithm 4 for the formal\\ndescription.\\nThe algorithm description is the same when k = 1. The only difference is that we invoke Push-CutMatch\\nfrom Theorem 3.3 instead of ES-CutMatch for the improved quality and running time. Note that when\\nk = 1, the graph G′ from Item 5 of Algorithm 4 is a singleton.\\nAnalysis. We first state the following algorithm for Item 6 of Algorithm 4.\\nLemma 5.6. There is an algorithm called SlowCutAugment for solving the CutAugment problem\\nthat, given (G,φ, β) where G = (V,E) is an n-node m-edge (multi)-graph, φ ∈ [0, 1] and β ≤ 1/3, in time\\nO(m+ n5) returns either\\n1. a set S ⊂ V such that βvolG ≤ volG(S) ≤ volG/2 and ΦG(S) ≤ φ, or\\n2. a set F ⊆ V × V of edges (not necessarily in E) where |F | ≤ βvolG and ΦG∪F ≥ φ/40 logm.\\nThis lemma is obtained iteratively applying a deterministic polynomial-time approximate low-conductance\\ncut algorithm (e.g. [LR99]) and removing the returned cuts from G until we cannot. The proof is straight-\\nforward and is deferred to the Appendix. We note that if G is a singleton, then SlowCutAugment\\nreturns F = ∅. In particular, when k = 1, we have F out = ∅ in Algorithm 4.\\nThe running time of Algorithm 4 is straightforward. Note that m ≤ n ≤ 2m. The total time Item 4a\\ntakes is kR·TvBalCutPrune(2m/k,R, 1/4, α). The total time Item 4b takes isR·TCutMatch(2m, k, φU , cong, 160βcut).\\nNote that we have cong ≥ cmatch log2 2m\\nφ2U\\nwhen k > 1 and cong ≥ cmatch log 2mφU when k = 1. So the con-\\nditions for calling ES-CutMatch from Theorem 3.2 and the conditions for calling Push-CutMatch\\nfrom Theorem 3.3 are indeed satisfied. Item 6 takes O(m+ k5) time by Lemma 5.6. Other steps clearly\\ntakes O(m) time.\\nNow, we proof the correctness. First, by the guarantees of Definition 3.1 and Lemma 5.6, observe\\nthat the following:\\nLemma 5.7. If Algorithm 4 returns a set S from Item 4c or Item 7, then βcutvolG ≤ volG(S) ≤ volG/2\\nand ΦG(S) ≤ φU .\\n24\\nAlgorithm 4 Algorithm for CutAugment using vBalCutPrune (Lemma 5.1; Definition 4.3)\\nInput: An m-edge graph G = (V,E) and parameters (φU , φL, βcut, βaug) satisfying the conditions in\\nLemma 5.1.\\nOutput: As in Definition 4.3, either\\n1. a set S ⊂ V such that βcutvolG ≤ volG(S) ≤ volG/2 and ΦG(S) ≤ φU , or\\n2. a set F of edges where |F | ≤ βaugvolG and ΦG∪F ≥ φL.\\nAlgorithm:\\n1. Invoke the algorithm in Lemma 2.3 so we can assume that G is an n-node graph with maximum\\ndegree 20 and n ≤ 2m.\\n2. Partition V into V1, . . . , Vk where n/2k ≤ |Vi| ≤ n/k.\\n3. Set Wi as an empty graph on vertex set Vi for each 1 ≤ i ≤ k. Set F in ← ∅.\\n4. Repeat R iterations:\\n(a) For each 1 ≤ i ≤ k :\\ni. Ai ← vBalCutPrune(Wi, R, 1/4, α).\\nii. Bi be a subset of Vi −Ai where |Bi| = |Ai|.\\n(b) Run CutMatch(G,φU , cong, 160βcut, {A1, B1, . . . , Ak, Bk}) (using Theorem 3.2 when k > 1\\nand Theorem 3.3 when k = 1):\\n(c) If Item 4b returns a cut S, then return S. (The algorithm then terminates.)\\nElse, do the following.\\ni. Let M1, . . . ,Mk be the matchings returned from Item 4b.\\nii. For each 1 ≤ i ≤ k:\\nA. Let Fi be an arbitrary set of edges such that Mi ∪ Fi is a matching between Ai and\\nBi of size |Ai|.\\nB. Wi ←Wi ∪ (Mi ∪ Fi), F in ← F in ∪ Fi.\\n5. G′ ← G/V1/V2 . . . /Vk, i.e. G′ = (V ′, E′) is the graph after contracting each Vi into a vertex vi.\\n6. Run SlowCutAugment(G′, φU , βcut) (using Lemma 5.6):\\n7. If Item 6 returns a cut S′ ⊂ V (H), return S ← ∪vi∈S′Vi. (The algorithm then terminates.)\\nElse (a set F ′ ⊆ V ′ × V ′ of edges (not necessarily in G′) is returned), do the following.\\n(a) Let F out be an arbitrary set of edges such that degF out(v) = O(1) for each v ∈ V and\\nvolF out(Vi) = volF ′(vi) for each vi ∈ V (H).\\n(b) Return F = F in ∪ F out.\\n25\\nWe also have:\\nLemma 5.8. |F in| ≤ 320Rβm and |F out| ≤ βcutm. So |F | ≤ βaugm.\\nProof. In each iteration of Item 4 of Algorithm 4, we have that | ∪ki=1 Fi| ≤ 160βcutn by Definition 3.1.\\nAs there are R iterations and n ≤ 2m, we have |F in| ≤ 320Rβcutm. Also, |F out| ≤ βcutm by Lemma 5.6.\\nSo we have |F | ≤ 330Rβcutm ≤ βaugm.\\nNow, we need the notion of embedding from Section 5.1. Observe the following:\\nLemma 5.9. ∪ki=1Wi can be embedded into G ∪ F in with congestion R · cong.\\nProof. For each iteration of the repeat loop at Item 4, the matchings M1, . . . ,Mk returned are such that\\n∪ki=1Mi can be embedding into G with congestion cong. This is true by the definition of the matchings\\nfrom Definition 3.1. The claim follows because there are at most R iterations.\\nThe remaining part is to prove the following:\\nLemma 5.10. ΦG∪F in∪F out ≥ φL.\\nProof. First, we claim that mini ΦWi ≥ α/9R. Observe that the way Algorithm 4 inserts matchings into\\neach Wi is exactly the same way as how Algorithm 3 works. As we find the set Ai and Bi by invoking\\nvBalCutPrune(Wi, R, 1/4, α), Theorem 5.5 implies that ΦWi ≥ α/9R for each i.\\nLet Gˆ be obtained from G ∪ F in ∪ F out by contracting V1, . . . , Vk. As ∪ki=1Wi can be embedded into\\nG ∪ F in with congestion R · cong by Lemma 5.9 and the maximum degree of both G ∪ F in ∪ F out and\\nWi is O(R), by Lemma 5.4, we have\\nΦG∪F in∪F out ≥ Ω(1) ·\\nΦGˆ · (mini ΦWi)\\nR4cong\\n≥ Ω(1) · α · ΦGˆ\\nR5cong\\n.\\nThere are two cases. If k = 1, then ΦGˆ = 1 as Gˆ is a singleton. In this case, as we assume that\\nφL ≤ c0α/(R5cong) where c0 is small enough. So ΦG∪F in∪F out ≥ φL.\\nNow, we consider the case when k > 1. Observe that G′ ∪ F ′ is the graph G ∪ F out after contracting\\nV1, . . . , Vk. By Lemma 5.6, SlowBalCutPrune guarantees ΦG′∪F ′ ≥ φU/40R logm.\\nAs each node is incident to at most R edges from F in, ΦGˆ ≥ ΦG′∪F ′/R ≥ φU/40R logm.\\nAs φL ≤ c0αφU/(R6cong logm) where c0 is small enough. We conclude ΦG∪F in∪F out ≥ φL as well in\\nthis case.\\nWe summarize the property of our algorithm when k = 1 in the following remark. We will need this\\nlater in Section 8.\\nRemark 5.1. When k = 1, in the case the set F is returned, we have that Algorithm 4 computes the\\ngraph W1 where ΦW1 ≥ α/9R, E(Wi) ⊃ F , and W1 can be embedded into G∪F with congestion R ·cong.\\nMoreover, max∅6=S⊂V {volW (S)volG(S) ,\\nvolG(S)\\nvolW (S)\\n} ≤ R = O(logm) because the max degree of both W1 and G are\\nat most R.\\n6 Combining Everything: A Fast Algorithm for BalCutPrune\\nIn this section, we show a fast algorithm for solving BalCutPrune:\\n26\\nTheorem 6.1. There is a large enough constant cBalCutPrune such that the following holds. For any\\n` ≥ 1, there is an algorithm for solving the BalCutPrune problem such that, for any (m,φU , φL) where\\nφU ∈ [0, 1] and φL ≤ φU\\n(cBalCutPrune logm)12`+8\\n,\\nhas running time TBalCutPrune(m,φU , φL) ≤ O(m1+3/`φ−2U logO(`\\n2)m).\\nBy setting ` = Θ((logm/ log logm)1/3), we immediately obtain an almost-linear time algorithm for\\nφU ≥ 1/no(1):\\nCorollary 6.2. There is an algorithm of BalCutPrune that, for any (m,φU , φL) where\\nφU ∈ [0, 1] and φL ≤ φU/2O(log1/3 n·(log logn)2/3),\\nhas running time TBalCutPrune(m,φU , φL) ≤ mφ−2U 2O(log\\n2/3 n·(log logn)1/3).\\nTo prove Theorem 6.1, in Section 6.1 we combine the chain of reductions from Sections 4 and 5\\nand show that BalCutPrune can be solved by recursively solving BalCutPrune itself on several\\nmuch smaller graphs. Then, in Section 6.2 we analyze the algorithm when we recurse for at most\\n` levels, and then apply a slow polynomial time algorithm when the graph in the recursion is small\\nenough. This already gives us an almost-linear-time algorithm for BalCutPrune with a condition that\\nφU = Θ(1/ logm), i.e. it works only for the high conductance regime. Finally, Section 6.3 we extend the\\nalgorithm to work for any φU ∈ [0, 1] and obtain Theorem 6.1.\\nAll the proofs in this section are quite straightforward but a bit tedious because we need to check all\\nthe conditions of parameters whenever we would like to invoke any reduction or algorithm.\\n6.1 One level of recursion\\nIn this section, we show a recursive algorithm for BalCutPrune which calls itself in much smaller\\ngraphs. This is done by combining the main lemma (Lemma 5.1) with other basic reductions from (10)\\nin Section 4:\\nLemma 6.3. Let ckkov, cmatch, c0, cPrune be constants from Theorem 5.5, Theorem 3.2, Lemma 5.1,\\nTheorem 4.4 respectively. For any k ≥ 1, there is an algorithm called RecurBalCutPrune for solving\\nBalCutPrune such that, for any (m,φU , φL) where\\nR ≥ ckkov log(2m/k),Ω( 1\\nR\\n) ≤ φU ≤ 1\\n240R\\n, cong ≥ cmatch log\\n2 2m\\nφ2U\\n, γ ≥ 18R\\n6cong logm\\nc0φU\\n, φL ≤ 1\\n4γ\\nhas running time\\nTBalCutPrune(m,φU , φL) ≤ O(kφ−1L R3) · TBalCutPrune(2mR/k, φU , φLγ) + O˜((km+ k5)R5φ−2L ).\\nProof. Let βcut =\\nφL\\n330R·cPrune and α = φLγ. By Lemma 4.6, we have\\nTBalCutPrune (m,φU , φL) ≤ O (1/βcut) · TCutPrune (m,φU/3, φL, βcut) .\\nThen, Lemma 4.5 implies that\\nTCutPrune (m,φU/3, φL, βcut) = TCutAugment\\n(\\nm,φU/3, 6φL, βcut,\\nφL\\ncPrune\\n)\\n+ TPrune\\n(\\nm,\\n2mφL\\ncPrune\\n, 6φL\\n)\\n.\\n27\\nNote that TPrune(m,\\n2mφL\\ncPrune\\n, 6φL) = O˜(m/φL) by Theorem 4.4. Now, as R ≥ ckkov log(2m/k), cong ≥\\ncmatch log\\n2 2m\\nφ2U\\n, α = φLγ ≤ 14 , 6φL = 6αγ ≤ c0αφU/3R6cong logm and βcut ≤ φL330R·cPrune , all parameters satisfy the\\nconditions of Lemma 5.1. So we obtain\\nTCutAugment\\n(\\nm,φU/3, 6φL, βcut,\\nφL\\ncPrune\\n)\\n= kR · TvBalCutPrune (2m/k,R, 1/4, α) +R · TCutMatch (2m, k, φU/3, cong, 160βcut) +O\\n(\\nm+ k5\\n)\\n.\\nNote that TCutMatch(2m, k, φU/3, cong, 160βcut) = O˜(km/φ\\n3\\nU ) by Theorem 3.2. Finally, by Lemma 4.7,\\nwe have\\nTvBalCutPrune (2m/k,R, 1/4, α) = O (R) · TBalCutPrune (2mR/k, φU , α) .\\nPutting everything together, we obtain a recursion for TBalCutPrune:\\nTBalCutPrune (m,φU , φL)\\n= O (1/βcut) ·\\n(\\nO˜ (m/φL) +O\\n(\\nm+ k5\\n)\\n+R · O˜ (km/φ3U)+O (kR2) · TBalCutPrune (2mR/k, φU , α))\\n= O(kφ−1L R\\n3) · TBalCutPrune (2mR/k, φU , φLγ) + O˜\\n((\\nkm+ k5\\n)\\nR5φ−2L\\n)\\n.\\n6.2 High Conductance Regime\\nIn this section, we show a fast algorithm for solving BalCutPrune in the high conductance regime,\\nmore specifically, when φU = Θ(1/ logm).\\nFirst, we note that it is easy to see that BalCutPrune can be solved deterministically in polynomial\\ntime (e.g. by [LR99]). We will use this algorithm as a base case in our recursive algorithm.\\nLemma 6.4. There is a constant cslow such that there is an algorithm called SlowBalCutPrune for\\nsolving the BalCutPrune problem such that, for any (m,φU , φL) where\\nφL ≤ cslowφU/ logm,\\nhas running time TBalCutPrune(m,φU , φL) = O(m\\n4).\\nWe now describe a straightforward recursive algorithm: We invoke RecurBalCutPrune from\\nLemma 6.3 recursively until the size of the graph is small enough. Once the graph becomes very small, we\\nthen apply the slow polynomial time SlowBalCutPrune algorithm. The lemma below gives a formal\\nanalysis of this idea:\\nLemma 6.5. Let ckkov be constants from Theorem 5.5. Let c1 be a big enough constant. For any ` ≥ 1,\\nthere is an algorithm for BalCutPrune such that, for any (m,φU , φL) where\\nckkov log 2m ≤ R ≤ O (logm) ,Ω\\n(\\n1\\nR\\n)\\n≤ φU ≤ 1\\n240R\\n,φL ≤ 1\\n(c1 logm)\\n12`+2\\nhas running time TBalCutPrune(m,φU , φL) ≤ O(m1+3/` logO(`\\n2)m).\\nProof. Fix a parameter ` ≥ 1. Given (G,φU , φL) where G has m edges. We set global variables\\nk = m1/`, cong =\\ncmatch log\\n2 2m\\nφ2U\\nand γ =\\n18R6cong logm\\nc0φU\\n.\\n28\\nFor each 1 ≤ i ≤ `, we set\\nφ\\n(i)\\nL =\\ncslowφU\\nγi logm\\nand m(i) = m(\\n2R\\nk\\n)`−i.\\nAs γ = Θ(log12m), we have that φ\\n(i)\\nL ≥ 1/(c1 logm)12i+2 where c1 is a big enough constant and m(i) =\\nmi/`(logΘ(1)m)`−i. Our algorithm is recursive, but we emphasize that these global variables do not\\ndepend on the size of graphs inside the recursion. Throughout the recursion, we keep an invariant that\\nthe recursive input to BalCutPrune must be of the form (G′, φU , φ\\n(i)\\nL ), for some 1 ≤ i ≤ `, where G′ is\\na graph with m′ ≤ m(i) edges.\\nNow, we describe the algorithm. Initially, given an input (G,φU , φL), we invokeRecurBalCutPrune\\nfrom Lemma 6.3 with parameter (G,φU , φ\\n(`)\\nL ). In the recursion, given (G\\n′, φU , φ\\n(i)\\nL ) as input, if G\\n′ has\\nm′ > m(1) edges, then we invoke RecurBalCutPrune, otherwise we invoke SlowBalCutPrune. By\\nLemma 6.3, it is easy to see that we indeed maintain the invariant.\\nWe next prove the validity of the outputs and the inputs of BalCutPrune throughout the recursion.\\nThe output from the topmost call to RecurBalCutPrune a valid output for the BalCutPrune\\nproblem with parameter (G,φU , φL) because φL ≤ 1/(c1 logm)12`+2 = φ(`)L . For any 1 ≤ i ≤ `, given\\n(G′, φU , φ\\n(i)\\nL ) where G\\n′ has m′ ≤ m(i) edges, the conditions of RecurBalCutPrune from Lemma 6.3\\nhold because\\nR ≥ ckkov log(2m′/k), cong ≥ cmatch log\\n2 2m′\\nφ2U\\n, γ ≥ 18R\\n6cong logm′\\nc0φU\\n, φ\\n(i)\\nL ≤\\n1\\n4γ\\n.\\nAlso, the condition φ\\n(i)\\nL ≤ cslowφU/ logm of SlowBalCutPrune from Lemma 6.4 holds. So, (G′, φU , φ(i)L )\\nis valid for both RecurBalCutPrune or SlowBalCutPrune. This completes the correctness of the\\nalgorithm. It remains to analyze the running time.\\nAs φ\\n(i−1)\\nL = γφ\\n(i)\\nL and m\\n(i−1) = m(i)( 2Rk ), we have that for all i ≥ 2\\nTBalCutPrune(m\\n(i), φU , φ\\n(i)\\nL ) ≤ O\\n(\\nkR3\\nφ\\n(i)\\nL\\n)\\n· TBalCutPrune(m(i−1), φU , φ(i−1)L ) + O˜\\n(\\n(km(i) + k5)R5(φ\\n(i)\\nL )\\n−2\\n)\\n≤ O\\n(\\nkR3\\nφ\\n(i)\\nL\\n)\\n· TBalCutPrune(m(i−1), φU , φ(i−1)L ) +O(m(i+3)/` logO(`)m)\\nby Lemma 6.3. When i = 1, by calling SlowBalCutPrune, we have\\nTBalCutPrune(m\\n(1), φU , φ\\n(1)\\nL ) = O(m(\\n2R\\nk\\n)`−1)4 = O(m4/` logO(`)m).\\nTherefore, we conclude\\nTBalCutPrune(m,φU , φL)\\n≤ TBalCutPrune(m(`), φU , φ(`)L )\\n≤ O\\n(\\nkR3\\nφ\\n(`)\\nL\\n)`−1\\n·O\\n(\\nm4/` logO(`)m\\n)\\n+\\n∑`\\ni=2\\nO\\n(\\nkR3\\nφ\\n(`)\\nL\\n)`−i\\n· O˜\\n(\\nm(i+1)/` logO(`)m\\n)\\n= O\\n(\\nm1−1/` logO(`\\n2)m\\n)\\n·O\\n(\\nm4/` logO(`)m\\n)\\n+\\n`−1∑\\ni=1\\nO\\n(\\nm1−i/` logΘ(`\\n2)m\\n)\\n·O\\n(\\nm(i+3)/` logO(`)m\\n)\\n= O\\n(\\nm1+3/` logO(`\\n2)m\\n)\\n.\\n29\\n6.3 General Regime\\nBased on the algorithm for high conductance regime from Lemma 6.5, we now extend it to work on the\\ngeneral regime and prove Theorem 6.1 in this section.\\nThe high level idea is simple. The main observation is that Lemma 5.1 implies that, to solve\\nCutAugment in a general regime, it suffices to only have an algorithm for vBalCutPrune in the\\nhigh conductance regime, which we have from Lemma 6.5. So we apply Lemma 5.1 (with k = 1 for more\\nefficiency). Now, given an algorithm for CutAugment in general regime, the basic reductions from (10)\\nin Section 4 immediately give us an algorithm for BalCutPrune for a general regime.\\nLemma 6.6. There are large enough constant c2 and c3 such that the following holds. For any ` ≥ 1,\\nthere is an algorithm for CutAugment such that, for any (m,φU , φL, βcut, βaug) where\\nφL ≤ φU\\n(c2 logm)12`+8\\nand βcut ≤ βaug\\nc3 logm\\nhas running time TCutAugment(m,φU , φL, βcut, βaug) = O(m\\n1+3/`φ−1U log\\nO(`2)m).\\nProof. Let ckkov, cmatch, c0, c1 be constants from Theorem 5.5, Theorem 3.2, Lemma 5.1, Lemma 6.5,\\nrespectively. We set\\nR = 2ckkov log(2m), cong =\\ncmatch log 2m\\nφU\\nand α =\\n1\\n(c1 logm)12`+2\\n.\\nObserve that φL ≤ φU(c2 logm)12`+8 ≤ c0αR5cong and βcut ≤\\nβaug\\nc3 logm\\n≤ βaug330R because c2 and c3 are large enough.\\nAs the parameters satisfy the condition of Item 2 of Lemma 5.1, we have\\nTCutAugment(m,φU , φL, βcut, βaug)\\n≤ R · TvBalCutPrune(2m,R, 1/4, α) +R · TCutMatch(2m, 1, φU , cong, 160βcut) +O(m).\\nBy Lemma 4.7, we have\\nTvBalCutPrune(2m,R, 1/4, α) = O(R) · TBalCutPrune(2mR, 1/240R,α).\\nLet m0 = 2mR and R0 = ckkov log 2m0. We would like to bound TBalCutPrune(m0, 1/240R,α). Observe\\nthat R0 ≤ R = O(R0). So we have ckkov log 2m0 ≤ R0 ≤ O(logm0) and Ω( 1R0 ) ≤ 1240R ≤ 1240R0 . Note\\nalso that α ≤ 1\\n(c1 logm)12`+2\\n. These parameters satisfy the conditions of Lemma 6.5 and so\\nTBalCutPrune(m0, 1/240R,α) = O(m\\n1+3/`\\n0 log\\nO(`2)m0) = O(m\\n1+3/` logO(`\\n2)m).\\nPutting everything together, we obtain\\nTCutAugment(m,φU , φL, βcut, βaug) = O(m\\n1+3/` logO(`\\n2)m) +R · TCutMatch(2m, 1, φU , cong, 160βcut)\\n(12)\\n= O(m1+3/`φ−1U log\\nO(`2)m)\\nwhere the last equality is by invoking Push-CutMatch from Theorem 3.3.\\nProof of Theorem 6.1. Given an algorithm for CutAugment in general regime, we immediately\\nobtain another for BalCutPrune as follows. Let φ∗L =\\nφU\\n(cBalCutPrune logm)12`+8\\nand βcut =\\nφ∗L\\n330R·cPrune . We\\n30\\nhave:\\nTBalCutPrune (m,φU , φL)\\n≤ TBalCutPrune (m,φU , φ∗L)\\n≤ O(1/βcut) · TCutPrune (m,φU/3, φ∗L, βcut) (Lemma 4.6)\\n≤ O(1/βcut) ·\\n(\\nTCutAugment\\n(\\nm,φU/3, 6φ\\n∗\\nL, βcut,\\nφ∗L\\ncPrune\\n)\\n+ TPrune\\n(\\nm,\\n2mφ∗L\\ncPrune\\n, 6φ∗L\\n))\\n(Lemma 4.5)\\nNote that 6φ∗L ≤ φU/3(c2 logm)12`+8 because cBalCutPrune is large enough. The parameters satisfy the conditions\\nof Lemma 6.6 which implies that\\nTCutAugment\\n(\\nm,φU/3, 6φ\\n∗\\nL, βcut,\\nφ∗L\\ncPrune\\n)\\n= O\\n(\\nm1+3/`φ−1U log\\nO(`2)m\\n)\\n.\\nBy Theorem 4.4, we have TPrune(m,\\n2mφ∗L\\ncPrune\\n, 6φ∗L) = O˜(m/φ\\n∗\\nL) = O(mφ\\n−1\\nU log\\nO(`)m). As 1/βcut =\\nO(φ−1U log\\nO(`)m), we have\\nTBalCutPrune(m,φU , φL) = O(m\\n1+3/`φ−2U log\\nO(`2)m).\\n7 Applications of BalCutPrune\\nIn this section, we give a list of applications of the fast BalCutPrune algorithm from Corollary 6.2.\\nTables 1 and 2 summarizes our results. We use the Ô(·) notation to hide sup-polynomial lower order terms.\\nFormally Ô(f(n)) = O(f(n)1+o(1)), or that for any constant θ > 0, we have Ô(f(n)) ≤ O(f(n)1+θ). It\\ncan be viewed as a direct generalization of the O˜(·) notation for hiding logarithmic factors, and behaves\\nin the same manner.\\n7.1 Expander Decomposition\\nAn (\\x0f, φ)-expander decomposition of a graph G = (V,E) is a partition P = {V1, . . . , Vk} of V such that\\nΦG[Vi] ≥ φ and\\n∑\\ni δG(Vi) ≤ \\x0fvolG. This decomposition was introduced in [KVV04, GR99] and has been\\nused as a key tool for various applications in many setting (including the ones in this paper).\\nSpielman and Teng give the first near-linear O˜(m/poly(\\x0f)) time algorithm for computing a weak\\nvariant of the (\\x0f, \\x0f2/polylogn)-expander decomposition, where each Vi is only guaranteed to be contained\\nin some Wi ⊆ Vi where ΦG[Wi] ≥ \\x0f2/polylogn.\\nThis caveat is first removed in [NS17]: one can compute an (\\x0f, \\x0f/no(1))-expander decomposition in\\nÔ(m) time ([Wul17] also uses the same approach but has worse running time). Then, [SW19] shows how\\nto compute an (\\x0f, \\x0f/polylogn)-expander decomposition in O˜(m/\\x0f) time. However, all previous algorithms\\nare randomized.\\nThe only subquadratic time deterministic algorithm is implicit in [GLN+]: an (\\x0f, \\x0f/no(1))-expander\\ndecomposition can be computed in Ô(m1.5) time. Here, we show the first deterministic algorithm with\\nalmost-linear running time for computing this decomposition.\\nCorollary 7.1. There is an algorithm that, given a graph G = (V,E) with m edges and a parameter\\n\\x0f ∈ (0, 1], returns a (\\x0f, \\x0f/2O(log1/3m log2/3 logm))-expander decomposition of G in time Ô(m\\x0f−2).\\nProof. Let φU = \\x0f/100 logm and φL = φU/2\\nO(logm log logm)2/3 . Let P ← ∅ initially. Using Corollary 6.2,\\nWe apply BalCutPrune(G,φU , φL), which returns either\\n1. (Cut): a cut S ⊂ V such that volG/3 ≤ volG(S) ≤ volG/2 and ΦG(S) ≤ φU , or\\n31\\n2. (Prune): a cut S ⊂ V such that volG(S) ≤ volG/2, δG(S) ≤ φUvolG and ΦG−S ≥ φL.\\nIf Case 1 happens, we recurse on G[S] and G[V − S]. If Case 2 happens, we let P ← P ∪ {V − S} (since\\nΦG[V−S] = \\x0f/2O(logm log logm)\\n2/3\\n) and only recurse on G[S]. After the recursion, P is an partition of V\\nand each Vi ∈ P satisfies ΦG[Vi] = \\x0f/2O(logm log logm)\\n2/3\\n. Since volG(S) ≤ volG/2, volV−S ≤ 2volG/3 in\\nCase 1 and volG(S) ≤ volG/2 in Case 2, the depth of recursion is at most 100 logm. Thus, the running\\ntime is bounded by O(logm · mφ−2U 2O(logm log logm)\\n2/3\\n) = O(m\\x0f−22O(logm log logm)\\n2/3\\n). Finally, in both\\ncases, by splitting V into S and V − S, the number of edges between different Vi’s increase by at most\\nφUvolG ≤ (\\x0f/100 logm)volG. Since the depth of recursion is bounded by 100 logm,\\n∑\\nVi∈P δG(Vi) ≤\\n\\x0fvolG.\\n7.2 Dynamic Connectivity and Minimum Spanning Forests\\nNext, we resolve the long standing open problem whether the classic deterministic dynamic connectivity\\nalgorithm with O(\\n√\\nn) worst-case update time by Frederickson [Fre85, EGIN97] can be sped up by a\\npolynomial factor. The previous best deterministic algorithm has O(\\n√\\nn(log log n)2/ log n) worst-case\\nupdate time [KKPT16]. We improve this bound to subpolynomial:\\nCorollary 7.2. There is a deterministic algorithm that, given an n-node graph G with undergoing edge\\ninsertion and deletions, can explicitly maintain a minimum spanning forest of G using 2O(logn log logn)\\n2/3\\nworst-case update time for each edge update.\\nThe above dynamic minimum spanning forest algorithm immediately implies a dynamic connectivity\\nalgorithm with the same update time and O(log n) query time (for answering whether a pair of nodes is\\nconnected).\\nWe only sketch the argument here. There are only two randomized components in the fully dynamic\\nminimum spanning forests algorithm by Nanongkai, Saranurak and Wulff-Nilsen [NSW17]. The first\\nrandomized component is the MSF decomposition (Theorem 8.3) in Section 8 in [NSW17]. In that\\nsection, one can observed that the only reason their algorithm is randomized is because of the algorithm\\nfor computing the expander decomposition (i.e. Lemma 8.7 in their paper) which is derandomized by\\nCorollary 7.1. So the MSF decomposition is also derandomized.\\nThe second randomized component is Theorem 6.1 from Section 6 in [NSW17], which is an extension\\nof Theorem 5.1 in [NSW17]. Both theorems are about dynamic expander pruning algorithms. In Theorem\\n5.1, the theorem assumes that the initial input graph is guaranteed to has high conductance. With this\\nassumption, the algorithm for Theorem 5.1 is deterministic. Then, the purpose of Theorem 6.1 is to\\nremove this assumption, however the algorithm becomes randomized. The only reason they need to use\\nTheorem 6.1 instead of Theorem 5.1 is because their MSF decomposition was Monte Carlo randomized\\nand hence the initial input graph to Theorem 6.1, with small probability, can have low conductance.\\nHowever, given that the MSF decomposition is derandomized, one can instead apply Theorem 5.1 which\\nis deterministic.\\nGiven that the two components are deterministic, one can verify that the whole algorithm is also\\ndeterministic. By implementing the MSF decomposition using Corollary 7.1 and by implementing dynamic\\nexpander pruning in Theorem 5.1 using the improved expander pruning algorithm from [SW19], we believe\\nthat the final update time of the dynamic MSF algorithm is 2O(logn log logn)\\n2/3\\nwhich is the factor from\\nCorollary 6.2.9\\n7.3 Spectral Sparsifiers\\nDeterministic expander decompositions can be immediately used to to derandomize their original applica-\\ntion: constructing spectral sparsifiers [ST11]. Given a undirected weighted n-node graph G = (V,E,w),\\n9However, we note even if the running time of algorithm from Corollary 6.2 is O(mpoly logm), the approach of [NSW17]\\ncannot give the update time below 2O(\\n√\\nlogn).\\n32\\na Laplacian LG of G is a matrix of size n× n where\\nLG(u, v) =\\n{\\n−w(u,v) u 6= v∑\\ne=(u,u′) w(u,u′) u = v.\\nWe say that a graph H is a α-approximate spectral sparsifier of G is x ∈ Rn, x>LGx ≤ x>LHx ≤\\nα · x>LGx.\\nAll previous deterministic graph sparsification algorithms, even for cut sparsifiers, use the explicit\\npotential function based approaches by Batson Spielman and Srivastava [BSS12]. All previous works\\nwith faster running times either perform random sampling [SS11], or use random projections to estimate\\nthe importances of edges [ALO15]. Below, we give the first deterministic almost-linear-time algorithm\\nfor computing a spectral sparsifier of a weighted graph. We emphasize that although all algorithms from\\nprevious sections are designed for unweighted graphs, the fact that spectral sparsifiers is “decomposable”\\nallows us to reduce the problem on weighted graphs to the one on unweighted graph easily.\\nCorollary 7.3. There is a deterministic algorithm that, given a undirected n-node m-edge graph G =\\n(V,E,w) with integral, polynomially bounded edge weights w , compute a no(1)-approximate spectral spar-\\nsifier H with O(n log2 n) edges of G in time Ô(m).\\nProof. First we decompose each edge of G by its binary representation and let G(i) = (V, {e ∈ E|we =\\n2i}). Since we can approximate each G(i) separately, we assume G is an unweighted graph.\\nWe compute an (1/2, 1/2O(logn log logn)\\n2/3\\n)-expander decomposition P = {V1, V2, . . . , Vk} of G by\\nCorollary 7.1. If Ê :=\\n⋃\\ni∈[k] δ(Vi) is nonempty, let G← (V, Ê) and recurse. The depth of this recursion\\nis at most O(logm) and G is decomposed into sum of its subgraphs whose conductances are at least\\n1/2O(logn log logn)\\n2/3\\n.\\nThis shows that it suffices to approximate a graph H with conductance at least 1/2O(logn log logn)\\n2/3\\n.\\nWe will first approximate H by a “product demand graph” D as defined in [KLP+16] and then use the\\nconstruction (which is a strengthen version of Fact 2.6) from [KLP+16] to sparsify D.\\nDefinition 7.4 (Definition G.13, [KLP+16]). The product demand graph of a vector d ∈ (R>0)n, G(d),\\nis a complete weighted graph on n vertices whose weight between vertices i and j is given by w ij = d id j .\\nFor any n-node graph G, we let degG ∈ Zn be a vector indicating the degree of each node. Let\\nD = 1volHG(degH) be a product demand graph. Observe that degD = degH by construction. In the\\nfollowing lemma, we show D and H approximates each other:\\nLemma 7.5. Let D and H be two undirected weighted graphs on the same set of vertices such that\\ndegD = degH . If ΦD,ΦH ≥ φ for some conductance threshold φ, φx>LHx ≤ x>LDx ≤ 1φx>LHx for\\nany vector x ⊥ deg 12D where deg\\n1\\n2\\nD is the vector such that the u-th entry is\\n√\\ndegD u.\\nProof. Let L̂D and L̂H be normalized Laplacians of D and H, respectively. Since the degree vector of D\\nand H are the same, we only need to prove φx>L̂Hx ≤ x>L̂Dx ≤ 1φx>L̂Hx for any vector x ⊥ deg\\n1\\n2\\nD.\\nLet λ be the second eigenvalue of L̂H . For any vector x ⊥ deg\\n1\\n2\\nD, we have\\nλ\\n2\\nx>L̂Dx ≤ λ‖x‖2 ≤ x>L̂Hx\\nsince the largest eigenvalue of L̂D is at most 2. By Cheeger’s inequality [Alo86],\\nφx>L̂Dx ≤ ΦDx>L̂Dx ≤ λ\\n2\\nx>L̂Dx ≤ x>L̂Hx .\\nSame proof applies to the other direction.\\n33\\nBy construction, for any cut S of D, its conductance is\\nδD(S)\\nmin{volD(S), volD(V − S)} =\\nvolD(S) · volD(V − S)\\nvolD ·min{volD(S), volD(V − S)} ≥\\n1\\n2\\n.\\nLemma 7.5 with φ = 1/2O(logn log logn)\\n2/3\\n= no(1) implies that D is a no(1)-approximate spectral sparsifier\\nof H. Finally, a sparsifier of D can be constructed in nearly linear time:\\nLemma 7.6 (Lemma G.15, [KLP+16]). There exists an algorithm, given any demand vector d ∈ Rn,\\nreturns in O(n\\x0f−4) work a graph K with O(n\\x0f−4) edges such that e−\\x0fK is an e2\\x0f-approximate spectral\\nsparsifier of G(d).\\nBy letting \\x0f = 2 and d = degD in Lemma 7.6, we have an 100-approximate spectral sparsifier of D\\n(by scaling K), which is a no(1)-approximate spectral sparsifier of H. Each expander is preserved up to\\nan approximation factor of no(1), so we get an no(1) approximation of the overall graph.\\n7.4 Laplacian Solvers and Laplacian-based Graph Algorithms\\nPrior to our work, the fastest deterministic Laplacian solver is by Spielman and Teng [ST03] has running\\ntime O˜(m1.31 log 1\\x0f ). All faster solvers with near-linear running time are based on randomized spectral\\nsparsifiers (e.g. [ST14]) or inherently randomized [KS16]. By applying the deterministic algorithm for\\ncomputing spectral sparsifiers from Corollary 7.3, we immediately obtain that deterministic Laplacian\\nsolvers with almost linear running time.\\nFormally stating such results requires the definition of errors, which are based on matrix norms. For\\nany matrix A, an A-norm of a vector x is defined by ||x||A =\\n√\\nx>Ax and let A† denote the Moore-Penrose\\npseudoinverse of A, which is the matrix with the same nullspace as A that acts as the inverse of A on its\\nimage.\\nCorollary 7.7. There is a deterministic algorithm that, given a Laplacian L size n×n with m non-zeroes\\nand a vector b ∈ Rn, compute a vector x such that ||x− L†b||L ≤ \\x0f||L†b||L in time Ô(m log 1\\x0f ).\\nThis result follows because spectral sparsifiers are the only randomized components in the Spielman-\\nTeng Laplacians solvers [ST14]. Although Spielman and Teng employ 1 + \\x0f-approximate spectral spar-\\nsifiers in their solvers, by paying no(1) factor in the running time, one can show that exactly the same\\napproach work even if we use no(1)-approximate spectral sparsifiers from Corollary 7.3.\\nThere are several graph algorithms [Mad10b, Mad16, CMSV17] based on interior point method that\\nneed to iteratively solve Laplacian systems several times. In those algorithms, solving Laplacians is the\\nonly randomized subroutine. Therefore, the Ô(m3/2 logW ) bound of interior point methods for graph\\nstructured matrices by Daitch and Spielman [DS08] becomes deterministic. This immediately implies\\nalgorithms for\\n• maximum flow in directed graphs with m edges and edge capacities up to W ,\\n• minimum-cost, and loss generalized flows in directed graphs with m edges and edge capacities in\\n[0,W ] and edge costs in [−W,W ].\\nthat run in deterministic Ô(m3/2 logW ) time. See [DS08] for the discussion about the history of these\\nproblems. Furthermore, by derandomizing the interior-point-methods-based results from [Mad13, Mad16,\\nCMSV17], the following problems in directed m-edge graphs with edge costs/weights in the range [−W,W ]\\ncan also be solved in deterministic Ô(m10/7 logW ) time. The problems includes\\n• unit-capacity maximum flow and maximum bipartite matching,\\n• single-source shortest path (with negative weight),\\n34\\n• minimum-cost bipartite perfect matching,\\n• minimum-cost bipartite perfect b-matching, and\\n• minimum-cost unit-capacity maximum flow.\\nSee [CMSV17] for the discussion about the history of these problems.\\n7.5 Approximate Max-Flows\\nA (1+\\x0f)-approximate max flow in undirected edge-capacitated graphs can be computed in near-linear time\\nby randomized algorithms [She13, KLOS14, Pen16]. The currently fastest algorithm takes O˜(m/\\x0f) time\\nand is by Sherman [She17]. Below, we present a Ô(m/\\x0f)-time deterministic algorithm. The deterministic\\nspectral sparsifier algorithm from Corollary 7.3 also implies derandomization of the following results.\\n1. the mixed `2 and `p norm sparsifier in unit-capacitated graphs by Kyng, Peng, Sachdeva, and\\nWang [KPSW19].\\n2. the cut-approximation algorithm in capacitated undirected graphs by Madry [Mad10b], and the\\ncongestion-approximator by Sherman [She13],\\n3. the oblivious routing scheme in capacitated undirected graphs by Kelner, Lee, Orecchia, and Sid-\\nford [KLOS14], and\\nitem 1, namely sparse approximations that preserve mixed `2 and `p objectives also implies a de-\\nrandomization of the overall `p-norm flow algorithm in [KPSW19]. This is due to the direct analog\\nbetween that algorithm and the Spielman-Teng solver [ST14], namely that it builds ultra-sparsifiers by\\nmoving edges around a low-stretch spanning tree, and then sparsifies the graph on a smaller subset of\\nvertices. This gives an algorithm for computing the minimum energy `p flows to an additive error of\\n1/poly(n) in undirected unit weighted graphs that runs in time Ô(exp(O(p3/2))m\\n1+O( 1√p )). This sub-\\nroutine can also be applied to the more general weighted setting, leading to running times of the form\\nÔ(exp(O(p))(m + n\\n|p−2|\\n2p+|p−2| )) for 1/poly(n) relative error solutions [AKPS19]. It can also be used in\\nconjunction with subsequent improvements on the p dependences [APS19, AS19].\\nCombining the oblivious routings/congestion approximators with the area convexity based accelerated\\nmethods by Sherman [She17] gives a deterministic, almost linear time, approximate max-flow algorithm\\nwith 1/\\x0f dependencies.\\nCorollary 7.8. There is a deterministic algorithm that, given (G, b, \\x0f) where G = (V,E, c) is an m-edge\\nundirected capacitated graph with edge capacity ratio C = maxe cemine ce , b ∈ RV is a demand vector where∑\\nv∈V bv = 0, and \\x0f > 0 is an accuracy parameter, returns in time TMaxFlow(m,C, \\x0f) = Ô(m\\x0f\\n−1 logC)\\neither\\n• (Flow): a flow satisfying the demand b with congestion (1 + \\x0f), or\\n• (Cut): a cut S such that ∑e∈E(S,S) ce <∑v∈S bv.\\nThis result, in fact, generalizes to multi-commodity flow [She17] as well. On the other hand, the more\\nrecent method by Sidford and Tian [ST18] also uses randomization in its intermediate steps: it remains\\nan interesting question whether it can be derandomized as well.\\nThe single-tree based routing scheme by Ra¨cke, Shah, and Taubig [RST14] can also be derandomized\\nby using our balanced cut routine as the cut player in its cut-matching game. However, this use introduces\\nagain an overhead of no(1) in the approximation ratio. Therefore, matching the best randomized overhead\\nof polylog(n) for approximate maximum flow [Pen16] may be the most direct goal for improving the\\nperformances of these derandomized graph preconditioning routines.\\n35\\n7.6 Low Vertex Expansion Cut\\nNext, we show that a fast algorithm for computing a cut with low vertex expansion, a notion similar to\\nconductance and sparsity. For any graph G = (V,E), a vertex cut (L, S,R) is a partition of V such that\\nL,R 6= ∅ and EG(L,R) = ∅. A vertex expansion of a vertex cut (L, S,R) is denoted by hG(L, S,R) =\\n|S|\\nmin{|L|,|R|}+|S| and a vertex expansion of a graph is denoted by hG = min(L,S,R) hG(L, S,R). We define\\nbalance of (L, S,R) as balG(L, S,R) =\\nmin{|L|,|R|}+|S|\\nmax{|L|,|R|}+|S| .\\nCorollary 7.9. There is a deterministic algorithm that, given (G,φ, β) where G is an m-edge n-node\\nundirected graph G, φ ∈ [0, 1] is a vertex expansion parameter, and β ∈ [0, 1] is a balanced parameter, in\\ntime Ô(min{m/φ,m√n}) either\\n1. declares there is no vertex cut (L, S,R) where hG(L, S,R) < φ and balG(L, S,R) > β, or\\n2. finds a vertex cut (L, S,R) where hG(L, S,R) ≤ φ2O(logn log logn)2/3 and balG(L, S,R) ≥ β/2O(logn log logn)2/3 .\\nThis algorithm is a vertex expansion variant of the MostBalance algorithm that we will prove for-\\nmally later in Lemma 8.9. The main difference is that our goal now is to embed an expander with small\\ncongestion on nodes instead of on edges. So, in each round of the cut-matching game, we need to solve\\na variant of the CutMatch problem with small vertex congestion. This can be implemented in either\\nO˜(m/φ) time (based on the bounded-height push relabel or blocking flow similar the Push-CutMatch\\nalgorithm, or in time O˜(m\\n√\\nn) (based on the deterministically vertex capacitated exact max flow al-\\ngorithm by Theorem 1.4 of [NSY19]). Following the same proof of Lemma 8.9, we obtain the above\\nclaim.\\n7.7 Vertex Max Flow, Min Cut, and Lowest Vertex Expansion Cut\\nWe can also derandomize the decremental single-source shortest path (SSSP) algorithm by Chuzhoy and\\nKhanna [CK19].\\nCorollary 7.10. There is a deterministic algorithm that, given a simple undirected weighted n-node\\ngraph G = (V,E) undergoing vertex deletions, a special source vertex s ∈ V , and a parameter \\x0f > 0,\\nsupports queries path-query(v). For each query path-query(v), the algorithm returns a path from s to v,\\nif such a path exists, whose length is at most (1 + \\x0f)dist(s, v). The total running time of the algorithm\\nis Ô(n2\\x0f−2 log3(1/\\x0f) logL) where L is the ratio of largest to smallest edge weights, and each query is\\nanswered in Ô(n logL log(1/\\x0f)) time.\\nProof. By applying Corollary 7.9 with φ = 1/no(1), we can derandomize the construction of the core\\ndecomposition in [CK19] which is the only randomized component in the whole paper.\\nChuzhoy and Khanna [CK19] also show applications of their algorithm to computing approximate\\nvertex max flow, min cut, and cut with lowest vertex expansion. As the reductions are deterministic, we\\nimmediately obtain:\\nCorollary 7.11. There is a deterministic algorithm that, given an undirected n-node graph G = (V,E)\\nwith capacities c(v) ≥ 0 on vertices, a source s, a sink t, and a parameter \\x0f ∈ (0, 1], computes a (1 + \\x0f)-\\napproximate maximum s-t flow and a (1 + \\x0f)-approximate minimum vertex s-t cut in Ô(n2/poly\\x0f) time.\\nCorollary 7.12. There is a deterministic algorithm that, given an undirected n-node graph G = (V,E),\\ncomputes a O(log4 n)-approximate to the lowest vertex expansion cut in Ô(n2) time.\\nNote that this algorithm is incomparable to one from Corollary 7.9.\\n36\\n8 Faster BalCutPrune in Low Conductance Regime\\nThe algorithm for BalCutPrune from Section 6 is slow in the low conductance regime. More specifically,\\nthe running time is m1+Ω(1) when φU < 1/n\\n\\x0f for any constant \\x0f > 0. In this section, we remove the\\ndependency on φU from the running time of BalCutPrune while still taking almost linear time.\\nTheorem 8.1. For any number ` ≥ 1, there is an algorithm for BalCutPrune that, for any (m,φU , φL)\\nwhere\\nφU ∈ [0, 1] and φL ≤ φU/ logO(`\\n2)m,\\nhas running time TBalCutPrune(m,φU , φL) = Ô(m\\n1+4/` logO(`\\n2)m).\\nBy setting ` = Θ((logm/ log logm)1/3), this gives Corollary 8.2 which is a restatement of our main\\nresult of this paper in Theorem 1.2.\\nCorollary 8.2. There is an algorithm for BalCutPrune that, for any (m,φU , φL) where\\nφU ∈ [0, 1] and φL ≤ φU/2O(log2/3 n·(log logn)1/3),\\nhas running time TBalCutPrune(m,φU , φL) = O(m2\\nO(log2/3 n·(log logn)1/3)) = Ô(m).\\nThe outline for proving Theorem 8.1 is as follows. In Section 8.1, we first show theMaxflow-CutMatch\\nalgorithm for solving CutMatch problem whose running time does not depend on φ by exploiting the\\ndeterministic approximate max flow algorithm from Corollary 7.8. Combining with the almost-linear-time\\nBalCutPrune algorithm in the high conductance regime from Theorem 6.1, we obtain an algorithm for\\nCutAugment without dependency on φU in the running time.\\nOne way to obtain BalCutPrune algorithm from the one for CutAugment is to again apply\\nthe chain of reductions from (10) in Section 4. However, because of the Prune algorithm, this would\\nintroduce dependency on φU . In Section 8.2, we show how to obtain BalCutPrune algorithm from\\nCutAugment algorithm without paying 1/φU factors. We note that all ideas of the proofs in this section\\nwere implicit in Section 5 of [NS17]. However, the problem definitions in our paper are quite different\\nfrom theirs. Therefore, we include the proof for completeness.\\nFinally, we discuss applications of Corollary 8.2 in Section 8.3.\\n8.1 CutMatch and CutAugment via Max Flow\\nIn this section, we show the Maxflow-CutMatch algorithm for solving the CutMatch problem where\\nthe running time does not depend on the conductance parameter φ.\\nLemma 8.3. There is an algorithm called Maxflow-CutMatch for solving the CutMatch problem\\nthat, for any (n, k, φ, cong, β) where k = 1 and cong ≥ 100φ−1 log n, has running time TCutMatch(n, 1, φ, cong, β) =\\nÔ(n).\\nRecall that we are given an n-node m-edge graph G = (V,E) with maximum degree 20, a conductance\\nparameter φ ∈ [0, 1], a congestion parameter cong ≥ 1, a balance parameter β ∈ [0, 1] where cong ≥\\n2φ−1 log n, and A,B are disjoint subsets of V where |A| = |B|. The algorithm description is shown\\nAlgorithm 5. Note that the idea of the algorithm and the proof is similar to Lemma B.18 of [NS17].\\nClaim 8.4. If a set S is returned from Item 4c of Algorithm 5, then βvolG/160 ≤ volG(S) ≤ volG/2 and\\nΦG(S) ≤ φ.\\nProof. We assume that volG(S) ≤ volG/2 because another case is symmetric. Let capG′({s}∪S, S ∪{t})\\nbe the total capacity of edges in G′ crossing the cut ({s}∪S, S∪{t}). Note that capG′({s}∪S, S∪{t}) <\\nx < |Aˆ|/2 ≤ |Aˆ| − βn/4 because |Aˆ| = |Tˆ |/2 and |Tˆ | ≥ βn. Observe that\\ncapG′({s} ∪ S, S ∪ {t}) = δG(S)/φ+ |Aˆ ∩ S|+ |Bˆ ∩ S|. (13)\\n37\\nAlgorithm 5 The implementation of Maxflow-CutMatch (for Lemma 8.3).\\nInput: An n-node graph G = (V,E) with maximum degree 20, a conductance parameter φ ∈ [0, 1], a\\ncongestion parameter cong ≥ 1 a balance parameter β ∈ [0, 1] where cong ≥ 2φ−1 log n, and A,B are\\ndisjoint subsets of V where |A| = |B|.\\nOutput: Let T = A ∪B be the set of terminals. Output either\\n1. a cut S where βvolG/160 ≤ volG(S) ≤ volG/2 and ΦG(S) ≤ φ, or\\n2. a collection P of paths connecting u ∈ A to v ∈ B such that each terminal is an endpoint of exactly\\none path, except at most βn many terminals which are not an endpoint of any path, and every\\nedge is contained in at most cong paths.\\nAlgorithm:\\n1. Initialize Aˆ← A, Bˆ ← B, P ← ∅\\n2. Let MaxFlow be an approximate max flow algorithm from Corollary 7.8.\\n3. Maintain as a graph Gˆ obtained from G by adding a source node s and edges connecting s to each\\nnode in Aˆ with capacity 1, adding a sink t and edges connecting t to each node in Bˆ with capacity\\n1, and setting the capacity of each original edge in G to be 1/φ. Maintain Tˆ = Aˆ ∪ Bˆ as a current\\nterminal set.\\n4. Repeat until |Tˆ | < βn:\\n(a) Let b(x) be a demand vector such that b\\n(x)\\ns = −b(x)t = x and bu = 0 for all u ∈ V .\\n(b) Binary search for x such that\\ni. MaxFlow(G′, 1, b(x)) returns a cut ({s} ∪ S, S ∪ {t}) of G′ and\\nii. MaxFlow(G′, 1, b(x/2)) return a flow f .\\n(c) If x < |Aˆ|/2, then return S if volG(S) ≤ volG/2, otherwise return S.\\n(d) Else,\\ni. Compute a collection of paths P̂ connecting Aˆ and Bˆ from a scaled flow f/2 using the\\nRoundDecompose algorithm from Claim 8.5.\\nii. P ← P ∪ P̂ and remove endpoints of paths in P̂ from Aˆ and Bˆ.\\n5. Return P.\\n38\\nAs |Aˆ ∩ S| ≤ capG′({s} ∪ S, S ∪ {t}) < |Aˆ| − βn/4, we have βn/4 < |Aˆ| − |Aˆ ∩ S| = |Aˆ ∩ S|. So\\nvolG(S) ≥ |S| ≥ |Aˆ ∩ S| ≥ βn/4 ≥ βvolG/80\\nwhere volG ≤ 20n because G has maximum degree at most 20. Next, to see that ΦG(S) ≤ φ, we have\\nby Equation (13)\\nδG(S)/φ ≤ capG′({s} ∪ S, S ∪ {t})− |Aˆ ∩ S|\\n< |Aˆ| − |Aˆ ∩ S|\\n= |Aˆ ∩ S| ≤ |S| ≤ volG(S).\\nWe have δG(S) ≤ φvolG(S). So ΦG(S) ≤ φ.\\nNext, we show the property of the collection of paths P̂ in Item 4(d)i of Algorithm 5.\\nClaim 8.5. We can compute P̂ from the scaled flow f/2 at Item 4(d)i of Algorithm 5 in O˜(m) = O˜(n)\\ntime such that\\n• Each path P in P̂ connects a node in Aˆ to another node in Bˆ.\\n• |P̂| ≥ |Tˆ |/8.\\n• Each terminal node is an endpoint of at most one path in P̂.\\n• Each edge e is contained in at most 1/φ many paths in P̂.\\nProof. We assume that 1/φ is integral, otherwise the proof is similar. There is an algorithm called\\nRoundDecompose such that, given an s-t arbitrary flow f ′ with value |f ′| in an n-node m-edge graph\\nwith integral capacity, computes in time O(m log n) a collection P ′ of s-t paths such that |P ′| ≥ |f ′| and\\neach edge e is contained in at most d|f ′e|e many paths. This can be done by first rounding the flow f ′ so\\nthat f ′ is integral [KP15], and then performing the standard flow-path decomposition using the link-cut\\ntree [ST83]. These algorithms are deterministic.\\nWe apply RoundDecompose to f/2. Let P̂ be obtained from the set of returned paths by removing\\nthe endpoint s and t from each path. It is easy to see that each path connects a node in Aˆ to another\\nnode in Bˆ. Also, we have |P̂| ≥ |f |/2 ≥ x/2. Observe that x ≥ |Tˆ /4| because x ≥ |Aˆ|/2 = |Tˆ |/4. So\\n|P̂| ≥ |Tˆ |/8.\\nRecall that we set \\x0f = 1 when we invoke MaxFlow to compute f . So original edges of G can have\\nflow value at most 2 × 1/φ and edges incident to s and t can have flow value at most 2. As we invoke\\nRoundDecompose on f/2, terminal node is an endpoint of at most 2/2 = 1 path in P̂ and each edge e\\nis contained in at most 1/φ many paths in P̂.\\nClaim 8.6. There are at most 30 log n iterations in the loop, and so the total running time of Algorithm 5\\nis Ô(n).\\nProof. By Claim 8.5, |P̂| ≥ |Tˆ |/8. As we immediately the endpoints of paths from P̂ from Tˆ . |Tˆ | is\\nreduced by 7/8 factor in every iteration. So there are 30 log n iterations. Each call to MaxFlow from\\nCorollary 7.8 takes Ô(m) = Ô(n) time. The time spent on other steps are dominated by this. As there\\nare only poly log n calls to MaxFlow, the total running time is Ô(n).\\nClaim 8.7. If P is returned from Item 5 of Algorithm 5, then P satisfies the conditions of the CutMatch\\nproblem (Item 2 of Definition 3.1).\\n39\\nProof. By Claim 8.5, each path P ∈ P connects a node u ∈ A to another node v ∈ B. Each terminal is\\nan endpoint of at most one path because we immediately remove the endpoints of P whenever we add P\\nto P in Item 4(d)ii of Algorithm 5. As |Tˆ | < βn in the end, at most βn many terminals are not endpoints\\nof paths from P.\\nBy Claim 8.5, each edge is contained in at most 3/φ paths from P̂ in each iteration. As there are only\\nO(log n) iterations, we have that every edge is contained in at most 3φ−1×30 log n ≤ 100φ−1 log n = cong\\npaths from P.\\nClaim 8.4 and Claim 8.7 show correctness of the algorithm. Claim 8.6 bounds the running time. This\\ncompletes the proof of Lemma 8.3. From this, we also obtain an algorithm for CutAugment whose\\nrunning time does not depend on the conductance parameters:\\nLemma 8.8. Let c2 and c3 be the constants from Lemma 6.6. For any ` ≥ 1, there is an algorithm for\\nsolving the CutAugment problem such that, for any (m,φU , φL, βcut, βaug) where\\nφL ≤ φU\\n(c2 logm)12`+8\\nand βcut ≤ βaug\\nc3 logm\\nhas running time TCutAugment(m,φU , φL, βcut, βaug) = Ô(m\\n1+3/` logO(`\\n2)m). Moreover, given a graph\\nG = (V,E), in the case that the algorithm returns an augmenting set F (Item 2 in Definition 4.3), a\\ngraph W = (V,E′) is also returned where\\n• F ⊆ E′,\\n• ΦW ≥ 1/9(c1 logm)12`+3,\\n• W can be embedded into G ∪ F with congestion O( log2mφU ), and\\n• max∅6=S⊂V {volW (S)volG(S) ,\\nvolG(S)\\nvolW (S)\\n} = O(logm).\\nProof. Let ckkov, cmatch, c1 be constants from Theorem 5.5, Theorem 3.2, and Lemma 6.5 respectively.\\nWe set\\nR = 2ckkov log(2m), cong =\\ncmatch log 2m\\nφU\\nand α =\\n1\\n(c1 logm)12`+2\\n.\\nThese parameters are the exactly same as in the proof of Lemma 6.6. So, from Equation (12) in the proof\\nof Lemma 6.6, we have\\nTCutAugment(m,φU , φL, βcut, βaug) = O(m\\n1+3/` logO(`\\n2)m) +R · TCutMatch(2m, 1, φU , cong, 160βcut).\\nBy Lemma 8.3, we have TCutMatch(2m, 1, φU , cong, 160βcut) = Ô(m). Putting them together completes\\nthe first part of the lemma. For the moreover part, we the algorithm from Lemma 6.6 invokes Algorithm 4\\nfrom Lemma 5.1. By Remark 5.1, the graph W = (V,E′) is also computed with such that F ⊆ E′, ΦW ≥\\nα/9R ≥ 1/9(c1 logm)12`+3 and W can be embedded into G∪F with congestion R ·cong = O( log\\n2m\\nφU\\n).\\n8.2 BalCutPrune without calling Prune\\nIn this section, we prove Theorem 8.1. We first show that the CutAugment algorithm from previous\\nsection implies an algorithm for computing approximately most-balanced low conductance cut as formally\\nstated in Lemma 8.9. Then, we invoke the known reduction to obtain the BalCutPrune algorithm for\\nTheorem 8.1.\\nWe say that S∗ is a most-balanced cut with conductance φ if volG(S∗) ≤ volG/2 and S∗ has max-\\nimum volume among all cuts with conductance at most φ. The following algorithm follows from the\\nCutAugment algorithm from Lemma 8.8.\\n40\\nLemma 8.9. For any ` ≥ 1, there is a deterministic algorithm called MostBalance that, given (G,φ)\\nwhere G is an m-edge graph and φ ∈ [0, 1] is a conductance parameter, in time TMostBalance(m, `) =\\nÔ(m1+3/` logO(`\\n2)m) does the following: Let γsize = γsize(m, `) = log\\nO(`)m and γexp = γexp(m, `) =\\nlogO(`)m . The algorithm either:\\n• returns a cut S where volG(S∗)/γsize ≤ volG(S) ≤ volG/2 and ΦG(S) ≤ φγexp, where S∗ is a\\nmost-balanced cut with conductance φ.\\n• declares that ΦG ≥ φ.\\nProof. Let c2 and c3 be a constant from Lemma 8.8. Given φ, we set φL = 2φ and\\nφU = max{φL(c2 logm)12`+8, 9c′(c1 logm)12`+6}\\nwhere c′ is a large enough constant. Let A be the algorithm for CutAugment from Lemma 8.8. We let\\nA(β) denote the algorithm A when given parameters (G,φU , φL, βc3 logm , β). Recall from Definition 4.3\\nthat A(β) either return a set S of nodes or return a set F of edges.\\nNote that we can assume that A(1) returns a set F because there always a exists |F | ≤ volG where\\nΦG∪F ≥ φL. Next, we check if A(0) returns a set F of edges where |F | = 0. In this case, we declare that\\nΦG ≥ φL ≥ φ. Now, we assume A(0) returns a node set S and A(1) returns an edge set F . Therefore,\\nwe can binary search for βS and βF where βS ≤ βF ≤ 2βS where A(βS) returns a node set S and A(βF )\\nreturns an edge set F together with a graph W . We will just return S as our output. The running time\\nis clearly Ô(m1+3/` logO(`\\n2)m) by Lemma 8.8.\\nConsider any set Sˆ where 2|F |/ΦW ≤ volW (Sˆ) ≤ volW /2. We have that δW−F (Sˆ) ≥ ΦWvolW (Sˆ) −\\n|F | ≥ |F |. Now, as W can be embedded into G ∪ F with congestion cong = O( log2mφU ). So W − F can\\nbe embedded into G with congestion cong as well. This means that δG(Sˆ) ≥ |F |/cong. As volG(Sˆ) and\\nvolW (Sˆ) are within O(logm) factor away. We have that\\nΦG(Sˆ) =\\nδG(Sˆ)\\nmin{volG(Sˆ), volG(V − Sˆ)}\\n≥ |F |/cong\\nO(logm) ·min{volW (Sˆ), volW (V − Sˆ)}\\n≥ |F |/cong\\nO(logm) · 2|F |/ΦW\\n≥ Ω(φUΦW / log3m)\\n> φ\\nwhere the last inequality is because ΦW ≥ 1/9(c1 logm)12`+3 and the choice of φU .\\nLet S∗ be a most-balanced cut with conductance φ. The above argument implies that volG(S∗) ≤\\n2|F |/ΦW ≤ ( 4ΦW )βSvolG because |F | ≤ βFvolG and βF ≤ 2βS . On the other hand, we have that the set\\nS is such that ΦG(S) ≤ φU ≤ φγexp, volG(S) ≤ volG/2, and\\nvolG(S) ≥ βSvolG\\n≥ volG(S∗)ΦW /4\\n≥ volG(S∗)/γsize(m).\\nGiven an algorithm MostBalance for computing nearly most balanced low conductance cut as a\\nblack-box, we can obtain an algorithm BalCutPrune immediately (and even an expander decompo-\\nsition) using the known reduction. This reduction is first shown independently by [NS17] and [Wul17].\\nThen, it is also used in [CK19] and [CS19]. We omit the proof of this reduction in this version.\\n41\\nLemma 8.10. Let TMostBalance(m, `), γexp(m, `), γsize(m, `) be the functions from Lemma 8.9. For\\nany number d ≥ 1, there is an algorithm for solving the BalCutPrune problem such that, for any\\n(m,φU , φL) where\\nφU ∈ [0, 1] and φL ≤ φU\\n(γexp(m, `))d\\n,\\nhas running time TBalCutPrune(m,φU , φL) ≤ O(TMostBalance(m) · d · γsize(m, `) ·m1/d).\\nBy setting d = `, we complete the proof of Theorem 8.1.\\n8.3 Applications in Low Conductance Regime\\nFirst, observe that we can no(1)-approximate graph conductance in almost-linear time independent of the\\nvalue of the conductance itself:\\nCorollary 8.11. There is a deterministic algorithm that, given an m-edge undirected graph G, compute\\na (2O(logn log logn)\\n2/3\\n)-approximation to the lowest-conductance cut in time Ô(m).\\nThis follows by calling the CutAugment algorithm from Lemma 8.8 where βcut = 0, βaug = 0,\\nφL ≤ φU(c2 logm)12`+8 , ` = Θ((logm/ log logm)1/3) and doing binary search on φU . It is not hard to adjust\\nthe proof so that we can approximate sparsity of the graph instead of conductance in Ô(m) time as well.\\nNext, observe that Corollary 8.12 immediately implies an almost-linear time algorithm for computing\\n(\\x0f, \\x0f/no(1))-expander decomposition even for very small \\x0f. The proof is the same as the one in Corol-\\nlary 7.1:\\nCorollary 8.12. There is an algorithm that, given a graph G = (V,E) with m edges and a parameter\\n\\x0f ∈ (0, 1], returns a (\\x0f, \\x0f/2O(logm log logm)2/3)-expander decomposition of G in time Ô(m).\\nGiven Corollary 8.12, we can significantly simplify Kawarabayashi and Thorup’s [KT19] deterministic\\nalgorithm for computing edge connectivity. The idea is as follows: Suppose we are given an n-node\\nm-edge graph with minimum degree δ. The key step in [KT19] is to compute the KT-decomposition of\\na graph into clusters such that, each cluster can overlap any min cut by at most two nodes. In [KT19],\\nthey give a quite involved algorithm for computing this decomposition.\\nWe argue that, by paying a no(1) in the running time, this decomposition algorithm can be simplified\\ninto a three-step algorithm: First, compute a δ-sparse certificate [NI92] as in [KT19]. Second, compute\\nexpander decomposition on the sparse certificate using Corollary 8.12 with \\x0f = no(1)/δ. Third, trim each\\nexpander from the previous step where the operation trim is defined in [KT19]. It can be shown in each\\ntrimmed expander must be a cluster according to KT-decomposition (see [DHNS19]). After contracting\\nthe core of each cluster (as defined in [KT19]), we will obtain a graph with n1+o(1) edges and n1+o(1)/δ\\nnodes where all non-trivial min cuts are preserved. So we can compute the edge connectivity on this\\ncontracted graph in Ô(m) time using Gabow’s algorithm as [KT19] does.\\n9 Open Problems\\nThe guarantee of our algorithm (Theorem 1.2) contains the no(1) term in both the approximation factor\\n(the ratio φUφL ) and time complexity. An obvious open problem is to remove the n\\no(1) term in both\\nplaces to achieve a deterministic polylogarithmic-approximation in near-linear time. This will also imply\\na near-linear time deterministic algorithm for computing an expander decomposition (a randomized one\\nwas known, due to [SW19]).\\nIt is typically desirable for dynamic graph algorithms to have polylogarithmic update time complexity.\\nOur result for dynamic connectivity (Theorem 1.4) only guarantees no(1) update time. Designing a\\ndeterministic algorithm with polylogarithmic update time for dynamic connectivity remains a major\\n42\\nopen problem. In fact, it is already very interesting to achieve such bound with a Las Vegas randomized\\nalgorithm. It is also very interesting to design a Monte Carlo randomized algorithm for maintaining\\na spanning forest in polylogarithmic update time that does not need the so-called oblivious adversary\\nassumption.10 Note that eliminating the no(1) terms in Theorem 1.2 is not enough to achieve any of these\\ngoals. This is because we also need the algorithm of Nanongkai et al. [NSW17] show Theorem 1.4, and\\nmany components of such algorithm incurs the no(1) term in the update time.\\nOur deterministic spectral sparsifier from Corollary 7.3 only gives no(1)-approximation. It is an\\nintriguing question whether (1+\\x0f)-approximate cut/spectral sparsifiers can be computed deterministically\\nin almost-linear time. It is also interesting if we can O(\\n√\\nlog n)-approximate a conductance of a graph\\ndeterministically. The best randomized algorithm requires O(m1+\\x0f) time for an arbitrarily small constant\\n\\x0f > 0 [She09]. We believe that both questions require significantly new ideas.\\nAcknowledgement\\nThis project has received funding from the European Research Council (ERC) under the European\\nUnion’s Horizon 2020 research and innovation programme under grant agreement No 715672. Nanongkai\\nwas also partially supported by the Swedish Research Council (Reg. No. 2015-04659).\\nReferences\\n[ACL07] Reid Andersen, Fan R. K. Chung, and Kevin J. Lang. Using pagerank to locally partition a\\ngraph. Internet Mathematics, 4(1):35–64, 2007. 1, 3, 7\\n[AGOT92] Ravindra K. Ahuja, Andrew V. Goldberg, James B. Orlin, and Robert Endre Tarjan. Finding\\nminimum-cost flows by double scaling. Math. Program., 53:243–266, 1992. 4\\n[AHK10] Sanjeev Arora, Elad Hazan, and Satyen Kale. O(\\n√\\nlog n) approximation to SPARSEST CUT\\nin O˜(n2) time. SIAM J. Comput., 39(5):1748–1771, 2010. 1, 22\\n[AKPS19] Deeksha Adil, Rasmus Kyng, Richard Peng, and Sushant Sachdeva. Iterative refinement\\nfor `p-norm regression. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on\\nDiscrete Algorithms, pages 1405–1424. SIAM, 2019. 35\\n[Alo86] Noga Alon. Eigenvalues and expanders. Combinatorica, 6(2):83–96, 1986. 33\\n[ALO15] Zeyuan Allen Zhu, Zhenyu Liao, and Lorenzo Orecchia. Spectral sparsification and regret\\nminimization beyond matrix multiplicative updates. In Proceedings of the Forty-Seventh\\nAnnual ACM on Symposium on Theory of Computing, STOC 2015, Portland, OR, USA,\\nJune 14-17, 2015, pages 237–245, 2015. 33\\n[APS19] Deeksha Adil, Richard Peng, and Sushant Sachdeva. Fast, provably convergent IRLS algo-\\nrithm for p-norm linear regression. CoRR, abs/1907.07167, 2019. To appear in NeurIPS 2019.\\n35\\n[ARV09] Sanjeev Arora, Satish Rao, and Umesh V. Vazirani. Expander flows, geometric embeddings\\nand graph partitioning. J. ACM, 56(2):5:1–5:37, 2009. 22\\n[AS19] Deeksha Adil and Sushant Sachdeva. Faster p-norm minimizing flows, via smoothed q-norm\\nproblems. To appear in SODA 2020, 2019. 35\\n10It was known, due to Kapron et al. [KKM13], that a spanning forest can be maintained in polylogarithmic update time\\nby a Monte Carlo randomized algorithm under the oblivious adversary assumption.\\n43\\n[BC16] Aaron Bernstein and Shiri Chechik. Deterministic decremental single source shortest paths:\\nbeyond the o(mn) bound. In Proceedings of the 48th Annual ACM SIGACT Symposium on\\nTheory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pages 389–397,\\n2016. 17\\n[Ber17] Aaron Bernstein. Deterministic partially dynamic single source shortest paths in weighted\\ngraphs. In LIPIcs-Leibniz International Proceedings in Informatics, volume 80. Schloss\\nDagstuhl-Leibniz-Center for Computer Science, 2017. 17\\n[BR11] Aaron Bernstein and Liam Roditty. Improved dynamic algorithms for maintaining approxi-\\nmate shortest paths under deletions. In SODA, pages 1355–1365. SIAM, 2011. 17\\n[BSS12] Joshua Batson, Daniel A Spielman, and Nikhil Srivastava. Twice-Ramanujan sparsifiers.\\nSIAM Journal on Computing, 41(6):1704–1721, 2012. 4, 33\\n[CC13] Chandra Chekuri and Julia Chuzhoy. Large-treewidth graph decompositions and applications.\\nIn Symposium on Theory of Computing Conference, STOC’13, Palo Alto, CA, USA, June\\n1-4, 2013, pages 291–300, 2013. 6\\n[CC16] Chandra Chekuri and Julia Chuzhoy. Polynomial bounds for the grid-minor theorem. J.\\nACM, 63(5):40:1–40:65, 2016. 6\\n[CGP+18] Timothy Chu, Yu Gao, Richard Peng, Sushant Sachdeva, Saurabh Sawlani, and Junxing\\nWang. Graph sparsification, spectral sketches, and faster resistance computation, via short\\ncycle decompositions. In 59th IEEE Annual Symposium on Foundations of Computer Science,\\nFOCS 2018, Paris, France, October 7-9, 2018, pages 361–372, 2018. 7\\n[CK19] Julia Chuzhoy and Sanjeev Khanna. A new algorithm for decremental single-source shortest\\npaths with applications to vertex-capacitated flow and cut problems. In STOC, pages 389–\\n400. ACM, 2019. 1, 4, 7, 8, 9, 12, 17, 36, 41\\n[CKM+14] Michael B. Cohen, Rasmus Kyng, Gary L. Miller, Jakub W. Pachocki, Richard Peng, Anup B.\\nRao, and Shen Chen Xu. Solving SDD linear systems in nearly mlog1/2n time. In Symposium\\non Theory of Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages\\n343–352, 2014. 4\\n[CL16] Julia Chuzhoy and Shi Li. A polylogarithmic approximation algorithm for edge-disjoint paths\\nwith congestion 2. J. ACM, 63(5):45:1–45:51, 2016. 6\\n[CMSV17] Michael B. Cohen, Aleksander Madry, Piotr Sankowski, and Adrian Vladu. Negative-weight\\nshortest paths and unit capacity minimum cost flow in O˜(m10/7 logW ) time (extended ab-\\nstract). In SODA, pages 752–771. SIAM, 2017. 1, 4, 34, 35\\n[CS19] Yi-Jun Chang and Thatchaphol Saranurak. Improved distributed expander decomposition\\nand nearly optimal triangle enumeration. In Proceedings of the 2019 ACM Symposium on\\nPrinciples of Distributed Computing, PODC 2019, Toronto, ON, Canada, July 29 - August\\n2, 2019., pages 66–73, 2019. 41\\n[dCSHS16] Marcel Kenji de Carli Silva, Nicholas J. A. Harvey, and Cristiane M. Sato. Sparse sums of\\npositive semidefinite matrices. ACM Trans. Algorithms, 12(1):9:1–9:17, 2016. 4\\n[DHNS19] Mohit Daga, Monika Henzinger, Danupon Nanongkai, and Thatchaphol Saranurak. Dis-\\ntributed edge connectivity in sublinear time. In Proceedings of the 51st Annual ACM SIGACT\\nSymposium on Theory of Computing, STOC 2019, Phoenix, AZ, USA, June 23-26, 2019.,\\npages 343–354, 2019. 42\\n44\\n[Din70] Efim A Dinic. Algorithm for solution of a problem of maximum flow in networks with power\\nestimation. In Soviet Math. Doklady, volume 11, pages 1277–1280, 1970. 17\\n[DS08] Samuel I. Daitch and Daniel A. Spielman. Faster approximate lossy generalized flow via\\ninterior point algorithms. In Proceedings of the 40th annual ACM symposium on Theory\\nof computing, STOC ’08, pages 451–460, New York, NY, USA, 2008. ACM. Available at\\nhttp://arxiv.org/abs/0803.0988. 4, 34\\n[EGIN97] David Eppstein, Zvi Galil, Giuseppe F. Italiano, and Amnon Nissenzweig. Sparsification - a\\ntechnique for speeding up dynamic graph algorithms. J. ACM, 44(5):669–696, 1997. 1, 5, 32\\n[ES81] Shimon Even and Yossi Shiloach. An on-line edge-deletion problem. Journal of the ACM\\n(JACM), 28(1):1–4, 1981. 8, 12, 17\\n[Fre85] Greg N. Frederickson. Data structures for on-line updating of minimum spanning trees, with\\napplications. SIAM J. Comput., 14(4):781–798, 1985. Announced at STOC’83. 1, 5, 32\\n[GG81] Ofer Gabber and Zvi Galil. Explicit constructions of linear-sized superconcentrators. J.\\nComput. Syst. Sci., 22(3):407–420, 1981. announced at FOCS’79. 11\\n[GKKT15] David Gibb, Bruce M. Kapron, Valerie King, and Nolan Thorn. Dynamic graph connectivity\\nwith improved worst case update time and sublinear space. CoRR, abs/1509.06464, 2015. 5\\n[GLN+] Yu Gao, Jason Li, Danupon Nanongkai, Richard Peng, Thatchaphol Saranurak, and Sorrachai\\nYingchareonthawornchai. Deterministic graph cuts in subquadratic time: Sparse, balanced,\\nand k-vertex. unpublished. 1, 4, 6, 7, 23, 31\\n[Gol95] Andrew V. Goldberg. Scaling algorithms for the shortest paths problem. SIAM J. Comput.,\\n24(3):494–504, 1995. 4\\n[GR98] Andrew V. Goldberg and Satish Rao. Beyond the flow decomposition barrier. J. ACM,\\n45(5):783–797, 1998. 4\\n[GR99] Oded Goldreich and Dana Ron. A sublinear bipartiteness tester for bounded degree graphs.\\nCombinatorica, 19(3):335–373, 1999. 31\\n[GT87] Andrew V. Goldberg and Robert Endre Tarjan. Solving minimum-cost flow problems by\\nsuccessive approximation. In Proceedings of the 19th Annual ACM Symposium on Theory of\\nComputing, 1987, New York, New York, USA, pages 7–18, 1987. 4\\n[GT88] Andrew V. Goldberg and Robert Endre Tarjan. A new approach to the maximum-flow\\nproblem. J. ACM, 35(4):921–940, 1988. 17\\n[GT89] Harold N. Gabow and Robert Endre Tarjan. Faster scaling algorithms for network problems.\\nSIAM J. Comput., 18(5):1013–1036, 1989. 4\\n[HdLT01] Jacob Holm, Kristian de Lichtenberg, and Mikkel Thorup. Poly-logarithmic deterministic\\nfully-dynamic algorithms for connectivity, minimum spanning tree, 2-edge, and biconnectivity.\\nJ. ACM, 48(4):723–760, 2001. Announced at STOC 1998. 5\\n[HHKP17] Shang-En Huang, Dawei Huang, Tsvi Kopelowitz, and Seth Pettie. Fully dynamic connec-\\ntivity in o(log n(log log n)2) amortized expected time. In SODA, 2017. 5\\n[HK97] Monika Rauch Henzinger and Valerie King. Maintaining minimum spanning trees in dynamic\\ngraphs. In ICALP, volume 1256 of Lecture Notes in Computer Science, pages 594–604.\\nSpringer, 1997. 5\\n45\\n[HK99] Monika Rauch Henzinger and Valerie King. Randomized fully dynamic graph algorithms with\\npolylogarithmic time per operation. J. ACM, 46(4):502–516, 1999. Announced at STOC 1995.\\n5\\n[HKN14a] Monika Henzinger, Sebastian Krinninger, and Danupon Nanongkai. Sublinear-time decre-\\nmental algorithms for single-source reachability and shortest paths on directed graphs. In\\nSTOC, pages 674–683. ACM, 2014. 17\\n[HKN14b] Monika Henzinger, Sebastian Krinninger, and Danupon Nanongkai. A subquadratic-time\\nalgorithm for decremental single-source shortest paths. In SODA, pages 1053–1072. SIAM,\\n2014. 17\\n[HKN16] Monika Henzinger, Sebastian Krinninger, and Danupon Nanongkai. Dynamic approximate\\nall-pairs shortest paths: Breaking the o(mn) barrier and derandomization. SIAM J. Comput.,\\n45(3):947–1006, 2016. Announced at FOCS’13. 17\\n[HKN18] Monika Henzinger, Sebastian Krinninger, and Danupon Nanongkai. Decremental single-source\\nshortest paths on undirected graphs in near-linear total update time. J. ACM, 65(6):36:1–\\n36:40, 2018. Announced at FOCS’14 and ICALP’15. 17\\n[HRW17] Monika Henzinger, Satish Rao, and Di Wang. Local flow partitioning for faster edge con-\\nnectivity. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete\\nAlgorithms, SODA 2017, Barcelona, Spain, Hotel Porta Fira, January 16-19, pages 1919–\\n1938, 2017. 17, 18\\n[HT97] Monika Rauch Henzinger and Mikkel Thorup. Sampling to provide or to bound: With\\napplications to fully dynamic graph algorithms. Random Struct. Algorithms, 11(4):369–379,\\n1997. 5\\n[Kin08] Valerie King. Fully dynamic connectivity. In Encyclopedia of Algorithms. Springer, 2008. 5\\n[Kin16] Valerie King. Fully dynamic connectivity. In Encyclopedia of Algorithms, pages 792–793.\\n2016. 5\\n[KKM13] Bruce M. Kapron, Valerie King, and Ben Mountjoy. Dynamic graph connectivity in polyloga-\\nrithmic worst case time. In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium\\non Discrete Algorithms, SODA 2013, New Orleans, Louisiana, USA, January 6-8, 2013, pages\\n1131–1142, 2013. 5, 43\\n[KKOV07] Rohit Khandekar, Subhash Khot, Lorenzo Orecchia, and Nisheeth K Vishnoi. On a cut-\\nmatching game for the sparsest cut problem. Univ. California, Berkeley, CA, USA, Tech.\\nRep. UCB/EECS-2007-177, 2007. 6, 7, 12, 21, 22, 23, 55\\n[KKPT16] Casper Kejlberg-Rasmussen, Tsvi Kopelowitz, Seth Pettie, and Mikkel Thorup. Faster worst\\ncase deterministic dynamic connectivity. In 24th Annual European Symposium on Algorithms,\\nESA 2016, August 22-24, 2016, Aarhus, Denmark, pages 53:1–53:15, 2016. 5, 32\\n[KLOS14] Jonathan A. Kelner, Yin Tat Lee, Lorenzo Orecchia, and Aaron Sidford. An almost-linear-\\ntime algorithm for approximate max flow in undirected graphs, and its multicommodity gen-\\neralizations. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete\\nAlgorithms, SODA 2014, Portland, Oregon, USA, January 5-7, 2014, pages 217–226, 2014.\\n1, 4, 35\\n[KLP+16] Rasmus Kyng, Yin Tat Lee, Richard Peng, Sushant Sachdeva, and Daniel A. Spielman.\\nSparsified cholesky and multigrid solvers for connection laplacians. In Proceedings of the 48th\\nAnnual ACM SIGACT Symposium on Theory of Computing, STOC 2016, Cambridge, MA,\\nUSA, June 18-21, 2016, pages 842–850, 2016. 33, 34\\n46\\n[KP15] Donggu Kang and James Payor. Flow rounding. CoRR, abs/1507.08139, 2015. 39\\n[KPSW19] Rasmus Kyng, Richard Peng, Sushant Sachdeva, and Di Wang. Flows in almost linear time\\nvia adaptive preconditioning. In Proceedings of the 51st Annual ACM SIGACT Symposium\\non Theory of Computing, STOC 2019, Phoenix, AZ, USA, June 23-26, 2019., pages 902–913,\\n2019. 1, 35\\n[KRV09] Rohit Khandekar, Satish Rao, and Umesh V. Vazirani. Graph partitioning using single\\ncommodity flows. J. ACM, 56(4):19:1–19:15, 2009. 1, 3, 4, 6, 7, 12, 22, 23\\n[KS16] Rasmus Kyng and Sushant Sachdeva. Approximate gaussian elimination for laplacians - fast,\\nsparse, and simple. In IEEE 57th Annual Symposium on Foundations of Computer Science,\\nFOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA, pages\\n573–582, 2016. 4, 34\\n[KT19] Ken-ichi Kawarabayashi and Mikkel Thorup. Deterministic edge connectivity in near-linear\\ntime. J. ACM, 66(1):4:1–4:50, 2019. 1, 42\\n[KVV04] Ravi Kannan, Santosh Vempala, and Adrian Vetta. On clusterings: Good, bad and spectral.\\nJ. ACM, 51(3):497–515, 2004. 31\\n[LR99] Frank Thomson Leighton and Satish Rao. Multicommodity max-flow min-cut theorems and\\ntheir use in designing approximation algorithms. J. ACM, 46(6):787–832, 1999. 22, 24, 28,\\n57\\n[LS14] Yin Tat Lee and Aaron Sidford. Path finding methods for linear programming: Solving\\nlinear programs in o˜(vrank) iterations and faster algorithms for maximum flow. In 55th IEEE\\nAnnual Symposium on Foundations of Computer Science, FOCS 2014, Philadelphia, PA,\\nUSA, October 18-21, 2014, pages 424–433, 2014. 4\\n[LS17] Yin Tat Lee and He Sun. An sdp-based algorithm for linear-sized spectral sparsification. In\\nProceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC\\n2017, Montreal, QC, Canada, June 19-23, 2017, pages 678–687, 2017. 4\\n[LSY19] Yang P. Liu, Sushant Sachdeva, and Zejun Yu. Short cycles via low-diameter decompositions.\\nIn SODA, pages 2602–2615. SIAM, 2019. 7\\n[Mad10a] Aleksander Madry. Fast approximation algorithms for cut-based problems in undirected\\ngraphs. In FOCS, pages 245–254. IEEE Computer Society, 2010. 4, 7\\n[Mad10b] Aleksander Madry. Faster approximation schemes for fractional multicommodity flow prob-\\nlems via dynamic graph algorithms. In Proceedings of the 42nd ACM Symposium on Theory\\nof Computing, STOC 2010, Cambridge, Massachusetts, USA, 5-8 June 2010, pages 121–130,\\n2010. 3, 34, 35\\n[Mad13] Aleksander Madry. Navigating central path with electrical flows: From flows to matchings,\\nand back. In Foundations of Computer Science (FOCS), 2013 IEEE 54th Annual Symposium\\non, pages 253–262. IEEE, 2013. Available at http://arxiv.org/abs/1307.2205. 4, 34\\n[Mad16] Aleksander Madry. Computing maximum flow with augmenting electrical flows. In FOCS,\\npages 593–602. IEEE Computer Society, 2016. 1, 4, 34\\n[NI92] Hiroshi Nagamochi and Toshihide Ibaraki. A linear-time algorithm for finding a sparse k-\\nconnected spanning subgraph of a k-connected graph. Algorithmica, 7(5&6):583–596, 1992.\\n42\\n47\\n[NS17] Danupon Nanongkai and Thatchaphol Saranurak. Dynamic spanning forest with worst-case\\nupdate time: adaptive, Las Vegas, and O(n1/2−\\x0f)-time. In Proceedings of the 49th Annual\\nACM SIGACT Symposium on Theory of Computing, STOC 2017, Montreal, QC, Canada,\\nJune 19-23, 2017, pages 1122–1129, 2017. 1, 3, 4, 5, 6, 20, 31, 37, 41\\n[NSW17] Danupon Nanongkai, Thatchaphol Saranurak, and Christian Wulff-Nilsen. Dynamic minimum\\nspanning forest with subpolynomial worst-case update time. In FOCS, pages 950–961. IEEE\\nComputer Society, 2017. 1, 3, 5, 6, 18, 32, 43\\n[NSY19] Danupon Nanongkai, Thatchaphol Saranurak, and Sorrachai Yingchareonthawornchai. Break-\\ning quadratic time for small vertex connectivity and an approximation scheme. In Proceedings\\nof the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC 2019, Phoenix,\\nAZ, USA, June 23-26, 2019., pages 241–252, 2019. 36\\n[OA14] Lorenzo Orecchia and Zeyuan Allen Zhu. Flow-based algorithms for local graph clustering.\\nIn Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms,\\nSODA 2014, Portland, Oregon, USA, January 5-7, 2014, pages 1267–1286, 2014. 17\\n[OSV12] Lorenzo Orecchia, Sushant Sachdeva, and Nisheeth K. Vishnoi. Approximating the expo-\\nnential, the lanczos method and an o˜(m)-time spectral algorithm for balanced separator. In\\nProceedings of the 44th Symposium on Theory of Computing Conference, STOC 2012, New\\nYork, NY, USA, May 19 - 22, 2012, pages 1141–1160, 2012. 3\\n[OSVV08] Lorenzo Orecchia, Leonard J. Schulman, Umesh V. Vazirani, and Nisheeth K. Vishnoi. On\\npartitioning graphs via single commodity flows. In Proceedings of the 40th Annual ACM\\nSymposium on Theory of Computing, Victoria, British Columbia, Canada, May 17-20, 2008,\\npages 461–470, 2008. 12\\n[OV11] Lorenzo Orecchia and Nisheeth K. Vishnoi. Towards an sdp-based approach to spectral meth-\\nods: A nearly-linear-time algorithm for graph partitioning and decomposition. In Proceedings\\nof the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2011,\\nSan Francisco, California, USA, January 23-25, 2011, pages 532–545, 2011. 3\\n[PD06] Mihai Patrascu and Erik D. Demaine. Logarithmic lower bounds in the cell-probe model.\\nSIAM J. Comput., 35(4):932–963, 2006. Announced at SODA’04 and STOC’04. 5\\n[Pen16] Richard Peng. Approximate undirected maximum flows in O(mpoly log(n)) time. In Proceed-\\nings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA\\n2016, Arlington, VA, USA, January 10-12, 2016, pages 1862–1867, 2016. 1, 4, 35\\n[PT07] Mihai Patrascu and Mikkel Thorup. Planning for fast connectivity updates. In FOCS, pages\\n263–271. IEEE Computer Society, 2007. 5\\n[Ra¨c02] Harald Ra¨cke. Minimizing congestion in general networks. In 43rd Symposium on Founda-\\ntions of Computer Science (FOCS 2002), 16-19 November 2002, Vancouver, BC, Canada,\\nProceedings, pages 43–52, 2002. 1\\n[RST14] Harald Ra¨cke, Chintan Shah, and Hanjo Ta¨ubig. Computing cut-based hierarchical decom-\\npositions in almost linear time. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Sym-\\nposium on Discrete Algorithms, SODA 2014, Portland, Oregon, USA, January 5-7, 2014,\\npages 227–238, 2014. 1, 4, 6, 35\\n[She09] Jonah Sherman. Breaking the multicommodity flow barrier for O(\\n√\\nlog n)-approximations to\\nsparsest cut. In 50th Annual IEEE Symposium on Foundations of Computer Science, FOCS\\n2009, October 25-27, 2009, Atlanta, Georgia, USA, pages 363–372, 2009. 22, 43\\n48\\n[She13] Jonah Sherman. Nearly maximum flows in nearly linear time. In FOCS, pages 263–269. IEEE\\nComputer Society, 2013. 1, 4, 35\\n[She17] Jonah Sherman. Area-convexity, l∞ regularization, and undirected multicommodity flow. In\\nProceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC\\n2017, Montreal, QC, Canada, June 19-23, 2017, pages 452–460, 2017. 4, 35\\n[SS11] D. Spielman and N. Srivastava. Graph sparsification by effective resistances. SIAM Journal\\non Computing, 40(6):1913–1926, 2011. 33\\n[ST83] Daniel Dominic Sleator and Robert Endre Tarjan. A data structure for dynamic trees. J.\\nComput. Syst. Sci., 26(3):362–391, 1983. 18, 39\\n[ST03] Daniel A. Spielman and Shang-Hua Teng. Solving sparse, symmetric, diagonally-dominant\\nlinear systems in time 0(m1.31). In 44th Symposium on Foundations of Computer Science\\n(FOCS 2003), 11-14 October 2003, Cambridge, MA, USA, Proceedings, pages 416–427, 2003.\\n4, 34\\n[ST04] Daniel A. Spielman and Shang-Hua Teng. Nearly-linear time algorithms for graph partition-\\ning, graph sparsification, and solving linear systems. In STOC, pages 81–90. ACM, 2004. 1,\\n3\\n[ST11] Daniel A. Spielman and Shang-Hua Teng. Spectral sparsification of graphs. SIAM J. Comput.,\\n40(4):981–1025, 2011. 4, 32\\n[ST14] Daniel A. Spielman and Shang-Hua Teng. Nearly linear time algorithms for precondition-\\ning and solving symmetric, diagonally dominant linear systems. SIAM J. Matrix Analysis\\nApplications, 35(3):835–885, 2014. 4, 34, 35\\n[ST18] Aaron Sidford and Kevin Tian. Coordinate methods for accelerating `∞ regression and faster\\napproximate maximum flow. In 59th IEEE Annual Symposium on Foundations of Computer\\nScience, FOCS 2018, Paris, France, October 7-9, 2018, pages 922–933, 2018. 35\\n[SW19] Thatchaphol Saranurak and Di Wang. Expander decomposition and pruning: Faster,\\nstronger, and simpler. In SODA, pages 2616–2635. SIAM, 2019. 1, 3, 4, 6, 7, 12, 18, 20,\\n31, 32, 42\\n[Tho00] Mikkel Thorup. Near-optimal fully-dynamic graph connectivity. In F. Frances Yao and Eu-\\ngene M. Luks, editors, Proceedings of the Thirty-Second Annual ACM Symposium on Theory\\nof Computing, May 21-23, 2000, Portland, OR, USA, pages 343–350. ACM, 2000. 5\\n[Tre05] Luca Trevisan. Approximation algorithms for unique games. In 46th Annual IEEE Symposium\\non Foundations of Computer Science (FOCS’05), pages 197–205. IEEE, 2005. 1\\n[Wul13] Christian Wulff-Nilsen. Faster deterministic fully-dynamic graph connectivity. In Proceedings\\nof the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2013,\\nNew Orleans, Louisiana, USA, January 6-8, 2013, pages 1757–1769, 2013. 5\\n[Wul17] Christian Wulff-Nilsen. Fully-dynamic minimum spanning forest with improved worst-case\\nupdate time. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of\\nComputing, STOC 2017, Montreal, QC, Canada, June 19-23, 2017, pages 1130–1143, 2017.\\n1, 3, 4, 5, 20, 31, 41\\n49\\nS vs T (Lemma 1.4)\\n𝑉1\\n𝑉2\\n𝑉3\\n𝑉4\\n𝑉1\\n𝑉2\\n𝑉3\\n𝑉4\\nFigure 1: Relationships between S, T and the clumps. The area of a part of each Vi represents its volume\\n(e.g. volG(S ∩ V3) is very high, thus V3 ∈ V ′′).\\nA Omitted Proofs\\nA.1 Proof of Lemma 2.4\\nProof. Consider any cut (S, S) in G where volG(S) ≤ volG(S). We will prove that either\\n(i) ΦG(S) = Ω(Φ\\nin\\nG ), or\\n(ii) we can obtain in O(m) time a cut T respecting the partition V where (a) ΦG(T ) = O(ΦG(S)/ΦinG ),\\n(b) volG(T ) = Ω(volG(S)), and (c) volG(T ) = Ω(volG(S)).\\nRoughly, the above means that either S is not so sparse, or we can quickly convert it into a cut T\\nrespecting V whose conductance is not much more than S and that is almost as balanced as S. Observe\\nthat this implies both Items 1 and 2 of Lemma 2.4. To see this for Item 1, consider any cut S. Either\\nΦG(S) = Ω(Φ\\nin\\nG ) by Item (i) or, by Item (ii), there is a cut T such that ΦG(S) = Ω(ΦG(T ) · ΦinG ) =\\nΩ(ΦoutG ·ΦinG ), where the first equality is because of (a) and the second is because T respects V. To prove\\nItem 2, we start from S as in Item 2 and use Item (ii) to get a cut T which, by (b) and (c), is Ω(β)-\\nbalanced (since S is β-balanced).11 It remains to prove that for any cut S, either Item (i) or Item (ii)\\nholds. Let\\nV ′ = {Vi | volG(S ∩ Vi) < 2volG(Vi − S)}, V ′′ = {Vi | volG(S ∩ Vi) ≥ 2volG(Vi − S)} and T =\\n⋃\\nVi∈V′′\\nVi.\\nIn other words, V ′ contains clumps that do not overlap much with S and V ′′ contains those with big\\noverlaps. The latter set defines a new cut T . Note that T can be computed in O(m) time. (Some readers\\nmay find Figure 1 useful for following the arguments below.) Note that as Vi is a clump, volG[Vi](S∩Vi) =\\nΘ(volG(S ∩ Vi)) and volG[Vi](Vi − S) = Θ(volG(Vi − S)). So,\\n∀Vi ∈ V ′ : δG[Vi](Vi ∩ S) ≥ Ω(ΦinG · volG[Vi](Vi ∩ S)) and (14)\\n∀Vi ∈ V ′′ : δG[Vi](Vi − S) ≥ Ω(ΦinG · volG[Vi](Vi − S)). (15)\\nNote a very important fact that both volG(T − S) and volG(S − T ) are O(δG(S)/ΦinG ). This exploits the\\ndefinition of T and the fact that each Vi is a clump, as in the inequalities below.\\n12\\n11In more details, by (b) and (c), T is such that volG(T ) = Ω(volG(S)) = Ω(βvol(V )) (the last equality is because\\nS is β-balanced) and volG(T ) = Ω(volG(S)) = Ω(βvol(V )). These imply that T is Ω(β)-balanced. Moreover, ΦG(T ) =\\nδG(T )\\nmin{volG(T ),volG(T )\\n= O(\\nδG(S)/Φ\\nin\\nG\\nmin{volG(S),volG(S)}\\n) = O(ΦG(S)/Φ\\nin\\nG ), where the second equality is because of (a), (b), and (c).\\n12Here we attempt to provide an intuition behind Equations (16) and (17). By the way we define T , we can write\\nvolG(S − T ) and volG(T − S) as a sum of volG(X) over subsets X ⊆ Vi (see the first lines). We can bound this by\\nvolG[Vi](X), exploiting that each Vi is a clump (see the second lines). This allows us to related volG[Vi](X) to δG[Vi](X)\\nwith a blow up of ΦinG , as in the third and fourth lines. We can then relate δG[Vi](X) back to δG(S) using the fact that\\neither X = S ∩ Vi or X = Vi − S; in both case we can conclude that EG[Vi](X,Vi −X) ⊆ E(S, V − S).\\n50\\nvolG (S − T ) =\\n∑\\nVi∈V′\\nvolG(S ∩ Vi) (by the definition of T )\\n≤ O (1) ·\\n∑\\nVi∈V′\\nvolG[Vi](S ∩ Vi) (Vi is a clump)\\n≤ O (1) ·\\n∑\\nVi∈V′\\nδG[Vi](S ∩ Vi)/ΦG[Vi] (Equation (14))\\n≤ O (1) ·\\n∑\\nVi∈V′\\nδG[Vi](S ∩ Vi)/ΦinG (ΦG[Vi] ≥ ΦinG )\\n≤ O (δG(S)/ΦinG .) (E(S ∩ Vi, Vi − S) ⊆ E(S, V − S)) (16)\\nand similarly for the other side of the symmetric difference:\\nvolG (T − S) =\\n∑\\nVi∈V′′\\nvolG(Vi − S) (by the definition of T )\\n≤ O (1) ·\\n∑\\nVi∈V′′\\nvolG[Vi](Vi − S) (Vi is a clump)\\n≤ O (1) ·\\n∑\\nVi∈V′′\\nδG[Vi](Vi − S)/ΦG[Vi] (Equation (15))\\n≤ O (δG(S)/ΦinG ) (ΦG[Vi] ≥ ΦinG and E(S ∩ Vi, Vi − S) ⊆ E(S, V − S))13\\n(17)\\nNow consider two cases.\\nCase 1: volG(S − T ) ≥ volG(S)/2. Inequalities below show that Item (i) holds, i.e. ΦG(S) = Ω(ΦinG ).\\nδG(S) = Ω\\n(\\nΦinG · volG(S − T )\\n)\\n(by Equation (16))\\n≥ Ω (ΦinG · volG(S)/2) (the assumption of this case).\\nCase 2: volG(S − T ) < volG(S)/2. We show that T satisfies (a), (b), and (c), thus Item (ii) holds. By\\nEquations (16) and (17), we have14\\nδG(T ) ≤ δG(S) + volG(T − S) + volG(S − T ) = O(δG(S)/ΦinG ).\\nThis implies (a) in Item (ii). For (b), we have\\nvolG(S) = volG(S ∩ T ) + volG(S − T ) ≤ volG(T ) + volG(S − T ) ≤ volG(T ) + volG(S)/2\\n13Note that the third lines of both Equations (16) and (17) follow from a more general fact that for all X ⊆ Vi,\\nδG[Vi](X) ≥ volG[Vi](X) · ΦG[Vi] (here X = S ∩ Vi and X = Vi − S). Also, the last lines follow from a more general fact\\nthat if X = S ∩ Vi or X = Vi − S, then E(X,Vi −X) ⊆ E(S, V − S).\\n14To see the first inequality, observe that δG(T ) = |E(T ∩ S, V − T − S)|+ |E(T ∩ S, S − T )|+ |E(T − S, V − T − S)|+\\n|E(T −S, S−T )|. Moreover, |E(T ∩S, V −T −S)| ≤ δG(S), |E(T ∩S, S−T )| ≤ volG(S−T ), and |E(T −S, V −T −S)|+\\n|E(T − S, S − T )| ≤ volG(T − S).\\n51\\nAlgorithm 6 A reduction from BalCutPrune to CutPrune\\n1. Let i← 1, G1 ← G, S ← ∅.\\n2. While volG(S) ≤ volG/3\\n(a) Let Si be the cut returned by CutPrune(Gi, φL, φU , β).\\n(b) S ← S ∪ Si.\\n(c) If Si is from Item 2 of CutPrune, then break.\\n(d) Gi+1 ← Gi{V (Gi)− Si}.\\n(e) i← i+ 1.\\n3. If volG(S) ≤ volG/2, then return S˜ = S. Else, return S˜ = V − S.\\nwhere the last inequality is because the assumption of Case 2. It follows that volG(S) = O(volG(T )),\\ncompleting (b). For (c), first observe that\\nvolG(T − S) =\\n∑\\nVi∈V′′\\nvolG(Vi − S)\\n<\\n∑\\nVi∈V′′\\nvolG(Vi ∩ S)/2 (by the definition of V ′′)\\n= volG(T ∩ S)/2 (since T ∩ S =\\n⋃\\nVi∈V′′ Vi ∩ S; see Figure 1).\\n≤ volG(S)/2 (T ∩ S ⊆ S)\\n≤ volG(S)/2 (S is defined to be the smaller side in the cut) (18)\\nSo we have\\nvolG(S) = volG(T ) + volG(T − S) (S = T ∪ (T − S); see Figure 1)\\n≤ volG(T ) + volG(S)/2 (by Equation (18)),\\nand so volG(S) = O(volG(T )).\\nA.2 Proof of Lemma 4.6\\nWe will show an equivalent statement of TBalCutPrune(m, 3φU , φL) ≤ O(TCutPrune(m,φU , φL, β) · 1β ).\\nProof. Given (G,φU , φL), we run Algorithm 6. Observe that there are at most O(1/β) iterations in the\\nwhile loop. Otherwise, volG(S) > volG/3. Therefore, the running time is obvious. It remains to show\\nthe correctness.\\nLet St be the last cut returned by calling CutPrune(Gt, φU , φL, β). Let S\\n′ = ∪i≤t−1Si. Note that\\nS = ∪i≤tSi = S′ ∪ St. Observe that volG(S′) ≤ volG/3 and ΦG(S′) ≤ φU by Proposition 2.2. We need\\ntwo small observations.\\nClaim A.1. δG(S) ≤ φUvolG.\\nProof. We know δG(S\\n′) ≤ φUvolG(S′). Also, note that δGt(St) ≤ φUvolGt = φUvolG(V − S′) where this\\nis true either St is from Item 1 or Item 2 of CutPrune. So we have\\nδG(S) ≤ δG(S′) + δGt(St)\\n≤ φUvolG(S′) + φUvolG(V − S′)\\n= φUvolG.\\n52\\nClaim A.2. If volG(S) > volG/2, then volG/3 ≤ volG(V − S) ≤ volG/2.\\nProof. volG(V − S) ≤ volG/2 follows directly from the assumption. To see another direction, we have\\nvolGt(St) ≤ volGt/2 = volG(V − S′)/2. Therefore,\\nvolG(S) ≤ volG(S′) + volG(V − S′)/2\\n= volG/2 + volG(S\\n′)/2 ≤ (2/3)volG.\\nThere are four cases: whether St is from Item 1 or Item 2 of CutPrune and whether volG(S) ≤\\nvolG/2.\\nFirst, suppose volG(S) ≤ volG/2 and St is from Item 1. We have ΦG(S) ≤ φU by Proposition 2.2 and\\nvolG/3 ≤ volG(S) ≤ volG/2. So S˜ = S satisfies the output conditions for Item 1 of BalCutPrune.\\nSecond, suppose volG(S) ≤ volG/2 and St is from Item 2. We claim that S˜ = S satisfies the output\\nconditions for Item 2 of BalCutPrune. This is because ΦG−S = ΦGt−St ≥ φL and δG(S) ≤ φUvolG.\\nThird, suppose that volG(S) > volG/2 and St is from Item 1. As volG(S) ≤ 2volG(V − S), and so\\nΦG(V − S) = δG(S)\\nmin{volG(S), volG(V − S)} ≤ 2\\nδG(S)\\nvolG(S)\\n≤ 2φU\\nwhere the last inequality is by Proposition 2.2. So S˜ = V − S satisfies the output conditions for Item 1\\nof BalCutPrune with parameter 2φU .\\nForth, suppose that volG(S) > volG/2 and St is from Item 2. We have\\nδG(S) ≤ φUvolG ≤ 3φUvolG(V − S)\\nby the two claims above. So S˜ = V −S satisfies the output conditions for Item 1 of BalCutPrune with\\nparameter 3φU .\\nA.3 Proof of Lemma 4.7\\nWe will show an equivalent statement of TvBalCutPrune(n,∆, 60∆φU , φL) ≤ O(TBalCutPrune(n∆, φU , φL)∆).\\nProof. Given (G,∆, φU , φL), we run Algorithm 6. Observe that there are at most t = O(∆) iterations in\\nthe while loop. This is because at each iteration i, we either remove S′i or V (Gi)−S′i from Gi. It suffices\\nto show the following:\\nmin{|S′i|, |V (Gi)− S′i|} ≥ min{volGi(S′i), volGi(V (Gi)− S′i)}/∆ (max degree is ∆)\\n≥ volGi/3∆ (S′i is from BalCutPrune)\\n≥ |V (Gi)|/3∆\\n≥ |V |/6∆. (|V (Gi)| ≥ 2|V |/3)\\nTherefore, the running time is obvious. It remains to show the correctness.\\nSuppose that the returned cut S˜ is from Step Item 2(b)i of Algorithm 7. In this case, |S| ≤ |V |/3 and\\nso\\n|S˜| = |S|+ |S′i| ≤ |S|+ |V − S|/4 = 3|S|/4 + |V |/4 ≤ |V |/2.\\nAlso, σG−S˜ ≥ ΦGi−S′i ≥ φL. So S˜ satisfies the output conditions for Item 2 of vBalCutPrune.\\nNow, we assume that S˜ is returned from Step Item 3. In this case, we have S = ∪i≤tSi.\\n53\\nAlgorithm 7 A reduction from vBalCutPrune to BalCutPrune\\n1. Let i← 1 and G1 ← G. Let S ← ∅.\\n2. While |S| ≤ |V |/3\\n(a) Let S′i be the cut returned by BalCutPrune(Gi, φL, φU ).\\n(b) If |S′i| ≤ |V (Gi)|/4, then\\ni. If S′i is from Item 2 of BalCutPrune, then return S˜ = S ∪ S′i.\\nii. Si ← S′i.\\n(c) Else |S′i| > |V (Gi)|/4, then Si ← V (Gi)− S′i.\\n(d) S ← S ∪ Si\\n(e) Gi+1 ← Gi{V (Gi)− Si}.\\n(f) i← i+ 1.\\n3. If |S| ≤ |V |/2, then return S˜ = S. Else, return S˜ = V − S.\\nClaim A.3. δG(S)|S| ≤ 12∆φU .\\nProof. We first analyze σGi(Si) = σGi(S\\n′\\ni) for each i. There are several cases. Suppose S\\n′\\ni is returned\\nfrom Item 1 of BalCutPrune, then by Proposition 2.1 σGi(S\\n′) ≤ ∆ΦGi(S′i) ≤ ∆φU and we are done.\\nNow, we assume that S′i is returned from Item 2 of BalCutPrune. In this case, we have\\nδGi(S\\n′\\ni) ≤ φUvolGi .\\nWe also have volGi(V (Gi)− S′i) ≥ volGi/2, so\\n|V (Gi)− S′i| ≥ volGi(V (Gi)− S′i)/∆ ≥ volGi/2∆.\\nAs we assume that S˜ is returned from Step Item 3, it must be the case that |S′i| > |V (Gi)|/4. So\\n|S′i| > |V (Gi)|/4 ≥ volGi/4∆.\\nPut these together, we have\\nσGi(Si) =\\nδGi(S\\n′\\ni)\\nmin{|S′i|, |V (Gi)− S′i|}\\n≤ φUvolGi\\nmin{volGi/4∆, volGi/2∆}\\n≤ 4∆φU .\\nThat is, σGi(Si) ≤ 4∆φU for each i. Given this, we have\\nδGi(Si)\\n|Si| ≤ 3 ·\\nδGi(Si)\\nmin{|Si|, |V (Gi)− Si|} (|Si|, |V (Gi)− Si| ≤ 3|V (Gi)|/4)\\n= 3σGi(Si)\\n≤ 12∆φU .\\nBy applying Proposition 2.2, this conclude the claim.\\n54\\nWe now use the above claim to prove the lemma. Let S′ = ∪i≤t−1Si. Observe that |S′| ≤ |V |/3 and\\nSt ≤ 34 |V (Gt)| = 34 |V − S′|. Therefore,\\n|S| = |S′|+ |St| ≤ |S\\n′|\\n4\\n+\\n3|V |\\n4\\n≤ ( 1\\n12\\n+\\n3\\n4\\n)|V | = 5\\n6\\n|V |.\\nSo |V − S| ≥ |S|/5. Therefore the returned cut S˜ satisfies |V |/6 ≤ |S˜| ≤ |V |/2, which in turn implies\\nσG\\n(\\nS˜\\n)\\n=\\nδG(S)\\nmin {|S| , |V − S|} ≤ 5 ·\\nδG (S)\\n|S| ≤ 60∆φU .\\nThat is, S˜ satisfies the output conditions for Item 1 of vBalCutPrune with parameter 60∆φU .\\nA.4 Proof of Theorem 5.5\\nWe call each iteration t in the for loop of Algorithm 3 a round, and call Mt a matching of round t.\\nConsider the following random-walk process along the matchings. Imagine a particle located at vertex u\\nbefore inserting Mt (i.e. before round t). Suppose there is an edge (u, v) incident to u in the matching\\nMt. Then, the particle stays at u with probability 1/2 and jumps across (u, v) with probability 1/2. Let\\npu,v(t) denote the probability that the particle starting at u reaches node v after inserting Mt (i.e. after\\nt rounds).\\nBelow, we let V = V (W ) and pA,B(t) =\\n∑\\nu∈A,v∈B pu,v(t).\\nProposition A.4 ([KKOV07]). We have the following:\\n• pu,u(0) = 1 for all u and pu,v(0) = 0 for u 6= v.\\n• For all t ≥ 0 and u ∈ V , pu,V (t) = pV,u(t) = 1.\\n• For all t ≥ 1, pu,v(t) = pu,w(t) = pu,v(t−1)+pu,w(t−1)2 if (v, w) ∈Mt.\\nLet Ψu(t) = −\\n∑\\nv∈V pu,v(t) ln pu,v(t) be the entropy of the distribution of the random walks starting\\nfrom u after t rounds. We call Ψ(t) =\\n∑\\nu∈V Ψu(t) the potential. We use the usual convention that\\n−0 ln 0 = limx→0+ −x lnx = 0. We have:\\nProposition A.5 ([KKOV07]). We have the following: Ψ(0) = 0 and Ψ(t) ≤ n lnn for any t.\\nThe key observation is that each matching, except the last one, must significantly increase the poten-\\ntial, which allows us to bound the number of rounds.\\nLemma A.6. For each round t, if the cut At is from Item 1 of BalCutPrune, then Ψ(t)−Ψ(t− 1) =\\nΩ(n).\\nProof. Note that |V |/6 ≤ |At| ≤ |V |/2. Let At = V −At. Observe that pAt,At(t−1) ≤ |EWt−1(At, At)|/2.\\nTo see this, when an edge (v, w) ∈ EWt−1(At, At) is inserted, there is exactly 1/2 units of probability mass\\n(summing up over all starting points) transfers from v to w. Moreover, the edge (v, w) ∈ EWt−1(At, At)\\nis used exactly once for transferring the probability mass.\\nAs σWt−1(At) ≤ 1/4, we have pAt,At(t − 1) ≤ |At|/8. By an averaging argument, there are at least|At|/2 nodes u ∈ At where pu,At(t−1) ≤ 1/4. Fix such node u. It suffices to prove that Ψu(t)−Ψu(t−1) =\\nΩ(1). This is because |At| = Ω(n), and the monotonic increase of entropy under averaging implying that\\nΨv(t)−Ψv(t− 1) ≥ 0 for any v ∈ V .\\nRecall that, for each v ∈ At, there is a unique matching edge (v, pi(v)) ∈Mt. Consider the sets\\nGood =\\n{\\nv ∈ St | pu,v (t− 1) ≥ 2pu,pi(v) (t− 1)\\n}\\nBad =\\n{\\nv ∈ St | pu,v (t− 1) < 2pu,pi(v) (t− 1)\\n}\\n.\\n55\\nFrom the definition, we have\\npu,Bad (t− 1) ≤ 2pu,St (t− 1) ≤ 1/2.\\nTherefore,\\npu,Good (t− 1) = pu,St (t− 1)− pu,Bad (t− 1) ≥ 3/4− 1/2 = 1/4.\\nFor each v ∈ Good, let q = pu,v(t − 1) and r = pu,pi(v)(t − 1), and q′ = pu,v(t) and r′ = pu,pi(v)(t).\\nNote that q′ = r′ = (q + r)/2 and q ≥ 2r. So\\n(−q′ ln q′ − r′ ln r′)− (−q ln q − r ln r) =\\n(\\n−2\\n(\\nq + r\\n2\\n)\\nln\\n(\\nq + r\\n2\\n))\\n− (−q ln q − r ln r)\\n= Ω (q) .\\nTherefore, Ψu(t)−Ψu(t−1) = Ω(pu,Good(t−1)) = Ω(1) as desired. Finally, we prove that−2\\n(\\nq+r\\n2\\n)\\nln\\n(\\nq+r\\n2\\n)−\\n(−q ln q − r ln r) = Ω(q) when r ∈ [0, q/2]. Observe that this expression is minimized when r = q/2. So\\nwe only show that the claim holds when r = q/2:\\n−2\\n(\\nq + r\\n2\\n)\\nln\\n(\\nq + r\\n2\\n)\\n− (−q ln q − r ln r) = −2\\n(\\n3q\\n4\\n)\\nln\\n(\\n3q\\n4\\n)\\n−\\n(\\n−q ln q − q\\n2\\nln\\nq\\n2\\n)\\n=\\nq\\n2\\nln\\n(\\n3q\\n4\\n)−3\\n+\\nq\\n2\\nln q2 +\\nq\\n2\\nln\\nq\\n2\\n=\\nq\\n2\\nln\\n43\\n332\\n=\\nq\\n2\\nln\\n32\\n27\\n= Ω(q).\\nLemma A.7. σW ≥ φ/9. So ΦW ≥ φ/9R.\\nProof. Let t be the first round where the cut At is from Item 2 of BalCutPrune. We know that\\nt = O(log n) because Ψ(t) ≤ n lnn and by Lemma A.6. We assume ckkov is such that R = ckkov log n > t.\\nWe will prove below that σWt ≥ φ/9. This will implies σW ≥ φ/9 because σW does not decrease under\\nedge insertions. By Proposition 2.1, we have ΦW ≥ φ/9R.\\nLet (At, At) be the cut returned by vBalCutPrune from Item 2 of Definition 4.2. We have\\nσWt−1{At} ≥ φ. Note that, for each node u ∈ At, there is a matching edge (u, v) in Mt where v ∈ At.\\nConsider any cut (T, T ) where |T | ≤ |T |. We will show that σWt(T ) ≥ φ/9. There are several cases.\\nFirst, suppose that |T ∩At| ≥ 3|T ∩At|. Then,\\n|At| = |T ∩At|+ |T ∩At| ≤ 4\\n3\\n|T ∩At| ≤ 4\\n3\\n|T | ≤ 2\\n3\\n|V |.\\nSo |At| ≥ 13 |V | and |T ∩ At| ≤ 13 |T ∩ At| ≤ 16 |V |. Therefore, the number of edges of Mt connecting At\\nand T ∩At is at least\\n|At| − |T ∩At| ≥ 1\\n6\\n|V |,\\nwhich in turn implies that σW (T ) is at least 1/3.\\nNext, suppose that |T ∩ At| ≥ 2|T |/3. That is, |T ∩ At| ≤ |T ∩ At|/2. Then, the number of edges of\\nMt connecting T ∩At and T ∩At is at least\\n|T ∩At| − |T ∩At| ≥ |T ∩At|/2 ≥ |T |/3,\\nso\\nσW (T ) ≥ |T |/3|T | ≥ 1/3.\\n56\\nThe remaining case is when |T ∩At| ≥ |T |/3 and |T ∩At| ≤ 3|T ∩At|. We claim that σWt(T ) ≥ φ/9.\\nThis is because σWt−1{At} ≥ φ and so\\nδWt−1{At}\\n(\\nT ∩At\\n) ≥ φmin{∣∣T ∩At∣∣ , ∣∣T ∩At∣∣} ≥ φ ∣∣T ∩At∣∣ /3 ≥ φ |T | /9.\\nThis completes the claim that σWt ≥ φ/9.\\nA.5 Proof of Lemma 5.6\\nProof. It is known that there is a deterministic SlowLowConductance algorithm (e.g. [LR99]) which,\\ngiven any graph H with total weight W and φ, in O(n4) either find a cut S where volH(S) ≤ volH/2 and\\nΦH(S) ≤ φ, or declare that ΦH ≥ φ/ logW .\\nThe algorithm is to iteratively find a sparse cut from G until we cannot. More formally, we first\\npreprocess G in O(m) so that G has at most n2 weighted edges with total weight W = m. Let H ← G\\nand Sˆ ← ∅. As long as SlowLowConductance(H,φ/2) return a cut S′, then we set H ← H{V − S′}\\nand Sˆ ← S ∪ S′. Whenever volG(Sˆ) ≥ βvolG, we set S = Sˆ if volG(S) ≤ volG/2, otherwise S = V − Sˆ.\\nIn this case, clearly βvolG ≤ volG(S) ≤ volG/2. We also that\\nΦG(S) =\\nδG(Sˆ)\\nmin{volG(Sˆ), volG(V − Sˆ)}\\n≤ 2 · δG(Sˆ)\\nvolG(Sˆ)\\n≤ φ\\nThe first inequality is because volG(V − Sˆ) ≤ 2volG(S) (as volH(S′) ≤ volH/2 in every iteration), and\\nthe second inequality is by Proposition 2.2.\\nSuppose that, before volG(Sˆ) ≥ βvolG, there is an iteration where SlowLowConductance(H,φ/2)\\ndeclares that ΦH ≥ φ/2. Let A = Sˆ and A = V − Sˆ. Let F be a set of edges connecting A and A such\\nthat degF (v) = degG(v) for each v ∈ A and degF (v) ≤ degG(v) for v ∈ A. It is obvious how to construct\\nthis set. We claim that ΦG∪F ≥ φ/12 logm. First, note that for any S, volG∪F (S) ≤ 2volG(2).\\nConsider any cut (T, T ) where volG(T ) ≤ volG(T ). There are two cases. The proof is analogous to\\nthe one in Lemma A.7.\\nSuppose that volG(T ∩ A) ≥ 2volG(T )/3. That is, volG(T ∩ A) ≤ volG(T ∩ A)/2. Then, the number\\nof edges of F connecting T ∩At and T ∩A is at least\\nvolG(T ∩A)− volG(T ∩A) ≥ volG(T ∩A)/2 ≥ volG(T )/3,\\nso ΦG∪F (T ) ≥ volG(T )/3volG∪F (T ) ≥ 1/6.\\nNext, suppose that volG(T∩A) ≥ volG(T )/3. Note that volG(T∩A) ≤ volG/2 and volG(A) ≥ 2volG/3,\\nso volG(T ∩A) ≤ 3volG(T ∩A). As ΦG{A} ≥ φ/2 logm and so\\nδG{A}(T ∩A) ≥\\nφ\\n2 logm\\nmin{volG{A}(T ∩At), volG{A}(T ∩At)}\\n≥ φ\\n6 logm\\nvolG(T ∩At) ≥ φ\\n18 logm\\nvolG(T ).\\nSo ΦG∪F (T ) ≥ φvolG(T )/(18 logm)volG∪F (T ) ≥ φ/40 logm.\\nThe running time is O(m+n5). Indeed, we preprocess G in O(m). We call SlowLowConductance\\nat most n times. Each time takes at most n4 time.\\n57\\n'}\n",
      "{'_id': ObjectId('6815e61add259dc7e6e1cd2d'), 'authors': 'Freer, Cameron E., Mansinghka, Vikash K., Rinard, Martin C., Saad, Feras A.', 'year': '2020', 'title': 'Optimal Approximate Sampling from Discrete Probability Distributions', 'full_text': '36\\nOptimal Approximate Sampling from Discrete Probability\\nDistributions\\nFERAS A. SAAD,Massachusetts Institute of Technology, USA\\nCAMERON E. FREER,Massachusetts Institute of Technology, USA\\nMARTIN C. RINARD,Massachusetts Institute of Technology, USA\\nVIKASH K. MANSINGHKA,Massachusetts Institute of Technology, USA\\nThis paper addresses a fundamental problem in random variate generation: given access to a random source\\nthat emits a stream of independent fair bits, what is the most accurate and entropy-efficient algorithm\\nfor sampling from a discrete probability distribution (p1, . . . ,pn ), where the probabilities of the output\\ndistribution (pˆ1, . . . , pˆn ) of the sampling algorithm must be specified using at most k bits of precision? We\\npresent a theoretical framework for formulating this problem and provide new techniques for finding sampling\\nalgorithms that are optimal both statistically (in the sense of sampling accuracy) and information-theoretically\\n(in the sense of entropy consumption). We leverage these results to build a system that, for a broad family\\nof measures of statistical accuracy, delivers a sampling algorithm whose expected entropy usage is minimal\\namong those that induce the same distribution (i.e., is “entropy-optimal”) and whose output distribution\\n(pˆ1, . . . , pˆn ) is a closest approximation to the target distribution (p1, . . . ,pn ) among all entropy-optimal\\nsampling algorithms that operate within the specified k-bit precision. This optimal approximate sampler\\nis also a closer approximation than any (possibly entropy-suboptimal) sampler that consumes a bounded\\namount of entropy with the specified precision, a class which includes floating-point implementations of\\ninversion sampling and related methods found in many software libraries. We evaluate the accuracy, entropy\\nconsumption, precision requirements, and wall-clock runtime of our optimal approximate sampling algorithms\\non a broad set of distributions, demonstrating the ways that they are superior to existing approximate samplers\\nand establishing that they often consume significantly fewer resources than are needed by exact samplers.\\nCCS Concepts: • Theory of computation→ Probabilistic computation; Numeric approximation algorithms; •\\nMathematics of computing→ Probability and statistics; Random number generation;Mathematical software\\nperformance; Combinatorial optimization; Discretization.\\nAdditional Key Words and Phrases: random variate generation, discrete random variables\\nACM Reference Format:\\nFeras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka. 2020. Optimal Approximate\\nSampling from Discrete Probability Distributions. Proc. ACM Program. Lang. 4, POPL, Article 36 (January 2020),\\n31 pages. https://doi.org/10.1145/3371104\\nAuthors’ addresses: Feras A. Saad, Department of Electrical Engineering & Computer Science, Massachusetts Institute of\\nTechnology, Cambridge, MA, 02139, USA, fsaad@mit.edu; Cameron E. Freer, Department of Brain & Cognitive Sciences,\\nMassachusetts Institute of Technology, Cambridge, MA, 02139, USA, freer@mit.edu; Martin C. Rinard, Department of\\nElectrical Engineering & Computer Science, Massachusetts Institute of Technology, Cambridge, MA, 02139, USA, rinard@\\ncsail.mit.edu; Vikash K. Mansinghka, Department of Brain & Cognitive Sciences, Massachusetts Institute of Technology,\\nCambridge, MA, 02139, USA, vkm@mit.edu.\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\\nthe full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses,\\ncontact the owner/author(s).\\n© 2020 Copyright held by the owner/author(s).\\n2475-1421/2020/1-ART36\\nhttps://doi.org/10.1145/3371104\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\nar\\nX\\niv\\n:2\\n00\\n1.\\n04\\n55\\n5v\\n1 \\n [c\\ns.D\\nS]\\n  1\\n3 J\\nan\\n 20\\n20\\n36:2 Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka\\n1 INTRODUCTION\\nSampling from discrete probability distributions is a fundamental activity in fields such as statis-\\ntics [Devroye 1986], operations research [Harling 1958], statistical physics [Binder 1986], financial\\nengineering [Glasserman 2003], and general scientific computing [Liu 2001]. Recognizing the im-\\nportance of sampling from discrete probability distributions, widely-used language platforms [Lea\\n1992; MathWorks 1993; R Core Team 2014] typically implement algorithms for sampling from\\ndiscrete distributions. As Monte Carlo methods move towards sampling billions of random variates\\nper second [Djuric 2019], there is an increasing need for sampling algorithms that are both efficient\\n(in terms of the number of random bits they consume to generate a sample) and accurate (in terms\\nof the statistical sampling error of the generated random variates with respect to the intended\\nprobability distribution). For example, in fields such as lattice-based cryptography and probabilistic\\nhardware [de Schryver et al. 2012; Roy et al. 2013; Dwarakanath and Galbraith 2014; Folláth 2014;\\nMansinghka and Jonas 2014; Du and Bai 2015], the number of random bits consumed per sample,\\nthe size of the registers that store and manipulate the probability values, and the sampling error\\ndue to approximate representations of numbers are all fundamental design considerations.\\nWe evaluate sampling algorithms for discrete probability distributions according to three criteria:\\n(1) the entropy consumption of the sampling algorithm, as measured by the average number of\\nrandom bits consumed from the source to produce a single sample (Definition 2.5); (2) the error of\\nthe sampling algorithm, which measures how closely the sampled probability distribution matches\\nthe specified distribution, using one of a family of statistical divergences (Definition 4.2); and (3) the\\nprecision required to implement the sampler, as measured by the minimum number of binary digits\\nneeded to represent each probability in the implemented distribution (Definition 2.13).\\nLet (M1, . . . ,Mn) be a list of n positive integers which sum toZ and write p B (p1, . . . ,pn) for the\\ndiscrete probability distribution over the set [n] B {1, . . . ,n} defined by pi B Mi/Z (i = 1, . . . ,n).\\nWe distinguish between two types of algorithms for sampling from p: (i) exact samplers, where the\\nprobability of returning i is precisely equal to pi (i.e., zero sampling error); and (ii) approximate\\nsamplers, where the probability of returning i is pˆi ≈ pi (i.e., non-zero sampling error). In exact\\nsampling, the numerical precision needed to represent the output probabilities of the sampler varies\\nwith the values pi of the target distribution; we say these methods need arbitrary precision. In\\napproximate sampling, on the other hand, the numerical precision needed to represent the output\\nprobabilities pˆi of the sampler is fixed independently of the pi (by constraints such as the register\\nwidth of a hardware circuit or arithmetic system implemented in software); we say these methods\\nneed limited precision. We next discuss the tradeoffs between entropy consumption, sampling error,\\nand numerical precision made by exact and approximate samplers.\\n1.1 Existing Methods for Exact and Approximate Sampling\\nInversion sampling is a universal method for obtaining a random sample from any probability\\ndistribution [Devroye 1986, Theorem 2.1]. The inversion method is based on the identity that if U\\nis a uniformly distributed real number on the unit interval [0, 1], then\\nPr\\n[∑j−1\\ni=1 pi ≤ U <\\n∑j\\ni=1 pi\\n]\\n= pj (j = 1, . . . ,n). (1)\\nKnuth and Yao [1976] present a seminal theoretical framework for constructing an exact sampler\\nfor any discrete probability distribution. The sampler consumes, in expectation, the least amount of\\nrandom bits per sample among the class of all exact sampling algorithms (Theorem 2.9). The Knuth\\nand Yao sampler is an implementation of the inversion method which compares (lazily sampled)\\nbits in the binary expansion ofU to the bits in the binary expansion of the pi . Despite its minimal\\nentropy consumption and zero sampling error, the method requires arbitrary precision and the\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\nOptimal Approximate Sampling from Discrete Probability Distributions 36:3\\nAlgorithm 1 Rejection Sampling\\nGiven probabilities (Mi/Z )ni=1:\\n(1) Let k be such that 2k−1 <Z ≤ 2k .\\n(2) Draw a k-bit integerW ∈ {0, . . . , 2k −1}.\\n(3) IfW <Z , return integer j ∈ [n] such that∑j−1\\ni=1 Mi ≤W <\\n∑j\\ni=1Mi ; else go to 2.\\nAlgorithm 2 Inversion Sampling\\nGiven probabilities (Mi/Z )ni=1, precision k :\\n(1) Draw a k-bit integerW ∈ {0, . . . , 2k −1}.\\n(2) LetU ′ BW /2k .\\n(3) Return smallest integer j ∈ [n] such that\\nU ′ <\\n∑j\\ni=1Mi/Z .\\ncomputational resources needed to implement the sampler are often exponentially larger than the\\nnumber of bits needed to encode the probabilities (Theorem 3.5), even for typical distributions\\n(Table 4). In addition to potentially requiring more resources than are available even on modern\\nmachines, the framework is presented from a theoretical perspective without readily-programmable\\nimplementations of the sampler, which has further limited its general application.1\\nThe rejection method [Devroye 1986], shown in Algorithm 1, is another technique for exact\\nsampling where, unlike the Knuth and Yao method, the required precision is polynomial in the\\nnumber of bits needed to encode p. Rejection sampling is exact, readily-programmable, and typically\\nrequires reasonable computational resources. However, it is highly entropy-inefficient and can\\nconsume exponentially more random bits than is necessary to generate a sample (Example 5.1).\\nWe now discuss approximate sampling methods which use a limited amount of numerical pre-\\ncision that is specified independently of the target distribution p. Several widely-used software\\nsystems such as the MATLAB Statistics Toolbox [MathWorks 1993] and GNU C++ standard li-\\nbrary [Lea 1992] implement the inversion method based directly on Eq. (1), where a floating-point\\nnumberU ′ is used to approximate the ideal real random variableU , as shown in Algorithm 2. These\\nimplementations have two fundamental deficiencies: first, the algorithm draws a fixed number\\nof random bits (typically equal to the 32-bit or 64-bit word size of the machine) per sample to\\ndetermineU ′, which may result in high approximation error (Section 2.4), is suboptimal in its use of\\nentropy, and often incurs non-negligible computational overhead in practice; second, floating-point\\napproximations for computing and comparingU ′ to running sums of pi produce significantly subop-\\ntimal sampling errors (Figure 3) and the theoretical properties are challenging to characterize [von\\nNeumann 1951; Devroye 1982; Monahan 1985]. In particular, many of these approximate methods,\\nunlike the method presented in this paper, are not straightforwardly described as producing samples\\nfrom a distribution that is close to the target distribution with respect to a specified measure of\\nstatistical error and provide no optimality guarantees.\\nThe interval method [Han and Hoshi 1997] is an implementation of the inversion method which,\\nunlike the previous methods, lazily obtains a sequenceUi of fair coin flips from the set {0, 1} and\\nrecursively partitions the unit interval until the outcome j ∈ [n] can be determined. Han and Hoshi\\n[1997] present an exact sampling algorithm (using arbitrary precision) and Uyematsu and Li [2003]\\npresent an approximate sampling algorithm (using limited precision). Although entropy consumed\\nby the interval method is close to the optimal limits of Knuth and Yao [1976], the exact sampler uses\\nseveral floating-point computations and has an expensive search loop during sampling [Devroye\\nand Gravel 2015, Algorithm 1]. The limited-precision sampler is more entropy-efficient than the\\nlimited-precision inversion sampler (Table 2) but often incurs a higher error (Figure 3).\\n1In reference to thememory requirements and programmability of the Knuth and Yao [1976] method, the authors note “most of the algorithms\\nwhich achieve these optimum bounds are very complex, requiring a tremendous amount of space”. Lumbroso [2013] also discusses these issues.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\n36:4 Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka\\n1.2 Optimal Approximate Sampling\\nThis paper presents a novel class of algorithms for optimal approximate sampling from discrete\\nprobability distributions. Given a target distribution p B (p1, . . . ,pn), any measure of statistical\\nerror in the family of (1-1 transformations of) f -divergences (Definition 4.2), and a number k\\nspecifying the allowed numerical precision, our system returns a sampler for p that is optimal in a\\nvery strong sense: it produces random variates with the minimal sampling error possible given the\\nspecified precision, among the class of all entropy-optimal samplers of this precision (Theorems 3.4\\nand 4.7). Moreover these samplers comprise, to the best of our knowledge, the first algorithms\\nthat, for any target distribution, measure of statistical accuracy, and specification of bit precision,\\nprovide rigorous guarantees on the entropy-optimality and the minimality of the sampling error.\\nThe key idea is to first find a distribution pˆ B (pˆ1, . . . , pˆn) whose approximation error of p is\\nminimal among the class of all distributions that can be sampled by any k-bit entropy-optimal\\nsampler (Section 4). The second step is to explicitly construct an entropy-optimal sampler for the\\ndistribution pˆ (Section 5). In comparison with previous limited-precision samplers, our samplers are\\nmore entropy-efficient and more accurate than any sampler that always consumes at most k random\\nbits (Proposition 2.16), which includes any algorithm that uses a finite number of approximately\\nuniform floating-point numbers (e.g., limited-precision inversion sampling and interval sampling).\\nThe time, space, and entropy resources required by our samplers can be significantly less than those\\nrequired by the exact Knuth and Yao and rejection methods (Section 6.3), with an approximation\\nerror that decreases exponentially quickly with the amount of precision (Theorem 4.17).\\nThe sampling algorithms delivered by our system are algorithmically efficient: they use integer\\narithmetic, admit straightforward implementations in software and probabilistic hardware systems,\\nrun in constant time with respect to the length n of the target distribution and linearly in the\\nentropy of the sampler, and can generate billions of random variates per second. In addition, we\\npresent scalable algorithms where, for a precision specification of k bits, the runtime of finding the\\nn optimal approximate probabilities pˆ is order n logn, and of building the corresponding sampler\\nis order nk . Prototype implementations of the system in C and Python are available in the online\\nartifact and at https://github.com/probcomp/optimal-approximate-sampling.\\n1.3 Contributions\\nThe main contributions of this paper are:\\nFormulation of optimal approximate sampling algorithms for discrete distributions. This\\nprecise formulation allow us to rigorously study the notion of entropy consumption, statistical\\nsampling error, and numerical precision. These three functional metrics are used to assess the\\nentropy-efficiency, accuracy, and memory requirements of a sampling algorithm.\\nTheoretical results for the class of entropy-optimal sampling algorithms. For a specified\\nprecision, we characterize the set of output probability distributions achievable by any entropy-\\noptimal sampler that operates within the given precision specification. We leverage these results to\\nconstrain the space of probability distributions for approximating a given target distribution to\\ncontain only those that correspond to limited-precision entropy-optimal samplers.\\nAlgorithms for finding optimal approximations to discrete distributions.We present a new\\noptimization algorithm that, given a target distribution p, a measure of statistical divergence, and\\na precision specification, efficiently searches the combinatorially large space of entropy-optimal\\nsamplers of the given precision, to find a optimal approximation sampler that most accurately\\napproximates the target distribution p. We prove the correctness of the algorithm and analyze its\\nruntime in terms of the size of the target distribution and precision specification.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\nOptimal Approximate Sampling from Discrete Probability Distributions 36:5\\nAlgorithms for constructing entropy-optimal sampling algorithms. We present detailed\\nalgorithms for sampling from any closest-approximation probability distribution pˆ in a way that is\\nentropy-optimal, using the guarantees provided by the main theorems of Knuth and Yao [1976].\\nOur prototype implementation can generate billions of random variates per second and executes\\nbetween 1.5x (for low-dimensional distributions) and 195x (for high-dimensional distributions)\\nfaster than the limited-precision linear inversion sampler provided as part of the GNU C++ standard\\nlibrary [Lea 1992].\\nComparisons to baseline limited-precision sampling algorithms. For several common prob-\\nability distributions, we empirically demonstrate that the proposed sampling algorithms consume\\nless entropy and are up to 1000x—10000x more accurate than the limited-precision inversion sam-\\npler from the GNU C++ standard library [Lea 1992] and interval algorithm [Uyematsu and Li 2003].\\nWe also show that (i) our sampler scales more efficiently as the size of the target distribution grows;\\nand (ii) using the information-theoretically minimal amount of bits per sample leads to up to 10x\\nless wall-clock time spent calling the underlying pseudorandom number generator.\\nComparisons to baseline exact sampling algorithms.We present a detailed study of the exact\\nKnuth and Yao method, the rejection method, and the proposed method for a canonical discrete\\nprobability distribution. We demonstrate that our samplers can use 150x less random bits per\\nsample than rejection sampling and many orders of magnitude less precision than exact Knuth and\\nYao sampling, and can (unlike exact sampling algorithms) trade off greater numerical precision in\\nexchange for exponentially smaller sampling accuracy, all while remaining entropy-optimal.\\nThe remainder of this paper is structured as follows: Section 2 describes the random bit model of\\ncomputation for sampling algorithms and provides formal definitions used throughout the paper.\\nSection 3 presents theoretical results on the class of entropy-optimal samplers which are leveraged\\nin future sections. Section 4 presents an efficient algorithm for finding a closest-approximation\\ndistribution to any given target distribution. Section 5 presents algorithms for constructing entropy-\\noptimal samplers. Section 6 investigates the properties of the optimal samplers and compares them\\nto multiple existing sampling methods in terms of accuracy, precision, entropy, and runtime.\\n2 COMPUTATIONAL MODELS OF SAMPLING ALGORITHMS\\nIn the algebraic model of computation over the real numbers (also known as the real RAM\\nmodel [Blum et al. 1998]), a sampling algorithm has access to an ideal register machine that\\ncan (i) sample a real random variable U uniformly distributed on the unit interval [0, 1] using a\\nprimitive called uniform(), which forms the basic unit of randomness; and (ii) store and perform\\nalgebraic operations on infinitely-precise real numbers in unit time [Devroye 1986, Assumptions 1,\\n2, and 3]. The algebraic model is useful for proving the correctness of exact mathematical trans-\\nformations applied to a uniform random variate U and for analyzing the algorithmic runtime and\\nstorage costs of preprocessing and sampling, assuming access to infinite amounts of entropy or\\nprecision [Walker 1977; Vose 1991; Smith 2002; Bringmann and Panagiotou 2017].\\nHowever, sampling algorithms that access an infinite amount of entropy and computewith infinite\\nprecision real arithmetic cannot be implemented on physical machines. In practice, these algorithms\\nare implemented on machines which use a finite amount of entropy and compute with approximate\\nreal arithmetic (e.g., double-precision floating point). As a result, sampling algorithms typically\\nhave a non-zero sampling error, which is challenging to systematically assess in practice [Devroye\\n1982].2 While the quality of sampling algorithms implemented in practice is often characterized\\n2von Neumann [1951] objected that “the amount of theoretical information about the statistical properties of the round-off\\nmechanism is nil” and, more humorously, that “anyone who considers arithmetic methods of producing random digits is, of\\ncourse, in a state of sin.”\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\n36:6 Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka\\nusing ad-hoc statistical goodness-of-fit tests on a large number of simulations [Walker 1974; Leydold\\nand Chaudhuri 2014], these empirical metrics fail to give rigorous statistical guarantees about\\nthe accuracy and/or theoretical optimality of the algorithm [Monahan 1985]. In this paper, we\\nconsider an alternative computational model that is more appropriate in applications where limited\\nnumerical precision, sampling error, or entropy consumption are of interest.\\n2.1 The Random Bit Model\\nIn the random bit model, introduced by von Neumann [1951], the basic unit of randomness is a\\nrandom symbol in the set {0, 1, . . . ,b − 1} for some integer b ≥ 2, obtained using a primitive\\ncalled flip(). Since the random symbols are produced lazily by the source and the output of the\\nsampling algorithm is a deterministic function of the discrete symbols, this model is suitable for\\nanalyzing entropy consumption and sampling error. In this paper, we consider the random bit\\nmodel of computation where any sampling algorithm for a target distribution p over [n] operates\\nunder the following assumptions:\\nA1. each invocation of flip() returns a single fair (unbiased) binary digit in {0, 1} (i.e., b = 2);\\nA2. the bits returned by separate invocations of flip() are all mutually independent;\\nA3. the output of the sampling algorithm is a single outcome in [n], which is independent of all\\nprevious outputs of the algorithm; and\\nA4. the output probabilities of the sampling algorithm can be specified using at most k binary\\ndigits, where the numerical precision parameter k is specified independently of the target\\ndistribution p.\\nSeveral limited-precision algorithms for sampling from discrete probability distributions in\\nthe literature operate under assumptions similar to A1–A4; examples include samplers for the\\nuniform [Lumbroso 2013], discrete Gaussian [Folláth 2014], geometric [Bringmann and Friedrich\\n2013], random graph [Blanca and Mihail 2012], and general discrete [Uyematsu and Li 2003]\\ndistributions. Since these sampling algorithms use limited numerical precision that is specified\\nindependently of the target distribution (A4), they typically have some statistical sampling error.\\nWe also note that several variants of the random bit model for random variate generation,\\nwhich operate under different assumptions than A1–A4, have been thoroughly investigated in the\\nliterature. These variants include using a random source which provides flips of a biased b-sided\\ncoin (where the bias may be known or unknown); using a random source which provides non-i.i.d.\\nsymbols; sampling algorithms which return a random number of non-independent output symbols\\nin each invocation; and/or sampling algorithms which use arithmetic operations whose numerical\\nprecision depends on the probabilities in the target distribution [von Neumann 1951; Elias 1972;\\nStout and Warren 1984; Blum 1986; Roche 1991; Peres 1992; Han and Verdú 1993; Vembu and Verdú\\n1995; Abrahams 1996; Pae and Loui 2006; Cicalese et al. 2006; Kozen 2014; Kozen and Soloviev\\n2018]. For example, Pae and Loui [2006] solve the very general problem of optimally simulating an\\narbitrary target distribution using k independent flips of a b-sided coin with unknown bias, where\\noptimality is defined in the sense of the asymptotic ratio of output bits per input symbol. Kozen\\nand Soloviev [2018] provide a unifying coalgebraic framework for implementing and composing\\nentropy-preserving reductions between arbitrary input sources to output distributions, describe\\nseveral concrete algorithms for reductions between random processes, and present bounds on the\\ntrade-off between the latency and asymptotic entropy-efficiency of these protocols.\\nThe assumptions A1–A4 that we make in this paper are designed to explore a new set of trade-\\noffs compared to those explored in previous works. More specifically, the current paper trades\\noff accuracy with numerical precision in the non-asymptotic setting, while maintaining entropy-\\noptimality of the output distribution, whereas the works of Pae and Loui [2006] and Kozen and\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\nOptimal Approximate Sampling from Discrete Probability Distributions 36:7\\nSoloviev [2018], for example, trade off asymptotic entropy-efficiency with numerical precision,\\nwhile maintaining perfect accuracy. The trade-offs we consider are motivated by the standard\\npractice in numerical sampling libraries [Lea 1992; MathWorks 1993; R Core Team 2014; Galassi\\net al. 2019], which (i) use an entropy source that provides independent fair bits (modulo the fact\\nthat they may use pseudorandom number generators); (ii) implement samplers that guarantee\\nexactly one output symbol per invocation; (iii) implement samplers that have non-zero output\\nerror; and (iv) use arithmetic systems with a fixed amount of precision (using e.g., 32-bit or 64-bit\\nfloating point). For the trade-offs considered in this paper, we present results that conclusively\\nsolve the problem of finding entropy-optimal sampling algorithms operating within any precision\\nspecification that yield closest-approximation distributions among the class of all entropy-optimal\\nsamplers that also operate within the given precision. The next section formalizes these concepts.\\n2.2 Preliminaries\\nDefinition 2.1 (Sampling algorithm). Let n ≥ 1 be an integer. A sampling algorithm, or sampler,\\nA :\\n⊎∞\\nk=1{0, 1}k → {1, . . . ,n,⊥} is a map that sends each finite tuple of bits to either an outcome\\nin [n] or a special symbol ⊥ that indicates more bits are needed to determine the final outcome.\\nRemark 2.2. In Assumption A1 and Definition 2.1, the assumption that the source outputs binary\\ndigits in {0, 1} (i.e., b = 2) is made without loss of generality. All the definitions and results in this\\npaper generalize directly to the case of a source that outputs fair flips of any b-sided coin.\\nKnuth and Yao [1976] present a computational framework for expressing the set of all sampling\\nalgorithms for discrete probability distribution in the random bit model. Any sampling algorithm\\nA that draws random bits and returns an integer outcome i with probability pi (i = 1, . . . ,n) is\\nequivalent to some (possibly infinite) binary tree T . Each internal node of T has exactly 2 children\\nand each leaf node is labeled with an outcome in [n]. The sampling algorithm starts at the root\\nof T . It then draws a random bit b from the source and takes the left branch if b = 0 or the right\\nbranch if b = 1. If the child node is a leaf node, the label assigned to that leaf is returned and the\\ncomputation halts. Otherwise, the child node is an internal node, so a new random bit is drawn\\nfrom the source and the process repeats. The next definition presents a state machine model that\\nformally describes the behavior of any sampling algorithm in terms of such a computation tree.\\nDefinition 2.3 (Discrete distribution generating tree). Let A be a sampling algorithm. The computa-\\ntional behavior ofA is described by a state machineT = (S, r ,n, c,δ ), called the discrete distribution\\ngenerating (DDG) tree of A, where\\n• S ⊆ N is a set of states (nodes);\\n• r ∈ S is a designated start node;\\n• n ≥ 1 is an integer indicating the number of outcomes of the sampler;\\n• c : S → {1, . . . ,n} ∪ {branch} is a function that labels each node as either a branch node or\\na terminal (leaf) node assigned to an outcome in [n]; and\\n• δ : S × {0, 1} → S is a transition function that maps a node and a random bit to a new node.\\nLet bk B (b1, . . . ,bk ) ∈ {0, 1}k be a tuple of k ≥ 0 bits, i ∈ S a state, and j ∈ N. The operational\\nsemantics of T for a configuration ⟨i, j, bk ⟩ of the state machine are defined by the following rules\\n0 ≤ j < k ; c(i) = branch\\n⟨i, j, bk ⟩T →\\n〈\\nδ (i,bj+1), j + 1, bk\\n〉\\nT\\nk ≤ j; c(i) = branch\\n⟨i, j, bk ⟩T → ⊥\\n0 ≤ j ≤ k ; c(i) ∈ [n]\\n⟨i, j, bk ⟩T → c(i) (2)\\nIn Eq. (2), the arrow→ defines a transition relation from the current configuration (i.e., state i ,\\nconsumed bits j , and input bits bk ) to either a new configuration (first rule) or to a terminal outcome\\nin {1, . . . ,n,⊥} (second and third rules). The output ofA on input bk is given byA(bk ) B ⟨r , 0, bk ⟩T .\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\n36:8 Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka\\nDefinition 2.4 (Output distribution). Let T be the DDG tree of a sampler A, 1[·] the indicator\\nfunction, and bk ∼ Uniform\\n({0, 1}k ) a random draw of k ≥ 0 fair independent bits. Then\\nPr[A(bk ) = i] = 12k\\n∑\\nb′∈{0,1}k\\n1[⟨(r , 0, b′)⟩T = i] (i = 1, . . . ,n). (3)\\nThe overall probability of returning i , over an infinite length random stream b∞ from the source, is\\npi B Pr[A(b∞) = i] = lim\\nk→∞\\nPr[A(bk ) = i] (i = 1, . . . ,n). (4)\\nFor each k we have Pr[A(bk ) = ⊥] = 1 − ∑ni=1 Pr[A(bk ) = i]. The list of outcome probabilities\\n(p1, . . . ,pn) defined in Eq. (4) is the called the output distribution of T , and we say that T is well-\\nformed whenever these probabilities sum to one (equivalently, whenever A halts with probability\\none, so that Pr[A(b∞) = ⊥] = 0).\\nDefinition 2.5 (Number of consumed bits). For each k ≥ 0, let bk ∼ Uniform\\n({0, 1}k ) be a random\\ndraw of k bits from the source. The number of bits consumed by A is a random variable defined by\\nNk (A, bk ) B min(k, min\\n1≤j≤k\\n{j | A(b1, . . . ,bj ) ∈ [n]}) (k = 0, 1, . . . ). (5)\\n(where min(∅) B ∞), which is precisely the (random) number of steps executed in the evaluation\\nrules (2) on the (random) input bk . Furthermore, we define N (A) B limk→∞ Nk (A, bk ) to be the\\nlimiting number of bits per sample, which exists (in the extended reals) whenever T is well-formed.\\nDefinition 2.6 (Entropy [Shannon 1948]). Let p be a probability distribution over [n]. The Shannon\\nentropy H (p) B ∑ni=1 pi log(1/pi ) is a measure of the stochasticity of p (unless otherwise noted, all\\ninstances of log are base 2). For each integer n, a deterministic distribution has minimal entropy\\n(H (p) = 0) and the uniform distribution has maximal entropy (H (p) = log(n)).\\nDefinition 2.7 (Entropy-optimal sampler). A sampling algorithm A (or DDG tree T ) with output\\ndistribution p is called entropy-optimal if the expected number of random bits consumed from the\\nsource is minimal among all samplers (or DDG trees) that yield the same output distribution p.\\nDefinition 2.8 (Concise binary expansion). We say that a binary expansion of a rational number is\\nconcise if its repeating part is not of the form 1. In other words, to be concise, the binary expansions\\nof dyadic rationals must end in 0 rather than 1.\\nTheorem 2.9 (Knuth and Yao [1976]). Let p B (p1, . . . ,pn) be a discrete probability distribution\\nfor some positive integer n. Let A be an entropy-optimal sampler whose output distribution is equal to\\np. Then the number of bits N (A) consumed by A satisfies H (p) ≤ E[N (A)] < H (p) + 2. Further, the\\nunderlying DDG treeT ofA contains exactly 1 leaf node labeled i at level j if and only if pi j = 1, where\\n(0.pi1pi2 . . . )2 denotes the concise binary expansion of each pi .\\nWe next present three examples of target distributions and corresponding DDG trees that are\\nboth entropy-optimal, based on the construction from Theorem 2.9 and entropy-suboptimal. By\\nTheorem 2.9, an entropy-optimal DDG tree for p can be constructed directly from a data structure\\ncalled the binary probability matrix P, whose entry P[i, j] corresponds to the jth bit in the concise\\nbinary expansion of pi (i = 1, . . . ,n; j ≥ 0). In general, the matrix P can contain infinitely many\\ncolumns, but it can be finitely encoded when the probabilities of p are rational numbers.\\nIn the case where each pi is dyadic, as in Example 2.10, we may instead work with the finite\\nmatrix P that omits those columns corresponding to a final 0 in every row, i.e., whose width is the\\nmaximum number of non-zero binary digits to the right of “0.” in a concise binary expansion of pi .\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\nOptimal Approximate Sampling from Discrete Probability Distributions 36:9\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\np1\\np2\\np3\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1/2\\n1/4\\n1/4\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n. 1 0\\n. 0 1\\n. 0 1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nBinary probability matrix\\n1\\n23\\nEntropy-optimal DDG tree\\n23\\n1\\n32\\n1\\nEntropy-suboptimal DDG tree\\nExample 2.10. Consider the distribution p B (1/2, 1/4, 1/4) over {1, 2, 3}. Since p1 = (0.10)2 and\\np2 = p3 = (0.01)2 are all dyadic, the finite matrix P has two columns and the entropy-optimal tree\\nhas three levels (the root is level zero). Also shown above is an entropy-suboptimal tree for p.\\nNow consider the case where the values of p are all rational but not all dyadic, as in Example 2.11.\\nThen the full binary probability matrix can be encoded using a probability “pseudomatrix” P, which\\nhas a finite number of columns that contain the digits in the finite prefix and the infinitely-repeating\\nsuffix of the concise binary expansions (a horizontal bar is placed atop the columns that contain the\\nrepeating suffix). Similarly, the infinite-level DDG tree for p can be finitely encoded by using back-\\nedges in a “pseudotree”. Note that the DDG trees from Definition 2.3 are technically pseudotrees of\\nthis form, where δ encodes back-edges that finitely encode infinite trees with repeating structure.\\nThe terms “trees” and “pseudotrees” are used interchangeably throughout the paper.\\n[\\np1\\np2\\n]\\n=\\n[\\n3/10\\n7/10\\n]\\n=\\n[\\n. 0 1 0 0 1\\n. 1 0 1 1 0\\n]\\nBinary probability matrix\\n2\\n1\\n2\\n2\\n1\\nEntropy-optimal DDG tree\\n2\\n12\\n2\\nEntropy-suboptimal DDG tree\\nExample 2.11. Consider the distribution p B (3/10, 7/10) over {1, 2}. As p1 and p2 are non-dyadic\\nrational numbers, their infinite binary expansions can be finitely encoded using a pseudotree. The\\n(shortest) entropy-optimal pseudotree shown above has five levels and a back-edge (red) from level\\nfour to level one. This structure corresponds to the structure of P, which has five columns and a\\nprefix length of one, as indicated by the horizontal bar above the last four columns of the matrix.\\nIf any probability pi is irrational, as in Example 2.12, then its concise binary expansion will not\\nrepeat, and so we must work with the full binary probability matrix, which has infinitely many\\ncolumns. Any DDG tree for p has infinitely many levels, and neither the matrix nor the tree can be\\nfinitely encoded. Probability distributions whose samplers cannot be finitely encoded are not the\\nfocus of the sampling algorithms in this paper.\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\np1\\np2\\np3\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1/π\\n1/e\\n1 − 1/π − 1/e\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n. 0 1 0 1 0 0 0 . . .\\n. 0 1 0 1 1 1 1 . . .\\n. 0 1 0 1 0 0 0 . . .\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nBinary probability matrix\\n123\\n213\\n2. . .\\nEntropy-optimal DDG tree\\nExample 2.12 (Knuth and Yao [1976]). Consider the distribution p B (1/π , 1/e, 1 − 1/π − 1/e)\\nover {1, 2, 3}. The binary probability matrix has infinitely many columns and the corresponding\\nDDG tree shown above has infinitely many levels, and neither can be finitely encoded.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\n36:10 Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka\\n2.3 Sampling Algorithms with Limited Computational Resources\\nThe previous examples present three classes of sampling algorithms, which are mutually exclusive\\nand collectively exhaustive: Example 2.10 shows a sampler that halts after consuming at most k bits\\nfrom the source and has a finite DDG tree; Example 2.11 shows a sampler that needs an unbounded\\nnumber of bits from the source and has an infinite DDG tree that can be finitely encoded; and\\nExample 2.12 shows a sampler that needs an unbounded number of bits from the source and has an\\ninfinite DDG tree that cannot be finitely encoded. The algorithms presented in this paper do not\\nconsider target distributions and samplers that cannot be finitely encoded.\\nIn practice, any sampler A for a distribution p of interest that is implemented in a finite-resource\\nsystem must correspond to a DDG tree T with a finite encoding. As a result, the output probability\\nof the sampler is typically an approximation to p. This approximation arises from the fact that\\nfinite-resource machines do not have unbounded memory to store or even lazily construct DDG\\ntrees with an infinite number of levels—a necessary condition for perfectly sampling from an\\narbitrary target distribution—let alone construct entropy-optimal ones by computing the infinite\\nbinary expansion of each pi . Even for a target distribution whose probabilities are rational numbers,\\nthe size of the entropy-optimal DDG tree may be significantly larger than the available resources on\\nthe system (Theorem 3.5). Informally speaking, a “limited-precision” sampler A is able to represent\\neach probability pi using no more than k binary digits. The framework of DDG trees allows us to\\nprecisely characterize this notion in terms of the maximum depth of any leaf in the generating tree\\nof A, which corresponds to the largest number of bits used to encode some pi .\\nDefinition 2.13 (Precision of a sampling algorithm). Let A be any sampler andT B (S, r ,n, c,δ ) its\\nDDG tree. We say that A uses k bits of precision (or that A is a k-bit sampler) if S is finite and the\\nlongest simple path through δ starting from the root r to any leaf node l has exactly k edges.\\nRemark 2.14. Suppose A uses k bits of precision. If δ is cycle-free, as in Example 2.10, then A\\nhalts after consuming no more than k bits from the source and has output probabilities that are\\ndyadic rationals. If δ contains a back-edge, as in Example 2.11, then A can consume an unbounded\\nnumber of bits from the source and has output probabilities that are general rationals.\\nGiven a target distribution p, there may exist an exact sampling algorithm for p using k bits of\\nprecision which is entropy-suboptimal and for which the entropy-optimal exact sampler requires\\nk ′ > k bits of precision. Example 2.11 presents such an instance: the entropy-suboptimal DDG tree\\nhas depth k = 4whereas the entropy-optimal DDG tree has depth k ′ = 5. Entropy-suboptimal exact\\nsamplers typically require polynomial precision (in the number of bits used to encode p) but can be\\nslow and wasteful of random bits (Example 5.1), whereas entropy-optimal exact samplers are fast\\nbut can require precision that is exponentially large (Theorem 3.5). In light of these space–time\\ntrade-offs, this paper considers the problem of finding the “most accurate” entropy-optimal sampler\\nfor a target distribution p when the precision specification is set to a fixed constant (recall from\\nSection 1 that fixing the precision independently of p necessarily introduces sampling error).\\nProblem 2.15. Given a target probability distribution p B (p1, . . . ,pn), a measure of statistical\\nerror ∆, and a precision specification of k ≥ 1 bits, construct a k-bit entropy-optimal sampler Tˆ whose\\noutput probabilities pˆ achieve the smallest possible error ∆(p, pˆ).\\nIn the context of Problem 2.15, we refer to pˆ as a closest approximation to p, or as a closest-\\napproximation distribution to p, and say that Tˆ is an optimal approximate sampler for p.\\nFor any precision specification k , the k-bit entropy-optimal samplers that yield some closest\\napproximation to a given target distribution are not necessarily closer to p than all k-bit entropy-\\nsuboptimal samplers. The next proposition, however, shows they obtain the smallest error among\\nthe class of all samplers that always halt after consuming at most k random bits from the source.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\nOptimal Approximate Sampling from Discrete Probability Distributions 36:11\\nProposition 2.16. Given a target p B (p1, . . . ,pn), an error measure ∆, and k ≥ 1, suppose Tˆ is a\\nk-bit entropy-optimal sampler whose output distribution is a ∆-closest approximation to p. Then pˆ\\nis closer to p than the output distribution p˜ of any sampler T˜ that halts after consuming at most k\\nrandom bits from the source.\\nProof. Suppose for a contradiction that there is an approximation p˜ to p which is the output\\ndistribution of some sampler (either entropy-optimal or entropy-suboptimal) that consumes no\\nmore than k bits from the source such that ∆(p, p˜) < ∆(p, pˆ). But then all entries in p˜ must be\\nk-bit dyadic rationals. Thus, any entropy-optimal DDG tree T˜ for p˜ has depth k and no back-edges,\\ncontradicting the assumption that the output distribution pˆ of Tˆ is a closest approximation to p. □\\nRemark 2.17. In light of Proposition 2.16, we will also consider the restriction of Problem 2.15\\nto k-bit entropy-optimal samplers whose DDG trees do not have back-edges, which yields an\\nentropy-optimal sampler in the class of samplers that halt after consuming at most k random bits.\\n2.4 Pitfalls of Naively Truncating the Target Probabilities\\nLet us momentarily consider the class of samplers from Proposition 2.16. Namely, for given a preci-\\nsion specification k and target distribution p, solve Problem 2.15 over the class of all algorithms that\\nhalt after consuming at most k random bits (and thus have output distributions whose probabilities\\nare dyadic rationals). This section shows examples of how naively truncating the target probabilities\\npi to have k bits of precision (as in, e.g., Ladd [2009]; Dwarakanath and Galbraith [2014]) can fail to\\ndeliver accurate limited-precision samplers for various target distributions and error measures.\\nMore specifically, the naive truncation initializes pˆi = (0.p1p2 . . .pk )2 = ⌊2kpi ⌋/2k . As the pˆi may\\nnot sum to unity, lower-order bits can be arbitrarily incremented until the terms sum to one (this\\nnormalization is implicit when using floating-point computations to implement limited-precision\\ninversion sampling, as in Algorithm 2). The pˆi can be organized into a probability matrix Pˆ, which\\nis the truncation of the full probability matrix P to k columns. The matrix Pˆ can then be used to\\nconstruct a finite entropy-optimal DDG tree, as in Example 2.10. While such a truncation approach\\nmay be sensible when the error of the approximate probabilities pˆi is measured using total variation\\ndistance, the error in the general case can be highly sensitive to the setting of lower-order bits\\nafter truncation, depending on the target distribution p, the precision specification k , and the error\\nmeasure ∆. We next present three conceptual examples that highlight these numerical issues for\\ncommon measures of statistical error that are used in various applications.\\nExample 2.18 (Round-off with relative entropy divergence). Suppose the error measure is relative\\nentropy (Kullback-Leibler divergence), ∆(p, pˆ) B ∑ni=1 log(pˆi/pi )pi , which plays a key role in\\ninformation theory and data compression [Kullback and Leibler 1951]. Suppose n, k and p are such\\nthat n ≤ 2k and there exists i where pi = ϵ ≪ 1/2k . Then setting pˆi so that 2kpˆi = ⌊2kpi ⌋ = 0 and\\nfailing to increment the lower-order bit of pˆi results in an infinite divergence of pˆ from p, whereas,\\nfrom the assumption that n ≤ 2k , there exist approximations that have finite divergence.\\nIn the previous example, failing to increment a low-order bit results in a large (infinite) error. In\\nthe next example, choosing to increment a low-order bit results in an arbitrarily large error.\\nExample 2.19 (Round-off with Pearson chi-square divergence). Suppose the error measure is\\nPearson chi-square, ∆(p, pˆ) B ∑ni=1(pi − pˆi )2/pi , which is central to goodness-of-fit testing in\\nstatistics [Pearson 1900]. Suppose that k and p are such that there exists i where pi = c/2k + ϵ ,\\nfor 0 < ϵ ≪ 1/2k for some integer 0 ≤ c ≤ 2k − 1. Then setting pˆi so that 2kpˆi = ⌊2kpi ⌋ = c (not\\nincrementing the lower-order bit) gives a small contribution to the error, whereas setting pˆ+i so that\\n2kpˆ+i = ⌊2kpi ⌋ = c + 1 (incrementing the lower-order bit) gives a large contribution to the error.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\n36:12 Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka\\nMore specifically, the relative error of selecting pˆ+i instead of pˆi is arbitrarily large:\\n(pi − pˆ+i )2/(pi − pˆi )2 = (c/2k + ϵ − c/2k − 1/2k )2/(c/2k + ϵ − c/2k )2\\n= (1/2k − ϵ)2/ϵ2 ≈ 1/(2kϵ)2 ≫ 1.\\nThe next example shows that the first k bits of pi can be far from the globally optimal k-bit\\napproximation, even in higher-precision regimes where 1/2k ≤ min(p1, . . . ,pn).\\nExample 2.20 (Round-off with Hellinger divergence). Suppose the error measure is the Hellinger\\ndivergence, ∆(p, pˆ) B ∑ni=1(√pi−√pˆi )2, which is used in fields such as information complexity [Bar-\\nYossef et al. 2004]. Let k = 16 and n = 1000, with p1 = 5/8 and p2 = · · · = pn = 3/8(n − 1). Let\\n(pˆ1, . . . pˆn) be the k-bit approximation that minimizes ∆(p, pˆ). It can be shown that 2kpˆ1 = 40788\\nwhereas ⌊2kp1⌋ = 40960, so that\\n\\x0c\\x0c⌊2kp1⌋ − 2kpˆ1\\x0c\\x0c = 172.\\nIn light of these examples, we turn our attention to solving Problem 2.15 by truncating the\\ntarget probabilities in a principled way that avoids these pitfalls and finds a closest-approximation\\ndistribution for any target probability distribution, error measure, and precision specification.\\n3 CHARACTERIZING THE SPACE OF ENTROPY-OPTIMAL SAMPLING ALGORITHMS\\nThis section presents several results about the class of entropy-optimal k-bit sampling algorithms\\nover which Problem 2.15 is defined. These results form the basis of the algorithm for finding a closest-\\napproximation distribution pˆ in Section 4 and the algorithms for constructing the corresponding\\nentropy-optimal DDG tree Tˆ in Section 5, which together will form the solution to Problem 2.15.\\nSection 2.4 considered sampling algorithms that halt after consuming at most k random bits (so\\nthat each output probability is an integer multiple of 1/2k ) and showed that naively discretizing the\\ntarget distribution can result in poor approximations. The DDG trees of those sampling algorithms\\nare finite: they have depth k and no back-edges. For entropy-optimal DDG trees that use k ≥ 1\\nbits of precision (Definition 2.13) and have back-edges, the output distributions (Definition 2.4) are\\ndescribed by a k-bit number. The k-bit numbers x are those such that for some integer l satisfying\\n0 ≤ l ≤ k , there is some element (x1, . . . ,xk ) ∈ {0, 1}l × {0, 1}k−l , where the first l bits correspond\\nto a finite prefix and the final k − l bits correspond to an infinitely repeating suffix, such that\\nx = (0.x1 . . . xlxl+1 . . . xk )2. Write Bkl for the set of rationals in [0, 1] describable in this way.\\nProposition 3.1. For integers k and l with 0 ≤ l ≤ k , define Zkl B 2k − 2l1l<k . Then\\nBkl =\\n{\\n0\\nZkl\\n,\\n1\\nZkl\\n, . . . ,\\nZkl − 1\\nZkl\\n,\\nZkl\\nZkl\\n1l<k\\n}\\n. (6)\\nProof. For l = k , the number system Bkl = Bkk is the set of dyadic rationals in [0, 1) with\\ndenominator Zkk = 2k . For 0 ≤ l < k , any element x ∈ Bkl when written in base 2 has a (possibly\\nempty) non-repeating prefix and a non-empty infinitely repeating suffix, so that x has binary\\nexpansion (0.a1 . . . alsl+1 . . . sk )2. The first two lines of equalities (Eqs. (7) and (8)) imply Eq. (9):\\n2l (0.a1 . . . al )2 = (a1 . . . al )2 = ∑l−1i=0 al−i2i , (7)\\n(2k−l − 1)(0.sl+1 . . . sk )2 = (sl+1 . . . sk )2 = ∑k−(l+1)i=0 sk−i2i , (8)\\nx = (0.a1 . . . al )2 + 2−l (0.sl+1 . . . sk )2 =\\n(2k−l − 1)∑l−1i=0 al−i2i +∑k−(l+1)i=0 sk−i2i\\n2k − 2l . (9)\\n□\\nRemark 3.2. For a rational x ∈ [0, 1], we take a representative ((x1, . . . ,xl ), (xl+1, . . . ,xk )) ∈ Bkl\\nthat is both concise (Definition 2.8) and chosen such that the number k of digits is as small possible.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\nOptimal Approximate Sampling from Discrete Probability Distributions 36:13\\nRemark 3.3. When 0 ≤ l ≤ k , we have Bkl ⊆ Bk+1,l+1, since if x ∈ Bkl then Proposition 3.1\\nfurnishes an integer c such that x = c/(2k − 2l1l<k ) = 2c/(2k+1 − 2l+11l<k ) ∈ Bk+1,l+1. Further, for\\nk ≥ 2, we have Bk,k−1 \\\\ {1} = Bk−1,k−1 ⊆ Bkk , since any infinitely repeating suffix comprised of a\\nsingle digit can be folded into the prefix, except when the prefix and suffix are all ones.\\nTheorem 3.4. Let p B (p1, . . . ,pn) be a non-degenerate rational distribution for some integer\\nn > 1. The precision k of the shortest entropy-optimal DDG (pseudo)tree with output distribution p is\\nthe smallest integer such that every pi is an integer multiple of 1/Zkl (hence in Bkl ) for some l ≤ k .\\nProof. Suppose that T is a shortest entropy-optimal DDG (pseudo)tree and let k be its depth\\n(note that k ≥ 1, as k = 0 implies p is degenerate). Assume n = 2. From Theorem 2.9, Definition 2.13,\\nand the hypothesis that the transition function δ of T encodes that shortest possible DDG tree, we\\nhave that for each i = 1, 2, the probability pi is a rational number where the number of digits in the\\nshortest prefix and suffix of the concise binary expansion is at most k . Therefore, we can write\\np1 = (0.a1 . . . al1sl1+1 . . . sk ), p2 = (0.w1 . . .wl2ul2+1 . . .uk ), (10)\\nwhere li and k − li are the number of digits in the shortest prefix and suffix, respectively, of each pi .\\nIf l1 = l2 then the conclusion follows from Proposition 3.1. If l1 = k − 1 and l2 = k then the\\nconclusion follows from Remark 3.3 and the fact that p1 , 1, p2 , 1. Now, from Proposition 3.1, it\\nsuffices to establish that l1 = l2 C l , so that p1 and p2 are both integer multiples of 1/Zkl . Suppose\\nfor a contradiction that l1 < l2 and l1 , k − 1. Write p1 = a/c and p2 = b/d where each summand is\\nin reduced form. By Proposition 3.1, we have c = 2k − 2l1 and d = 2k − 2l21l2<k . Then as p1 +p2 = 1\\nwe have ad + bc = cd . If c , d then either b has a positive factor in common with d or a with c ,\\ncontradicting the summands being in reduced form. But c = d contradicts l1 < l2.\\nThe case where n > 2 is a straightforward extension of this argument. □\\nAn immediate consequence of Theorem 3.4 is that all back-edges in an entropy-optimal DDG tree\\nthat uses k bits of precision must originate at level k−1 and end at the same level l < k−1. The next\\nresult, Theorem 3.5, shows that at most Z − 1 bits of precision are needed by an entropy-optimal\\nDDG tree to exactly flip a coin with rational probability p = c/Z , which is exponentially larger\\nthan the log(Z ) bits needed to encode Z . Theorem 3.6 shows that this bound is tight for many Z\\nand, as we note in Remark 3.7, is likely tight for infinitely many Z . These results highlight the need\\nfor approximate entropy-optimal sampling from a computational complexity standpoint.\\nTheorem 3.5. LetM1, . . . ,Mn be n positive integers that sum to Z and let p B (M1/Z , . . . ,Mn/Z ).\\nAny exact, entropy-optimal sampler whose output distribution is p needs at most Z − 1 bits of precision.\\nProof. By Theorem 3.4, it suffices to find integers k ≤ Z − 1 and l ≤ k such that Zkl is a multiple\\nof Z , which in turn implies that any entropy-optimal sampler for p needs at most Z − 1 bits.\\nCase 1: Z is odd. Consider k = Z − 1. We will show that Z divides 2Z−1 − 2l for some l such\\n0 ≤ l ≤ Z −2. Let ϕ be Euler’s totient function, which satisfies 1 ≤ ϕ(Z ) ≤ Z −1 = k . Then 2ϕ(Z ) ≡ 1\\n(mod Z ) as gcd(Z , 2) = 1. Put l = Z − 1 − ϕ(Z ) and conclude that Z divides 2Z−1 − 2Z−1−ϕ(Z ).\\nCase 2: Z is even. Let t ≥ 1 be the maximal power of 2 dividing Z , and write Z = Z ′2t . Consider\\nk = Z ′ − 1 + t and l = j + t where j = (Z ′ − 1) − ϕ(Z ′). As in the previous case applied to Z ′, we\\nhave that Z ′ divides 2Z ′−1 − 2j , and so Z divides 2k − 2l . We have 0 ≤ l ≤ k as 1 ≤ ϕ(Z ) ≤ Z − 1.\\nFinally, k = Z ′ + t − 1 ≤ Z ′2t − 1 = Z − 1 as t < 2t . □\\nTheorem 3.6. LetM1, . . . ,Mn be n positive integers that sum to Z and put p = (M1/Z , . . . ,Mn/Z ).\\nIf Z is prime and 2 is a primitive root modulo Z , then any exact, entropy-optimal sampler whose output\\ndistribution is p needs exactly Z − 1 bits of precision.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\n36:14 Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka\\nProof. Since 2 is a primitive root modulo Z , the smallest integer a for which 2a −1 ≡ 0 (mod Z )\\nis precisely ϕ(Z ) = Z − 1. We will show that for any k ′ < Z − 1 there is no exact entropy-optimal\\nsampler that uses k ′ bits of precision. By Theorem 3.5, if there were such a sampler, then Zk ′l must\\nbe a multiple of Z for some l ≤ k ′. If l < k ′, then Zk ′l = 2k ′ − 2l . Hence 2k ′ ≡ 2l (mod Z ) and\\nso 2k ′−l ≡ 1 (mod Z ) as Z is odd. But k ′ < Z − 1 = ϕ(Z ), contradicting the assumption that 2 is\\na primitive root modulo Z . If l = k ′, then Zk ′l = 2k\\n′ , which is not divisible by Z since we have\\nassumed that Z is odd (as 2 is not a primitive root modulo 2). □\\nRemark 3.7. The bound in Theorem 3.5 is likely the tightest possible for infinitely many Z .\\nAssuming Artin’s conjecture, there are infinitely many primes Z for which 2 is a primitive root,\\nwhich in turns implies by Theorem 3.6 that any entropy-optimal DDG tree must have Z levels.\\n4 OPTIMAL APPROXIMATIONS OF DISCRETE PROBABILITY DISTRIBUTIONS\\nReturning to Problem 2.15, we next present an efficient algorithm for finding a closest-approximation\\ndistribution pˆ to any target distribution p, using Theorem 3.4 to constrain the set of allowable\\ndistributions to those that are the output distribution of some entropy-optimal k-bit sampler.\\n4.1 f -divergences: A Family of Statistical Divergences\\nWe quantify the error of approximate sampling algorithms using a broad family of statistical\\nerror measures called f -divergences [Ali and Silvey 1966], as is common in the random variate\\ngeneration literature [Cicalese et al. 2006]. This family includes well-known divergences such as\\ntotal variation (which corresponds to Euclidean L1 norm), relative entropy (used in information\\ntheory [Kullback and Leibler 1951]), Pearson chi-square (used in statistical hypothesis testing [Pear-\\nson 1900]), Jensen–Shannon (used in text classification [Dhillon et al. 2003]), and Hellinger (used\\nin cryptography [Steinberger 2012] and information complexity [Bar-Yossef et al. 2004]).\\nDefinition 4.1 (Statistical divergence). Let n be a positive integer and Sn be the (n−1)-dimensional\\nprobability simplex, i.e., the set of all probability distributions over [n]. A statistical divergence\\n∆ : Sn × Sn → [0,∞] is any mapping from pairs of distributions on [n] to non-negative extended\\nreal numbers, such that for all p, q ∈ Sn we have ∆(p, q) = 0 if and only if pi = qi (i = 1, . . . ,n).\\nTable 1. Common statistical divergences expressed as f -divergences.\\nDivergence Measure Formula ∆д(p, q) Generator д(t)\\nTotal Variation 12\\n∑n\\ni=1 |qi − pi | 12 |t − 1|\\nHellinger Divergence 12\\n∑n\\ni=1(\\n√\\npi − √qi )2 (\\n√\\nt − 1)2\\nPearson Chi-Squared\\n∑n\\ni=1 (qi − pi )2/qi (t − 1)2\\nTriangular Discrimination\\n∑n\\ni=1 (pi − qi )2/(pi + qi ) (t − 1)2/(t + 1)\\nRelative Entropy\\n∑n\\ni=1 log(qi/pi )qi t log t\\nα-Divergence 4(1 −∑ni=1(p(1−α )/2i q1+αi ))/(1 − α2) 4(1 − t (1+α )/2)/(1 − α2)\\nTotal Variation Relative Entropy Reverse Relative Entropy Hellinger Divergence Alpha Divergence Matern Jeffrey Divergence\\nFig. 1. Plots of generating functions д for various f -divergences, a subset of which are shown in Table 1.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\nOptimal Approximate Sampling from Discrete Probability Distributions 36:15\\nDefinition 4.2 (f -divergence). An f -divergence is any statistical divergence of the form\\n∆д(p, q) B ∑ni=1 д(qi/pi )pi , (11)\\nfor some convex function д : (0,∞) → R with д(1) = 0. The function д is called the generator of ∆д .\\nFor concreteness, Table 1 expresses several statistical divergence measures as f -divergences\\nand Figure 1 shows plots of generating functions. The class of f -divergences is closed under\\nseveral operations; for example, if ∆д(p, q) is an f -divergence then so is the dual ∆д∗ (q, p), where\\nд∗(t) = tд(1/t) is the perspective of д. A technical review of these concepts can be found in Liese\\nand Vajda [2006, Section III]. In this paper, we address Problem 4.6 assuming the error measure ∆ is\\nan f -divergence, which in turn allows us to optimize any error measure that is a 1-1 transformation\\nof an underlying f -divergence.\\n4.2 Problem Statement for Finding Closest-Approximation Distributions\\nRecall that Theorem 3.4 establishes that the probability distributions that can be simulated exactly\\nby an entropy-optimal DDG tree with k bits of precision have probabilities pi of the formMi/Zkl ,\\nwhereMi is a non-negative integer and Zkl = 2k − 2l1k<l is the denominator of the number system\\nBkl . This notion is a special case of the following concept.\\nDefinition 4.3 (Z -type distribution [Cover and Thomas 2006]). For any positive integer Z , a proba-\\nbility distribution p over [n] is said to be Z -type distribution if\\npi ∈\\n{\\n0\\nZ\\n,\\n1\\nZ\\n,\\n2\\nZ\\n, . . . ,\\nZ\\nZ\\n}\\n(i = 1, . . . ,n). (12)\\nDefinition 4.4. For positive integer n and non-negative integer Z , define the set\\nM[n,Z ] B {(M1, . . . ,Mn) \\x0c\\x0c Mi ≥ 0, Mi ∈ Z, ∑ni=1Mi = Z } , (13)\\nwhich can be thought of as the set of all possible assignments of Z indistinguishable balls into n\\ndistinguishable bins such that each bin i hasMi balls.\\nRemark 4.5. Each element M ∈ M[n,Z ] may be identified with a Z -type distribution q over\\n[n] by letting qi B Mi/Z (i = 1, . . . ,n), and thus adopt the notation ∆д(p,M) to indicate the\\nf -divergence between probability distributions p and q (cf. Eq. (11)).\\nBy Theorem 3.4 and Remark 4.5, Problem 2.15 is a special case of the following problem, since\\nthe output distribution of any k-bit entropy-optimal sampler is Z -type, where Z ∈ {Zk0, . . . ,Zkk }.\\nProblem 4.6. Given a target distribution p over [n], an f -divergence ∆д , and a positive integer Z ,\\nfind a tupleM = (M1, . . . ,Mn) ∈ M[n,Z ] that minimizes the divergence\\n∆д(p,M) =\\nn∑\\ni=1\\nд\\n(\\nMi\\nZpi\\n)\\npi . (14)\\nAs the set M[n,Z ] is combinatorially large, Problem 4.6 cannot be solved efficiently by enu-\\nmeration. In the next section, we present an algorithm that finds an assignmentM that minimizes\\nthe objective function (14) among the elements of M[n,Z ]. By Theorem 3.4, for any precision\\nspecification k ≥ 0, using Z = Zkl for each l = 0, . . . ,k and then selecting the value of l for which\\nEq. (14) is smallest corresponds to finding a closest-approximation distribution pˆ for the class of\\nk-bit entropy-optimal samplers, and thus solves the first part of Problem 2.15.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\n36:16 Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka\\nAlgorithm 3 Finding an error-minimal Z -type probability distribution.\\nInput: Probability distribution p B (p1, . . . ,pn); integer Z > 0; and f -divergence ∆д .\\nOutput: NumeratorsM B (M1, . . . ,Mn) of Z -type distribution that minimizes ∆д(p,M).\\n1. For each i = 1, . . . ,n:\\n1.1 If д\\n( ⌊Zpi ⌋\\nZpi\\n)\\n≤ д\\n( ⌊Zpi ⌋+1\\nZpi\\n)\\nthen setMi B ⌊Zpi ⌋;\\nElse setMi B ⌊Zpi ⌋ + 1.\\n2. ForW ∈ M[n,M1 + · · · +Mn], i ∈ [n], and δ ∈ {+1,−1}, define the function\\nϵ(W, i,δ ) B pi [д((Wi + δ )/(Zpi )) − д(Wi/(Zpi ))] , (15)\\nwhich is the cost of settingWi ←Wi + δ (or∞ if (Wi + δ ) < {0, . . . ,Z }).\\n3. Repeat until convergence:\\nLet (j, j ′) B argmin(i,i′)∈[n]2 |i,i′ {ϵ(M, i,+1) + ϵ(M, i ′,−1)}.\\nIf ϵ(M, j,+1) + ϵ(M, j ′,−1) < 0 then:\\nUpdateMj ← Mj + 1.\\nUpdateMj′ ← Mj′ − 1.\\n4. Let S B (M1 + · · ·+Mn) −Z be the number of units that need to be added toM (if S < 0)\\nor subtracted fromM (if S > 0) in order to ensure thatM sums to Z .\\n5. If S = 0, then returnM as the optimum.\\n6. Let δS B 1[S < 0] − 1[S > 0].\\n7. Repeat S times:\\nLet j B argmini=1, ...,n(ϵ(M, i,δS )).\\nUpdateMj ← Mj + δS .\\n8. ReturnM as the optimum.\\n4.3 An Efficient Optimization Algorithm\\nAlgorithm 3 presents an efficient procedure that solves Problem 4.6. We now state the main theorem.\\nTheorem 4.7. For any probability distribution p, f -divergence ∆д , and denominator Z > 0, the\\ndistribution returned by Algorithm 3 minimizes the objective function (14) over all Z -type distributions.\\nThe remainder of this section contains the proof of Theorem 4.7. Section 4.3.1 establishes\\ncorrectness and Section 4.3.2 establishes runtime.\\n4.3.1 Theoretical Analysis: Correctness. In this section, let p,n,Z , andд be defined as in Algorithm 3.\\nDefinition 4.8. Let t > 0 be an integer andM ∈ M[n, t]. For integers a and b, define\\n∆i [a → b;M,Z ] B pi\\n[\\nд\\n(\\nMi + b\\nZpi\\n)\\n− д\\n(\\nMi + a\\nZpi\\n)]\\n(i = 1, . . . ,n). (16)\\nFor typographical convenience, we write ∆i [a → b;M] (or ∆i [a → b]) when Z (andM) are clear\\nfrom context. We define ∆i [a → b] B ∞ whenever (Mi + b) or (Mi + a) are not in {0, . . . , t}.\\nRemark 4.9. The convexity of д implies that for any real number j,\\nд\\n(\\nMi + j + 1\\nZpi\\n)\\n− д\\n(\\nMi + j\\nZpi\\n)\\n1/(Zpi ) ≤\\nд\\n(\\nMi + j + 2\\nZpi\\n)\\n− д\\n(\\nMi + j + 1\\nZpi\\n)\\n1/(Zpi ) (i = 1, . . . ,n). (17)\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\nOptimal Approximate Sampling from Discrete Probability Distributions 36:17\\nLetting j range over the integers gives\\n· · · < ∆i [−2→ −1] < ∆i [−1→ 0] < ∆i [0→ 1] < ∆i [1→ 2] < ∆i [2→ 3] < · · · . (18)\\nBy telescoping (18), if a < b < c then\\n∆i [a → b] < ∆i [a → c] . (19)\\nFinally, it is immediate from the definition that ∆i [a → b] = −∆i [b → a] for all a and b.\\nTheorem 4.10. Let t > 0 be an integer andM B (M1, . . . ,Mn) be any assignment inM[n, t]. If,\\ngiven initial values M the loop defined in Step 3 of Algorithm 3 terminates, then the final values of M\\nminimize ∆д(p, ·) over the setM[n, t].\\nProof. We argue that the locally optimal assignments performed at each iteration of the loop\\nare globally optimal. Assume toward a contradiction that the loop in Step 3 terminates with a\\nsuboptimal assignment (W1, . . . ,Wn) ∈ M[n, t]. Then there exist indices i and j with 1 ≤ i < j ≤ n\\nsuch that for some positive integers a and b,\\npjд\\n(\\nWj + a\\nZpj\\n)\\n+ piд\\n(\\nWi − b\\nZpj\\n)\\n< pjд\\n(\\nWj\\nZpj\\n)\\n+ piд\\n(\\nWi\\nZpj\\n)\\n(20)\\n⇐⇒ ∆j [0→ a] + ∆i [0→ −b] < 0 (21)\\n⇐⇒ ∆j [0→ a] < −∆i [0→ −b] (22)\\n⇐⇒ ∆j [0→ a] < ∆i [−b → 0] . (23)\\nCombining (23) with (19) gives\\n∆j [0→ 1] < ∆j [0→ a] < ∆i [−b → 0] < ∆i [−1→ 0] , (24)\\nwhich implies ϵj (+1) + ϵi (−1) < 0, and so the loop can execute for one more iteration. □\\nWe now show that the value ofM at the termination of the loop in Step 7 of Algorithm 3 optimizes\\nthe objective function overM[n,Z ].\\nTheorem 4.11. For some positive integer t < Z , suppose that M B (M1, . . . ,Mn) minimizes the\\nobjective function ∆д(p, ·) over the set M[n, t]. Then M+ defined by M+i B Mi + 1i=u minimizes\\n∆д(p, ·) overM[n, t + 1], where\\nu B argmin\\ni=1, ...,n\\n{\\npi\\n[\\nд\\n(\\nMi + 1\\nZpi\\n)\\n− д\\n(\\nMi\\nZpi\\n)]}\\n. (25)\\nProof. Assume, for a contradiction, that there existsM′ B (M ′1, . . . ,M ′n) that minimizes ∆д(p, ·)\\noverM[n, t + 1] with ∆д(p,M′) < ∆д(p,M+). ClearlyM′ , M+. We proceed in cases.\\nCase 1: M ′u = Mu . Then there exists integers j , t and a ≥ 1 such thatM ′j = Mj + a. Hence\\n∆u [0→ 1] < ∆j [0→ 1] ≤ ∆j [(a − 1) → a] = −∆j [a → (a − 1)] (26)\\n=⇒ ∆u [0→ 1] + ∆j [a → (a − 1)] < 0, (27)\\nwhere the first inequality of (26) follows from the minimality of u in (25) and the second inequality\\nof (26) follows from (18). Therefore, setting M ′u ← Mu + 1 and M ′j ← Mj + (a − 1) gives a net\\nreduction in the cost, a contradiction to the optimality ofM′.\\nCase 2: M ′u = Mu +1. Assume without loss of generality (for this case) thatu = 1. SinceM′ , M+,\\nthere exists an index j > 1 such that M ′j , Mj . There are t + 1 − (M1 + 1) = t − M1 remaining\\nunits to distribute among (M ′2, . . . ,M ′n). From the optimality ofM, the tail (M2, . . . ,Mn) minimizes∑n\\ni=2 piд(Mi/Zpi ) among all tuples using t −M1 units; otherwise a more optimal solution could be\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\n36:18 Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka\\nobtained by holdingM1 fixed and optimizing (M2, . . . ,Mn). It follows that the tail (M ′2, . . . ,M ′n) of\\nM′ is less optimal than the tail (M2, . . . ,Mn) ofM+, a contradiction to the optimality ofM′.\\nCase 3: M ′u = Mu + c for some integer c ≥ 2. Then there exists some j , t such thatM ′j = Mj −a\\nfor some integer a ≥ 1. From the optimality ofM, any move must increase the objective, i.e.,\\n∆u [0→ 1] > ∆j [−1→ 0] . (28)\\nCombining (18) with (28) gives\\n∆u [(c − 1) → c] ≥ ∆u [0→ 1] > ∆j [−1→ 0] ≥ ∆j [−a → −(a − 1)] (29)\\n=⇒ ∆u [c → (c − 1)] + ∆j [−a → −(a − 1)] < 0 (30)\\nTherefore, settingM ′u ← Mu + (c − 1) andM ′j ← Mj − (a − 1) gives a net reduction in the cost, a\\ncontradiction to the optimality ofM′.\\nCase 4: M ′u = Mu − a for some integer a ≥ 1. This case is symmetric to the previous one. □\\nBy a proof symmetric to that of Theorem 4.11, we obtain the following.\\nCorollary 4.12. IfMminimizes ∆д(p, ·) overM[n, t] for some t ≤ Z , then the assignmentM− with\\nM−i B Mi − 1i=u minimizes ∆д(p, ·) overM[n, t − 1], where u B argmini=1, ...,n ∆i [0→ −1;M,Z ].\\n4.3.2 Theoretical Analysis: Runtime. We next establish that Algorithm 3 halts by showing the loops\\nin Step 3 and Step 4 execute for at most n iterations. Recall that Theorem 4.10 established that if\\nthe loop in Step 3 halts, then it halts with an optimal assignment. The next two theorems together\\nestablish this loop halts in at most n iterations.\\nTheorem 4.13. In the loop in Step 3 of Algorithm 3, there is no index j ∈ [n] for which Mj is\\nincremented at some iteration of the loop and then decremented at a later iteration.\\nProof. The proof is by contradiction. Suppose that iteration s is the first iteration of the loop\\nwhere some index j was decremented, having only experienced increments (if any) in the previous\\niterations 1, 2, . . . , s − 1. Let r ≤ s − 1 be the iteration at which j was most recently incremented,\\nand j ′′ the index of the element which was decremented at iteration r so that\\n∆j [0→ 1;Mr ] + ∆j′′ [0→ −1;Mr ] < 0, (31)\\nwhereMq denotes the assignment at the beginning of any iteration q (q = 1, . . . , s).\\nThe following hold:\\n∆j\\n′ [0→ 1;Ms ] + ∆j [0→ −1;Ms ] < 0, (32)\\n∆j [0→ 1;Mr ] = −∆j [0→ −1;Ms ] , (33)\\n∆j\\n′ [0→ 1;Mr ] ≤ ∆j′ [0→ 1;Ms ] , (34)\\nwhere (32) follows from the fact that j is decremented at iteration s and j ′ is the corresponding index\\nwhich was incremented that gives a net decrease in the error; (33) follows from the hypothesis that\\nr was the most recent iteration at which j was incremented; and (34) follows from the hypothesis\\non iteration s , which implies that j ′ must have only experienced increments at iterations 1, . . . , s − 1\\nand the property of ∆j′ from (18). These together yield\\n∆j\\n′ [0→ 1;Mr ] ≤ ∆j′ [0→ 1;Ms ] < −∆j [0→ 1;Ms ] = ∆j [0→ 1;Mr ] , (35)\\nwhere the first inequality follows from (34), the second inequality from (32), and the final equality\\nfrom (33). But (35) implies that the pair of indices (j, j ′′) selected (31) at iteration r was not an\\noptimal choice, a contradiction. □\\nTheorem 4.14. The loop in Step 3 of Algorithm 3 halts in at most n iterations.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\nOptimal Approximate Sampling from Discrete Probability Distributions 36:19\\nProof. Theorem 4.13 establishes that once an item is decremented it will never incremented at a\\nfuture step; and once an item is incremented it will never be decremented at a future step. To prove\\nthe bound of halting within n iterations, we show that there are at most n increments/decrements\\nin total. We proceed by a case analysis on the generating function д.\\nCase 1: д > 0 is a positive generator. In this case, we argue that the values (M1, . . . ,Mn) obtained\\nin Step 1 are already initialized to the global minimum, and so the loop in Step 3 is never entered.\\nBy the hypothesis д > 0, it follows that д is decreasing on (0, 1) and increasing on (1,∞):\\nд\\n(\\n0\\nZpi\\n)\\n> · · · > д\\n( ⌊Zpi ⌋\\nZpi\\n)\\n, д\\n( ⌊Zpi ⌋ + 1\\nZpi\\n)\\n< · · · < д\\n(\\nZ\\nZpi\\n)\\n. (36)\\nTherefore, the function дi (m) B piд(m/(Zpi )) attains its minimum at eitherm = ⌊Zpi ⌋ orm =\\n⌊Zpi ⌋ + 1. Since the objective function is a linear sum of the дi , minimizing each term individually\\nattains the global minimum. The loop in Step 3 thus executes for zero iterations.\\nCase 2: д > 0 on (1,∞) and д < 0 on an interval (γ , 1) for some 0 < γ < 1. The main indices i of\\ninterest are those for which\\nγ <\\n⌊Zpi ⌋\\nZpi\\n< 1 < ⌊Zpi ⌋ + 1\\nZpi\\n, (37)\\nsince all indices for which д(⌊Zpi ⌋/(Zpi )) > 0 and д((⌊Zpi ⌋ + 1)/(Zpi )) > 0 are covered by the\\nprevious case. Therefore we may assume that\\nγ ≤ min\\ni=1, ...,n\\n( ⌊Zpi ⌋\\nZpi\\n)\\n, (38)\\nwith д increasing on (γ ,∞). (The proof for general γ is a straightforward extension of the proof\\npresented here.) We argue that the loop maintains the invariantMi ≤ ⌊Zpi ⌋+1 for each i = 1, . . . ,n.\\nThe proof is by induction on the iterations of the loop. For the base case, observe that\\nд\\n( ⌊Zpi ⌋\\nZpi\\n)\\n< 0 < д\\n( ⌊Zpi ⌋ + 1\\nZpi\\n)\\n(i = 1, . . . ,n), (39)\\nwhich follows from the hypothesis on д in this case. The values after Step 1 are thusMi = ⌊Zpi ⌋\\nfor each i = 1, . . . ,n. The first iteration performs one increment/decrement so the bound holds.\\nFor the inductive case, assume that the invariant holds for iterations 2, . . . , s−1. Assume, towards\\na contradiction, that in iteration s there existsMj = ⌊Zpj ⌋ + 1 andMj is incremented. LetMu be\\nthe corresponding element that is decremented. We analyze two cases onMu .\\nSubcase 2.1: Mu/(Zpu ) ≤ 1. ThenMu = ⌊Zpu ⌋ − a for some integer a ≥ 0. But then\\n(Mu − a − 1)/Zpu < (Mu − a)/Zpu < 1 < (Mj + 1)/Zpj < (Mj + 2)/Zpj (40)\\nand\\npjд\\n(\\nMj + 2\\nZpj\\n)\\n+ puд\\n(\\nMu − a − 1\\nZpu\\n)\\n< pjд\\n(\\nMj + 1\\nZpj\\n)\\n+ puд\\n(\\nMu − a\\nZpu\\n)\\n(41)\\n⇐⇒\\nд\\n(\\nMj + 2\\nZpj\\n)\\n− д\\n(\\nMj + 1\\nZpj\\n)\\n1/(Zpj ) <\\nд\\n(\\nMu − a\\nZpu\\n)\\n− д\\n(\\nMu − a − 1\\nZpu\\n)\\n1/(Zpu ) , (42)\\na contradiction to the convexity of д.\\nSubcase 2.2: 1 ≤ Mu/(Zpu ). By the inductive hypothesis, it must be that Mu = ⌊Zpu ⌋ + 1.\\nSince the net error of the increment and corresponding decrement is negative in the if branch of\\nStep 3, ∆j [1→ 2] + ∆l [1→ 0] < 0, which implies\\n∆j [1→ 2] < −∆l [1→ 0] = ∆l [0→ 1] . (43)\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\n36:20 Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka\\nSince ∆j [0→ 1] < ∆j [1→ 2] from (18), it follows thatMj should have been incremented at two\\nprevious iterations before having incrementedMu ← Mu + 1, contradicting the minimality of the\\nincrements at each iteration.\\nSince each Mi is one greater than the initial value at the termination of the loop, and at each\\niteration one value is incremented, the loop terminates in at most n iterations.\\nCase 3: д > 0 on (0, 1) and д < 0 on some interval (1,γ ) for 1 < γ ≤ ∞. The proof is symmetric\\nto the previous case, with initial valuesMi = ⌊Zpi ⌋ + 1 from Step 1 and invariantMi ≥ ⌊Zpi ⌋. □\\nRemark 4.15. The overall cost of Step 3 is O(n logn), since (j, j ′) can be found in O(logn) time\\nby performing order-preserving insertions and deletions on a pair of initially sorted lists.\\nTheorem 4.16. The value S defined in Step 4 of Algorithm 3 always satisfies −(n − 1) ≤ S ≤ n − 1.\\nProof. The smallest value of S is obtained when eachMi = ⌊Zpi ⌋, in which case\\n0 ≤\\nn∑\\ni=1\\n(Zpi − ⌊Zpi ⌋) B\\nn∑\\ni=1\\nχ (Zpi ) ≤ n − 1, (44)\\nwhere the first inequality follows from ⌊x⌋ ≤ x and the final inequality from the fact that 0 ≤\\nχ (x) < 1 so that the integer∑ni=1 χ (Zpi ) < n. Therefore, −S ≤ (n − 1) =⇒ −(n − 1) ≤ S . Similarly,\\nthe largest value of S is obtained when eachMi = ⌊Zpi ⌋ + 1, so that\\nn∑\\ni=1\\n(⌊Zpi ⌋ + 1 − Zpi ) =\\nn∑\\ni=1\\n(1 − χ (Zpi )) = n −\\nn∑\\ni=1\\nχ (Zpi ) ≤ n − 1. (45)\\nTherefore, S ≤ n − 1, where the final inequality uses the fact that χ (Zpi ) , 0 for some i (otherwise,\\nMi = ⌊Zpi ⌋ would be the optimum for each i). □\\nTheorems 4.10–4.16 together imply Theorem 4.7. Furthermore, using the implementation given\\nin Remark 4.15, the overall runtime of Algorithm 3 is order n logn.\\nReturning to Problem 2.15, from Theorems 3.4 and Theorem 4.7, the approximation error can\\nbe minimized over the set of output distributions of all entropy-optimal k-bit samplers as follows:\\n(i) for each j = 0, . . . ,k , letMj be the optimal Zk j -type distribution for approximating p returned by\\nAlgorithm 3; (ii) let l = argmin0≤j≤k ∆д(p,Mj ); (iii) set pˆi B Ml i/Zkl (i = 1, . . . ,n). The optimal\\nprobabilities for any sampler that halts after consuming at most k bits (as in Proposition 2.16)\\nare given by pˆi B Mki/Zkk . The next theorem establishes that, when the f -divergence is total\\nvariation, the approximation error decreases proportionally to 1/Z (the proof is in Appendix B).\\nTheorem 4.17. If ∆д is the total variation divergence, then any optimal solution M returned by\\nAlgorithm 3 satisfies ∆д(p,M) ≤ n/2Z .\\n5 CONSTRUCTING ENTROPY-OPTIMAL SAMPLERS\\nNow that we have described how to find a closest-approximation distribution for Problem 4.6 using\\nAlgorithm 3, we next describe how to efficiently construct an entropy-optimal sampler.\\nSuppose momentarily that we use the rejection method (Algorithm 1) described in Section 1.1,\\nwhich can sample exactly from any Z -type distribution, which includes all distributions returned\\nby Algorithm 3. Since any closest-approximation distribution that is the output distribution of a\\nk-bit entropy-optimal sampler has denominator Z = Zkl ≤ 2k , rejection sampling needs exactly k\\nbits of precision. The expected number of trials is 2k/Z and k random bits are used per trial, so that\\nk2k/Z ≤ 2k bits per sample are consumed on average. The following example illustrates that the\\nentropy consumption of the rejection method can be exponentially larger than the entropy of p.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\nOptimal Approximate Sampling from Discrete Probability Distributions 36:21\\nExample 5.1. Let p = (a1/2k , . . . ,an/2k ) with n = k . An entropy-optimal sampler uses at most\\nlogn bits per sample (Theorem 2.9), whereas rejection (Algorithm 1) uses n bits per sample.\\nWe thus turn our attention toward constructing an entropy-optimal sampler that realizes the\\nentropy-optimality guarantees from Theorem 2.9. For the data structures in this section we use a\\nzero-based indexing system. For positive integers l and k , letM B (M0, . . . ,Mn−1) be the return\\nvalue of Algorithm 3 given denominator Zkl . Without loss of generality, we assume that (i) k , l ,\\nandMi have been reduced so that some probabilityMi/Zkl is in lowest terms; and (ii) for each j\\nwe haveMj < Zkl (ifMj = Zkl for some j, then the sampler is degenerate: it always returns j).\\nAlgorithm 4 shows the first stage of the construction, which returns the binary probability matrix\\nP ofM. The ith row contains the first k bits in the concise binary expansion ofMi/Zkl , where first\\nl columns encode the finite prefix and the final k − l columns encode the infinitely repeating suffix.\\nAlgorithm 5 shows the second stage, which converts P from Algorithm 4 into an entropy-optimal\\nDDG tree T . From Theorem 2.9, T has a node labeled r at level c + 1 if and only if P[r , c] = 1 (recall\\nthe root is at level 0, so column c of P corresponds to level c+1 ofT ). TheMakeLeafTable function\\nreturns a hash table L that maps the level-order integer index i of any leaf node in a complete binary\\ntree to its label L[i] ∈ {1, . . . ,n} (the index of the root is zero). The labeling ensures that leaf nodes\\nare filled right-to-left and are labeled in increasing order. Next, we define a node data structure\\nwith fields left, right, and label, indicating the left child, right child, and outcome label (for leaf\\nnodes). The MakeTree function builds the tree T from L, returning the root node. The function\\nstores the list A of ancestors at level l in right-to-left order, and constructs back-edges from any\\nnon-leaf node at level k − 1 to the next available ancestor at level l to encode the recurring subtree.\\nAlgorithm 6 shows the third stage, which converts the root node of the entropy-optimal DDG\\ntree T returned from Algorithm 5 into a sampling-efficient encoding enc. The PackTree function\\nfills the array enc such that for an internal node i , enc[i] and enc[i + 1] store the indexes of enc for\\nthe left and right child (respectively) if i is a branch; and for an leaf node i , enc[i] stores the label\\n(as a negative integer). The field node.loc tracks back-edges, pointing to the ancestor instead of\\nmaking a recursive call whenever a node has been visited by a previous recursive call.\\nNow that preprocessing is complete, Algorithm 7 shows the main sampler, which uses the enc\\ndata structure from Algorithm 6 and the flip() primitive to traverse the DDG tree starting from\\nthe root (at enc[0]). Since PackTree uses negative integers to encode the labels of leaf nodes, the\\nSampleEncoding function returns the outcome −enc[c] whenever enc[c] < 0. Otherwise, the\\nsampler goes to the left child (at enc[c]) if flip() returns 0 or the right child (at enc[c + 1]) if flip()\\nreturns 1. The resulting sampler is very efficient and only stores the linear array enc in memory,\\nwhose size is order nk . (The DDG tree of an entropy-optimal k-bit sampler is a complete depth-k\\nbinary tree with at most n nodes per level, so there are at most nk leaf nodes and nk branch nodes.)\\nFor completeness, we also present Algorithm 8, which implements an entropy-optimal sampler\\nby operating directly on the n × k matrix P returned from Algorithm 4. This algorithm is based\\non a recursive extension of the Knuth and Yao sampler given in Roy et al. [2013], where we allow\\nfor an infinitely repeating suffix by resetting the column counter c to l whenever c = k − 1 is at\\nthe last columns. (The algorithm in Roy et al. [2013] is designed for hardware and samples from a\\nprobability matrix with a finite number of columns and no repeating suffixes. Unlike the focus of this\\npaper, Roy et al. [2013] does not deliver closest-approximation distributions for limited-precision\\nsampling.) Algorithm 7 is significantly more efficient as its runtime is dictated by the entropy\\n(upper bounded by logn) whereas the runtime of Algorithm 8 is upper bounded by n logn due to\\nthe order n inner loop. Figure 4 in Section 6.2.3 shows that, when implemented in software, the\\nincrease in algorithmic efficiency from using a dense encoding can deliver wall-clock gains of up\\nto 5000x on a representative sampling problem.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\n36:22 Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka\\nAlgorithm 4 Building the probability matrix for a Zkl -type probability distribution.\\nInput: Integers k, l with 0 ≤ l ≤ k ; integers (M0, . . .Mn−1) with sum Zkl B 2k − 2l1l<k .\\nOutput: n × k probability matrix P of distribution (M0/Zkl , . . . ,Mn−1/Zkl ).\\n1. Repeat for each i = 0, . . . ,n − 1:\\n1.1. If l = k , then let xi B Mi and yi B 0;\\nElse if l = 0, then let xi B 0 and yi B Mi ;\\nElse if 0 < l < k , then let xi B\\n⌊\\nMi/(2k−l − 1)\\n⌋\\n,yi B Mi − (2k−l − 1)xi .\\n1.2. Let ai be the length-l binary string encoding xi ,\\n1.3. Let si be the length k − l binary string encoding yi .\\n1.4. Let bi B ai ⊕ si be their concatenation.\\n2. Return the n × k matrix P B\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nb01 b12 . . . b0,k−1\\n...\\n...\\n...\\n...\\nbn−1,1 bn−1,2 . . . bn−1,k−1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb .\\nAlgorithm 5 Building an entropy-optimal DDG tree from a probability matrix.\\nInput: n × k matrix P representing n k-bit binary expansions with length-l suffix.\\nOutput: root node of discrete distribution generating tree for P, from Theorem 2.9.\\n1. Define the following functions:\\nfunctionMakeLeafTable(P) ▷ returns map of node indices to outcomes\\nL ← ∅; i ← 2 ▷ initialize dictionary and root index\\nfor c = 0, . . . ,k − 1 do ▷ for each level c + 1 in the tree\\nfor r = 0, . . . ,n − 1 do ▷ for each outcome r\\nif P[r , c] = 1 then ▷ if the outcome is a leaf\\nL[i] ← r + 1 ▷ mark i with the outcome\\ni ← i − 1 ▷ move i one node left\\ni ← 2i + 2 ▷ index of next first leaf node\\nreturn L\\nfunctionMakeTree(i,k, l ,A,L) ▷ returns DDG tree with labels L\\nnode ← Node() ▷ initialize node for current index\\nif i ∈ L then ▷ if node is a leaf\\nnode.label ← L[i] ▷ label it with outcome\\nelse ▷ if node is a branch\\nlevel ← ⌊log2(i + 1)⌋ ▷ compute level of current node\\nif level = l then A.Append(node) ▷ add node to list of ancestors\\nnode.right ← A.Pop(0) if [level = k − 1 and (2i + 2) < L] ▷ make right child\\nelse MakeTree(2i + 2,k, l ,A,L)\\nnode.left ← A.Pop(0) if [level = k − 1 and (2i + 1) < L] ▷ make left child\\nelse MakeTree(2i + 1,k, l ,A,L)\\nreturn node\\n2. Let L ← MakeLeafTable(P).\\n3. Let root ← MakeTree(0,k, l , [ ],L).\\n4. Return root.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\nOptimal Approximate Sampling from Discrete Probability Distributions 36:23\\nAlgorithm 6 Building a sampling-efficient linear encoding from a DDG tree.\\nInput: root node of a discrete distribution generating tree.\\nOutput: Dense linear array enc that encodes the branch and leaf nodes of the tree.\\n1. Define the following function:\\nfunction PackTree(enc, node, offset) ▷ returns sampling-efficient data structure\\nnode.loc ← offset ▷ mark node at this location\\nif node.label , Nil then ▷ node is a leaf\\nenc[offset] ← −node.label ▷ label it with outcome\\nreturn offset + 1 ▷ return the next offset\\nif node.left.loc , Nil then ▷ left child has been visited\\nenc[offset] ← node.left.loc ▷ mark location of left child\\nw ← offset + 2 ▷ setw two cells to the right\\nelse\\nenc[offset] ← offset + 2 ▷ point to left child\\nw ← PackTree[enc, node.left, offset + 2] ▷ recursively build left subtree\\nif node.right.loc , Nil then ▷ right child has been visited\\nenc[offset + 1] ← node.right.loc ▷ mark location of right child\\nelse\\nenc[offset + 1] ← w ▷ point to right child\\nw ← PackTree(enc, node.right,w) ▷ recursively build right subtree\\nreturnw ▷ return next empty cell\\n2. Create array enc[] and call PackTree(enc, root, 0).\\n3. Return enc.\\nAlgorithm 7 Sampling a DDG tree given the\\nlinear encoding from Algorithm 6.\\nfunction SampleEncoding(enc)\\nLet c ← 0\\nwhile True do\\nb ← flip\\nc ← enc[c + b]\\nif enc[c] < 0 then\\nreturn −enc[c]\\nAlgorithm 8 Sampling a DDG tree given the\\nprobability matrix from Algorithm 4.\\nfunction SampleMatrix(P,k, l )\\nd ← 0\\nc ← 0\\nwhile True do\\nb ← flip\\nd ← 2d + (1 − b)\\nfor r = 0, . . . ,n − 1 do\\nd ← d − P[r ][c]\\nif d = −1 then\\nreturn r + 1\\nif c = k − 1 then\\nc ← l\\nelse\\nc ← c + 1\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\n36:24 Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka\\n6 EXPERIMENTAL RESULTS\\nWe next evaluate the optimal limited-precision sampling algorithms presented in this paper. Sec-\\ntion 6.1 investigates how the error and entropy consumption of the optimal samplers vary with\\nthe parameters of common families of discrete probability distributions. Section 6.2 compares the\\noptimal samplers with two limited-precision baselines samplers, showing that our algorithms are\\nup to 1000x-10000x more accurate, consume up to 10x fewer random bits per sample, and are\\n10x–100x faster in terms of wall-clock time. Section 6.3 compares our optimal samplers to exact\\nsamplers on a representative binomial distribution, showing that exact samplers can require high\\nprecision or consume excessive entropy, whereas our optimal approximate samplers can use less\\nprecision and/or entropy at the expense of a small sampling error. Appendix A contains a study\\nof how the closest-approximation error varies with the precision specification and entropy of the\\ntarget distribution, as measured by three different f -divergences. The online artifact contains the\\nexperiment code. All C algorithms used for measuring performance were compiled with gcc level\\n3 optimizations, using Ubuntu 16.04 on AMD Opteron 6376 1.4GHz processors.\\n6.1 Characterizing Error and Entropy for Families of Discrete Distributions\\nWe study how the approximation error and entropy consumption of our optimal approximate sam-\\nplers vary with the parameter values of four families of probability distributions: (i) Binomial(n,p):\\nthe number of heads in n independent tosses of a biased p-coin; (ii) Beta Binomial(n,α , β): the\\nnumber of heads in n independent tosses of a biased p-coin, where p is itself randomly drawn\\nfrom a Beta(α , β) distribution; (iii) Discrete Gaussian(n,σ ): a discrete Gaussian over the integers\\n{−n, . . . ,n} with variance σ 2; and (iv) Hypergeometric(n,m,d): the number of red balls obtained\\nafter d draws (without replacement) from a bin that hasm red balls and n −m blue balls.\\nFigure 2 shows how the closest-approximation error (top row) and entropy consumption (bottom\\nrow) vary with two of the parameters of each family (x and y-axes) when using k = 32 bits of\\nprecision. Since Beta Binomial and Hypergeometric have three parameters, we fix n = 80 and vary\\nthe remaining two parameters. Closest-approximation distributions are obtained from Algorithm 3,\\nusing Z = 232 and the Hellinger divergence (which is most sensitive at medium entropies). The\\nplots show that, even with the same family, the closest-approximation error is highly dependent on\\nthe target distribution and the interaction between parameter values. For example, in Figure 2a\\n0.0 0.1 0.2 0.3 0.4 0.5\\nCoin Weight\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\nN\\num\\nb\\ner\\nof\\nT\\nri\\nal\\ns\\nTheoretically Optimal Error\\n10−20\\n10−18\\n10−16\\n10−14\\n10−12\\n10−10\\n0.0 0.1 0.2 0.3 0.4 0.5\\nCoin Weight\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\nN\\num\\nb\\ner\\nof\\nT\\nri\\nal\\ns\\nNumber of Bits per Sample\\n3\\n4\\n5\\n6\\n(a) Binomial\\n1 2 3 4 5 6 7 8 9 10\\nBeta\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nA\\nlp\\nha\\nTheoretically Optimal Error\\n10−16\\n10−14\\n10−12\\n10−10\\n1 2 3 4 5 6 7 8 9 10\\nBeta\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nA\\nlp\\nha\\nNumber of Bits Per Sample\\n3\\n4\\n5\\n6\\n7\\n8\\n(b) Beta Binomial (n = 80)\\n10 20 30 40 50\\nVariance\\n10\\n20\\n30\\n40\\n50\\nN\\num\\nb\\ner\\nof\\nO\\nut\\nco\\nm\\nes\\nTheoretically Optimal Error\\n10−20\\n10−18\\n10−16\\n10−14\\n10−12\\n10−10\\n10 20 30 40 50\\nVariance\\n10\\n20\\n30\\n40\\n50\\nN\\num\\nb\\ner\\nof\\nO\\nut\\nco\\nm\\nes\\nNumber of Bits per Sample\\n4.0\\n4.5\\n5.0\\n5.5\\n6.0\\n6.5\\n(c) Discrete Gaussian\\n10 20 30 40 50 60 70\\nNumber of Draws\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\nN\\num\\nb\\ner\\nof\\nR\\ned\\nB\\nal\\nls\\nTheoretically Optimal Error\\n10−32\\n10−28\\n10−24\\n10−20\\n10−16\\n10−12\\n10 20 30 40 50 60 70\\nNumber of Draws\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\nN\\num\\nb\\ner\\nof\\nR\\ned\\nB\\nal\\nls\\nNumber of Bits Per Sample\\n2.5\\n3.0\\n3.5\\n4.0\\n4.5\\n5.0\\n(d) Hypergeometric (n = 80)\\nFig. 2. Characterization of the theoretically optimal approximation error (top row) and average number of bits\\nper sample (bottom row) for four common families of probability distributions using k = 32 bits of precision.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\nOptimal Approximate Sampling from Discrete Probability Distributions 36:25\\n(top panel), the black spikes at coin weight 0.25 and 0.50 correspond to pairs (n,p) where the\\nbinomial distribution can be sampled exactly. Moreover, for a fixed coin weight (x-axis), the error\\nincreases as the number of trials (y-axis) increases. The rate at which the error increases with\\nthe number of trials is inversely proportional to the coin weight, which is mirrored by the fact\\nthat the average number of bits per sample (bottom panel) varies over a wider range and at a\\nfaster rate at low coin weights than at high coin weights. In Figure 2c, for a fixed level of variance\\n(x-axis), the error increases until the number of outcomes (y-axis) exceeds the variance, after\\nwhich the tail probabilities become negligible. In Figure 2d when the number of red ballsm and\\nnumber of draws d are equal to roughly half of the population size n, the bits per sample and\\napproximation error are highest (grey in center of both panels). This relationship stands in contrast\\nto Figure 2b, where approximation error is lowest (black/purple in lower left of top panel) when\\nbits per sample is highest (grey in lower left of bottom panel). The methods presented in this paper\\nenable rigorous and systematic assessments of the effects of bit precision on theoretically-optimal\\nentropy consumption and sampling error, as opposed to empirical, simulation-based assessments\\nof entropy and error which can be very noisy in practice (e.g., Jonas [2014, Figure 3.15]).\\n6.2 Comparing Error, Entropy, and Runtime to Baseline Limited-Precision Algorithms\\nWe next show that the proposed sampling algorithm is more accurate, more entropy-efficient, and\\nfaster than existing limited-precision sampling algorithms. We briefly review two baselines below.\\nInversion sampling. Recall from Section 1.1 that inversion sampling is a universal method based\\non the key property in Eq. (1). In the k-bit limited-precision setting, a floating-point number U ′\\n(with denominator 2k ) is used to approximate a real uniform variate U . The GNU C++ standard\\nlibrary [Lea 1992] v5.4.0 implements inversion sampling as in Algorithm 2 (using ≤ instead of <).3\\nAsW ∼ Uniform({0, 1/2k , . . . , (2k − 1)/2k }), it can be shown that the limited-precision inversion\\nsampler has the following output probabilities pˆi , where p˜j B\\n∑j\\ns=1 ps (j = 1, . . . ,n) and 2 ≤ i ≤ n:\\npˆ1 ∝ ⌊2kp˜1⌋ + 1p˜1,1; pˆi ∝\\n{\\nmax(0, ⌈2kp˜i ⌉ − ⌊2kp˜i−1⌋) (if 2kp˜i = ⌊2kp˜i ⌋ and p˜i , 1)\\nmax(0, ⌈2kp˜i ⌉ − ⌊2kp˜i−1⌋ − 1) (otherwise)\\n. (46)\\nInterval algorithm [Han and Hoshi 1997]. This method implements inversion sampling by recur-\\nsively partitioning the unit interval [0, 1] and using the cumulative distribution of p to lazily find\\nthe bin in which a uniform random variable falls. We refer to Uyematsu and Li [2003, Algorithm 1]\\nfor a limited-precision implementation of the interval algorithm using k-bit integer arithmetic.\\n6.2.1 Error Comparison. Both the inversion and interval samplers use at most k bits of precision,\\nwhich, from Proposition 2.16, means that these algorithms are less accurate than the optimal\\napproximate samplers from Algorithm 3 (using Z = 2k ) and less entropy-efficient than the sampler\\nin Algorithm 7. To compare the errors, 500 distributions are obtained by sweeping through a\\ngrid of values that parameterize the shape and dimension for each of six families of probability\\ndistributions. For each target distribution, probabilities from the inversion method (from Eq. (46)),\\nthe interval method (computed by enumeration), and the optimal approximation (from Algorithm 3)\\nare obtained using k = 16 bits of precision. In Figure 3, the x-axis shows the approximation error\\n(using the Hellinger divergence) of each method relative to the theoretically-optimal error achieved\\nby our samplers. The y-axis shows the fraction of the 500 distributions whose relative error is less\\nthan or equal to the value on the x-axis. The results show that, for this benchmark set, the output\\ndistributions of inversion and interval samplers are up to three orders of magnitude less accurate\\nrelative to the output distribution of the optimal k-bit approximation delivered by our algorithm.\\n3Steps 1 and 2 are implemented in generate_canonical and Step 3 is implemented in discrete_distribution::operator() using a\\nlinear scan; see /gcc-5.4.0/libstdc++v3/include/bits/random.tcc in https://ftp.gnu.org/gnu/gcc/gcc-5.4.0/gcc-5.4.0.tar.gz.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\n36:26 Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka\\nTable 2. Comparison of the average number of input bits per sample used by inversion sampling, interval\\nsampling, and the proposed method, in each of the six parameterized families using k = 16 bits of precision.\\nDistribution Average Number of Bits per Sample\\nInversion Sampler (Alg. 2) Interval Sampler [Uyematsu and Li 2003] Optimal Sampler (Alg. 7)\\nBenford 16 6.34 5.71\\nBeta Binomial 16 4.71 4.16\\nBinomial 16 5.05 4.31\\nBoltzmann 16 1.51 1.03\\nDiscrete Gaussian 16 6.00 5.14\\nHypergeometric 16 4.04 3.39\\n100 101 102\\nRelative Approximation Error\\n0\\n20\\n40\\n60\\n80\\n100\\n%\\nof\\nD\\nis\\ntr\\nib\\nut\\nio\\nns\\nBenford\\nInversion Sampler\\nInterval Sampler\\nOptimal Sampler\\n100 101 102 103 104\\nRelative Approximation Error\\n0\\n20\\n40\\n60\\n80\\n100\\n%\\nof\\nD\\nis\\ntr\\nib\\nut\\nio\\nns\\nBeta Binomial\\n100 101 102 103\\nRelative Approximation Error\\n0\\n20\\n40\\n60\\n80\\n100\\n%\\nof\\nD\\nis\\ntr\\nib\\nut\\nio\\nns\\nBinomial\\n100 101 102 103\\nRelative Approximation Error\\n0\\n20\\n40\\n60\\n80\\n100\\n%\\nof\\nD\\nis\\ntr\\nib\\nut\\nio\\nns\\nBoltzmann\\n100 101 102\\nRelative Approximation Error\\n0\\n20\\n40\\n60\\n80\\n100\\n%\\nof\\nD\\nis\\ntr\\nib\\nut\\nio\\nns\\nDiscrete Gaussian\\n100 101 102\\nRelative Approximation Error\\n0\\n20\\n40\\n60\\n80\\n100\\n%\\nof\\nD\\nis\\ntr\\nib\\nut\\nio\\nns\\nHypergeometric\\nFig. 3. Comparison of the approximation error of limited-precision implementations of interval sampling\\n(green) and inversion sampling (blue) relative to error obtained by the optimal sampler (red), for six families\\nof probability distributions using k = 16 of bits precision. The x-axis shows the approximation error of each\\nsampler relative to the optimal error. The y-axis shows the fraction of 500 distributions from each family\\nwhose relative error is less than or equal to the corresponding value on the x-axis.\\n6.2.2 Entropy Comparison. Next, we compare the efficiency of each sampler measured in terms\\nof the average number of random bits drawn from the source to produce a sample, shown in\\nTable 2. Since these algorithms are guaranteed to halt after consuming at most k random bits, the\\naverage number of bits per sample is computed by enumerating over all 2k possible k-bit strings\\n(using k = 16 gives 65536 possible input sequences from the random source) and recording, for\\neach sequence of input bits, the number of consumed bits until the sampler halts. The inversion\\nalgorithm consumes all k available bits of entropy, unlike the interval and optimal samplers, which\\nlazily draw bits from the random source until an outcome can be determined. For all distributional\\nfamilies, the optimal sampler uses fewer bits per sample than are used by interval sampling.\\n6.2.3 Runtime Comparison. We next assess the runtime performance of our sampling algorithms as\\nthe dimension and entropy of the target distribution increases. For each n ∈ {10, 100, 1000, 10000},\\nwe generate 1000 distributions with entropies ranging from 0, . . . , log(n). For each distribution,\\nwe measure the time taken to generate a sample based on 100000 simulations according to four\\nmethods: the optimal sampler using SampleEncoding (Algorithm 7); the optimal sampler using\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\nOptimal Approximate Sampling from Discrete Probability Distributions 36:27\\n0 1 2 3\\nEntropy of Target Distribution\\n10−8\\n10−7\\n10−6\\n10−5\\n10−4\\n10−3\\nS\\nec\\non\\nds\\np\\ner\\nS\\nam\\npl\\ne\\n10 Dimensional Distributions\\nOptimal (Matrix, Alg. 8)\\nInversion (Linear)\\nInversion (Binary)\\nOptimal (Encoding, Alg. 7)\\n0 2 4 6\\nEntropy of Target Distribution\\n100 Dimensional Distributions\\n0.0 2.5 5.0 7.5 10.0\\nEntropy of Target Distribution\\n1000 Dimensional Distributions\\n0 5 10\\nEntropy of Target Distribution\\n10000 Dimensional Distributions\\nFig. 4. Comparison of wall-clock time per sample and order of growth of two implementations of the optimal\\nsamplers (using Algorithms 7 and 8) with inversion sampling (using linear and binary search in Algorithm 2).\\nTable 3. Comparison of runtime and number of calls to the random number generator using limited-precision\\nentropy-optimal and inversion sampling to generate 100 million samples from 100 dimensional distributions\\nMethod Entropy of Target Distribution Number of PRNG Calls PRNG Wall-Clock Time (ms)\\nOptimal Approximate\\nSampler (Alg. 7)\\n0.5 7,637,155 120\\n2.5 11,373,471 160\\n4.5 18,879,900 260\\n6.5 24,741,348 350\\nInversion Sampler (Alg. 2) (all) 100,000,000 1410\\nSampleMatrix (Algorithm 8); the inversion sampler using a linear scan (Algorithm 2, as in the\\nGNU C++ standard library); and the inversion sampler using binary search (fast C implementation).\\nFigure 4 shows the results, where the x-axis is the entropy of the target distribution and the y-axis\\nis seconds per sample (log scale). In general, the difference between the samplers increases with the\\ndimension n of the target distribution. For n = 10, the SampleEncoding sampler executes a median\\nof over 1.5x faster than any other sampler. For n = 10000, SampleEncoding executes a median of\\nover 3.4x faster than inversion sampling with binary search and over 195x faster than the linear\\ninversion sampler implemented in the C++ library. In comparison with SampleMatrix [Roy et al.\\n2013], SampleEncoding is faster by a median of 2.3x (n = 10) to over 5000x (n = 10000).\\nThe worst runtime scaling is given by SampleMatrix which, although entropy-optimal, grows\\norder nH (p) due to the inner loop through the rows of the probability matrix. In contrast, Sam-\\npleEncoding uses the dense linear array described in Section 5 and is asymptotically more efficient:\\nits runtime depends only on the entropy H (p) ≤ logn. As for the inversion methods, there is a\\nsignificant gap between the runtime of SampleEncoding (orange) and the binary inversion sampler\\n(red) at low values of entropy, which is especially visible at n = 1000 and n = 10000. The binary\\ninversion sampler scales order logn independently of the entropy, and is thus less performant than\\nSampleEncoding when H (p) ≪ logn (the gap narrows as H (p) approaches logn).\\nTable 3 shows the wall-clock improvements from using Algorithm 7. Floating-point sampling\\nalgorithms implemented in standard software libraries typically make one call to the pseudorandom\\nnumber generator per sample, consuming a full 32-bit or 64-bit pseudorandom word, which in\\ngeneral is highly wasteful. (As a conceptual example, sampling Bernoulli(1/2) requires sampling\\nonly one random bit, but comparing an approximately-uniform floating-point numberU ′ < 0.5 as\\nin inversion sampling uses e.g., 64 bits.) In contrast, the optimal approximate sampler (Algorithm 7)\\nis designed to lazily consume random bits (following Lumbroso [2013], our implementation of flip\\nstores a buffer of pseudorandom bits equal to the word size of the machine) which results in fewer\\nfunction calls to the underlying pseudorandom number generator and 4x–12x less wall-clock time.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\n36:28 Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka\\nTable 4. Precision, entropy consumption, and sampling error of Knuth and Yao sampling, rejection sampling,\\nand optimal approximate sampling, at various levels of precision for the Binomial(50, 61/500) distribution.\\nMethod Precision k(l ) Bits per Sample Error (L1)\\nExact Knuth and Yao Sampler (Thm. 2.9) 5.6 × 10104(100) 5.24 0.0\\nExact Rejection Sampler (Alg. 1) 449(448) 735 0.0\\nOptimal Approximate\\nSampler (Alg. 3+7)\\n4(4) 5.03 2.03 × 10−1\\n8(4) 5.22 1.59 × 10−2\\n16(0) 5.24 6.33 × 10−5\\n32(12) 5.24 1.21 × 10−9\\n64(29) 5.24 6.47 × 10−19\\n6.3 Comparing Precision, Entropy, and Error to Exact Sampling Algorithms\\nRecall that two algorithms for sampling from Z -type distributions (Definition 4.3) are: (i) exact\\nKnuth and Yao sampling (Theorem 2.9), which samples from any Z -type distribution using at\\nmost H (p) + 2 bits per sample and precision k described in Theorem 3.4; and (ii) rejection sam-\\npling (Algorithm 1), which samples from any Z -type distribution using k bits of precision (where\\n2k−1 < Z ≤ 2k ) using k2k/Z bits per sample. Consider the Binomial(50, 61/500) distribution p,\\nwhich is the number of heads in 50 tosses of a biased coin whose probability of heads is 61/500.\\nThe probabilities are pi B\\n(50\\ni\\n) (61/500)i (39/500)n−i (i = 0, . . . ,n) and p is a Z -type distribution\\nwith Z = 8.881 784 197 001 252 323 389 053 344 726 562 5 × 10134. Table 4 shows a comparison of the\\ntwo exact samplers to our optimal approximate samplers. The first column shows the precision k(l ),\\nwhich indicates k bits are used and l (where 0 ≤ l ≤ k) is the length of the repeating suffix in the\\nnumber system Bkl (Section 3). Recall that exact samplers use finite but arbitrarily high precision.\\nThe second and third columns show bits per sample and sampling error, respectively.\\nExact Knuth and Yao sampler. This method requires a tremendous amount of precision to\\ngenerate an exact sample (following Theorem 3.5), as dictated by the large value of Z for the\\nBinomial(50, 61/500) distribution. The required precision far exceeds the amount of memory avail-\\nable on modern machines. Although at most 5.24 bits per sample are needed on average (two more\\nthan the 3.24 bits of entropy in the target distribution), the DDG tree has more than 10104 levels.\\nAssuming that each level is a byte, storing the sampler would require around 1091 terabytes.\\nExact rejection sampler. This method requires 449 bits of precision (roughly 56 bytes), which\\nis the number of bits needed to encode common denominator Z . This substantial reduction in\\nprecision as compared to the Knuth and Yao sampler comes at the cost of higher number of bits per\\nsample, which is roughly 150x higher than the information-theoretically optimal rate. The higher\\nnumber of expected bits per sample leads to wasted computation and higher runtime in practice\\ndue to excessive calls to the random number generator (as illustrated in Table 3).\\nOptimal approximate sampler. For precision levels ranging from k = 4 to 64, the selected value of\\nl delivers the smallest approximation error across executions of Algorithm 3 on inputs Zkk , . . . ,Zk0.\\nAt each precision, the number of bits per sample has an upper bound that is very close to the\\nupper bound of the optimal rate, since the entropies of the closest-approximation distributions are\\nvery close to the entropy of the target distribution, even at low precision. Under the L1 metric, the\\napproximation error decreases exponentially quickly with the increase in precision (Theorem 4.17).\\nThese results illustrate that exact Knuth and Yao sampling can be infeasible in practice, whereas\\nrejection sampling requires less precision (though higher than what is typically available on low\\nprecision sampling devices [Mansinghka and Jonas 2014]) but is wasteful in terms of bits per sample.\\nThe optimal approximate samplers are practical to implement and use significantly less precision\\nor bits per sample than exact samplers, at the expense of a small approximation error that can be\\ncontrolled based on the accuracy and entropy constraints of the application at hand.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\nOptimal Approximate Sampling from Discrete Probability Distributions 36:29\\n7 CONCLUSION\\nThis paper has presented a new class of algorithms for optimal approximate sampling from discrete\\nprobability distributions. The samplers minimize both statistical error and entropy consumption\\namong the class of all entropy-optimal samplers and bounded-entropy samplers that operate within\\nthe given precision constraints. Our samplers lead to improvements in accuracy, entropy-efficiency,\\nand wall-clock runtime as compared to existing limited-precision samplers, and can use significantly\\nfewer computational resources than are needed by exact samplers.\\nMany existing programming languages and systems include libraries and constructs for random\\nsampling [Lea 1992; MathWorks 1993; R Core Team 2014; Galassi et al. 2019]. In addition to the\\nareas of scientific computing mentioned in Section 1, relatively new and prominent directions in\\nthe field of computing that leverage random sampling include probabilistic programming languages\\nand systems [Gordon et al. 2014; Saad and Mansinghka 2016; Staton et al. 2016; Cusumano-Towner\\net al. 2019]; probabilistic program synthesis [Nori et al. 2015; Saad et al. 2019]; and probabilistic\\nhardware [de Schryver et al. 2012; Dwarakanath and Galbraith 2014; Mansinghka and Jonas 2014].\\nIn all these settings, the efficiency and accuracy of random sampling procedures play a key role in\\nmany implementation techniques. As uncertainty continues to play an increasingly prominent role\\nin a range of computations and as programming languages move towards more support for random\\nsampling as one way of dealing with this uncertainty, trade-offs between entropy consumption,\\nsampling accuracy, numerical precision, and wall-clock runtime will form an important set of\\ndesign considerations for sampling procedures. Due to their theoretical optimality properties,\\nease-of-implementation, and applicability to a broad set of statistical error measures, the algorithms\\nin this paper are a step toward a systematic and practical approach for navigating these trade-offs.\\nACKNOWLEDGMENTS\\nThis research was supported by a philanthropic gift from the Aphorism Foundation.\\nREFERENCES\\nJulia Abrahams. 1996. Generation of Discrete Distributions from Biased Coins. IEEE Trans. Inf. Theory 42, 5 (Sept. 1996),\\n1541–1546.\\nS. M. Ali and S. D. Silvey. 1966. A General Class of Coefficients of Divergence of One Distribution from Another. J. R. Stat.\\nSoc. B. 28, 1 (Jan. 1966), 131–142.\\nZiv Bar-Yossef, Thathachar S. Jayram, Ravi Kumar, and D. Sivakumar. 2004. An Information Statistics Approach to Data\\nStream and Communication Complexity. J. Comput. Syst. Sci. 68, 4 (June 2004), 702–732.\\nKurt Binder (Ed.). 1986. Monte Carlo Methods in Statistical Physics (2 ed.). Topics in Current Physics, Vol. 7. Springer-Verlag,\\nBerlin.\\nAntonio Blanca and Milena Mihail. 2012. Efficient Generation ϵ -close to G(n, p) and Generalizations. (April 2012).\\narXiv:1204.5834\\nLenore Blum, Felipe Cucker, Michael Shub, and Steve Smale. 1998. Complexity and Real Computation. Springer-Verlag, New\\nYork.\\nManuel Blum. 1986. Independent Unbiased Coin Flips from a Correlated Biased Source: A Finite State Markov Chain.\\nCombinatorica 6, 2 (June 1986), 97–108.\\nKarl Bringmann and Tobias Friedrich. 2013. Exact and Efficient Generation of Geometric Random Variates and Random\\nGraphs. In ICALP 2013: Proceedings of the 40th International Colloquium on Automata, Languages and Programming (Riga,\\nLatvia). Lecture Notes in Computer Science, Vol. 7965. Springer, Heidelberg, 267–278.\\nKarl Bringmann and Konstantinos Panagiotou. 2017. Efficient Sampling Methods for Discrete Distributions. Algorithmica\\n79, 2 (Oct. 2017), 484–508.\\nFerdinando Cicalese, Luisa Gargano, and Ugo Vaccaro. 2006. A Note on Approximation of Uniform Distributions from\\nVariable-to-Fixed Length Codes. IEEE Trans. Inf. Theory 52, 8 (Aug. 2006), 3772–3777.\\nThomas M. Cover and Joy A. Thomas. 2006. Elements of Information Theory (2 ed.). John Wiley & Sons, Inc., Hoboken.\\nMarco F. Cusumano-Towner, Feras A. Saad, Alexander K. Lew, and Vikash K. Mansinghka. 2019. Gen: A General-purpose\\nProbabilistic Programming System with Programmable Inference. In PLDI 2019: Proceedings of the 40th ACM SIGPLAN\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\n36:30 Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka\\nConference on Programming Language Design and Implementation (Phoenix, AZ, USA). ACM, New York, 221–236.\\nChristian de Schryver, Daniel Schmidt, Norbert Wehn, Elke Korn, Henning Marxen, Anton Kostiuk, and Ralf Korn. 2012. A\\nHardware Efficient Random Number Generator for Nonuniform Distributions with Arbitrary Precision. Int. J. Reconf.\\nComput. 2012, Article 675130 (2012), 11 pages.\\nLuc Devroye. 1982. A Note on Approximations in Random Variate Generation. J. Stat. Comput. Simul. 14, 2 (1982), 149–158.\\nLuc Devroye. 1986. Non-Uniform Random Variate Generation. Springer-Verlag, New York.\\nLuc Devroye and Claude Gravel. 2015. Sampling with Arbitrary Precision. (Feb. 2015). arXiv:1502.02539\\nInderjit S. Dhillon, Subramanyam Mallela, and Rahul Kumar. 2003. A Divisive Information-Theoretic Feature Clustering\\nAlgorithm for Text Classification. J. Mach. Learn. Res. 3 (March 2003), 1265–1287.\\nDragan Djuric. 2019. Billions of Random Numbers in a Blink of an Eye. Retrieved June 15, 2019 from https://dragan.rocks/\\narticles/19/Billion-random-numbers-blink-eye-Clojure\\nChaohui Du and Guoqiang Bai. 2015. Towards Efficient Discrete Gaussian Sampling For Lattice-Based Cryptography. In FPL\\n2015: Proceedings of the 25th International Conference on Field Programmable Logic and Applications (London, UK). IEEE\\nPress, Piscataway, 1–6.\\nNagarjun C. Dwarakanath and Steven D. Galbraith. 2014. Sampling from Discrete Gaussians for Lattice-Based Cryptography\\nOn a Constrained Device. Appl. Algebr. Eng. Comm. 25, 3 (June 2014), 159–180.\\nPeter Elias. 1972. The Efficient Construction of an Unbiased Random Sequence. Ann. Math. Stat. 43, 3 (June 1972), 865–870.\\nJános Folláth. 2014. Gaussian Sampling in Lattice Based Cryptography. Tatra Mount. Math. Pub. 60, 1 (Sept. 2014), 1–23.\\nMark Galassi, Jim Davies, James Theiler, Brian Gough, Gerard Jungman, Patrick Alken, Michael Booth, Fabrice Rossi, and\\nRhys Ulerich. 2019. GNU Scientific Library. Free Software Foundation.\\nPaul Glasserman. 2003. Monte Carlo Methods in Financial Engineering. Stochastic Modeling and Applied Probability, Vol. 53.\\nSpringer Science+Business Media, New York.\\nAndrew D. Gordon, Thomas A. Henzinger, Aditya V. Nori, and Sriram K. Rajamani. 2014. Probabilistic Programming. In\\nFOSE 2014: Proceedings of the on Future of Software Engineering (Hyderabad, India). ACM, New York, 167–181.\\nTe Sun Han and Mamoru Hoshi. 1997. Interval Algorithm for Random Number Generation. IEEE Trans. Inf. Theory 43, 2\\n(March 1997), 599–611.\\nTe Sun Han and Sergio Verdú. 1993. Approximation Theory of Output Statistics. IEEE Trans. Inf. Theory 39, 3 (May 1993),\\n752–772.\\nJohn Harling. 1958. Simulation Techniques in Operations Research—A Review. Oper. Res. 6, 3 (June 1958), 307–319.\\nEric Jonas. 2014. Stochastic Architectures for Probabilistic Computation. Ph.D. Dissertation. Massachusetts Institute of\\nTechnology.\\nDonald E. Knuth and Andrew C. Yao. 1976. The Complexity of Nonuniform Random Number Generation. In Algorithms\\nand Complexity: New Directions and Recent Results, Joseph F. Traub (Ed.). Academic Press, Inc., Orlando, FL, 357–428.\\nDexter Kozen. 2014. Optimal Coin Flipping. In Horizons of the Mind. A Tribute to Prakash Panangaden: Essays Dedicated to\\nPrakash Panangaden on the Occasion of His 60th Birthday. Lecture Notes in Computer Science, Vol. 8464. Springer, Cham,\\n407–426.\\nDexter Kozen and Matvey Soloviev. 2018. Coalgebraic Tools for Randomness-Conserving Protocols. In RAMiCS 2018:\\nProceedings of the 17th International Conference on Relational and Algebraic Methods in Computer Science (Groningen, The\\nNetherlands). Lecture Notes in Computer Science, Vol. 11194. Springer, Cham, 298–313.\\nS. Kullback and R. A. Leibler. 1951. On Information and Sufficiency. Ann. Math. Stat. 22, 1 (March 1951), 79–86.\\nAnthony J. C. Ladd. 2009. A Fast Random Number Generator for Stochastic Simulations. Comput. Phys. Commun. 180, 11\\n(2009), 2140–2142.\\nDopug Lea. 1992. User’s Guide to the GNU C++ Library. Free Software Foundation, Inc.\\nJosef Leydold and Sougata Chaudhuri. 2014. rvgtest: Tools for Analyzing Non-Uniform Pseudo-Random Variate Generators.\\nhttps://CRAN.R-project.org/package=rvgtest R package version 0.7.4.\\nFriedrich Liese and Igor Vajda. 2006. On Divergences and Informations in Statistics and Information Theory. IEEE Trans. Inf.\\nTheory 52, 10 (Oct. 2006), 4394–4412.\\nJun S. Liu. 2001. Monte Carlo Strategies in Scientific Computing. Springer, New York.\\nJérmie Lumbroso. 2013. Optimal Discrete UniformGeneration fromCoin Flips, andApplications. (April 2013). arXiv:1304.1916\\nVikash Mansinghka and Eric Jonas. 2014. Building Fast Bayesian Computing Machines Out of Intentionally Stochastic\\nDigital Parts. (Feb. 2014). arXiv:1402.4914\\nThe MathWorks. 1993. Statistics Toolbox User’s Guide. The MathWorks, Inc.\\nJohn F. Monahan. 1985. Accuracy in Random Number Generation. Math. Comput. 45, 172 (Oct. 1985), 559–568.\\nAditya V. Nori, Sherjil Ozair, SriramK. Rajamani, and Deepak Vijaykeerthy. 2015. Efficient Synthesis of Probabilistic Programs.\\nIn PLDI 2015: Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation\\n(Portland, OR, USA). ACM, New York, 208–217.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\nOptimal Approximate Sampling from Discrete Probability Distributions 36:31\\nSung-il Pae and Michael C Loui. 2006. Randomizing Functions: Simulation of a Discrete Probability Distribution Using a\\nSource of Unknown Distribution. IEEE Trans. Inf. Theory 52, 11 (Nov. 2006), 4965–4976.\\nKarl Pearson. 1900. On the Criterion That a Given System of Deviations from the Probable in the Case of a Correlated\\nSystem of Variables Is Such That It Can Be Reasonably Supposed to Have Arisen from Random Sampling. Philos. Mag. 5\\n(July 1900), 157–175.\\nYuval Peres. 1992. Iterating von Neumann’s Procedure for Extracting Random Bits. Ann. Stat. 20, 1 (March 1992), 590–597.\\nR Core Team. 2014. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing,\\nVienna, Austria. http://www.R-project.org/\\nJames R. Roche. 1991. Efficient Generation of Random Variables from Biased Coins. In ISIT 1991: Proceedings of the IEEE\\nInternational Symposium on Information Theory (Budapest, Hungary). IEEE Press, Piscataway, 169–169.\\nSinha S. Roy, Frederik Vercauteren, and Ingrid Verbauwhede. 2013. High Precision Discrete Gaussian Sampling on FPGAs. In\\nSAC 2013: Proceedings of the 20th International Conference on Selected Areas in Cryptography (Burnaby, Canada). Lecture\\nNotes in Computer Science, Vol. 8282. Springer, Berlin, 383–401.\\nFeras Saad and Vikash Mansinghka. 2016. Probabilistic Data Analysis with Probabilistic Programming. (Aug. 2016).\\narXiv:1608.05347\\nFeras A. Saad, Marco F. Cusumano-Towner, Ulrich Schaechtle, Martin C. Rinard, and Vikash K. Mansinghka. 2019. Bayesian\\nSynthesis of Probabilistic Programs for Automatic Data Modeling. Proc. ACM Program. Lang. 3, POPL, Article 37 (Jan.\\n2019), 32 pages.\\nClaude E. Shannon. 1948. A Mathematical Theory of Communication. Bell Sys. Tech. Journ. 27, 3 (July 1948), 379–423.\\nWarren D. Smith. 2002. How To Sample from a Probability Distribution. Technical Report DocNumber17. NEC Research.\\nSam Staton, Hongseok Yang, FrankWood, Chris Heunen, and Ohad Kammar. 2016. Semantics for Probabilistic Programming:\\nHigher-order Functions, Continuous Distributions, and Soft Constraints. In LICS 2016: Proceedings of the 31st Annual\\nACM/IEEE Symposium on Logic in Computer Science (New York, NY, USA). ACM, New York, 525–534.\\nJohn Steinberger. 2012. Improved Security Bounds for Key-Alternating Ciphers via Hellinger Distance. Technical Report\\nReport 2012/481. Cryptology ePrint Archive.\\nQuentin F. Stout and Bette Warren. 1984. Tree Algorithms for Unbiased Coin Tossing with a Biased Coin. Ann. Probab. 12, 1\\n(Feb. 1984), 212–222.\\nTomohiko Uyematsu and Yuan Li. 2003. Two Algorithms for Random Number Generation Implemented by Using Arithmetic\\nof Limited Precision. IEICE Trans. Fund. Elec. Comm. Comp. Sci 86, 10 (Oct. 2003), 2542–2551.\\nSridhar Vembu and Sergio Verdú. 1995. Generating Random Bits from an Arbitrary Source: Fundamental Limits. IEEE Trans.\\nInf. Theory 41, 5 (Sept. 1995), 1322–1332.\\nJohn von Neumann. 1951. Various Techniques Used in Connection with Random Digits. In Monte Carlo Method, A. S.\\nHouseholder, G. E. Forsythe, and H. H. Germond (Eds.). National Bureau of Standards Applied Mathematics Series,\\nVol. 12. U.S. Government Printing Office, Washington, DC, Chapter 13, 36–38.\\nMichael D. Vose. 1991. A Linear Algorithm for Generating Random Numbers with a Given Distribution. IEEE Trans. Softw.\\nEng. 17, 9 (Sept. 1991), 972–975.\\nAlistair J. Walker. 1974. New Fast Method for Generating Discrete Random Numbers with Arbitrary Frequency Distributions.\\nElectron. Lett. 10, 8 (April 1974), 127–128.\\nAlastair J. Walker. 1977. An Efficient Method for Generating Discrete Random Variables with General Distributions. ACM\\nTrans. Math. Softw. 3, 3 (Sept. 1977), 253–256.\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\n36:32 Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka\\nAppendix A OPTIMAL APPROXIMATION ERROR AT VARIOUS LEVELS OF BIT\\nPRECISION\\nWe study how the theoretically-optimal approximation error realized by our samplers (using\\nAlgorithm 3) varies with the entropy of the target distribution p and the number of bits of precision\\nk available to the sampling algorithm. We obtain 10000 probability distributions {p1, . . . , p10000}\\nover n = 100 dimensions with entropies ranging from 0 (deterministic distribution) to log(100) ≈ 6.6\\n(uniform distribution). For each pi (i = 1, . . . , 10000) and precision values k = 1, . . . , 20, we obtain\\nan optimal approximation pˆik using Algorithm 3 with Z = 2k and measure the approximation\\nerror ∆ik B ∆(pi , pik ). Figure 5 shows a heatmap of the approximation errors ∆ik according to\\nthree common f -divergences: total variation, Hellinger divergence, and relative entropy, which\\nare defined in Table 1. Under the relative entropy divergence, all approximation errors are infinite\\nwhenever the precision k < 7 (white area; Figure 5c), since the sampler needs at least 7 bits of\\nprecision to assign a non-zero probability to each of the n = 100 outcomes of the target distributions.\\n2 4 6 8 10 12 14 16 18 20\\nNumber of Bits of Precision\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\nE\\nnt\\nro\\npy\\nof\\nT\\nar\\nge\\nt\\nD\\nis\\ntr\\nib\\nut\\nio\\nn\\nTheoretically Optimal Error\\n10−5\\n10−4\\n10−3\\n10−2\\n10−1\\n100\\n(a) Total Variation\\n2 4 6 8 10 12 14 16 18 20\\nNumber of Bits of Precision\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\nE\\nnt\\nro\\npy\\nof\\nT\\nar\\nge\\nt\\nD\\nis\\ntr\\nib\\nut\\nio\\nn\\nTheoretically Optimal Error\\n10−8\\n10−6\\n10−4\\n10−2\\n100\\n(b) Hellinger Divergence\\n2 4 6 8 10 12 14 16 18 20\\nNumber of Bits of Precision\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\nE\\nnt\\nro\\npy\\nof\\nT\\nar\\nge\\nt\\nD\\nis\\ntr\\nib\\nut\\nio\\nn\\nTheoretically Optimal Error\\n10−8\\n10−6\\n10−4\\n10−2\\n100\\n(c) Relative Entropy\\nFig. 5. Characterization of theoretically optimal approximation errors according to three f -divergences (total\\nvariation, Hellinger, and relative entropy) for target distributions over n = 100 dimensions.\\nIn all three plots, for a fixed level of entropy (y-axis), the approximation error tends to zero\\nas the precision increases from k = 1 to k = 20 (x-axis). However, the relationship between\\napproximation error and entropy of the target distribution under each divergence. For total variation,\\nthe approximation error increases as the entropy increases at both low-precision values (gray area;\\ntop-left of Figure 5a) and high-precision values (purple area; bottom-right of Figure 5a). In contrast,\\nfor relative entropy, the approximation error decreases as the entropy increases at both low-\\nprecision values (gray area; bottom-center-left of Figure 5a) and high-precision values (purple area;\\ntop-right of Figure 5a). For the Hellinger divergence, the approximation error contains both of these\\ncharacteristics; more specifically, it behaves like the error under total variation at low precision\\n(gray area; top-left of Figure 5b) and like the error under relative entropy at high precision (purple\\narea; top-right of Figure 5b). More generally, the distributions with highest approximation error\\nunder the Hellinger divergence lie in the center of the entropy values and the distributions with\\nthe lowest approximations lie at the low and high end of the entropy values.\\nThese studies provide systematic guidelines for obtaining theoretically-minimal errors of entropy-\\noptimal approximate samplers according to various f -divergences in applications where precision\\nand accuracy are key design considerations. For example, Jonas [2014] empirically measure the\\neffects of bit precision (using 4 to 12 bits) on the sampling error (measured by the relative entropy)\\nof a 1000-dimensional multinomial hardware gate. In cryptographic applications, a common re-\\nquirement for various security guarantees is to sample from a discrete Gaussian lattice with an\\napproximation error (measured by total variation) of at most 2−90 [Dwarakanath and Galbraith\\n2014], and various limited-precision samplers aim to operate within these bounds [Folláth 2014].\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\nOptimal Approximate Sampling from Discrete Probability Distributions 36:33\\nAppendix B DEFERRED PROOFS\\nThis section proves Theorem 4.17 from the main text, which is restated below.\\nTheorem B.1. If ∆д is the total variation divergence, then any optimal solution M returned by\\nAlgorithm 3 satisfies ∆д(p,M) ≤ n/2Z .\\nWe begin by first establishing the following result.\\nTheorem B.2. Let p B (p1, . . . ,pn) be a probability distribution, Z > 0 an integer, and ∆д be total\\nvariation divergence. Any assignmentM ∈ M[n,Z ] that minimizes ∆д(p,M) satisfies:\\n⌊Zpi ⌋ ≤ Mi ≤ ⌊Zpi ⌋ + 1 (i = 1, . . . ,n). (47)\\nProof. Write χ (w) B w − ⌊w⌋ to denote the fractional part of a real number w . From the\\ncorrespondence of the total variation to the L1 distance, the objective function may be rewritten as\\n∆д(p,M) = 12\\nn∑\\ni=1\\n|Mi/Z − pi | . (48)\\nOptimizing ∆д(p, ·) is equivalent to optimizing ∆′д(p, ·), defined by\\n∆′д(p,M) B 2Z∆д(p,M) =\\nn∑\\ni=1\\n|Mi − Zpi | . (49)\\nLetM be any assignment that minimizes ∆′д(p, ·). We will show the upper bound and lower bound\\nin (47) separately.\\n(Upper bound). Assume toward a contradiction that there is some t ∈ [n] such thatMt = ⌊Zpt ⌋+c\\nfor some integer c > 1.\\nWe first claim that there must be some j , t such thatMj < Zpj . Assume not. Then Zpi ≤ Mi\\nfor all i ∈ [n], which gives\\nn∑\\ni=1\\nMi ≥\\nn∑\\ni=1\\ni,t\\nZpi + ⌊Zpt ⌋ + c =\\nn∑\\ni=1\\ni,t\\nZpi + ⌊Zpt ⌋ + c (50)\\n=\\nn∑\\ni=1\\ni,t\\nZpi + Zpt − χ (Zpit) + c (51)\\n=\\nn∑\\ni=1\\nZpt + (c − χ (Zpt )) = Z + (c − χ (Zpt )) > Z , (52)\\nwhere the final inequality follows from c > 1 > χ (Zpt ). But (52) contradictsM ∈ M[n,Z ].\\nConsider the assignmentW B (W1, . . . ,Wn) ∈ M[n,Z ] defined by\\nWi B\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\nMi − 1 if i = t ,\\nMi + 1 if i = j,\\nMi otherwise.\\n(i = 1, . . . ,n) (53)\\nWe will establish that ∆′д(p,W) < ∆′д(p,M), contradicting the optimality ofM. From cancellation\\nof like-terms, we have\\n∆′д(p,W) − ∆′д(p,M) = [|Wt − Zpt | − |Mt − Zpt |] +\\n[|Wj − Zpj | − |Mj − Zpj |] . (54)\\nFor the first term in the right-hand side of (54), we have\\n|Wt − Zpt | − |Mt − Zpt | = (Mt − 1 − Zpt ) − (Mt − Z ) = −1, (55)\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\n36:34 Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka\\nwhere the first equality uses the fact that c ≥ 2, so that\\nWt = Mt − 1 = ⌊Zpt ⌋ + c − 1 ≥ ⌊Zpt ⌋ + 1 > Zpt . (56)\\nWe now consider the second term of (54), and proceed by cases.\\nCase 1: Mj < ⌊Zpj ⌋. Then clearly\\n|Wj − Zpj | − |Mj − Zpj | = (Zpj − (Mj + 1)) − (Zpj −Mj ) = −1. (57)\\nCase 2: Mj = ⌊Zpj ⌋. SinceMj < Zpj , we have ⌊Zpj ⌋ < Zpj and 0 < χ (Zpj ) < 1, which gives\\n|Wj − Zpj | − |Mj − Zpj | = (Mj + 1 − Zpj ) − (Zpj −Mj ) (58)\\n= 1 − 2(Zpj −Mj ) (59)\\n= 1 − 2χ (Zpj ) (60)\\n< 1. (61)\\nCombining (57) and (61) from these two cases gives the upper bound\\n|Wj − Zpj | − |Mj − Zpj | < 1. (62)\\nUsing (57) and (62) in (54), we obtain\\n∆′д(p,W) − ∆′д(p,M) = [|Wt − Zpt | − |Mt − Zpt |] +\\n[|Wj − Zpj | − |Mj − Zpj |] (63)\\n< −1 + 1 = 0, (64)\\nestablishing a contradiction to the optimality ofM.\\n(Lower Bound). Assume toward a contradiction that there exists t ∈ [n] such thatMt < ⌊Zpt ⌋.\\nWe first claim that there must exist j , t such that Zpj < Mj . Assume not. ThenMi ≤ Zpi for\\nall i ∈ [n], which gives\\nn∑\\ni=1\\nMi <\\nn∑\\ni=1\\ni,t\\nZpi + ⌊Zpt ⌋ ≤\\nn∑\\ni=1\\ni,t\\nZpi + Zpt = Z , (65)\\nwhich again contradictsM ∈ M[n,Z ].\\nThe remainder of the proof is symmetric to that of the upper bound, where the assignment\\nW ∈ M[n,Z ] defined by\\nWi B\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\nMi + 1 if i = t ,\\nMi − 1 if i = j,\\nMi otherwise\\n(i = 1, . . . ,n) (66)\\ncan be shown to satisfy ∆′д(p,W) < ∆′д(p,M), contradicting the optimality ofM. □\\nProof of Theorem B.1. From Theorem B.2, we have\\n⌊Zpi ⌋ ≤ Mi ≤ ⌊Zpi ⌋ + 1 =⇒ |Mi − Zpi | ≤ 1 =⇒ |Mi/Z − pi | ≤ 1/Z (i = 1, . . . ,n), (67)\\nwhich along with (48) yields ∆д(p,M) ≤ n/2Z . □\\nProc. ACM Program. Lang., Vol. 4, No. POPL, Article 36. Publication date: January 2020.\\n'}\n",
      "{'_id': ObjectId('6815e61add259dc7e6e1cd2e'), 'authors': 'Belazzougui, Djamal, Venturini, Rossano', 'year': '2017', 'title': 'Compressed String Dictionary Search with Edit Distance One', 'full_text': 'Noname manuscript No.\\n(will be inserted by the editor)\\nCompressed String Dictionary Search with Edit Distance\\nOne\\nDjamal Belazzougui · Rossano Venturini\\nthe date of receipt and acceptance should be inserted later\\nAbstract In this paper we present different solutions for the problem of indexing a\\ndictionary of strings in compressed space. Given a pattern P, the index has to report\\nall the strings in the dictionary having edit distance at most one with P. Our first\\nsolution is able to solve queries in (almost optimal) O(|P|+ occ) time where occ is\\nthe number of strings in the dictionary having edit distance at most one with P. The\\nspace complexity of this solution is bounded in terms of the k-th order entropy of the\\nindexed dictionary. A second solution further improves this space complexity at the\\ncost of increasing the query time. Finally, we propose randomized solutions (Monte\\nCarlo and Las Vegas) which achieve simultaneously the time complexity of the first\\nsolution and the space complexity of the second one.\\n1 Introduction\\nModern web search, information retrieval, data base and data mining applications\\noften require solving string processing and searching tasks. Most of such tasks boil\\ndown to some basic algorithmic primitives which involve a large dictionary of variable-\\nlength strings. Solving approximate searches over dictionaries of strings is an impor-\\ntant primitive that appears frequently in many practical scenarios. In Web search, for\\nThe work is an extended version of the paper [8] appeared in Proceedings of 23rd Annual Symposium on\\nCombinatorial Pattern Matching, 2012. This work has been partially supported by Academy of Finland un-\\nder grant 250345 (CoECGR), the French ANR-2010-COSI-004 MAPPI project, PRIN ARS Technomedia\\n2012, the Midas EU Project, Grant Agreement no. 318786, and the eCloud EU Project, Grant Agreement\\nno. 325091.\\nDjamal Belazzougui\\nHelsinki Institute for Information Technology HIIT, Department of Computer Science, University of\\nHelsinki, Finland.\\nRossano Venturini\\nDepartment of Computer Science, University of Pisa, Italy.\\nE-mail: djamal.belazzougui@cs.helsinki.fi, rossano.venturini@unipi.it\\n2example, users query the engine with possibly misspelled terms that can be corrected\\nby choosing among the closest terms stored in a trustable dictionary. In data min-\\ning and data base applications, instead, an automatically built dictionary may contain\\nnoise in the form of misspelled strings. Thus, we may need to resort to approximate\\nsearches in order to identify the closest dictionary strings with respect to a (correct)\\ninput string.\\nThe edit distance (also known as Levenstein distance) is the most commonly\\nused distance to deal with misspelled strings. The edit distance between two strings\\nis defined as the minimal number of edit operations required to transform the first\\nstring into the second string. There are three possible edit operations: deletion of a\\nsymbol, insertion of a symbol and substitution of a symbol with another.\\nThe problem string dictionary search with edit distance one is defined as follows.\\nLet D = {S1,S2, . . . ,Sd} be a dictionary of d strings of total length n drawn from an\\nalphabet Σ = [σ ]. We want to build a (compressed) index that, given any string P[1, p],\\nreports all the strings in D having edit distance at most 1 with P. For simplicity, we\\nassume that the strings in D are all distinct and sorted lexicographically (namely, for\\nany 1≤ i< d, Si < Si+1).\\nIn this paper we provide two compressed solutions for the problem. The first\\nsolution guarantees (almost) optimal query time while requiring compressed space.\\nNamely, we show how to obtain an index of 2nHk + n · o(logσ) + 2d logd bits,\\nthat is able to report all the occ strings having edit distance at most 1 with P in\\nO(p+occ) time. Here Hk denotes the k-th order entropy of the strings in the dictio-\\nnary for any fixed k = o(logσ n). Interestingly, the time complexity of this solution\\nis independent of alphabet size. This is quite an uncommon result for compressed\\ndata structures dealing with texts. The second solution provides space/time tradeoffs\\nby using a completely different approach. Its space occupancy, indeed, decreases to\\nnHk + n · o(logσ) bits. This better space bound is obtained at the cost of increasing\\nthe query time to O(p log logσ +occ).\\nInterestingly, our first solution can be easily extended to support an additional\\noperation which has interesting practical applications. Assume that each string Si in\\nD has been assigned a score c(Si), which, for example, could establish the relative\\nimportance of any string with respect to the others. The first solution can be extended\\nto support the extra operation Top(P[1, p],k) that reports the k highest scored strings\\nin D having edit distance at most one with P. This operation is solved in O(p+k logk)\\ntime and returns the occurrences sorted by their scores.\\nWe finally show how to introduce randomization in these solutions to derive\\nMonte Carlo and Las Vegas solutions. These solutions are able to either reduce the\\nspace occupancy or improve the query time of the deterministic solutions.\\n2 Related work\\nThe literature presents several solutions to the problem of indexing string dictionaries\\nto efficiently search with error distance one.\\nThe theoretical study of the problem has been initiated by Yao and Yao in [34].\\nThey present a solution in the bit probe model for the related problem of indexing a\\n3dictionary of strings supporting searches with Hamming distance one. Their solution\\nindexes d binary strings of length m each (i.e., n = dm) by using O(n logm) bits and\\nsolves a query with O(m log logd) bit probes. Subsequently, Brodal and Venkatesh\\n[12] improve the above time complexity by a factor log logd: their solution uses\\nO(n logd) bits of space and O(m) bit probes. In the RAM model, the time complexity\\nis O(m/w), where w is the size of a machine word.\\nBrodal and Ga¸sieniec [11] propose two solutions to solve searches with Hamming\\ndistance one1. The first solution reports all the occ strings having Hamming distance\\nat most one with P in time O(p+occ) by using O(σ ·n logn) bits of space. The main\\ndata structure is a trie that indexes strings in D plus extra strings. An extra string is a\\nstring that does not belong to D but has Hamming distance one with at least a string in\\nD. Clearly, each root-to-leaf path in the trie represents either a string in D or an extra\\nstring. The leaf representing a string S is associated with the list of indices of strings in\\nD having Hamming distance one with S. The query for P is solved by navigating the\\ntrie. If a leaf is reached, the list of indices stored in the leaf is reported. Construction\\ntime and space occupancy for non-constant size alphabets are the major drawbacks\\nof this solution. Indeed, it is unknown how to build this data structure in O(nσ) time\\nor use o(nσ logn) bits of space. Their second solution is slower than the previous one\\nby an additive term O(logd) (namely, query time is O(p+ logd + occ)) [11]. The\\nadvantage is represented by its space occupancy which is O(n logn) and, thus, it is\\nbetter for non-constant size alphabets2. The solution resorts to two tries augmented\\nwith a sorted list of identifiers. These tries index, respectively, the strings in D and\\nthe strings in D reversed. The query algorithm exploits the following property: if\\nthere exists a string S in D having distance one with P[1, p], it can be factorized as\\nS = P[1, i] · c ·P[i+ 2, p], for some index i and symbol c ∈ Σ . This is a key property\\nthat has been exploited by almost all the subsequent solutions, including ours. These\\nsolutions differ from each other in the data structures and the algorithms they use to\\ndiscover all these factorizations. For each string S[1,s] in D, Brodal and Ga¸sieniec\\nconsider all the triplets (npi(S), S[i+ 1], nsi+2(S)), where npi(S) is the identifier of\\nthe node corresponding to prefix S[1, i] in the first trie and nsi+2(S) is the identifier\\nof the node corresponding to S[i+2,s] reversed in the second trie. It is easy to index\\nthese triples by inserting them in a search tree that is able to report, given a pair\\nof node identifiers u and v, all the triples with u in the first component and v in\\nthe third component. This is the core operation of an algorithm solving any query\\nin O(p logn+ occ) time. For any index i, the algorithm identifies the nodes npi(P)\\nand nsi+2(P) and uses the search tree by querying for these two nodes. If the triple\\n(npi(P), c, nsi+2(P)) is returned, then the string S = P[1, i]c ·P[i+ 2, p] is in D and\\nhas distance one from P. The above query time can be further improved by replacing\\nthe search tree with a sorted list of string identifiers in each node of the reverse trie\\nand by resorting to (a sort of) fractional cascading during the query resolution.\\nThe current best solution for the string dictionary search with edit distance one\\nproblem has been presented by Belazzougui [4]. This solution follows a similar ap-\\n1 However, they can be easily extended to deal with the more general edit distance.\\n2 Actually, the paper [11] described only a solution for binary alphabet. However, it is not hard to obtain\\nthe claimed space and time complexities also for non-constant alphabet sizes.\\n4proach but obtains significantly better time and space complexities. Indeed, this so-\\nlution achieves O(p+occ) query time by requiring optimal O(n logσ) bits of space.\\nThis is obtained by carefully combining compact tries, (minimal) perfect hash func-\\ntions and Karp-Rabin signatures.\\nFinally, we observe that this problem can be seen as a simpler instance of either\\napproximate full-text indexing or approximate dictionary matching with one or more\\nerrors. However, currently best solutions for these more general problems are not\\ncompetitive with the solutions presented in this paper (see e.g., [1, 5, 15]).\\n3 Background\\nIn this section we collect a set of algorithmic tools that will be used in our solutions.\\nIn the following we report each result together with a brief description of the solved\\nproblem. More details can be obtained by consulting the corresponding references.\\nAll the results hold in the unit cost word-RAM model, where each memory word has\\nsize w bits.\\nEmpirical Entropy. Let T [1,n] be a string drawn from the alphabet Σ = {a1, . . . ,ah}.\\nFor each ai ∈ Σ , we let ni denote the number of occurrences of ai in T . The zero-th\\norder empirical entropy of T is defined as follows.\\nH0(T ) =\\n1\\n|T |\\nh\\n∑\\ni=1\\nni log\\nn\\nni\\n(1)\\nNote that |T |H0(T ) provides an information-theoretic lower bound for the output\\nsize of any compressor that encodes each symbol of T with a fixed code [33].\\nFor any string w of length k, we denote by wT the string of single symbols fol-\\nlowing the occurrences of w in T , taken from left to right. For example, if T =\\nmississippi and w = si, we have wT = sp since the two occurrences of si in T\\nare followed by the symbols s and p, respectively. The k-th order empirical entropy\\nof T is defined as follows.\\nHk(T ) =\\n1\\n|T | ∑\\nw∈Σ k\\n|wT |H0(wT ) (2)\\nWe have Hk(T )≥ Hk+1(T ) for any k ≥ 0. The term |T |Hk(T ) is an information-\\ntheoretic lower bound for the output size of any compressor that encodes each symbol\\nof T with a code that depends on the symbol itself and on the k immediately preceding\\nsymbols [26].\\nCompressed strings with fast random access. In the following we will require the\\navailability of a storage scheme for strings that uses compressed space still being\\nable to access in O(1) time any symbol of the represented string T . To this aim, we\\nuse the following result [21].\\n5Lemma 1 Given a text T [1,n] drawn from an alphabet Σ = [σ ], σ ≤ n, there exists a\\ncompressed data structure that supports the access in constant time of any substring\\nof T of length O(logn) bits requiring nHk(T )+ρ bits, where Hk(T ) denotes the k-th\\nempirical entropy of T and k= o(logσ n). The redundancy ρ depends on the alphabet\\nsize σ : ρ = o(n) if logσ = o(\\n√\\nlogn/k), ρ = n ·o(logσ) otherwise.\\nThe scheme can be also used in cases in which T is the concatenation of a set of\\nstrings (namely, T = S1 ·S2 · . . . ·Sd). The starting positions of strings in T are stored\\nby resorting to Elias-Fano’s representation [17, 18] within d log( nd )+O(d) bits. This\\nadditional structure allows us to access an arbitrary portion of any string in optimal\\ntime.\\nKarp-Rabin signature. Given a string S[1,s], the Karp-Rabin signature function [25]\\nkr(S) is equal to ∑si=1 S[i] · t i (mod M), where M is a prime number and t is a ran-\\ndomly chosen integer in [1,M − 1]. Given a dictionary of strings D containing d\\nstrings of total length n, we can obtain an instance kr of the Karp-Rabin function that\\nmaps strings in D to the integers in [M−1] without collisions, with M chosen among\\nthe first O(n ·d2) integers. It is known that a value of t that guarantees injectivity can\\nbe found with constant number of attempts in expectation (see the analysis in [16]).\\nNotice that the representation of kr requires O(logn+ logd) =O(logn) bits of space.\\nInterestingly, the Karp-Rabin function guarantees that, after a preprocessing phase\\nover a string S, signatures of strings close enough to S can be computed in constant\\ntime. This property is formally stated by the following lemma.\\nLemma 2 Given a string S[1,s], for every prefix P of S, kr(P) can be computed in\\nconstant time. Moreover, for every string Q at distance 1 from S, kr(Q) can be com-\\nputed in constant time, after a preprocessing phase that takes O(s) time.\\nMinimal perfect hash function. Given a subset of S = {x1,x2, . . . ,xn} ⊆ U = [2w]\\nof size n, a minimal perfect hash function has to injectively map keys in S to the\\nintegers in [n]. Hagerup and Tholey [24] show how to build a space/time optimal\\nminimal perfect hash function as stated by the following lemma.\\nLemma 3 Given a subset of S⊆U = [2w] of size n, one can construct in O(n) time a\\nminimal perfect hash function for S that can be evaluated in O(1) time and requires\\nn loge+o(n) bits of space.\\nCompressed static function. Often keys in S = {x1,x2, . . . ,xn} have associated satel-\\nlite data (called values) from an alphabet Σ = [σ ] and we are asked to build a dictio-\\nnary that, given a key x∈ S, returns its associated value. An arbitrary value is returned\\nwhenever x 6∈ S. Essentially, we are defining a static function F whose domain is the\\nset S and whose codomain is formed by the values associated with those keys (i.e.,\\nΣ = {F(x1), . . . ,F(xn)}). The problem asks to evaluate F on its domain with the pos-\\nsibility of returning any value for keys in U \\\\S. Often the values associated with the\\nkeys follow a skewed distribution: few values are considerably more frequent than\\nothers. In these scenarios, it is desirable to achieve space that depends on the entropy\\nof the data rather than on the number of possible values. Thus, designing compressed\\n6abracadabra$\\nbracadabra$a\\nracadabra$ab\\nacadabra$abr\\ncadabra$abra\\nadabra$abrac\\ndabra$abraca\\nabra$abracad\\nbra$abracada\\nra$abracadab\\na$abracadabr\\n$abracadabra\\n=⇒\\nF L\\n$ abracadabr a\\na $abracadab r\\na bra$abraca d\\na bracadabra $\\na cadabra$ab r\\na dabra$abra c\\nb ra$abracad a\\nb racadabra$ a\\nc adabra$abr a\\nd abra$abrac a\\nr a$abracada b\\nr acadabra$a b\\nFig. 1 Example of Burrows-Wheeler transform for the string T = abracadabra$. The matrix on the right\\nhas the rows sorted in lexicographic order. The output of the Bwt is the column L = ard$rcaaaabb.\\nstatic functions asks to represent F with constant evaluation time by using space\\nclose to nH0 bits, where H0 denotes the 0-th order empirical entropy of the sequence\\nF(x1),F(x2), . . . ,F(xn). The current best result for this problem [9] is reported in the\\nfollowing theorem.\\nTheorem 1 A static function F defined over a subset of keys S = {x1, . . . ,xn} ⊆U =\\n[2w] into values drawn from an alphabet Σ = [σ ], σ ≤ n can be represented with\\nnH0 +O(\\nn(log logn+H0) log logn\\nlogn )+O(σ + logw) bits of space so that evaluating F(x)\\nfor any x takes constant time, where H0 denotes the empirical zero-th order entropy\\nof the sequence F(x1), . . . ,F(xn). The scheme can be built in O(n) expected time.\\nApproximate membership. An approximate membership data structure (AM for short)\\nstores an approximate representation of a set S = {x1,x2, . . . ,xn} ⊆ U = [2w]. The\\nrepresentation is approximate in the following sense: the query on an element x ∈ U\\nalways returns true if x∈ S, false with probability at least 1−ε if x∈U \\\\S, where the\\nreal parameter ε ∈ (0,1) (called false positive probability) is specified at construction\\ntime. The Bloom filter [10] is the most popular approximate membership data struc-\\nture, but only more recent data structures [9, 14, 30] are known to have optimal time\\nand space complexities (up to constant factors).\\nLemma 4 Given a set S = {x1, . . . ,xn} ⊆ U = [2w] and a parameter ε such that\\n0 < ε < 1, there exists an approximate membership data structure for the set S with\\nfalse positive probability ε requiring O(n log(1/ε)) bits of space and answering any\\nquery in constant time.\\nBurrows-Wheeler transform. In 1994 Burrows and Wheeler [13] introduced a new\\ncompression algorithm based on a reversible transformation, now called the Burrows-\\nWheeler Transform (Bwt from now on). The Bwt transforms the input string T into\\na new string that is easier to compress. The Bwt of T , hereafter denoted by BwtT , is\\nbuilt with three basic steps (see Figure 1):\\n1. append to T a special symbol $ smaller than any other symbol of Σ ;\\n72. form a conceptual matrix MT whose rows are the cyclic rotations of string T $ in\\nlexicographic order;\\n3. construct string L by taking the last column of the sorted matrix MT . We set\\nBwtT = L.\\nEvery column of MT , hence also the transformed string L, is a permutation of T $.\\nIn particular the first column of MT , call it F , is obtained by lexicographically sorting\\nthe symbols of T $ (or, equally, the symbols of L). Note that sorting the rows of MT\\nis essentially equivalent to sorting the suffixes of T , because of the presence of the\\nspecial symbol $. This shows that: (1) symbols following the same substring (context)\\nin T are grouped together in L, and thus give raise to clusters of nearly identical\\nsymbols; (2) there is an obvious relation between MT and the suffix array SAT of T .\\nProperty 1 is the key for devising modern data compressors (see e.g. [26]), Property\\n2 is crucial for designing compressed indexes (see e.g. [19, 28]) and, additionally,\\nsuggests a way to compute the Bwt through the construction of the suffix array of T :\\nL[1] = T [n] and, for any 2≤ i≤ n, set L[i] = T [SAT [i]−1].\\nBurrows and Wheeler [13] devised two properties for the invertibility of the Bwt:\\n(a) Since the rows in MT are cyclically rotated, L[i] precedes F [i] in the original\\nstring T ;\\n(b) For any c ∈ Σ , the `-th occurrence of c in F and the `-th occurrence of c in L\\ncorrespond to the same character of the string T .\\nAs a result, the original text T can be obtained backwards from L by resorting\\nto function LF (also called Last-to-First column mapping or LF-mapping) that maps\\nrow indexes to row indexes, and is defined as:\\nLF(i) =C[L[i]]+ rankL[i](L, i),\\nwhere C[L[i]] counts the number of occurrences in T of symbols smaller than L[i] and\\nrankL[i](L, i) is a function that returns the number of times symbol L[i] occurs in the\\nprefix L[1, i]. We talk about LF-mapping because the symbol c = L[i] is located in\\nthe first column of MT at position LF(i). The LF-mapping allows one to navigate T\\nbackwards: if T [k] = L[i], then T [k− 1] = L[LF(i)] because row LF(i) of MT starts\\nwith T [k] and thus ends with T [k− 1]. In this way, we can reconstruct T backwards\\nby starting at the first row, equal to $T , and repeatedly applying LF for n steps.\\nAs an example, see Figure 1 in which the 3rd a in L lies onto the row which starts\\nwith bracadabra$ and, correctly, the 3rd a in F lies onto the row which starts with\\nabracadabra$. That symbol a is T [1].\\nCompressed full-text indexing. Ferragina and Manzini [20] show that data structures\\nsupporting rank queries on the string L suffice to search for an arbitrary pattern P[1, p]\\nas a substring of the indexed text T . For any i ∈ [1, |L|] and c ∈ Σ , the query rank(i,c)\\non L returns the number of occurrences of symbol c in the prefix L[1, i]. The resulting\\nsearch procedure is now called backward search and is illustrated in Figure 2. It works\\nin p phases, each preserving the invariant: At the end of the i-th phase, [First,Last] is\\nthe range of contiguous rows in MT which are prefixed by P[i, p]. Backward search\\n8Algorithm Backward search(P[1, p])\\n1. i = p, c = P[p], First=C[c]+1, Last=C[c+1];\\n2. while ((First≤ Last) and (i≥ 2)) do\\n3. c = P[i−1];\\n4. First=C[c]+ rankc(L,First−1)+1;\\n5. Last=C[c]+ rankc(L,Last);\\n6. i = i−1;\\n7. if (Last< First) then return “no rows prefixed by P” else return [First,Last].\\nFig. 2 The algorithm to find the range [First,Last] of rows of MT prefixed by P[1, p].\\nstarts with i = p so that First and Last are determined via the array C (step 1). Fer-\\nragina and Manzini proved that the pseudo-code in Figure 2 maintains the invariant\\nabove for all phases, so [First,Last] delimits at the end the rows prefixed by P (if\\nany). Steps 4 and 5 are the dominant costs of each iteration of Backward search.\\nThey are computed efficiently by using appropriate data structures. Array C is small\\nand occupies O(σ logn) bits. Efficiently supporting rank queries over Bwt requires\\nmore sophisticated data structures. The literature offers many theoretical and practi-\\ncal solutions for this problem (see e.g., [2, 3, 6, 19, 28] and references therein). The\\nfollowing lemma summarizes the results we use in our solution.\\nLemma 5 Let T [1,n] be a string over alphabet Σ = [σ ], σ ≤ n, L = BwtT be its\\nBurrows-Wheeler transform and w be the size of a memory word.\\n1. For σ = O(poly(w)), there exists a data structure which supports rank queries\\nand the retrieval of any symbol of L in constant time, by using nHk(T )+o(n) bits\\nof space, for any k ≤ α logσ n and 0< α < 1.\\n2. For larger σ , there exists a data structure which supports rank queries and the\\nretrieval of any symbol of L in O(log logwσ) time, by using nHk(T )+ o(n)(1+\\nHk(T )) bits of space, for any k ≤ α logσ n and 0< α < 1\\nBy plugging Lemma 5 into Backward search, we obtain the following theorem.\\nTheorem 2 Given a text T [1,n] drawn from an alphabet Σ = [σ ], σ ≤ n, there exists\\na compressed index that takes p× trank time to support Backward search(P[1, p]),\\nwhere trank is the time cost of a single rank operation over L = BwtT . The space\\noccupancy is bounded by nHk(T )+ρ bits, for any k ≤ α logσ n and 0< α < 1.\\nThe redundancy ρ is o(n) bits and trank is O(1) when σ = O(poly(w)) while\\nρ = o(n)(1+Hk(T )) bits and trank = O(log logwσ) otherwise, where w is the size of\\na memory word.\\nNotice that compressed indexes support also other operations, like locate and\\nextract, which are slower than Backward search in that they require polylog(n) time\\nper occurrence [19, 28]. We do not go into further details on these operations because\\nthey are not required in our solution.\\n94 A hashing-based solution\\nOur first solution can be seen as a compressed variant of the solution presented in [4].\\nHowever, we need to apply significant and non-trivial changes to that solution in\\norder to achieve compressed space and to retain exactly the same (almost optimal)\\nquery time. More formally, in this section we prove the following theorem.\\nTheorem 3 Given a set D = {S1,S2, . . . ,Sd} of d strings of total length n drawn\\nfrom an alphabet Σ = [σ ], σ ≤ n, there exists an index that, given any pattern P[1, p],\\nreports in O(p+occ) worst-case time all the occ strings in D having edit distance at\\nmost one with P. The index requires\\n1. nHk +o(n)+2d logd bits of space, if σ = O(1);\\n2. 2nHk +o(n)+2d logd bits of space, if logσ = o(\\n√\\nlogn/k);\\n3. 2nHk +n ·o(logσ)+2d logd bits of space, otherwise,\\nfor any fixed k = o(logσ n).\\nAt a high level our solution works as follows. First, it identifies a set of O(p+occ)\\ncandidate strings being a superset of the strings that have edit distance at most one\\nwith P. Then, it discards all candidate strings that actually do not belong to D. For\\nthe moment, let us assume that establishing whether or not a candidate string belongs\\nto D costs constant time. Later, we will discuss how to efficiently perform this non-\\ntrivial task3.\\nOur solution asks to identify the strings in D that share prefixes and suffixes with\\nthe query string P. For this aim we resort to two patricia tries PT and PTr that in-\\ndex the strings in D and the strings in D written in reversed order, respectively. Each\\nnode in each patricia trie is uniquely identified by the time of its visit in the preorder\\nvisit of the tree. The tree structure of each patricia trie is represented in O(d) bits\\nwith standard succinct solutions [27]. In order to perform searches on patricia tries,\\nwe add data structures to compute the length of longest common prefix (lcp) and\\nlongest common suffix (lcpr) for any pair of strings in D. A standard constant time\\nsolution requiring O(d(1+ log nd )) bits of space is obtained by writing lcps between\\nlexicographically consecutive strings (resp. reverse strings) using Elias-Fano’s repre-\\nsentation [17, 18] and by resorting to Range Minimum Queries (rmq) (see e.g., [23])\\non these arrays. Fast percolation of the tries is obtained by augmenting the branch-\\ning nodes with monotone minimal perfect hash functions as described in [7]. In this\\nway choosing the correct edge to follow from the current node can be done in constant\\ntime regardless of the alphabet size. The extra space cost is bounded by O(d log logσ)\\nbits. Thus, the representation of the two patricia tries uses O(d(log logσ + log nd ))\\nbits. The following fact states that this space occupancy is O( n logσ log lognlogn ) bits and,\\nthus, within the lower-order terms of Theorem 3.\\nFact 1 d(log logσ + log nd ) = O(\\nn logσ log logn\\nlogn )\\n3 Notice that just accessing each symbol of these candidate strings would cost O(p+ p · occ) time in\\ntotal which is much higher than our claimed complexity.\\n10\\nProof Observe that, since all the d dictionary strings are distinct, their average length\\nmust be at least logd bits. This implies that their total length in bits, i.e., n logσ bits,\\nmust be at least d logd, i.e., d logd ≤ n logσ . Consider now two cases depending on\\nthe value of d. In the first case we assume that d ≤ n\\nlog2 n\\n, which gives d(log logσ +\\nlog nd ) ≤ nlog2 n (log logn+ logn) = O(\\nn\\nlogn ), and the result follows. Conversely, in\\nthe second case we assume that d > n\\nlog2 n\\n, which gives log(n/d) ≤ 2loglogn and\\nlogd ≥ logn−2loglogn and, thus, d(logn−2loglogn)≤ d logd ≤ n logσ . This al-\\nlows us to deduce that d ≤ n logσlogn−2loglogn and, thus, d(log logσ + log nd ) is at most\\nO(d log logn) ≤ O( n logσlogn log logn) = O( n logσ log lognlogn ) as claimed. In what preceded\\nwe assumed that log logσ ≤ log logn. If that was not the case, then σ > n and\\nd log logσ = O( n logσ log lognlogn ) trivially holds. uunionsq\\nThe correctness of the steps performed during the search on the patricia tries is\\nestablished by comparing the searched string and labels on the traversed edges. This\\nis done by directly accessing the appropriate portion of the strings in D from their\\ncompressed representations. For this aim D is represented by resorting to the com-\\npressed scheme of Lemma 1 that allows constant time access to any symbol of any\\nstring in D. The space required by this is bounded by the k-th order entropy according\\nto Lemma 1. Since the strings do not keep their original order in the trie PTr, we store\\na permutation pi of {1,2, . . . ,d} that keeps track of the original order in D of each leaf\\nof PTr. Namely, pi(i) is the index in D of the ith lexicographically smallest string in\\nPTr. Clearly, storing pi requires d logd+O(d) bits. Figure 3 shows most of the data\\nstructures above built on the set of strings D = {abcc,accb,baca,caac,cbcc}.\\nCandidate strings obtained by deleting a symbol. The identification of candidate\\nstrings for deletion is an easy task. Indeed, we observe that there are just p possi-\\nble candidate strings obtainable from P[1, p] by deleting one of its symbol. Thus, we\\nsimply consider any string P[1, i] ·P[i+ 2, p] as a candidate string. However, any of\\nthese strings is reported only after having checked that it actually belongs to D. As\\nsaid above, for the moment we assume that this non-trivial task can be done in O(1)\\n(amortized) time per string.\\nCandidate strings obtained by inserting or substituting a symbol. Identifying can-\\ndidate strings for insertion or substitution of a symbol is an easy task whenever the\\nalphabet has constant size. In this case there are, indeed, O(σ · p) = O(p) candidate\\nstrings obtained by inserting or substituting any possible symbol of Σ in any position\\nof P. This implies that the data structures above suffice for point 1 in Theorem 34.\\nIdentifying insertions and substitutions with a larger alphabet is a much harder task,\\nwhich requires an additional data structure. Our additional data structure follows the\\nidea presented in [4] which allows us to reduce the number of candidate strings from\\nO(σ · p) to O(p+occ). However, our solution is forced to use more sophisticated ar-\\nguments in order to achieve a space bounded in terms of the k-th order entropy. Given\\nthe set of strings D and the two patricia tries PT and PTr, our first step consists in\\n4 Recall that we are still assuming that we can check in O(1) whether a candidate string belongs to D.\\n11\\n0\\n5\\n54\\naac bcc\\n31\\n21\\nbcc ccb\\na\\nbaca\\nc\\n(a) PT\\n0\\n3\\n5\\n51\\na c\\n4\\naac cb\\n23\\nacab\\nbcca\\nc\\n(b) PTr\\n1 2 3 4 5\\nlcp 1 0 0 1 ×\\nlcpr 0 0 1 3 ×\\npi 3 2 4 1 5\\n(c) lcp, lcpr , and pi\\n〈0, 0, a, 3, 5〉\\n〈1, 1, b, 2, 3〉\\n〈1, 2, c, 1, 3〉\\n〈1, 3, c, 0, 0〉\\n(d) Tuples induced by\\nstring abcc\\nFig. 3 The picture shows a running example for the set of strings D = {abcc,accb,baca,caac,cbcc}.\\nFigures (a) and (b) show the patricia tries of, respectively, the strings in D and the strings in D written in\\nreversed order. For each internal node we report the time of its visit in a preorder visit of the tree while for\\neach leaf we report the identifier of the corresponding string in D. We report complete edges labels, even\\nif a patricia trie stores only the first symbol of each label. Figure (c) reports lcp, lcpr , and pi . Figure (d)\\nshows the four tuples induced by the string abcc.\\nbuilding a set T of tuples. For each string S in D of length s, we consider each of\\nits factorizations of the form S = S[1, i] · c · S[i+ 2,s]. For each of them, we add to\\nT the tuple 〈np, i,c = S[i+ 1],s− (i+ 1),ns〉 where np (resp. ns) is the index of the\\nnode in PT (resp. PTr) whose locus has the longest common prefix with S[1, i] (resp.\\nS[i+ 2,s] reversed). Observe that the cardinality of T is at most n, since we add at\\nmost s tuples for a string S of length s. Figure 3(d) shows the four tuples induced by\\nstring S= abcc. For example, the second tuple is equal to 〈1,1,b,2,3〉 since the locus\\nof node 1 in PT has the longest common prefix with S[1,1] = a, the locus of node 3\\nin PTr has the longest common prefix with S[3,4] = cc reversed, and S[2] = b.\\nThe set T contains enough information to allow the identification of all the can-\\ndidate strings. In the following we consider only insertions since substitutions are\\nsolved similarly. For insertions we consider all the factorizations of P having the\\nform P= P[1, i] ·P[i+1, p]. For each of them, we identify the (highest) nodes npi and\\nnsi+1 in PT and PTr that are prefixed respectively by P[1, i] and P[i+1, p] reversed.\\nClearly, identifying all these nodes for all the factorizations of P requires O(p) time.\\n12\\nThe key observation to identify candidate strings is the following: If there exists\\na tuple 〈npi, i,c, p− i,nsi+1〉 in T , then the string S = P[1, i] · c ·P[i+ 1, p] belongs\\nto D and, obviously, has distance one from P.5 As an example, consider the pattern\\nP = acc. The node np1 = 1 has the longest common prefix with P[1,1] = a in PT\\nand the node ns2 = 3 has the longest common prefix with P[2,3] = cc reversed in\\nPTr. Since the triple 〈1,1,b,2,3〉 belongs to T , the string P[1,1] ·b ·P[2,3] = abcc\\nhas distance one from P and belongs to D.\\nOur data structure is built on top of T and allows us to easily identify the required\\ntuples (and without requiring to store T explicitly). We notice that there may exist\\nseveral tuples of the form 〈np, i,?,ns, i′〉. These groups of tuples share the same four\\ncomponents np, i, ns and i′, and differ just for the symbol c. In order to distinguish\\nthem, we arbitrarily rank tuples in the same group and we assign to each of them its\\nposition in the ranking. We build a data structure that, given the indexes np and ns\\nof two nodes, two lengths i and i′ and rank r, returns the symbol c of the rth tuple\\nof the form 〈np, i,?,ns, i′〉 in T . The data structure is allowed to return an arbitrary\\nsymbol whenever such a tuple does not exist. The use of such a data structure to\\nsolve our problem is simple. For each factorization P[1, i] ·P[i+1, p] of P, we query\\nthe data structure above several times by passing the parameters npi, i, p− i− 1,\\nnsi+1 and r. The value of r is initially set to 0 and increased by 1 for each of the\\nsubsequent queries. After every query, we check if the string S = P[1, i] ·c ·P[i+1, p]\\nbelongs to D, where c is the symbol returned by the data structure. We pass to the\\nnext factorization as soon as we discover that either the string S does not belong to D\\nor symbol c has been already seen for the same factorization. Both these conditions\\nprovide the evidence that no tuple 〈npi, i,?, p− i−1,nsi+1〉 with rank r or larger can\\nbelong to T . It is easy to see that the overall number of queries is O(p+occ).\\nWe are now ready to present a data structure to index T as described above that\\nrequires O(1) time per query and uses entropy bounded space. The first possible\\ncompressed solution consists in defining a function F which is then represented by\\nusing the solution of Theorem 1. For any tuple 〈np, i,c,ns, i′〉 having rank r in T , we\\nset F(np, i,ns, i′,r) equal to c. Queries above are solved by appropriately evaluating\\nfunction F . According to Theorem 1, each query is solved in constant time. As far\\nas space occupancy is concerned, we observe that F is defined for at most n values\\nand that any symbol of any string in D is assigned at most once. Thus, by combining\\nthese considerations with Theorem 1, it follows that the representation of F requires\\nat most nH0 +O(\\nn(H0+log logn) log logn\\nlogn ) bits. A boost of this space complexity to nHk\\nis obtained by defining several functions F , one for each possible context of length\\nk. Here k = o(logσ n) is an arbitrary but fixed parameter. The function Fcntxt is de-\\nfined only for tuples 〈np, i,c,ns, i′〉 where the symbol c is preceded by the context\\ncntxt in the string that induced the tuple. By summing up the cost of storing the\\nrepresentations of these functions, we have that the space occupancy is bounded by\\nnHk +O(\\nn(Hk+log logn) log logn\\nlogn ) bits for the fixed k = o(logσ n). Notice that splitting F\\nin several functions is not an issue for our aim. In the algorithm above, indeed, we\\ncan always query the correct function since we are aware of the correct context.\\n5 Observe that similar considerations hold also for substitutions with the difference that we skip the ith\\nsymbol in factorizations of the form P = P[1, i−1] ·P[i] ·P[i+1, p].\\n13\\nChecking candidate strings. We are left to explain how to establish, in constant time,\\nwhether a candidate string belongs to D. Observe that any candidate string has the\\nform S = P[1, i] ·P[i+ 2, p] in case of deletion, S = P[1, i] · c ·P[i+ 1, p] in case of\\ninsertion, or S = P[1, i] · c ·P[i+ 2, p] in case of substitution, for some symbol c and\\nindex i. One of the issues behind this task is the fact that S may not fit in a constant\\nnumber of memory words and, thus, it cannot be managed directly in constant time.\\nFor this aim Karp-Rabin function kr is used to create small sketches of the strings in\\nD that fit in O(1) memory words and that uniquely identify each string. Observe that\\nthe signatures assigned by function kr are values smaller than M and, thus, each of\\nthem fits in O(1) words of memory.\\nOnce we have these perfect signatures, we use a minimal perfect hash function\\nto connect each signature to the corresponding string in D. Let Dkr be the set of sig-\\nnatures assigned by kr to strings in D (i.e., Dkr = {kr(S) | S ∈ D}). We construct a\\nminimal perfect hash function mph that maps signatures in Dkr to the first d integers.\\nLemma 3 guarantees O(1) evaluation time by requiring O(d) bits of space. As satel-\\nlite data, the entry for the string S stores in logd+O(1) bits the index of the leaf in\\nPTr that corresponds to S reversed. Clearly, if S belongs to D, mph(kr(S)) gives us in\\nconstant time the index of S reversed in PTr while pi(mph(kr(S))) reports the index\\nof S in PT. It is worth noticing that the result of these operations are meaningless\\nwhenever S does not belong to D.\\nThe checking of candidate strings requires a preprocessing phase shared among\\nall the candidate strings. Firstly, we compute in O(p) time the Karp-Rabin signatures\\nof all prefixes and suffixes of P. In this way, the signature of any candidate string S can\\nbe computed in constant time by appropriately combining two of those signatures (see\\nLemma 2). Then, we identify a leaf pleaf in PT that shares the longest common prefix\\nwith P. Similarly, we identify a leaf sleaf in PTr having the longest common prefix\\nwith P reversed. Given the properties of patricia tries and our succinct representation,\\nidentifying these two leaves costs O(p) time.\\nThe check for the single candidate string S = P[1, i] ·c ·P[i+1, p] obtained by in-\\nserting symbol c in the (i+1)th position is done as follows6. We compute in constant\\ntime the values k = pi(mph(kr(S))) and k′ = mph(kr(S)). Then, we have to check\\nthat the candidate string S is equal to the string Sk in D. Instead of comparing S and\\nSk symbol by symbol, we exploit the fact that S and Sk coincide if and only if the\\nfollowing three conditions are satisfied:\\n– lcp(k,pleaf) is at least i;\\n– lcpr(k′,sleaf) is at least p− i;\\n– (i+1)th symbol of Sk is equal to c.\\nClearly, these three conditions are checkable in constant time. The O(p) preprocess-\\ning time is amortized over the O(p+occ) candidate strings.\\nFinding Top-k strings. As we mentioned in the introduction, our solution can be ex-\\ntended to support an additional operation which has interesting practical applications.\\nAssume that each string Si in D is assigned a score c(Si). For example, the score could\\n6 Checks for other types of errors are done in a similar way.\\n14\\nestablish the relative importance of any string with respect to the others. It is possible\\nto extend our solution in order to support the extra operation Top(P[1, p],k) that re-\\nports the k highest scored strings in D having edit distance at most one with P. This\\noperation is solved in O(p+k logk) time. We assume that values c() are available for\\nfree. Notice that we can easily avoid this assumption by storing in d logd+O(d) bits\\nthe ranking of strings in D induced by c().\\nWe first present a simpler O((p+ k) logk) time algorithm which is, then, modi-\\nfied in order to achieve the claimed time complexity. We said above that an arbitrary\\nrank is assigned to tuples in T belonging to the same group (namely, tuples of the\\nform 〈np, i,?,ns, i′〉 that differ just for the symbol ?). Instead, this algorithm requires\\nthat the assigned ranks respect the order induced by c(). Namely, lower ranks are\\nassigned to tuples corresponding to strings with higher values of c(). The searching\\nalgorithm is similar to the previous one. The main difference is in the order in which\\nthe factorizations of P[1, p] are processed. The algorithm works in steps and keeps\\na heap. The role of the heap is to keep track of the top-k candidate strings seen so\\nfar. Each factorization is initially considered active and becomes inactive later in the\\nexecution. Once a factorization becomes inactive, it is no longer taken into consider-\\nation. Each factorization also has an associated score which is initially set to +∞. At\\neach step, we process the active factorization with the largest score. We query func-\\ntion F with the correct value of r for the current factorization. Let S be the candidate\\nstring identified by resorting to F . If S does not belong to D, the current factorization\\nbecomes inactive and we continue with the next factorization. Otherwise, we insert S\\ninto the heap with its score c(S) and we decrease the score associated with the current\\nfactorization to c(S). At each step we also check the number of strings in the heap. If\\nthere are k+1 strings in the heap, we remove the string with the lowest score and we\\ndeclare the factorization that introduced that string inactive.\\nNotice that, apart from the first k steps, in each step a factorization becomes in-\\nactive. Since there are O(p) factorizations, our algorithm performs at most O(p+ k)\\ninsertions into a heap containing at most k strings. Thus, the claimed time complexity\\neasily follows. The improvement is obtained by observing that most of the time (i.e.,\\nO(p logk)) is spent on inserting the first string of each factorization into the heap.\\nThis is no longer necessary if we use the following strategy. We first collect the first\\nstring of each factorization together with its score and we apply the classical linear\\ntime selection algorithm to identify the k-th smallest score. This step costs O(p) time.\\nWe immediately declare the p− k factorizations whose strings have a smaller score\\ninactive. We insert the remaining k strings into the heap and we use the previous al-\\ngorithm to complete the task. The latter step costs now O(k logk) time, since we have\\njust k active factorizations.\\n5 A Bwt-based solution\\nThe term d logd and the factor 2 multiplying the Hk term in the space bound of The-\\norem 3 may be too large for some applications.\\nIn this section we provide a solution that is able to overcome this limitation at the\\ncost of (slightly) increasing the query time.\\n15\\nFormally, we prove the following theorem.\\nTheorem 4 Given a set D= {S1,S2, . . . ,Sd} of d strings of total length n drawn from\\nan alphabet Σ = [σ ], σ ≤ n and let w be the size of a memory word, there exists an\\nindex requiring nHk +ρ bits of space for any k ≤ α logσ n and 0< α < 1 that, given\\nany pattern P[1, p] reports all the occ strings in D having edit distance at most one\\nwith P in\\n1. O(p logσ n log logn+occ) worst-case time for σ =O(poly(w)) with redundancy\\nρ = O(n logσlog logn )+o(n) bits;\\n2. O(p log logwσ+occ) worst-case time for larger σ with redundancy ρ = o(n)(1+\\nHk)+O(n log(logσ n log logn)) bits.\\nThis solution uses a completely different approach with respect to the previous\\none and solves the problem by building a collection of compressed permuterm in-\\ndexes [22] on the dictionary D. More precisely, we divide the strings in D into subsets\\nbased on their lengths and we build a compressed permuterm index R` for each set\\nD`, where D` denotes the subset of strings in D of length `.7 This solution introduces\\nseveral possible trade-offs but, for simplicity, we report in Theorem 4 only the most\\ninteresting ones.\\nThe compressed permuterm index [22] is a compressed index for dictionaries\\nof strings based on the Burrows-Wheeler Transform (Bwt). Among other types of\\nqueries, it solves efficiently the PrefixSuffix(P,S) query which is useful for our prob-\\nlem. This query, given a prefix P and a suffix S, identifies all the strings in the dictio-\\nnary having P as prefix and S as suffix. Below we resort to a slightly different variant\\nof the compressed permuterm index. The main difference is the sorting strategy used\\nto obtain the underlying Burrows-Wheeler Transform (Bwt) [13]. In [22] a text is ob-\\ntained by concatenating the strings in the dictionary by using a special symbol # not in\\nΣ as separator. Then, all the suffixes of this text are sorted lexicographically to obtain\\nthe rows of the Burrows-Wheeler matrix. In our variant we first append the symbol\\n# to each string, then we construct the (conceptual) matrix M by lexicographically\\nsorting all the cyclic rotations of all the strings in the set. Since every row is a rota-\\ntion of a single string from the dictionary it is guaranteed that any row contains only\\nsymbols belonging to the same string. This fact turns out to be useful below when we\\nwill define parent and depth operations on a proper (conceptual) trie. This different\\nconstruction of the Burrows-Wheeler transform was already implicitly used by Fer-\\nragina and Venturini [22] and simulated at query time by means of function jump2end\\n(see [22] for more details). Figure 4 shows this variant of the Burrows-Wheeler Trans-\\nform for the cyclic rotations of the strings in D4 = {abcc,accb,baca,caac,cbcc}.\\nA query PrefixSuffix(P,S) can be easily solved by searching the pattern S#P with the\\nstandard Backward search [22]. The procedure returns the range of rows of M that\\nare prefixed by S#P which are exactly all the strings in the dictionary that are both\\nprefixed by P and suffixed by S.\\nGiven a pattern P[1, p], we solve our problem by querying only three compressed\\npermuterm indexes: Rp−1 for deletions, Rp for substitutions and Rp+1 for insertions.\\n7 We notice that the number of distinct lengths and, thus, compressed permuterm indexes is O(\\n√\\nn).\\n16\\ncb\\nbc\\ncc\\nacc\\nbaca\\na\\ncb#\\na#b\\n#bac\\nc\\nbcc#\\nac#c#b\\nac\\nc\\nacc#\\naca#\\n#ac\\nc\\nab\\naa\\nc\\nabc\\nac#\\n#ac\\ncc#\\n#ac\\ncb\\nab\\nb#a\\n#\\nc\\nb\\na\\n#\\nc\\nb\\na\\n#\\nF L\\n# a b c c\\n# a c c b\\n# b a c a\\n# c a a c\\n# c b c c\\na # b a c\\na a c # c\\na b c c #\\na c # c a\\na c a # b\\na c c b #\\nb # a c c\\nb a c a #\\nb c c # a\\nb c c # c\\nc # a b c\\nc # c a a\\nc # c b c\\nc a # b a\\nc a a c #\\nc b # a c\\nc b c c #\\nc c # a b\\nc c # c b\\nc c b # a\\nFig. 4 The matrix M4 for our variant of the Burrows-Wheeler Transform Bwt (right) which is obtained by\\nsorting lexicographically all the cyclic rotations of strings in the set D4 = {abcc,accb,baca,caac,cbcc}.\\nThe resulting Burrows-Wheeler Transform is the last column of this matrix (i.e., Bwt4 = L). The Figure\\nshows also the compact trie built on all these cyclic rotations. Dashed arrows show the existing relation\\nbetween T4 and M4.\\nIn the following we will only describe the solution for insertion since deletion and\\nsubstitution are solved in a similar way. The basic idea behind our searching algo-\\nrithm is the following. For each cyclic rotation Pi = P[i, p]#P[1, i−1] of P[1, p]#, we\\nuse the compressed permuterm index Rp+1 to identify the range of rows (say, [l,r])\\nof Mp+1 that are prefixed by Pi, if any, where Mp+1 is the matrix for the strings in\\nDp+1. We observe that having that range suffices for identifying the strings in D ob-\\ntained by inserting a symbol at the ith position of P. These symbols are, indeed, the\\nones contained in Bwtp+1[l,r], where Bwtp+1 is the Burrows-Wheeler Transform for\\nthe strings in Dp+1. However, we cannot compute all these ranges in a naı¨ve way by\\nsearching each Pi separately using the backward search, since it would cost Ω(p2)\\ntime. Thus, a faster solution has to efficiently move from rows prefixed by Pi to rows\\nprefixed by Pi−1. This is achieved by augmenting the compressed permuterm index\\nwith a data structure that supports the two operations: parent and depth on a (con-\\nceptual) compact trie Tp+1 which indexes all the cyclic rotations of strings in Dp+1.\\nThe trie for our set of five strings D4 is shown in Figure 4. There exists a very strong\\nrelation between Tp+1 and Mp+1: the locus of the ith leaf of Tp+1 is equal to the\\nith row of Mp+1. Moreover, any internal node u of Tp+1 is in correspondence with a\\n17\\nrange of rows in Mp+1 (namely, the rows corresponding to the leaves in the subtree\\nrooted at u). These rows share a longest common prefix which is equal to the locus\\nof u.\\nLet u be a node of the above trie corresponding a range of rows [l,r] in Mp+1.\\nThe two operations are defined as follows:\\n1. parent(u) returns the range [l′,r′] corresponding to the parent of the node u;\\n2. depth(u) returns the length of the locus of node u.\\nIt is possible to support both these operations in O(logσ n log logn) time by re-\\nquiring O(nˆ logσlog logn ) bits of additional space when σ = O(poly(w)) [31], where nˆ is\\nthe total size of the indexed dictionary.\\nOur solution works in two phases. In the first phase, it identifies the range of rows\\nof Bwtp+1 sharing the longest common prefix with P0 = #P[1, p]. This is done by\\nusing the following strategy. We search P0 backwards. At any step j, we keep the\\nfollowing invariant: [l j,r j] is the range of all rows of Bwtp+1 prefixed by q j, where\\nq j is the longest prefix of P0[p− j+ 2, p+ 1] that is a prefix of at least one row of\\nBwtp+1. We also keep a counter ` that tells us the length of q j. Notice that it may\\nhappen that a backward step from [l j,r j] with the next symbol P[p− j+1] returns an\\nempty range. In this case, we repeatedly enlarge the range [l j,r j] via parent operations\\nuntil the subsequent backward step is successful. The value of ` is kept updated by\\nincreasing it by one after every successful backward step or by setting it equal to the\\nvalue returned by depth after every call to parent. This approach has been already\\nused to compute the so-called matching statistics [29].\\nSimilarly, the second phase matches rotations of P# backwards. The main dif-\\nference is given by the fact that the starting range and the value of ` are the ones\\ncomputed at the end of the previous phase. At each step i, we identify the range of\\nrows [li,ri] that share the maximal common prefix with P[i, p]#P[1, p]. The correct-\\nness of each step follows from the following fact.\\nFact 2 The range of rows prefixed by Pi = P[i, p]#P[1, i−1] is non-empty if and only\\nif the value of ` reaches p+1 or p+2.\\nProof In the former case, the obtained range [li,ri] is clearly the range of rows pre-\\nfixed by Pi = P[i, p]#P[1, i− 1]. In the latter case8, we need one additional step. In\\norder to identify Pi = P[i, p]#P[1, i−1], we enlarge the range [li,ri] via parent opera-\\ntion, apply depth operation on it and check whether the returned value equals p+1.\\nIf that is the case, then the range of rows prefixed by Pi = P[i, p]#P[1, i− 1] is the\\nenlarged range. Otherwise, it is the original range. uunionsq\\nEven if this solution works for any alphabet size, we state its time and space\\ncomplexities in point 1 of Theorem 4 for σ = O(poly(w)) only. The overall time\\ncomplexity of these two phases is O(p logσ n log logn), since we have at most 2p\\ncalls to parent and depth which dominate the cost of the O(p) calls to rank in the\\nbackward search. The overall space occupancy is nHk +o(n) bits for the compressed\\n8 Notice that this case occurs only when PiP[i] ∈ D. In order to properly deal with this case, the value\\nof ` is not increased after a successful backward step if it already reached the maximal value p+2.\\n18\\npermuterm indexes and O(n logσlog logn ) bits for the data structures to support parent and\\ndepth operations. This proves Point 1 of Theorem 4.\\nA better solution for larger alphabet sizes is reported in point 2 of Theorem 4.\\nThis solution is obtained by showing that, if we are allowed to use more space, a\\nfaster solution is possible. More precisely, we can improve the time of parent (for all\\ncases) and depth (for the case of large depths) by augmenting every permuterm index\\nR` with some auxiliary data structures:\\n1. The operation parent can be supported in constant time using O(nˆ) additional bits\\nof space. This is feasible by using Sadakane’s compressed suffix tree [32] on top\\nthe permuterm index R`.\\n2. The operation depth can be supported in constant time using O(nˆ log t) bits of\\nspace when the string depth is at least `− t, for some parameter t. For this aim,\\nwe resort to a table ∆` which stores log(t +1) bits per node. For any node u, we\\nstore the difference between the depth of the node u and p whenever the string\\ndepth of u is at least `− t. Otherwise, we store a special symbol indicating that\\nthe string depth of u is less than `− t.\\nNow that we have a constant time parent operation, the depth operation remains\\nas the only bottleneck for achieving faster query time. Indeed, for larger alphabets,\\neach depth operation requires O(logσ n(log logn)\\n2) time and uses O(nˆ logσlog logn ) bits\\nof additional space [31]. To circumvent this, we introduce a lazy strategy that com-\\nputes the correct value of ` only whenever its value may be p+ 1 or p+ 2 (i.e, we\\nhave a match), thus, avoiding most of the calls to depth operation. Assume that the\\ncompressed suffix tree supports the depth operation in time t. Instead of performing\\ndepth operation after each parent, the algorithm keeps track of the last node u ob-\\ntained by parent operation with an associated variable du, which may be undefined.\\nThe value is always the depth of u whenever u’s depth is at least p+1− t, but may be\\nundefined otherwise. Every time we perform a parent operation, we use table ∆` to\\ntry to compute in constant time the value of du, but we set it to undefined whenever\\n∆` does not contain its depth (i.e., ∆` returns the special symbol). The algorithm also\\nkeeps track of the number t ′ of successive backward step after the last parent oper-\\nation. This way, if du is defined, we can compute ` as du + t ′, this is because every\\nbackward step after the last parent operation have increased it by one. Instead, if du\\nis undefined, at least t backward steps are required for ` to be at least p+1. Thus, we\\ncompute the value du with a depth operation as soon as t ′ becomes equal to t. This\\nway, we can check whether `= du+ t ′ is at least p+1.\\nIt follows that a depth operation is computed only after exactly t successive back-\\nward steps from the last parent operation and that the result is kept for the subsequent\\nsuccessive backward steps. As a first consequence of this fact, the cost of depth oper-\\nation can be amortized over the t successive backward steps. Another consequence is\\nthat the value du is always defined whenever t ′ ≥ t. The correctness of the algorithm\\nfollows by observing that the range may correspond to a Pi only if either t ′ ≥ t or ∆`\\ndoes not return the special symbol, and in both cases, du will be defined.\\nPoint 2 of Theorem 4 is obtained by setting t =O(logσ n(log logn)\\n2). Indeed, the\\nbackward search becomes the dominant time cost, namely, O(p log logwσ) time ac-\\ncording to Theorem 2. To the space occupancy of Theorem 2 we have to add O(n) bits\\n19\\nfor the Sadakane’s compressed suffix trees and O(n log t) =O(n log(logσ n log logn))\\nbits to store the tables ∆`.\\n6 Randomized solutions\\nIn this section we provide two randomized solutions. The first one is a Monte Carlo\\nsolution which may report O(ε) false positives in expectation (i.e., spurious non ex-\\nisting occurrences). Formally, we prove the following theorem.\\nTheorem 5 Given a set D = {S1,S2, . . . ,Sd} of d strings of total length n drawn\\nfrom an alphabet Σ = [σ ], σ ≤ n, there exists an index that, given any pattern P[1, p],\\nreports all the occ strings in D having edit distance at most one with P in O(p+\\nocc) randomized time. A query may report O(ε) false positives in expectation (i.e.,\\nspurious non existing occurrences), for any parameter ε with 0 < ε < 1. The index\\nhas size nHk+O(\\nn(log logn+logσ) log logn\\nlogn )+O(d log\\n1\\nε ) bits, for any fixed k = o(logσ n).\\nThe second solution is a Las Vegas solution which guarantees the space and time\\ncomplexities reported in the following theorem.\\nTheorem 6 Given a set D= {S1,S2, . . . ,Sd} of d strings of total length n drawn from\\nan alphabet Σ = [σ ], σ ≤ n and let w be the size of a memory word, there exists an\\nindex of size nHk +ρ+O(d log 1ε ) bits, for any k ≤ α logσ n with 0< α < 1 and any\\nparameter ε with 0 < ε < 1, such that, given any pattern P[1, p] reports all the occ\\nstrings in D having edit distance at most one with P in\\n1. O(p+ occ) time with probability ε and redundancy ρ = O(n) bits, for σ =\\nO(poly(w));\\n2. O(p log logwσ+occ) time with probability ε and redundancy ρ =O(n)+o(n)(1+\\nHk) bits, for larger σ and logσ = O(logn/ log logn).\\nNotice that these solutions could provide strong probabilistic guarantees by set-\\nting ε = 1nc for some constant c at the expense of using O(d logn) more bits of space.\\nIn this case the first solution only returns O( 1nc ) spurious occurrences in expectation,\\nwhile the second one guarantees that the query time holds with high probability.\\nMonte Carlo solution. In this paragraph we show how to derive a Monte Carlo so-\\nlution from the result of Theorem 3 in Section 4. The possibility of returning spu-\\nrious answers combined with the use of approximate membership data structures\\nsuffices for reducing the dominant term in the space of this solution from 2nHk to\\nnHk without increasing the query time. Fix the parameter ε and build an approximate\\nmembership AM` for each subset of strings D` of length ` by fixing the false pos-\\nitive probability to ε` . According to Lemma 4 the space of AM` is O(log`+ log\\n1\\nε )\\nbits for each string in D`. Overall the space used by all the approximate member-\\nship data structures is O(d(log nd + log\\n1\\nε )) bits, which is O(\\nn logσ log logn\\nlogn + d log\\n1\\nε )\\nbits according to Fact 1. The presence of these data structures and the relaxed goal\\nallow us to remove the compressed scheme of Lemma 1 and the permutation pi\\n20\\nin the solution of Section 4. This reduces the space occupancy in Theorem 3 to\\nnHk +O(\\nn(log logn+logσ) log logn\\nlogn )+O(d log\\n1\\nε ) bits.\\nBecause of the suppression of the compressed string representations, the correct-\\nness of the steps performed during the percolation of the patricia tries can no longer be\\nestablished by comparing the searched string and labels on the traversed edges. How-\\never, the percolation never introduces any false negative. Indeed, if lcp(|P|,pleaf) is\\nat least i, then npi is correctly computed. Similarly, if lcpr(|P|,sleaf) is at least p− i,\\nthen nsi+1 is also correctly computed. This is because we can always follow the cor-\\nrect edge thanks to the monotone minimal perfect hash function stored at each node.\\nNotice that a tuple 〈npi, i,c, p− i,nsi+1〉 can exist in T only if lcp(|P|,pleaf)≥ i and\\nlcpr(|P|,sleaf)≥ p− i. Thus, we conclude that the traversal of the trie introduces no\\nfalse negatives.\\nAlso although it is no longer possible to establish whether a candidate string\\nbelongs to D, we can use the approximate membership data structure to ensure that\\na non-existing string is reported with probability at most ε . The check for a single\\ncandidate string S = P[1, i] · c ·P[i+ 1, p] obtained by inserting symbol c in the (i+\\n1)th position is done simply by querying AMp+1. Notice that a single non-existing\\ncandidate is reported only with probability ε` . Moreover, a second candidate S =\\nP[1, i] · c′ ·P[i+ 1, p] with c′ 6= c is checked only if the first candidate was reported\\nas existing 9. Thus, on expectation the number of false positive candidates obtained\\nby inserting a symbol in the (i+1)th position is bounded by O( ε` ). This gives O(ε)\\nfalse positive candidates when summing up over all positions of insertion. Similar\\nconsiderations can be used to deal with substitutions and deletions.\\nLas Vegas solution. We now present the Las Vegas solution of Theorem 6 which is\\nobtained by introducing randomization in the solution of Section 5. The goal here is\\nto remove the use of depth operation whose time complexity was the dominant cost in\\nTheorem 4. Similarly to the previous randomized solution, we build an approximate\\nmembership data structure AM` for each set D`. As in Section 5 we use Sadakane’s\\ncompressed suffix tree [32] to support parent operation by using a constant number\\nof bits per symbol. Given a pattern P[1, p], we show how to find insertions using the\\ncompressed permuterm index Rp+1 in conjunction with the approximate member-\\nship data structure AMp+1, the parent operation, and the decompression of strings\\nwith LF steps on Bwtp+1. As for the other solutions, substitutions and deletions are\\nsolved with a similar approach. Recall from Section 5 that our goal is to identify the\\nrange of rows (say, [l,r]) of Mp+1 that are prefixed by Pi, for each cyclic rotation\\nPi = P[i, p]#P[1, i−1] of P[1, p]#. Indeed, the symbols to be inserted at position i of P\\nare the ones contained in Bwtp+1[l,r]. As in Section 5 the solution identifies, for each\\ni, the range of rows sharing the maximal common prefix with P[i, p]#P[1, p] by com-\\nbining the use of Backward search and parent operation. The solution in Section 5\\nestablishes that one of these ranges contains answers for insertion by checking that\\n9 If we have false positives then the same character may be checked and thus potentially reported twice.\\nTo avoid this case, we can use a dynamic hash table at query time which stores all the characters reported\\nso-far. Whenever we find that a character has been already reported, then the query stops and does not\\nreport more characters, since a correct query answer can not return the same character twice at the same\\nposition.\\n21\\ndepth operation returns a value at least p+1 (i.e., the length of the maximal common\\nprefix above is at least p+ 1). We show here how to replace the use of depth oper-\\nation with LF steps. Indeed, we use the function LF to extract symbols backwards\\nfrom one position in each of these ranges to check whether the depth is at least p+1.\\nNotice that doing this in a naı¨ve way would require to extract Ω(p2) symbols. In the\\nfollowing we show an approach that extracts only O(p) symbols from ranges having\\ndepth at least p+ 1. Proper queries to AMp+1 are used to filter (most of the) ranges\\nwith a shorter depth.\\nMore in detail, let [li,ri] denote the range sharing the maximal common prefix\\nwith P[i, p]#P[1, p] computed with the only use of Backward search and parent op-\\neration. For each i, we query AMp+1 for the candidate string P[1, i− 1] · c ·P[i, p]\\nobtained by inserting the symbol c in ith position of P, where c is any symbol in\\nBwtp+1[li,ri]. This is the preliminary filter that removes a range with depth smaller\\nthan p+1 with probability at least 1−ε/p. Let I = {i1, i2, . . . , it}, with i1 < i2, . . . < it ,\\nbe set of indices for which AMp+1 returns true. A naı¨ve approach would work as fol-\\nlows. For each index i in I, it starts to extract p+ 2 symbols with p+ 2 LF steps\\nstarting from any row in [li,ri] and computes the maximal common prefix between\\nthe extracted string and P[i, p]#P[1, p]. By definition, the depth of the range coin-\\ncides with the length of this common prefix. Unfortunately, this approach requires\\nO(p) LF steps for each of the O(p) indices in I. In our solution we use the ap-\\nproach above starting from the smallest index i1 to check that the range of rows\\nM[li1 ,ri1 ] is prefixed by Pi1 = P[i1, p]#P[1, i1− 1] . If this check succeeds, then we\\nknow that for any other index j > i1, the range of rows M[l j,r j] and P[ j, p]#P[1, p]\\nshare a prefix of length at least p+1− ( j− i1). Indeed, if we rotate the range of rows\\nM[li1 ,ri1 ] by j− i1 positions to the left, we get a range prefixed by P[ j, p]#P[1, i1−1]\\nthat shares a prefix with P[ j, p]#P[1, p] of length at least p+ 1− ( j− i1). Thus, we\\ncan check whether M[li2 ,ri2 ] is prefixed by Pi2 = P[i2, p]#P[1, i2− 1] by performing\\nonly i2− i1 + 1 LF steps to check whether any row in M[li2 ,ri2 ] is preceded by the\\nstring P[i1, i2− 1]c where c is some symbol in Bwtp+1[li2 ,ri2 ] (i.e., the row ends by\\nP[i1, i2−1]c). If the check for i1 fails (i.e., the range of rows M[li1 ,ri1 ] is not prefixed\\nby Pi1 = P[i1, p]#P[1, i1−1]), then the check for i2 would require p+2 LF steps. This\\nprocedure is repeated for the subsequent indices in I.\\nTo compute the number of LF steps required by this approach, it is convenient to\\nsplit the set I in the two subsets Ia and Ib that contain indices that pass and fail the\\nabove check, respectively. Assume Ia = {i′1, i′2, . . . , i′t ′}, with i′1 < i′2 < .. . < i′t ′ . The\\ncheck for i′1 asks to perform p+ 2 LF steps. The check for any other i\\n′\\nj > i\\n′\\n1 in Ia is\\ndone by making i′j− i′j−1 +1 LF steps. Summing up over all the elements in Ia, this\\ngives O(p) LF steps, which, by using the data structure in Lemma 5, require O(p)\\nor O(p log logwσ) time depending on the alphabet size. Indices in Ib, instead, require\\nO(p) LF steps each. However, since the AMp+1 has a false positive probability εp+2 ,\\nIb contains on expectation O(ε) indices, incurring O(ε · p) additional LF steps.\\nWe have to address a small technical detail to conclude the proof of Theorem 5.\\nOnce we have determined the indices i such that the rows in M[li,ri] are prefixed by\\nPi = P[i, p]#P[1, i−1], we need one more step. If li < ri, we report all the symbols in\\nBwtp+1[li,ri] because we are sure that S= P[1, i−1] ·c ·P[i, p] is a correct answer. On\\nthe other hand, if ri = li, it may happen that the row M[li] equals P[i, p]#P[1, i−1]P[i].\\n22\\nIn this case, there may exist a larger range of rows prefixed by P[i, p]#P[1, i−1] that\\nlead to correct answers. This larger range is identified by taking the parent for the\\ninterval [li,ri] and by checking that the resulting new interval corresponds to a node\\nof depth exactly p+ 1. If it is the case, we replace [li,ri] with the larger interval.\\nInstead of computing the depth for that node, we associate one additional bit to mark\\nevery internal node of the trie that has depth exactly p+ 1. This solves the issue by\\nadding O(n) additional bits.\\n7 Conclusion\\nIn this paper we described two different compressed solutions for searching with\\nedit distance one in a dictionary of strings. The first solution requires 2Hk(S) +\\nn · o(logσ) + 2d logd bits of space for any fixed k = o(logσ n). The query time\\nis (almost optimal) O(|P|+ occ) time where occ is the number of strings in the\\ndictionary having edit distance at most one with the query pattern P. The second\\nsolution further improves this space complexity but the time complexity grows to\\nO(|P| logσ n log logn+ occ) or O(|P| log logwσ + occ) depending on the amount of\\nredundancy, where w is the size of a memory word. Interestingly enough, the two so-\\nlutions solve the problem at hand with two different approaches: the former is based\\non (perfect) hashing while the latter is based on the compressed permuterm index.\\nFinally, we have shown how to introduce randomization in these solutions to derive\\nMonte Carlo and Las Vegas solutions in order to either reduce the space occupancy\\nor improve the query time of the deterministic solutions.\\nAn interesting open problem asks for a deterministic approach that obtains si-\\nmultaneously the time complexity of our first deterministic solution and the space\\ncomplexity of the second one. Furthermore, it is still open whether one can design a\\nsolution that solves the problem in O(|P| · logσ/w+occ) time. At the moment, there\\ndoes not exist any solution achieving such a time complexity, even a non compressed\\nsolution (using say O(n polylog(n)) space).\\nFinally, building efficient dictionaries for edit distance d larger than 1 is still an\\nopen problem. However, the approaches we used in our two solutions are not eas-\\nily extendible to efficiently solve query for higher edit distance. Indeed, we could\\njust solve a query in O(σd−1|P|d + occ) time for edit distance d by resorting to the\\nstandard dynamic programming approach.\\nReferences\\n1. Amihood Amir, Dmitry Keselman, Gad M. Landau, Moshe Lewenstein, Noa\\nLewenstein, and Michael Rodeh. Text indexing and dictionary matching with\\none error. Journal of Algorithms, 37(2):309–325, 2000.\\n2. Je´re´my Barbay, Meng He, J. Ian Munro, and Srinivasa Rao Satti. Succinct in-\\ndexes for strings, binary relations and multilabeled trees. ACM Transactions on\\nAlgorithms, 7(4):52, 2011.\\n23\\n3. Jrmy Barbay, Francisco Claude, Travis Gagie, Gonzalo Navarro, and Yakov\\nNekrich. Efficient fully-compressed sequence representations. Algorithmica,\\n69(1):232–268, 2014.\\n4. Djamal Belazzougui. Faster and space-optimal edit distance ”1” dictionary. In\\nProceedings of the 20th Annual Symposium on Combinatorial Pattern Matching\\n(CPM), pages 154–167, 2009.\\n5. Djamal Belazzougui. Improved space-time tradeoffs for approximate full-text\\nindexing with one edit error. Algorithmica, pages 1–27, 2011.\\n6. Djamal Belazzougui and Gonzalo Navarro. New lower and upper bounds for\\nrepresenting sequences. In Proceedings of the 20th Annual European Symposium\\non Algorithms (ESA), pages 181–192, 2012.\\n7. Djamal Belazzougui and Gonzalo Navarro. Alphabet-independent compressed\\ntext indexing. ACM Transactions on Algorithms, 10(4):23, 2014.\\n8. Djamal Belazzougui and Rossano Venturini. Compressed string dictionary look-\\nup with edit distance one. In Proceedings of the 23rd Annual Symposium on\\nCombinatorial Pattern Matching (CPM), pages 280–292, 2012.\\n9. Djamal Belazzougui and Rossano Venturini. Compressed static functions with\\napplications. In Proceedings of the 24th Annual ACM-SIAM Symposium on Dis-\\ncrete Algorithms (SODA), pages 229–240, 2013.\\n10. Burton H. Bloom. Space/time trade-offs in hash coding with allowable errors.\\nCommunications of the ACM, 13(7):422–426, July 1970.\\n11. Gerth Stølting Brodal and Leszek Ga¸sieniec. Approximate dictionary queries. In\\nProceedings of the 7th Annual Symposium on Combinatorial Pattern Matching,\\npages 65–74. Springer Verlag, 1996.\\n12. Gerth Stølting Brodal and Venkatesh Srinivasan. Improved bounds for dictionary\\nlook-up with one error. Information Processing Letters, 75(1-2):57–59, 2000.\\n13. Michael Burrows and David Wheeler. A block sorting lossless data compression\\nalgorithm. Technical Report 124, Digital Equipment Corporation, 1994.\\n14. Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and Ayellet Tal. The bloomier\\nfilter: an efficient data structure for static support lookup tables. In Proceed-\\nings of the 15th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),\\npages 30–39, 2004.\\n15. Richard Cole, Lee-Ad Gottlieb, and Moshe Lewenstein. Dictionary matching\\nand indexing with errors and don’t cares. In Proceedings of the 36th Annual\\nACM Symposium on Theory of Computing (STOC), pages 91–100, 2004.\\n16. Martin Dietzfelbinger, Joseph Gil, Yossi Matias, and Nicholas Pippenger. Poly-\\nnomial hash functions are reliable (extended abstract). In Proceeding of the 19th\\nInternational Colloquium on Automata, Languages and Programming (ICALP),\\npages 235–246, 1992.\\n17. Peter Elias. Efficient storage and retrieval by content and address of static files.\\nJournal of the ACM, 21:246–260, 1974.\\n18. Robert Mario Fano. On the number of bits required to implement anassociative\\nmemory. Memorandum 61, Computer Structures Group, Project MAC, 1971.\\n19. Paolo Ferragina, Rodrigo Gonza´lez, Gonzalo Navarro, and Rossano Venturini.\\nCompressed text indexes: From theory to practice. ACM Journal of Experimental\\nAlgorithmics, 13, 2008.\\n24\\n20. Paolo Ferragina and Giovanni Manzini. Indexing compressed text. Journal of\\nthe ACM, 52(4):552–581, 2005.\\n21. Paolo Ferragina and Rossano Venturini. A simple storage scheme for strings\\nachieving entropy bounds. Theorectical Computer Science, 372(1):115–121,\\n2007.\\n22. Paolo Ferragina and Rossano Venturini. The compressed permuterm index. ACM\\nTransactions on Algorithms, 7(1):10, 2010.\\n23. Johannes Fischer and Volker Heun. Space-efficient preprocessing schemes\\nfor range minimum queries on static arrays. SIAM Journal on Computing,\\n40(2):465–492, 2011.\\n24. Torben Hagerup and Torsten Tholey. Efficient minimal perfect hashing in nearly\\nminimal space. In Proceedings of the 18th Annual Symposium on Theoretical\\nAspects of Computer Science (STACS), pages 317–326, 2001.\\n25. Richard M. Karp and Michael O. Rabin. Efficient randomized pattern-matching\\nalgorithms. IBM Journal of Research and Development, 31(2):249–260, 1987.\\n26. Giovanni Manzini. An analysis of the Burrows-Wheeler transform. Journal of\\nthe ACM, 48(3):407–430, 2001.\\n27. J. Ian Munro and Venkatesh Raman. Succinct representation of balanced paren-\\ntheses and static trees. SIAM Journal on Computing, 31(3):762–776, 2001.\\n28. Gonzalo Navarro and Veli Ma¨kinen. Compressed full text indexes. ACM Com-\\nputing Surveys, 39(1), 2007.\\n29. Enno Ohlebusch, Simon Gog, and Adrian Ku¨gel. Computing matching statistics\\nand maximal exact matches on compressed full-text indexes. In SPIRE, pages\\n347–358, 2010.\\n30. Anna Pagh, Rasmus Pagh, and S. Srinivasa Rao. An optimal bloom filter replace-\\nment. In Proceedings of the 16th Annual ACM-SIAM Symposium on Discrete\\nAlgorithms (SODA), pages 823–829, 2005.\\n31. Luı´s M. S. Russo, Gonzalo Navarro, and Arlindo L. Oliveira. Fully compressed\\nsuffix trees. ACM Transactions on Algorithms, 7(4):53, 2011.\\n32. Kunihiko Sadakane. Compressed suffix trees with full functionality. Theory of\\nComputing Systems, 41(4):589–607, 2007.\\n33. Ian H. Witten, Alistair Moffat, and Timothy C. Bell. Managing Gigabytes: Com-\\npressing and Indexing Documents and Images. Morgan Kaufmann Publishers,\\n1999.\\n34. Andrew Chi-Chih Yao and Frances F. Yao. Dictionary look-up with one error.\\nJournal of Algorithms, 25(1):194–202, 1997.\\n'}\n",
      "{'_id': ObjectId('6815e61add259dc7e6e1cd2f'), 'authors': 'Agarwal, Udit, Ramachandran, Vijaya', 'year': '2019', 'title': 'New and Simplified Distributed Algorithms for Weighted All Pairs\\n  Shortest Paths', 'full_text': 'ar\\nX\\niv\\n:1\\n81\\n0.\\n08\\n54\\n4v\\n1 \\n [c\\ns.D\\nS]\\n  1\\n8 O\\nct \\n20\\n18\\nNew and Simplified Distributed Algorithms for Weighted All\\nPairs Shortest Paths\\nUdit Agarwal ⋆ and Vijaya Ramachandran∗\\nOctober 22, 2018\\nAbstract\\nWe consider the problem of computing all pairs shortest paths (APSP) and shortest paths for\\nk sources in a weighted graph in the distributed Congest model. For graphs with non-negative\\ninteger edge weights (including zero weights) we build on a recent pipelined algorithm [1] to\\nobtain a O˜(λ1/4 · n5/4)-round bound for graphs with edge-weight at most λ, and O˜(n · △1/3)-\\nround bound for shortest path distances at most △. Additionally, we simplify some of the\\nprocedures in the earlier APSP algorithms for non-negative edge weights in [8, 2]. We also\\npresent results for computing h-hop shortest paths and shortest paths from k given sources.\\nIn other results, we present a randomized exact APSP algorithm for graphs with arbitrary\\nedge weights that runs in O˜(n4/3) rounds w.h.p. in n, which improves the previous best O˜(n3/2)\\nbound, which is deterministic. We also present an O˜(n/ǫ2)-round deterministic (1 + ǫ) ap-\\nproximation algorithm for graphs with non-negative poly(n) integer weights (including zero\\nedge-weights), improving results in [13, 11] that hold only for positive integer weights.\\n1 Introduction\\nDesigning distributed algorithms for various network and graph problems such as shortest paths [2,\\n8, 12, 14, 10] is a extensively studied area of research. The Congest model (described in Sec 1.2)\\nis a widely-used model for these algorithms, see [2, 5, 8, 12]. In this paper we consider distributed\\nalgorithms for the computing all pairs shortest paths (APSP) and related problems in a graph with\\nnon-negative edge weights in the Congest model.\\nIn sequential computation, shortest paths can be computed much faster in graphs with non-negative\\nedge-weights (including zero weights) using the classic Dijkstra’s algorithm [4] than in graphs with\\nnegative edge weights. Additionally, negative edge-weights raise the possibility of negative weight\\ncycles in the graph, which usually do not occur in practice, and hence are not modeled by real-world\\nweighted graphs. Thus, in the distributed setting, it is of importance to design fast shortest path\\nalgorithms that can handle non-negative edge-weights, including edges of weight zero.\\nThe presence of zero weight edges creates challenges in the design of distributed algorithms as\\nobserved in [8]. (We review related work in Section 1.3.) One approach used for positive integer\\nedge weights is to replace an edge of weight d with d unweighted edges and then run an unweighted\\n∗Dept. of Computer Science, University of Texas, Austin TX 78712. Email: udit@cs.utexas.edu,\\nvlr@cs.utexas.edu. This work was supported in part by NSF Grant CCF-1320675.\\n1\\nAPSP algorithm such as [12, 14] on this modified graph. This approach is used in approximate APSP\\nalgorithms [13, 11]. However such an approach fails when zero weight edges may be present. There\\nare a few known algorithms that can handle zero weights, such as the O˜(n5/4)-round randomized\\nAPSP algorithm of Huang et al. [8] (for polynomially bounded non-negative integer edge weights)\\nand the O˜(n3/2)-round deterministic APSP algorithm of Agarwal et al. [2] (for graphs with arbitrary\\nedge weights including zero weights). A deterministic pipelined algorithm for this problem that runs\\nin at most 2 · n√∆+2n was recently given in [1], where ∆ is an upper bound on the shortest path\\nlength.\\n1.1 Our Results\\nWe present several new results for computing APSP and related problems on an n-node graph\\nG = (V,E) with non-negative edge weights w(e), e ∈ E, including deterministic distributed sub-\\nn3/2-round algorithms for moderate weights (including zero weights) [1]. All of our results hold for\\nboth directed and undirected graphs and we will assume w.l.o.g. that G is directed.\\nMany of our results build on a recent deterministic distributed pipelined algorithm we developed\\nfor APSP and k-SSP for graphs with non-negative integer weights (including zero weights) [1]. This\\nalgorithm computes the h-hop shortest path problem for k sources ((h, k)-SSP), with an additional\\nconstraint that the shortest paths have distance at most △ in G, together with the corresponding\\nshortest path trees, defined as follows.\\nDefinition 1.1. An h-hop shortest path from u to v in G is a path from u to v of minimum weight\\namong all paths with at most h edges (or hops).\\nAn h-SSP tree for source s and shortest path distance ∆ is a tree rooted at s that contains an h-hop\\nshortest path from s to every other vertex to which there exists an h-hop path with weight at most △\\nin G. In the case of multiple h-hop shortest paths from s to a vertex v, this tree contains the path\\nwith the smallest number of hops, breaking any further ties by choosing the predecessor vertex with\\nsmallest ID.\\nThe pipelined algorithm achieves the bounds in the following theorem.\\nTheorem 1.2. [1] Let G = (V,E) be a directed or undirected edge-weighted graph, where all edge\\nweights are non-negative integers (with zero-weight edges allowed). The following deterministic\\nbounds can be obtained in the Congest model for shortest path distances at most △.\\n(i) (h, k)-SSP in 2\\n√△kh+ k + h rounds.\\n(ii) APSP in 2n\\n√△+ 2n rounds.\\n(iii) k-SSP in 2\\n√△kn+ n+ k rounds.\\nThe new results we present in this paper are the following.\\n1. Faster Deterministic APSP for Non-negative, Moderate Integer Weights. We improve\\non the bounds given in (ii) and (iii) of Theorem 1.2 by combining the pipelined algorithm in [1]\\nwith a modified version of the APSP algorithm in [2] to obtain our improved Algorithm 1, with the\\nbounds stated in the following Theorems 1.3 and 1.4. To obtain these improved bounds we also\\npresent an improved deterministic distributed algorithm to find a ‘blocker set’ [2].\\nTheorem 1.3. Let G = (V,E) be a directed or undirected edge-weighted graph, where all edge\\nweights are non-negative integers bounded by λ (with zero-weight edges allowed). The following\\n2\\ndeterministic bounds can be obtained in the Congest model.\\n(i) APSP in O(λ1/4 · n5/4 log1/2 n) rounds.\\n(ii) k-SSP in O(λ1/4 · nk1/4 log1/2 n) rounds.\\nTheorem 1.4. Let G = (V,E) be a directed or undirected edge-weighted graph, where all edge\\nweights are non-negative integers (with zero edge-weights allowed), and the shortest path distances\\nare bounded by △. The following deterministic bounds can be obtained in the Congest model.\\n(i) APSP in O(n(△ log2 n)1/3) rounds.\\n(ii) k-SSP in O((△kn2 log2 n)1/3) rounds.\\nOur results in Theorem 1.3 and 1.4 improve on the O˜(n3/2) deterministic APSP bound of Agarwal\\net al. [2] for significant ranges of values for both λ and ∆, as stated below.\\nCorollary 1.5. Let G = (V,E) be a directed or undirected edge-weighted graph with non-negative\\nedge weights (and zero-weight edges allowed). The following deterministic bounds hold for the Con-\\ngest model for 1 ≥ ǫ ≥ 0.\\n(i) If the edge weights are bounded by λ = n1−ǫ, then APSP can be computed in O(n3/2−ǫ/4 log1/2 n)\\nrounds.\\n(ii) For shortest path distances bounded by ∆ = n3/2−ǫ, APSP can be computed in O(n3/2−ǫ/3 log2/3 n)\\nrounds.\\nThe corresponding bounds for the weighted k-SSP problem are: O(n5/4−ǫ/4k1/4 log1/2 n) (when\\nλ = n1−ǫ) and O(n7/6−ǫ/3k1/3 log2/3 n) (when∆ = n3/2−ǫ). Note that the result in (i) is independent\\nof the value of ∆ (depends only on λ) and the result in (ii) is independent of the value of λ (depends\\nonly on ∆).\\n2. Simplifications to Earlier Algorithms. Our techniques give simpler methods for some\\nof procedures in the two previous distributed weighted APSP algorithms that handle zero weight\\nedges. In Section 3 we present simple deterministic algorithms that match the congest and dilation\\nbounds in [8] for two of the three procedures used there: the short-range and short-range-extension\\nalgorithms. Our simplified algorithms are both obtained using a streamlined single-source version\\nof the pipelined APSP algorithm in [1].\\nA key contribution in the deterministic APSP algorithm in [2] is a fast deterministic distributed\\nalgorithm for computing a blocker set. The performance of the blocker set algorithm in [2] does not\\nsuffice for our faster APSP algorithms (Theorems 1.3 and 1.4). In Section 2 we present a faster\\nblocker set algorithm, which is also a simplification of the blocker set algorithm in [2]. The improved\\nbound that we obtain here for computing a blocker set will not improve the overall bound in [2],\\nbut our method could be used there to achieve the same bound with a more streamlined algorithm.\\n3. Faster (Randomized) APSP for Arbitrary Edge-Weights. For exact APSP in directed\\ngraphs with arbitrary edge-weights the only prior nontrivial result known is the O˜(n3/2)-round\\ndeterministic algorithm in [2]. We present an algorithm with the following improved randomized\\nbound in Section 4.1.\\nTheorem 1.6. Let G = (V,E) be a directed or undirected edge-weighted graph with arbitrary edge\\nweights. Then, we can compute weighted APSP in G in the Congest model in O˜(n4/3) rounds,\\nw.h.p. in n.\\nThe corresponding bound for k-SSP is O˜(n + n2/3k2/3).\\n3\\nTable 1: Table comparing our new results for non-negative edge-weighted graphs (including zero edge weights) with previous\\nknown results. Here λ is the maximum edge weight and ∆ is the maximum weight of a shortest path in G.\\nProblem: Exact Weighted APSP\\nAuthor Arbitrary/ handle zero Randomized/ Undirected/ Round\\nInteger weights weights Deterministic (Directed & Undirected) Complexity\\nHuang et al. [8] Integer Yes Randomized Directed & Undirected O˜(n5/4)\\nElkin [5] Arbitrary Yes Randomized Undirected O˜(n5/3)\\nAgarwal et al. [2] Arbitrary Yes Deterministic Directed & Undirected O˜(n3/2)\\nThis paper\\nInteger Yes Deterministic Directed & Undirected\\nO˜(n3/2−ǫ/4) (when λ ≤ n1−ǫ)\\nO˜(n3/2−ǫ/3) (when ∆ ≤ n3/2−ǫ)\\nArbitrary Yes Randomized Directed & Undirected O˜(n4/3)\\nProblem: (1 + ǫ)-Approximation Weighted APSP\\nNanongkai [13] Integer No Randomized Directed & Undirected O˜(n/ǫ2)\\nLenzen & Integer No Deterministic Directed & Undirected O˜(n/ǫ2)\\nPatt-Shamir [11]\\nThis paper Integer Yes Deterministic Directed & Undirected O˜(n/ǫ2)\\n4. Approximate APSP for Non-negative Edge Weights. In Section 4.2 we present an\\nalgorithm that matches the earlier bound for computing approximate APSP in graphs with positive\\ninteger edge weights [13, 11] by obtaining the same bound for non-negative edge weights.\\nTheorem 1.7. Let G = (V,E) be a directed or undirected edge-weighted graph, where all edge\\nweights are non-negative integers polynomially bounded in n, and where zero-weight edges are al-\\nlowed. Then, for any ǫ > 0 we can compute (1 + ǫ)-approximate APSP in O((n/ǫ2) · log n) rounds\\ndeterministically in the Congest model.\\nRoadmap. The rest of the paper is organized as follows. In Sections 1.2 and 1.3 we review the\\nCongest model and discuss related work. In Section 2 we present our faster APSP and k-SSP\\ndeterministic distributed algorithms, including our improved deterministic method to compute a\\nblocker set. Section 3 describes our simple algorithms for the short-range and short-range extension\\nproblems from Huang et al. [8]. Section 4 presents our results that give Theorems 1.6 and 1.7, and\\nwe conclude with Section 5.\\n1.2 Congest Model\\nIn the Congest model, there are n independent processors interconnected in a network by bounded-\\nbandwidth links. We refer to these processors as nodes and the links as edges. This network is\\nmodeled by graph G = (V,E) where V refers to the set of processors and E refers to the set of links\\nbetween the processors. Here |V | = n and |E| = m.\\nEach node is assigned a unique ID between 1 and poly(n) and has infinite computational power.\\nEach node has limited topological knowledge and only knows about its incident edges. For the\\ninteger-weighted APSP problem we consider, each edge has a non-negative integer weight (zero\\nweights allowed) that can be represented with B = O(log n) bits. Also if the edges are directed,\\nthe corresponding communication channels are bidirectional and hence the communication network\\ncan be represented by the underlying undirected graph UG of G (this is also the assumption used\\nin [8, 7, 2]). The pipelined algorithm in [1] does not need this feature, and uses only the directed\\nedges in the graph for communication.\\n4\\nThe computation proceeds in rounds. In each round each processor can send an O(log n)-bit message\\nalong edges incident to it, and it receives the messages sent to it in the previous round. (If the graph\\nhas arbitrary edge weights, a node can send a constant number of distance values and node IDs\\nalong each edge in a message.) The model allows a node to send different message along different\\nedges though we do not need this feature in our algorithm. The performance of an algorithm in the\\nCongest model is measured by its round complexity, which is the worst-case number of rounds of\\ndistributed communication.\\n1.3 Related Work\\nWeighted APSP. The current best bound for the weighted APSP problem is due to the randomized\\nalgorithm of Huang et al. [8] that runs in O˜(n5/4) rounds. This algorithm works for graphs with\\npolynomially bounded integer edge weights (including zero-weight edges), and the result holds with\\nw.h.p. in n. For graphs with arbitrary edge weights, the recent result of Agarwal et al. [2] gives\\na deterministic APSP algorithm that runs in O˜(n3/2) rounds. This is the current best bound\\n(both deterministic and randomized) for graphs with arbitrary edge weights as well as the best\\ndeterministic bound for graphs with integer edge weights.\\nIn this paper we present an algorithm for non-negative integer edge-weights (including zero-weighted\\nedges) that runs in O˜(n△1/3) rounds where the shortest path distances are at most △ and in\\nO˜(n5/4λ1/4) rounds when the edge weights are bounded by λ. This result improves on the O˜(n3/2)\\ndeterministic APSP bound of Agarwal et al. [2] when either edge weights are at most n1−ǫ or shortest\\npath distances are at most n3/2−ǫ, for any ǫ > 0.\\nWe also give an improved randomized algorithm for APSP in graphs with arbitrary edge weights\\nthat runs in O˜(n4/3) rounds, w.h.p. in n.\\nWeighted k-SSP. The current best bound for the weighted k-SSP problem is due to the Huang\\net al’s [8] randomized algorithm that runs in O˜(n3/4 · k1/2 + n) rounds. This algorithm is also\\nrandomized and only works for graphs with integer edge weights. The recent deterministic APSP\\nalgorithm in [2] can be shown to give an O(n · √k log n) round deterministic algorithm for k-SSP.\\nIn this paper, we present a deterministic algorithm for positive including zero integer edge-weighted\\ngraphs that runs in O˜((△ · n2 · k)1/3) rounds where the shortest path distances are at most △ and\\nin O˜((λk)1/4n) rounds when the edge weights are bounded by λ.\\n(1+ ǫ)-Approximation Algorithms. For graphs with positive integer edge weights, deterministic\\nO˜(n/ǫ2)-round algorithms for a (1 + ǫ)-approximation to APSP are known [13, 11]. But these\\nalgorithms do not handle zero weight edges. In this paper we present a deterministic algorithm that\\nhandles zero-weight edges and matches the O˜(n/ǫ2)-round bound for approximate APSP known\\nbefore for positive edge weights.\\n2 Faster k-SSP Algorithm Using a Blocker Set\\nIn this section we give faster deterministic APSP and k-SSP algorithms than the O˜(n3/2) bound\\nin [2] for moderate non-negative edge weights (including zero weights). The overall Algorithm 1\\nhas the same structure as the deterministic O(n3/2 ·√log n)) round weighted APSP algorithm in [2]\\nbut we use a variant of the pipelined APSP algorithm in [1] in place of Bellman-Ford, and we also\\npresent new methods within two of the steps.\\n5\\nWe first define the following notion of an h-hop Consistent SSSP (CSSSP) collection. This notion\\nis implicit in [2] but is not explicitly defined there.\\nDefinition 2.1 (CSSSP). Let H be a collection of rooted h-hop trees in a graph G = (V,E). Then\\nH is an h-hop CSSSP collection (or simply an h-hop CSSSP) if for every u, v ∈ V the path from\\nu to v is the same in each of the trees in H (in which such a path exists), and is the h-hop shortest\\npath from u to v in the h-hop tree Tu rooted at u. Further, each Tu contains every vertex v that has\\na shortest path from u in G with at most h hops.\\nIn our improved Algorithm 1, Steps 3-5 are unchanged from the algorithm in [2]. However we\\ngive an alternate method for Step 1 to compute h-hop CSSSP (see Section 2.1) since the method\\nin [2] takes Θ(n · h) rounds, which is too large for our purposes. Our new method is very simple\\nand using the pipelined algorithm in [1] it runs in O(\\n√△hk) rounds. (An implementation using\\nBellman-Ford [3] would give an O(n · h)-round bound, which could be used in [2] to simplify that\\nblocker set algorithm.)\\nStep 2 computes a blocker set, defined as follows.\\nDefinition 2.2 (Blocker Set [9, 2]). Let H be a collection of rooted h-hop trees in a graph G =\\n(V,E). A set Q ⊆ V is a blocker set for H if every root to leaf path of length h in every tree in H\\ncontains a vertex in Q. Each vertex in Q is called a blocker vertex for H.\\nFor Step 2 we use the overall blocker set algorithm from [2], which runs in O(n · h+ (n2 log n)/h)\\nrounds and computes a blocker set of size q = O((n log n)/h) for the h-hop trees constructed in\\nStep 1 of algorithm 1. But this gives only an O˜(n3/2) bound for Step 2 (by setting h = O˜(\\n√\\nn)), so\\nit will not help us to improve the bound on the number of rounds for APSP. Instead, we modify and\\nimprove a key step where that earlier blocker set algorithm has a Θ(n ·h) round preprocessing step.\\n(Our improved method here will not help to improve the bound in [2] but does help to obtain a\\nbetter bound here in conjunction with the pipelined algorithm.) We give the details of our method\\nfor Step 2 in Section 2.2.\\nAlgorithm 1 Overall k-SSP algorithm (adapted from [2])\\nInput: set of sources S, number of hops h\\n1: Compute h-hop CSSSP rooted at each source x ∈ S (described in Section 2.1).\\n2: Compute a blocker set Q of size Θ(n log n\\nh\\n) for the h-hop CSSSP computed in Step 1 (described in Section 2.2).\\n3: for each c ∈ Q in sequence: compute SSSP tree rooted at c.\\n4: for each c ∈ Q in sequence: broadcast ID(c) and the shortest path distance values δh(x, c) for each x ∈ S.\\n5: Local Step at node v ∈ V : for each x ∈ S compute the shortest path distance δ(x, v) using the received values.\\nLemma 2.3. Algorithm 1 computes k-SSP in O(n\\n2 logn\\nh +\\n√△hk) rounds.\\nProof. The correctness of Algorithm 1 is established in [2]. Step 1 runs in O(\\n√△hk) rounds by\\nLemma 2.5 in Section 2.1. In Section 2.2 we will give an O(n · q+√△hk) rounds algorithm to find\\na blocker set of size q = O(n lognh ). Simple O(n · q) round algorithms for Steps 3 and 4 are given\\nin [2]. Step 5 has no communication. Hence the overall bound for Algorithm 1 is O(n · q +√△hk)\\nrounds. Since q = O(n lognh ) this gives the desired bound.\\nProofs of Theorem 1.3 and 1.4: Using h = n\\n4/3·log2/3 n\\n(2k·△)1/3 in Lemma 2.3 we obtain the bounds in The-\\norem 1.4.\\nIf edge weights are bounded by λ, the weight of any h-hop path is at most hλ. Hence by\\nLemma 2.3, the k-SSP algorithm (Algorithm 1) runs in O(n\\n2 logn\\nh + h\\n√\\nλk) rounds. Setting h =\\nn log1/2 n/(λ1/4k1/4) we obtain the bounds stated in Theorem 1.3.\\n6\\nab\\nd c\\n1\\n1 8\\n1\\n(i) Example graph G.\\na\\nb\\nd c\\nb\\nd c\\n(ii) 2-hop SSSP for source nodes a (on\\nleft) and b (on right).\\na\\nb\\nd\\nb\\nd c\\n(iii) 2-hop CSSSP for source set {a, b}.\\nFigure 1: This figure illustrates an example graph G where the collection of 2-hop SSSPs is different from the 2-hop CSSSP\\ngenerated for the source set S = {a, b}.Observe that the edge (b, c) is part of the 2-hop SSSP rooted at a (Fig. (ii)) but is not\\npart of the 2-hop CSSSP (Fig. (iii)) since there is a shorter path from b to c of hop-length 2 (path 〈b, d, c〉 in 2-SSP rooted at\\nb).\\n2.1 Computing Consistent h-hop trees\\nIn Section 1 we defined a natural notion of an h-hop SSSP tree rooted at a source s, as a rooted\\ntree which contains an h-hop shortest path from s to every other vertex to which there exists a path\\nfrom s with at most h hops. We also defined tie-breaking rules for the case when multiple paths\\nfrom s to v satisfy this definition: a path with the smallest number of hops is chosen, with further\\nties broken by choosing the predecessor vertex with smallest ID. Each h-hop tree constructed by the\\npipelined (h, k)-SSP algorithm [1] satisfies the definition of an h-hop SSSP tree (as constructed using\\nthe Z.p pointers) and these trees can also be constructed for each source using the Bellman-Ford\\nalgorithm [3].\\nThe definition of a CSSSP collection (Def. 2.1) places additional stringent conditions on the struc-\\nture of h-hop SSSP trees in the collection, and neither the pipelined algorithm in [1] nor Bellman-\\nFord is guaranteed to construct this collection. At the same time, the trees in a CSSSP collection\\nmay not satisfy the definition of h-hop SSSP in Sec. 1 since we may not have a path from vertex u\\nto vertex v in a h-hop CSSSP tree even if there exists a path from u to v with at most h hops. See\\nFig. 1.\\nOur method to construct an h-hop CSSSP collection is very simple: We execute the pipelined\\nalgorithm in [1] to construct 2h-hop SSSP trees instead of h-hop SSSP trees. Our CSSSP collection\\nwill retain the initial h hops of each of these 2h-hop SSSP trees. We now show that this simple\\nconstruction results in an h-hop CSSSP collection. Thus we are able to construct h-hop CSSSPs\\nby incurring just a constant factor overhead in the number of rounds over the bound for pipelined\\nalgorithm.\\nLemma 2.4. Let A be a distributed algorithm that computes (h, k)-SSP trees of shortest path dis-\\ntance at most ∆ in an n-node graph in f(h, k, n,∆) round. Consider running Algorithm A using\\nthe hop-length bound 2h, and let C be the collection of h-hop trees formed by retaining the initial\\nh hops in each of these 2h-hop trees. Then the collection C forms an h-hop CSSSP collection, and\\nthis collection can be computed in f(2h, k, n,∆) rounds.\\nProof. If not, then there exist vertices u, v and trees Tx, Ty such that the paths from u to v in Tx\\nand Ty are different. Let π\\nx\\nu,v and π\\ny\\nu,v be the corresponding paths in these trees.\\nThere are three possible cases: (1) when wt(πxu,v) 6= wt(πyu,v) (2) when paths πxu,v and πyu,v have\\nsame weight but different hop-lengths (3) when both πxu,v and π\\ny\\nu,v have same weight and hop-length.\\n(1) wt(πxu,v) 6= wt(πyu,v): w.l.o.g. assume that wt(πxu,v) < wt(πyu,v). Now if we replace πyu,v in Ty\\nwith πxu,v, we get a path of smaller weight from y to v of hop-length at most 2h and weight at\\nmost ∆. But this violates the definition of h-SSP (Definition 1.1) since Ty is a 2h-SSP and hence\\nit should contain a minimum weight path from y to v of hop-length at most 2h, and not the path\\nπyy,v, resulting in a contradiction.\\n7\\n(2) paths πxu,v and π\\ny\\nu,v have same weight but different hop-lengths. w.l.o.g. assume that path πxu,v\\nhas smaller hop-length than πyu,v. Then κ(πxu,v) < κ(π\\ny\\nu,v) and hence κ(π\\ny\\ny,u ◦ πxu,v) < κ(πyy,u ◦ πyu,v).\\nThis again violates the h-SSP definition (Definition 1.1) since Ty is a 2h-SSP and hence it should\\ncontain the path πyy,u ◦ πxu,v from y to v as it has smaller number of hops than the path πyy,v.\\n(3) both πxu,v and π\\ny\\nu,v have same weight and hop-length. Let (a, v) be the last edge on the path πxu,v\\nand let (b, v) be the last edge on the path πyu,v. w.l.o.g. assume that ID(a) < ID(b). Then again\\nsince Ty is a 2h-SSP, by h-SSP definition (Definition 1.1) Ty must contain the path π\\ny\\ny,u ◦ πxu,v from\\ny to v since its predecessor vertex has smaller ID than the predecessor vertex of path πyy,v.\\nLemma 2.5. An h-hop CSSSP collection can be computed in O(\\n√\\n∆hk) rounds using the pipelined\\nAPSP algorithm in [1] and in O(nh) rounds using Bellman-Ford [3].\\nWe now show two useful properties of an h-hop CSSSP collection that we will use in our blocker\\nset algorithm in the next section. (Lemma 2.7 is also implicitly established in [2]).\\nLemma 2.6. Let c be a vertex in G and let T be the union of the edges in the collection of subtrees\\nrooted at c in the trees in a h-hop CSSSP collection C. Then T forms an out-tree rooted at c.\\nProof. If not, there exist nodes u and v and trees Tx and Ty such that the path from c to u in Tx\\nand path from c to v in Ty first diverge from each other after starting from c and then coincide\\nagain at some vertex z. But since C is an h-hop CSSSP collection, by Lemma 2.4 the path from c\\nto z in the collection C is unique.\\nLemma 2.7. Let c be a vertex in G and let T be the collection of paths from source node x ∈ S to\\nc in the trees in a h-hop CSSSP collection C. Then T forms an in-tree rooted at c.\\n2.2 Computing a Blocker Set\\nOur overall blocker set algorithm runs in O(n\\n2 logn\\nh +\\n√△hk) rounds. It differs from the blocker set\\nalgorithm in [2] by developing faster algorithms for two steps that take O(nh) rounds in [2].\\nThe first step in [2] that takes O(nh) rounds is the step that computes the initial ‘scores’ at all\\nnodes for all h-hop trees in the CSSSP collection. The score of node v in an h-hop tree is the number\\nof v’s descendants in that tree. Instead of the O(nh) rounds, we can compute scores for all trees at\\nall nodes in O(\\n√△hk) rounds with a timestamp technique given in [14] for propagating values from\\ndescendants to ancestors in the shortest path trees within the same bound as the APSP algorithm.\\nTo explain the second O(nh)-round step in [2], we first give a brief recap of the blocker set algorithm\\nin [2]. This algorithm picks nodes to be added to the blocker set greedily. The next node that is\\nadded to the blocker set is one that lies in the maximum number of paths in the h-hop trees that\\nhave not yet been covered by the already selected blocker nodes. To identify such a node, the\\nalgorithm maintains at each node v a count (or score) of the number of descendant leaves in each\\ntree, since the sum of these counts is precisely the number of root-to-leaf paths in which v lies.\\nOnce all vertices have their overall score, the new blocker node c can be identified as one with the\\nmaximum score. It now remains for each node v to update its scores to reflect the fact that paths\\nthrough c no longer exist in any of the trees. This update computation is divided into two steps\\nin [2]. In both steps, the main challenge is for a given node to determine, in each tree Tx, whether\\nit is an ancestor of c, a descendant of c, or unrelated to c.\\n8\\n1. Updates at Ancestors. For each v, in each tree Tx where v is an ancestor of c, v needs to reduce\\nits score for Tx by c’s score for Tx since all of those descendant leaves have been eliminated. In [2] an\\nO(n)-round pipelined algorithm (using the in-tree property in Lemma 2.7) is given for this update\\nat all nodes in all trees, and this suffices for our purposes.\\n2. Updates at Descendants. For each v, in each tree Tx where v is a descendant of c, v needs to\\nreduce its score for Tx to zero, since all descendant leaves are eliminated once c is removed. In [2]\\nthis computation is performed by an O(nh)-round precomputation in which each vertex identifies\\nall of its ancestors in all of the h-hop trees and thereafter can readily identify the trees in which it\\nis a descendant of a newly chosen blocker node c once c broadcasts its identity to all nodes. But\\nthis is too expensive for our purposes.\\nAlgorithm 2 Pipelined Algorithm for updating scores at v in trees Tx in which v is a descendant of newly chosen blocker\\nnode c\\nInput: Q: blocker set, c: newly chosen blocker node, S: set of sources\\n(only for c)\\n1: Local Step at c: create listc to store the ID of each source x ∈ S such that scorex(c) 6= 0; for each x ∈ S do set\\nscorex(c) ← 0; set score(c)← 0\\n2: Send: Round i: let 〈x〉 be the i-th entry in listc; send 〈x〉 to c’s children in Tx.\\n(round r > 0 : for vertices v ∈ V −Q− {c})\\n3: send[lines 4-5]:\\n4: if v received a message 〈x〉 in round r − 1 then\\n5: if v 6= x then send 〈x〉 to v’s children in Tx\\n6: receive[lines 7-8]:\\n7: if v receives a message 〈x〉 then\\n8: score(v) ← score(v)− scorex(v); scorex(v) ← 0\\nHere, we perform no precomputation but instead in Algorithm 2 we use the property in Lemma 2.6\\nto develop a method similar to the one for updates at ancestors. Initially c creates a list, listc,\\nwhere it adds the IDs of all the source nodes x such that c lies in tree Tx. In round i, c sends\\nthe i-th entry 〈x〉 in listc to all its children in Tx. Since T (in Lemma 2.6) is a tree, every node v\\nreceives at most one message in a given round r. If v receives the message for source x in round r,\\nit forwards this message to all its children in Tx in the next round, r + 1, and also set its score for\\nsource x to 0. Similar to the algorithm for updating ancestors of c [2], it is readily seen that every\\ndescendant of c in every tree Tx receives a message for x by round k + h− 1.\\nLemma 2.8. Algorithm 2 correctly updates the scores of all nodes v in every tree Tx in which v is\\na descendant of c in k + h− 1 rounds.\\n3 Simplified Versions of Short-Range Algorithms in [8]\\nWe describe here simplified versions of the short-range and short-range-extension algorithms used\\nin the randomized O˜(n5/4) round APSP algorithm in Huang et al. [8]. Our short-range Algorithm 3\\nis inspired by the pipelined APSP algorithm in [1] and is much simpler than it since it is for a single\\nsource.\\nGiven a hop-length h and a source vertex x, the short-range algorithm in [8] computes the h-hop\\nshortest path distances from source x in a graph G′ (obtained through ‘scaling’) where ∆ ≤ n− 1.\\nThe scaled graph has different edge weights for different sources, and hence h-hop APSP is computed\\nthrough n h-hop SSSP (or short-range) computations, each of which runs with dilation (i.e., number\\nof rounds) O˜(n\\n√\\nh) and congestion (i.e., maximum number of messages along an edge) O(\\n√\\nh). By\\n9\\nrunning this algorithm using each vertex as source, h-hop APSP is computed inG′ in O(n\\n√\\nh) rounds\\nw.h.p. in n using a result in Ghaffari’s framework [6], which gives a randomized method to execute\\nthis collection of different short-range executions simultaneously in O˜(dilation + n · congestion) =\\nO˜(n\\n√\\nh) rounds.\\nThe short-range algorithm in [8] for a given source runs in two stages:. Initially every zero edge-\\nweight is increased to a positive value α = 1/\\n√\\nh and then h-hop SSSP is computed using a BFS\\nvariant in O˜(n/α) = O˜(n\\n√\\nh) rounds. This gives an approximation to the h-hop SSSP where the\\nadditive error is at most hα =\\n√\\nh. This error is then fixed by running Bellman-Ford algorithm [3]\\nfor h rounds. The total round complexity of this SSSP algorithm is O˜(n\\n√\\nh) and the congestion is\\nO(\\n√\\nh).\\nAlgorithm 3 Round r of short-range algorithm for source x\\n(initially d∗ ← 0; l∗ ← 0 at source x)\\n(at each node v ∈ V )\\n1: send: if ⌈d∗ · √h+ l∗⌉ = r then send (d∗, l∗) to all the neighbors\\n2: receive [Steps 2-6]: let I be the set of incoming messages\\n3: for each M ∈ I do\\n4: let M = (d−, l−) and let the sender be y.\\n5: d← d− + w(y, v); l ← l− + 1\\n6: if d < d∗ or (d = d∗ and l < l∗) then set d∗ ← d; l∗ ← l\\nWe now describe our simplified short-range algorithm (Algorithm 3) which has the same dilation\\nO(n\\n√\\nh) and congestion O(\\n√\\nh). Here d∗ is the current best estimate for the shortest path distance\\nfrom x at node v and l∗ is the hop-length of the corresponding path. Source node x initializes d∗\\nand l∗ values to zero and sends these values to its neighbors in round 0 (Step 1). At the start of a\\nround r, each node v checks if its current d∗ and l∗ values satisfy ⌈d∗ ·√h+ l∗⌉ = r, and if so, it sends\\nthis estimate to each of its neighbors. To bound the number of such messages v sends throughout\\nthe entire execution, we note that v will send another message in a future round only if it receives\\na smaller d∗ value with higher ⌈d∗ ·\\n√\\nh+ l∗⌉ value. But since l∗ ≤ h and d∗ values are non-negative\\nintegers, v can send at most\\n√\\nh messages to its neighbors throughout the entire execution.\\nWe now establish that vertex v will receive the message that creates the pair d∗, l∗ at v before round\\n⌈d∗ ·\\n√\\nh+ l∗⌉, and hence will be able to perform the send in Step 1 of Algorithm 3.\\nLemma 3.1. Let π∗x,v be a path from source x to vertex v with the minimum number of hops among\\nall h-hop shortest paths from x to v. Let π∗x,v have l∗ hops and weight (distance) d∗. If v receives\\nthe message for the pair d∗, l∗ in round r then r < ⌈d∗ · √h+ l∗⌉.\\nProof. We show this by induction on round r. The base case is trivially satisfied since x already\\nknows d∗ and l∗ values at the start (Round 0).\\nAssume inductively that the lemma holds at all vertices up to round r−1. Let y be the predecessor\\nof v on the path π∗x,v. Then y must have received the message for its pair (d∗ − w(y, v), l∗ − 1) in\\na round r′ < r. Let k = ⌈(d∗ − w(y, v)) · √h + l∗ − 1⌉. Then, r′ < k by the inductive assumption.\\nSo y will send the message (d∗ −w(y, v), l∗ − 1) to v in round r = k in Step 1 of Algorithm 3. But\\nk = ⌈(d∗ − w(y, v)) · √h + l∗ − 1⌉ < ⌈(d∗ − w(y, v)) · √h + l∗⌉ ≤ ⌈d∗ · √h + l∗⌉, since w(y, v) ≥ 0.\\nHence the round r in which v receives the message for the pair d∗, l∗ is less than ⌈d∗ · √h+ l∗⌉.\\nThis establishes the induction step and the lemma.\\nIf shortest path distances are bounded by∆, Algorithm 3 runs in ⌈∆·√h+h⌉ rounds with congestion\\n10\\nat most\\n√\\nh. And if ∆ ≤ n− 1 (as in [8]), then we can compute shortest path distances from x to\\nevery node v in O(n\\n√\\nh) rounds.\\nWe can similarly simplify the short-range-extension algorithm in [8], where some nodes already\\nknow their distance from source x and the goal is to compute shortest paths from x by extending\\nthese already computed shortest paths to u by another h hops. To implement this, we only need\\nto modify the initialization in Algorithm 3 so that each such node u initializes d∗ with this already\\ncomputed distance. The round complexity is again O(∆\\n√\\nh) and the congestion per source is O(\\n√\\nh).\\nThis gives us the following result.\\nLemma 3.2. Let G = (V,E) be a directed or undirected graph, where all edge weights are non-\\nnegative distances (and zero-weight edges are allowed), and where shortest path distances are bounded\\nby ∆. Then by using Algorithm 3, we can compute h-hop SSSP and h-hop extension in O(∆\\n√\\nh)\\nrounds with congestion bounded by\\n√\\nh.\\nAs in [8] we can now combine our Algorithm 3 with Ghaffari’s randomized framework [6] to compute\\nh-hop APSP and h-hop extensions (for all source nodes) in O˜(∆\\n√\\nh + n\\n√\\nh) rounds w.h.p. in n.\\nThe result can be readily modified to include the number of sources, k, by sending the current\\nestimates (d∗, l∗) in round ⌈d∗ · γ+ l∗⌉ , where γ =\\n√\\nhk/∆ as in pipelined algorithm in [1] (instead\\nof ⌈d∗ · √h + l∗⌉), and the resulting algorithm runs in O(√∆hk) rounds with congestion bounded\\nby\\n√\\n∆h/k. Then we can compute h-hop k-SSP and h-hop extensions for all k sources in O˜(\\n√\\n∆hk)\\nrounds.\\n4 Additional Results\\n4.1 A Faster Randomized Algorithm for Weighted APSP with Arbitrary Edge-\\nWeights\\nWe adapt the randomized framework of Huang et al. [8] to obtain a faster randomized algorithm\\nfor weighted APSP with arbitrary edge weights. Our randomized algorithm runs in O˜(n4/3) rounds\\nw.h.p. in n, improving on the previous best bound of O˜(n3/2) rounds (which is deterministic) in\\nAgarwal et al. [2]. We describe our randomized algorithm below.\\nAs described in Section 3, Huang et al.[8] use two algorithms short-range and short-range-extension\\nfor integer-weighted APSP for which they have randomized algorithms that run in O˜(n\\n√\\nh) rounds\\nw.h.p. in n. (We presented simplified versions of these two algorithms in Section 3.) Since we\\nconsider arbitrary edge weights here, we will instead use h rounds of the Bellman-Ford algorithm [3]\\nfor both steps, which will take O(kh) rounds for k source nodes.\\nWe keep the remaining steps in [8] unchanged: These steps involve having every ‘center’ c broadcast\\nits estimated shortest distances, δ(c′, c), from every other center c′, and each source node x ∈ S\\nsending its correct shortest distance, δ(x, c), to each center c. (The set of centers is a random\\nsubset of vertices in G of size O˜(\\n√\\nn).) These steps are shown in [8] to take O˜(n+\\n√\\nnkq) rounds in\\ntotal w.h.p. in n, where q = Θ(n lognh ). This gives an overall round complexity O˜(kh+ n +\\n√\\nnkq)\\nfor our algorithm. Setting h = n2/3/k1/3 and q = n1/3k1/3 log n, we obtain the desired bound of\\nO˜(n+ n2/3k2/3) in Theorem 1.6.\\n11\\n4.2 An O˜(n)-Rounds (1+ ǫ) Approximation Algorithm for Weighted APSP with\\nNon-negative Integer Edge-Weights\\nHere we deal with the problem of finding (1 + ǫ)-approximate solution to the weighted APSP\\nproblem. If edge-weights are strictly positive, the following result is known.\\nTheorem 4.1 ( [13, 11]). There is a deterministic algorithm that computes (1 + ǫ)-approximate\\nAPSP on graphs with positive polynomially bounded integer edge weights in O((n/ǫ2) · log n) rounds.\\nThe above result does not hold when zero weight edges are present. Here we match the deterministic\\nO((n/ǫ2)·log n)-round bound for this problem with an algorithm that also handles zero edge-weights.\\nWe first compute reachability between all pairs of vertices connected by zero-weight paths. This is\\nreadily computed in O(n) rounds, e.g., using [12, 14] while only considering only the zero weight\\nedges (and ignoring the other edges).\\nWe then consider shortest path distances between pairs of vertices that have no zero-weight path\\nconnecting them. The weight of any such path is at least 1. To approximate these paths we increase\\nthe zero edge-weights to 1 and transform every non-zero edge weight w(e) to n2 · w(e). Let this\\nmodified graph be G′ = (V,E,w′) . Thus the weight of an l-hop path p in G′, w′(p), satisfies\\nw′(p) ≤ w(p) ·n2+ l. Since the modified graph G′ has polynomially bounded positive edge weights,\\nwe can use the result in Theorem 4.1 to compute (1 + ǫ/3)-approximate APSP on this graph in\\nO˜(9n/ǫ2) rounds.\\nFix a pair of vertices u, v. Let p be a shortest path from u to v in G, and let its hop-length be l.\\nThen w′(p) ≤ n2 ·w(p) + l. Let p′ be a (1 + ǫ/3)-approximate shortest path from u to v, and let its\\nhop-length be l. Then w′(p′) ≤ (1 + ǫ/3) · w′(p) ≤ (1 + ǫ/3) · (n2 · w(p) + l). Dividing w′(p′) by n2\\ngives us w′(p′)/n2 < w(p)(1+ǫ/3)+(l/n2)(1+ǫ/3) < w(p)+w(p)ǫ/3+2/n ≤ w(p)(1+ǫ/3)+2ǫ/3 ≤\\nw(p)(1 + ǫ) (as long as ǫ > 3/n and since w(p) ≥ 1), and this establishes Theorem 1.7.\\n5 Conclusion\\nWe have presented new improved deterministic distributed algorithms for weighted shortest paths\\n(both APSP, and for k sources) in graphs with moderate non-negative integer weights. These re-\\nsults build on our recent pipelined algorithm for weighted shortest paths [1]. We have also presented\\nsimplications to two procedures in the randomized APSP algorithm in Huang et al. [8] by stream-\\nlining the pipelined APSP algorithm in [1] for SSSP versions. A key feature of our shortest path\\nalgorithms is that they can handle zero-weighted edges, which are known to present a challenge\\nin the design of distributed algorithms for non-negative integer weights. We have also presented a\\nfaster and more streamlined algorithm to compute a blocker set, an improved randomized APSP\\n(and k-SSP) algorithm for arbitrary edge-weights, and an approximate APSP algorithm that can\\nhandle zero-weighted edges.\\nAn important area for further research is to investigate further improvements to the deterministic\\ndistributed computation of a blocker set, beyond the algorithm in [2] and the improvements we have\\npresented here.\\n12\\nReferences\\n[1] U. Agarwal and V. Ramachandran. A faster deterministic distributed algorithm for weighted\\napsp through pipelining. arXiv preprint arXiv:1807.08824.v2, 2018.\\n[2] U. Agarwal, V. Ramachandran, V. King, and M. Pontecorvi. A deterministic distributed\\nalgorithm for exact weighted all-pairs shortest paths in O˜(n3/2) rounds. In Proc. PODC, pages\\n199–205. ACM, 2018.\\n[3] R. Bellman. On a routing problem. Quarterly of applied mathematics, 16(1):87–90, 1958.\\n[4] E. W. Dijkstra. A note on two problems in connexion with graphs. Numerische mathematik,\\n1(1):269–271, 1959.\\n[5] M. Elkin. Distributed exact shortest paths in sublinear time. In Proc. STOC, pages 757–770.\\nACM, 2017.\\n[6] M. Ghaffari. Near-optimal scheduling of distributed algorithms. In Proc. PODC, pages 3–12.\\nACM, 2015.\\n[7] M. Ghaffari and J. Li. Improved distributed algorithms for exact shortest paths. In Proc.\\nSTOC, pages 431–444. ACM, 2018.\\n[8] C.-C. Huang, D. Nanongkai, and T. Saranurak. Distributed exact weighted all-pairs shortest\\npaths in O˜(n5/4) rounds. In Proc. FOCS, pages 168–179. IEEE, 2017.\\n[9] V. King. Fully dynamic algorithms for maintaining all-pairs shortest paths and transitive\\nclosure in digraphs. In Proc. FOCS, pages 81–89. IEEE, 1999.\\n[10] S. Krinninger and D. Nanongkai. A faster distributed single-source shortest paths algorithm.\\nIn Proc. FOCS. IEEE, 2018.\\n[11] C. Lenzen and B. Patt-Shamir. Fast partial distance estimation and applications. In Proc.\\nPODC, pages 153–162. ACM, 2015.\\n[12] C. Lenzen and D. Peleg. Efficient distributed source detection with limited bandwidth. In\\nProc. PODC, pages 375–382. ACM, 2013.\\n[13] D. Nanongkai. Distributed approximation algorithms for weighted shortest paths. In Proc.\\nSTOC, pages 565–573. ACM, 2014.\\n[14] M. Pontecorvi and V. Ramachandran. Distributed algorithms for directed betweenness cen-\\ntrality and all pairs shortest paths. arXiv preprint arXiv:1805.08124, 2018.\\n13\\n'}\n",
      "{'_id': ObjectId('6815e61add259dc7e6e1cd30'), 'authors': 'DE STEFANI, LORENZO, SILVESTRI, FRANCESCO', 'year': '2014', 'title': 'Exploiting non-constant safe memory in resilient algorithms and data structures', 'full_text': 'Original Citation:Exploiting non-constant safe memory in resilient algorithms and data structuresElsevierPublisher:Published version:DOI:Terms of use:Open Access(Article begins on next page)This article is made available under terms and conditions applicable to Open Access Guidelines, as described athttp://www.unipd.it/download/file/fid/55401 (Italian only)Availability:This version is available at: 11577/3228318 since: 2017-05-14T16:58:34Z10.1016/j.tcs.2015.04.003Università degli Studi di PadovaPadua Research Archive - Institutional RepositoryExploiting non-constant safe memory in resilient algorithms anddata structuresLorenzo De Stefani, Francesco Silvestri∗aDipartimento di Ingegneria dell’Informazione, University of PadovaVia Gradenigo 6/B, I-35131 Padova, Italy{destefan,silvest1}@dei.unipd.itAbstractWe extend the Faulty RAM model by Finocchi and Italiano (2008) by adding a safe memory ofarbitrary size S, and we then derive tradeoffs between the performance of resilient algorithmictechniques and the size of the safe memory. Let δ and α denote, respectively, the maximumamount of faults which can happen during the execution of an algorithm and the actual number ofoccurred faults, with α ≤ δ. We propose a resilient algorithm for sorting n entries which requiresO (n log n+ α(δ/S + logS)) time and uses Θ (S) safe memory words. Our algorithm outperformsprevious resilient sorting algorithms which do not exploit the available safe memory and requireO (n log n+ αδ) time. Finally, we exploit our sorting algorithm for deriving a resilient priorityqueue. Our implementation uses Θ (S) safe memory words and Θ (n) faulty memory wordsfor storing n keys, and requires O (log n+ δ/S) amortized time for each insert and deleteminoperation. Our resilient priority queue improves the O (log n+ δ) amortized time required bythe state of the art.Keywords: resilient algorithm, resilient data structure, memory errors, sorting, priority queue,tradeoffs, fault tolerance1. IntroductionMemories of modern computational platforms are not completely reliable since a variety ofcauses, including cosmic radiations and alpha particles [1], may lead to a transient failure of amemory unit and to the loss or corruption of its content. Memory errors are usually silent andhence an application may successfully terminate even if the final output is irreversibly corrupted.This fact has been recognized in many systems, like in Sun Microsystems servers at major cus-tomer sites [1] and in Google’s server fleets [2]. Eventually, a few works have also shown thatmemory faults can cause serious security vulnerabilities (see, e.g., [3]).As hardware solutions, like Error Correcting Codes (ECC), are costly and reduce space andtime performance, a number of algorithms and data structures have been proposed that provide∗Corresponding author. Phone number: +39 049 8277954Preprint submitted to Theoretical Computer Science April 3, 2015arXiv:1305.3828v2  [cs.DS]  2 Apr 2015(almost) correct solutions even when silent memory errors occur. Algorithmic approaches fordealing with unreliable information have been widely targeted in literature under different set-tings, and we refer to [4] for a survey. In particular, a number of algorithms and data structures,which are called resilient, have been designed in the Faulty RAM (FRAM ) [5]. In this model, anadaptive adversary can corrupt up to δ memory cells of a large unreliable memory at any time(even simultaneously) during the execution of an algorithm. Resilient algorithmic techniqueshave been designed for many problems, including sorting [6], selection [7], dynamic program-ming [8], dictionaries [9], priority queues [10], matrix multiplication and FFT [11], K-d andsuffix trees [12, 13]. Resilient algorithms have also been experimentally evaluated [14, 11, 15, 16].1.1. Our resultsPrevious results in the FRAM model assume the existence of a safe memory of constant sizewhich cannot be corrupted by the adversary and which is used for storing crucial data such ascode and instruction counters. In this paper we follow up the preliminary investigation in [8]studying to which extent the size of the safe memory can affect the performance of resilientalgorithms and data structures. We enrich the FRAM model with a safe memory of arbitrarysize S and then give evidence that an increased safe memory can be exploited to notably improvethe performance of resilient algorithms. In addition to its theoretical interest, the adoptionof such a model is supported by recent research on hybrid systems that integrate algorithmicresiliency with the (limited) amount of memory protected by hardware ECC [17]. In this setting,S would denote the memory that is protected by the hardware.Let δ and α denote respectively the maximum amount of faults which can happen during theexecution of an algorithm and the actual number of occurred faults, with α ≤ δ. In Section 2,we show that n entries can be resiliently sorted in O (n log n+ α(δ/S + logS)) time when a safememory of size Θ (S) is available in the FRAM. As a consequence, our algorithm runs in optimalΘ (n log n) time as soon as δ = O(√nS log n)and S ≤ n/ log n. When S = ω(1), our algorithmoutperforms previous resilient sorting algorithms, which do not exploit non-constant safe memoryand require O (n log n+ αδ) time [6, 7]. Finally, we use the proposed resilient sorting algorithmfor deriving a resilient priority queue in Section 3. Our implementation uses Θ (S) safe memorywords and Θ (n) faulty memory words for storing n keys, and requires O (log n+ δ/S) amortizedtime for each insert and deletemin operation. This result improves the state of art for whichO (log n+ δ) amortized time is required for each operation [10].1.2. PreliminariesAs already mentioned, we use the FRAM model with a safe memory. Specifically, the adoptedmodel features two memories: the faulty memory whose size is potentially unbounded, and thesafe memory of size S. For the sake of simplicity, we allow algorithms to exceed the amount ofsafe memory by a multiplicative constant factor. The adversary can read the content of the faultymemory and corrupt at any time memory words stored in any position of the faulty memory for2up to a total δ times. Note that faults can occur simultaneously and the adversary is allowedto corrupt a value which was already previously altered. The safe memory can be read butnot corrupted by the adversary. A similar model was adopted in [8], however in this paper theadversary was not allowed to read the safe memory. We denote with α ≤ δ the actual number offaults injected by the adversary during the execution of the algorithm. Since the performance ofour algorithms do not increase as soon as S > δ, we assume through the paper that S ≤ δ; thisassumption can be easily removed by replacing S with min{S, δ} in our algorithms.A variable is reliably written if it is replicated 2δ+1 times in the faulty memory and its actualvalue is determined by majority: clearly, a reliably written variable cannot be corrupted. We saythat a value is faithful if it has never been corrupted and that a sequence is faithfully ordered ifall the faithful values in it are correctly ordered. Finally, we assume all faithful input values tobe distinct, each value to require a memory word, and that each sequence or buffer to be storedin adjacent memory words.2. Resilient Sorting AlgorithmIn the resilient sorting problem we are given a set of n keys and the goal is to correctly orderall the faithful input keys (corrupted keys can be arbitrarily positioned). We propose S-Sort, aresilient sorting algorithm which runs in O (n log n+ α (δ/S + logS)) time by exploiting Θ (S)safe memory words. Our approach builds on the resilient sorting algorithm in [6], however majorchanges are required to fully exploit the safe memory. In particular, the proposed algorithmforces the adversary to inject Θ (S) faults in order to invalidate part of the computation and toincrease the running time by an additive O (δ + S logS) term. In contrast, O (1) faults sufficeto increase by an additive O (δ) term the time of previous algorithms [5, 6, 7], even when ω(1)safe memory is available. Our algorithm runs in optimal Θ (n log n) time for δ = O(√Sn log n)and S ≤ n/ log n: this represents a Θ(√S)improvement with respect to the state of the art [6],where optimality is reached for δ = O(√n log n).S-Sort is based on mergesort and uses the resilient algorithm S-Merge for merging. TheS-Merge algorithm requires O (n+ α (δ/S + logS)) time for merging two faithfully ordered se-quences of length n each with Θ (S) safe memory. S-Merge is structured as follows. An incompletemerge of the two input sequences is initially computed with S-PurifyingMerge: this method re-turns a faithfully ordered sequence Z of length at least 2(n−α) that contains a partial merge ofthe input sequences, and a sequence F with the at most 2α remaining keys that the algorithm hasfailed to insert into Z. Finally, keys in F are inserted into Z using the S-BucketSort algorithm,obtaining the final faithfully ordered sequence of all input values. Procedures S-PurifyingMergeand S-BucketSort are respectively proposed in Sections 2.1 and 2.2, while Section 2.3 describesthe resilient algorithms S-Merge and S-Sort.3Z1δ + S/2 2nZnX4δ + SX1SX2nY Y14δ + SY2SZ2S/2Iteration(in safe memory)RoundInversion checkSafety checkFigure 1: Graphical representation of the S-PurifyingMerge algorithm. X and Y are the input sequences to merge,and Z is the output buffer. X1, Y1 and Z1 are support buffers stored in the faulty memory, while X2, Y2 and Z2are support buffers stored in the safe memory. The light gray (resp., dark gray) highlights the structures that areused in a round (resp., iteration). An inversion (resp., safety) check is invoked any time data are moved from X1to X2 or from Y1 to Y2 (resp., from Z1 to Z2).2.1. S-PurifyingMerge algorithmLet X and Y be the faithfully ordered input sequences of length n to be merged. The S-PurifyingMerge algorithm returns a faithfully ordered sequence Z of length at least 2(n− α) anda sequence F of length at most 2α: sequence Z contains part of the merging of X and Y , whileF stores the input keys that the algorithm has deemed to be potentially corrupted and has failedto insert into Z. The algorithm extends the PurifyingMerge algorithm presented in [6] by addinga two-level cascade of intermediate buffers, where the smallest ones are completely contained inthe safe memory. Specifically, the algorithm uses six support buffers1:• Buffers X1 and Y1 of length 4δ+S, and Z1 of length δ+S/2; they are stored in the faultymemory.• Buffers X2 and Y2 of length S, and Z2 of length S/2; they are stored in the safe memory.At high level, the algorithm works as follows (see Figure 1 for a graphical representation).The computation is organized in rounds. In each round, O (δ) input keys in X and Y arerespectively pumped into buffers X1 and Y1. Then, the algorithm merges these keys in Z1 byiteratively merging small amounts of data in safe memory: during each iteration, chunks of O (S)consecutive keys in X1 and Y1 are moved into buffers X2 and Y2, where they are merged in Z2using a standard merging algorithm. Keys in Z2 are shifted into Z1 at the end of each iteration,while keys in Z1 are appended to Z at the end of each round. The algorithm performs somechecks, which are explained in details later, when keys are moved among buffers in order toguarantee resiliency: an inversion check is done every time a key is shifted from X1 to X2 orfrom Y1 to Y2; a safety check is executed every time buffer Z1 is appended to Z. If a check is1It can be shown that a more optimized implementation of S-PurifyingMerge requires only two buffers (i.e., X2and Y2). However, we describe here the implementation with six support buffers for the sake of simplicity.4unsuccessful, some critical faults have occurred and then part of the computation must be rolledback and re-executed.We now provide a more detailed description. Each round starts by filling buffers X1 and Y1with the remaining keys in X and Y , starting from those occupying the smallest index positions(i.e., from the smallest faithful values). Subsequently, the algorithm fills Z1 with at least δ valuesfrom the sequence obtained by merging X1 and Y1 or until there are no further keys to merge.Specifically, buffer Z1 is filled by iterating the following steps until it contains at least δ valuesor there are no further keys to merge in X1, X2, Y1 and Y2:1. Buffers X2 and Y2 are filled with the remaining keys of X1 and Y1, respectively, startingfrom the smallest index position. With the exception of the first iteration of the first round,an inversion check is executed for each key inserted in X2 and Y2. If a check is unsuccessful,the current round is restarted. In the first iteration of the first round, each key is insertedin X2 and Y2 without any check.2. Buffers X2 and Y2 are merged in Z2, until buffer Z2 is full or there are no further entires inX2 and Y2. The merging is performed using the standard algorithm since input and outputbuffers are stored in safe memory.3. Buffer Z2 is appended to Z1 and then emptied.As soon as Z1 is full or there are no further keys, a safety check is performed on Z1: if itsucceeds, buffer Z1 is appended to Z and flushed and then a new round is started; otherwise, thecurrent round is restarted.The inversion check works as follows. The check is performed on every new key x of X1inserted into X2, and on every new key y of Y1 inserted into Y2. We describe the check performedon each entry x, being the control executed on y defined correspondingly. If X2 is empty, nooperation is done and the check ends successfully. Otherwise, the value x is compared with thelast inserted key x′ in X2. If x is larger than x′, no further operations are done and the checkends successfully. Otherwise, if x is smaller than or equal to x′, it is possible to conclude that atleast one of the two keys is corrupted since X2 is supposed to be faithfully ordered and each keyto be unique. Then, both keys are inserted into F and removed from X1 and X2; if there existsat least one value in X2 after the removal, the check ends successfully, and it ends unsuccessfullyotherwise. We observe that inversion checks guarantee X2 and Y2 to be perfectly ordered at anytime (recall that the two buffers are stored in safe memory).The safety check works as follows. The check is performed when Z1 contains at least δ keysor there are no more keys to merge. In the last case, the check always ends successfully. Supposenow that Z1 contains at least δ keys, and let z be the latest key inserted into Z1 which we assumeto be stored in safe memory. Denote with X ′ (resp., Y ′) the concatenation of keys in X2 andX1 (resp., Y2 and Y1). If there are less than S/2 keys in X′ and Y ′ smaller than or equal to z,the safety check ends successfully. Otherwise, the algorithm scans X ′ starting from the smallerposition and compares each pair of adjacent keys looking for inversions: if a pair is not ordered,5it is possible to conclude that at least one of the two values has been corrupted, and hence bothkeys are inserted in F and removed from X ′. A similar procedure is executed for Y ′ as well. Thecheck then ends unsuccessfully.When a round is restarted due to an unsuccessful check, the algorithm replaces keys in Zi,Xi and Yi, for any i ∈ {1, 2}, with the keys contained in the respective buffers at beginningof the round (specifically, just after the algorithm terminates to fill buffers X1 and Y1 withnew keys). However, keys that have been moved to F during the failed round are not restored(empty positions in X1 and Y1 are suitably filled with keys in X and Y ). This operation can beimplemented by storing a copy of X1, Y1 and Z1 in the faulty memory, and of X2, Y2 and Z2 inthe safe memory. For every key moved to F , the key is also removed from the copies2.Lemma 1. Let X and Y be two faithfully ordered sequences of length n. S-PurifyingMerge returnsa faithfully ordered sequence Z of length |Z| ≥ n− 2α containing part of the merge of X and Y ,and a sequence F of length |F | ≤ 2α containing the remaining input keys. The algorithm runsin O (n+ αδ/S) time and uses Θ (S) safe memory words.Proof: It is easy to see that the algorithm uses Θ (S) safe memory and that each input key mustbe in Z or F . We prove that Z is faithfully ordered as follows: we first show that Z1 is faithfullyordered at the end of each round; we then argue that Z1 can be appended to Z without affectingthe faithful order of Z. We say that a round is successful if the round is not restarted by anunsuccessful inversion or safety check. For proving the correctness of the algorithm we focus onsuccessful rounds since unsuccessful ones do not affect Z.Let us now show that buffer Z1 contains a faithfully ordered sequence at the end of a successfulround. Inversion checks guarantee that any key inserted in X2 is not smaller than the previousone, and then buffer X2 is sorted at any time. Moreover, since the round is successful, buffer X2always contains at least one key during the round and, in particular, the buffer contains a keybetween two consecutive iterations. This fact guarantees that the concatenation Xˆ of all keysinserted in X2 in each iteration of the round creates an ordered sequence. Similarly, we have thatthe concatenation Yˆ of all keys inserted in Y2 in each iteration of the round is ordered. Eachiteration of the round merges in safe memory a part of Xˆ and Yˆ . Then. the concatenation ofall keys written into Z2 during the round is the correct merge of Xˆ and Yˆ (note that the largestkeys in Xˆ and Yˆ are not merged and are kept in X2 and Y2). Since these output keys are firststored in Z2 and then in Z1, we can claim that Z1 is a faithfully ordered sequence: indeed, therecan be an out-of-order key in Z1 due to a corruption occurred after the key has been moved fromZ2 to Z1.We now prove that Z is faithfully ordered. If the algorithm ends in one successful round,then Z is faithful ordered by the previous claim. We now suppose that there are at least two2An entry can be removed from a sequence in constant time by moving the subsequent entries in the correctposition as soon as they are read and by maintaining the required pointers in the safe memory.6successful rounds. Let Zi1 be the two buffers appended to Z at the end of the i-th and (i+ 1)-stsuccessful round. We have that Zi1 and Zi+11 are faithfully ordered by the previous claim, and wenow argue that even the concatenation of Zi1 and Zi+11 is faithfully ordered. Let z be the latestvalue z inserted in Zi1: since z is maintained in safe memory, we have that z is larger than allthe faithful values in Zi1 and smaller than all values in X2 and Y2. Since the round is successful,there are at most S/2 keys smaller than or equal to z in X ′ and hence at least δ + 1 keys largerthan z in X ′: indeed, the 4δ + S entries in X1 at the beginning of the round can be moved inF (at most 2δ), in Z1 (at most δ + S/2− 1), or in X2 (thus remaining in X ′). Therefore, thereexists a faithful key in X ′ larger than z, and hence all the faithful values remaining in X mustbe larger than z. The at most S/2 keys smaller than or equal to z must be in X1, and they willbe removed by inversion checks in subsequent rounds since there are at least S/2 values in X2larger than z (note that there are always at least S/2 keys X2 at the end of an iteration). Sincea similar claim applies to Y , we have that all faithful keys in Zi+11 are larger than those in Zi1.It follows that Z, which is the concatenation of Z11 , Z21 , . . ., is faithfully ordered.Finally, we upper bound the running time of the algorithm. If no corruption occurs, thealgorithm requiresO (n) time since there areO (n/δ) rounds, each one requiringO (δ/S) iterationsof cost O (S). Faults injected by the adversary during the first iteration of the first round cannotrestart the round, but increase the running time by at most a factor O (α) since each fault cancause two keys to be moved in buffer F . Consider an unsuccessful round that fails due to aninversion check. Since at the beginning of each iteration there are at least S/2 keys in both X2and Y2, then at least S/2 inverted pairs are required for emptying X2 or Y2 and the adversarymust pay S/2 faults for getting an unsuccessful inversion check. Consider now an unsuccessfulround that fails due to a safety check. Since there are at leas S/2 keys larger than or equal to zin X2 and Y2 and there are at least S/2 keys smaller than z in X1 and Y1, there must be at leastS/2 inversions. Since at least S/2 of these inversions are removed during the safely check andcannot be used by the adversary for failing another round, the adversary must pay S/2 faults forgetting an unsuccessful safety check. In all cases, each unsuccessful round costs O (δ) time to thealgorithm and S/2 faults to the adversary: since there cannot be more than b2α/Sc unsuccessfulrounds, the overhead due to unsuccessful rounds is O (αδ/S). The lemma follows. \\x032.2. S-BucketSort AlgorithmLet X be a faithfully ordered sequence of length n1 and Y an arbitrary sequence of lengthn2. The S-BucketSort algorithm computes a faithfully ordered sequence containing all keys in Xand Y in O (n1 + (n2 + δ)n2/S + (n2 + α) logS) time using a safe memory of size Θ (S). Thisalgorithm extends and fuses the NaiveSort and UnbalancedMerge algorithms presented in [6].The algorithm consists of dn2/Se rounds. At the beginning of each round, the algorithmremoves the S smallest keys among those remaining in Y and stores them into an orderedsequence P maintained in safe memory. Subsequently, the algorithm scans the remaining keysof X, starting from the smallest position, and partitions them among S + 1 buckets Bi where B07contains keys in (−∞, P [1]), Bi keys in [P [i], P [i+ 1]) for 1 ≤ i < S and BS keys in [P [S],+∞).The scan of X ends once δ + 1 keys have been inserted into BS or there are no more keys leftin X. For each key x in X, the search of the bucket is crucial for improving performance andproceeds as follows: the algorithm checks if x belongs to the range of the last used bucket Bk,for some 0 ≤ k ≤ S; if the check fails, then the algorithm verifies if x belongs to the right/leftlogS adjacent buckets of Bk; if this search is again unsuccessful, the correct buffer is identified byperforming a binary search on P . When the correct bucket is found, entry x is removed from Xand appended to the sequence of keys already in the bucket, thus guaranteeing that each bucketis faithfully ordered. When the scan of X ends, the sequence given by the concatenation ofB0, P [1],B1, P [2], . . . ,BS−1, P [S] is appended to the output sequence Z, keys in BS are insertedagain in X (in suitable empty positions of X that maintain the faithful order of X), P and theS+1 buckets Bi are emptied and a new round is started. After the dn2/Se rounds, all remainingkeys in X are appended to Z.The S smallest values of Y , which are used in each round for determining the buckets, areextracted from Y using the following approach. At the beginning of S-BucketSort (i.e., beforethe first round), a support priority queue containing S nodes is constructed in safe memory asfollows. Keys in Y are partitioned into S segments of size dn2/Se and a node containing thesmallest key and a pointer to the segment is inserted into the priority queue (the segment isstored in the faulty memory). Each time the smallest value is required, the smallest value yof the queue is extracted (we note that y is the minimum faithful key among those in Y or acorrupted key even smaller). Then, y is removed from the queue and from the respective buffer,and a new node with the new smallest key and a pointer to the segment is inserted in the queue.When a value is removed from a segment, the remaining values in the segment are shifted tokeep keys stored in consecutive positions. At the beginning of each round, the S values usedfor defining the buckets are obtained by S subsequent extractions of the minimum value in thepriority queue and maintained in S safe memory words.Lemma 2. Let X be a faithfully ordered sequence of length n1 and let Y be a sequence of lengthn2. S-BucketSort returns a faithfully ordered sequence containing the merge of keys in X and Y .The algorithm runs in O (n1 + (n2 + δ)n2/S + (n2 + α) logS) time and uses Θ (S) safe memorywords.Proof: It is easy to see that the algorithm uses Θ (S) safe memory and that each key of Xand Y must be in the output sequence at the end of the algorithm. We now argue that theoutput sequence Z is faithfully ordered at the end of the algorithm: we first prove that thesequence appended to Z at the end of each round is faithfully ordered, and then show that itcan be appended to Z without affecting the faithful order of Z. We now prove that the sequenceappended to Z at the end of a round is faithfully ordered. For each 0 ≤ i ≤ S, each faithfulkey appended to bucket Bi is in the correct range (note that P cannot be corrupted being insafe memory) and the sequence of keys in Bi is faithfully ordered since the insertion maintains8the order of faithful keys in X. Therefore, the sequence B0, P [1],B1, P [2], . . . ,BS−1, P [S] that isappended to Z is faithfully ordered. We now show that the appended sequence guarantees thefaithful order of Z by proving that, at the end of a round, the faithful keys that remain in Xare larger than those already in Z (i.e., faithful keys that are appended in subsequent roundsare larger). If the round ends since there are no further keys in X, the claim is trivially true.Suppose now that the round ends since there are δ + 1 keys in BS . Then, there must exist atleast one faithful key larger than P [S] in BS and all remaining faithful values in X must be largerthan P [S]. Therefore, we can conclude that Z is faithfully ordered at the end of the algorithm:by the above argument Z is faithfully ordered at the end of the last round; the keys in X thatare appended to Z after the last round do not affect the faithful order of Z since X is faithfullyordered and faithful keys in X are larger than those already in Z.We now upper bound the running time. Suppose no corruptions occur during the execution ofthe algorithm. Extracting the S smallest keys from Y using the auxiliary priority queue requiresO (n2 + S logS) time (O (n2 log n2) time if n2 < S) for each of the dn2/Se rounds, and thereforea total O (n2(n2/S + logS)) time. The insertion of an entry X[i] in a bucket, for each 1 ≤ i ≤ n1,requires O (1 + min{fi, logS}) = O (1 + fi), where fi is the number of keys of Y in the range(X[i− 1], X[i]): indeed, the algorithm searches the bucket for X[i] among the O (min{fi, logS})buckets around the one containing X[i − 1] and then, only in case of failure, performs a binarysearch on P . When no fault occurs, each key of Y contributes to one of the fi’s since X is sorted.Therefore, the partitioning of X costs O (∑n1i=1(1 + fi)) = O (n1 + n2). We note that the aboveanalysis ignores the fact that in each round δ + 1 values are moved back from BS to X this factleads to an overall increase of the running time given by an additive component O (δdn2/Se),which follows by charging O (δ) additional operations to each round. A fault in X may affect therunning time required for partitioning X. In particular, each fault may force the algorithm topay O (logS) for the corrupted key and the subsequent one in X: indeed, a corruption of X[i]may force the algorithm to perform a binary search in order to find the right bucket for X[i]and for the subsequent key X[i+ 1]. The additive cost due to α faults is hence O (α logS). Thecorruption of keys in Y does not affect the running time since the algorithm does not exploit theordering of Y . The lemma follows. \\x032.3. S-Merge and S-Sort AlgorithmsAs previously described, S-Merge processes the two input sequences with S-PurifyingMergeand then the two output sequences are merged with S-BucketSort. We get the following lemma.Lemma 3. Let X and Y be two faithfully ordered sequences of length n. Algorithm S-Mergefaithfully merges the two sequences in O (n+ α (δ/S + logS)) time using Θ (S) safe memorywords.Proof: By Lemma 1, algorithm S-PurifyingMerge returns a faithful sequence Z of length at most2n and a sequence F of length at most 2α in O (n+ αδ/S) time. These output sequences are9then combined using the S-BucketSort algorithm: by Lemma 2, this algorithm returns a faithfullyordered sequence of all the input elements in O (n+ α (δ/S + logS)) time. The lemma follows.\\x03By using S-Merge in the classical mergesort algorithm3, we get the desired resilient sortingalgorithm S-Sort and the following theorem.Theorem 1. Let X be a sequence of length n. Algorithm S-Sort faithfully sorts the keys in Xin O (n log n+ α (δ/S + logS)) time using Θ (S) safe memory words.Proof: Let us assume, for the sake of simplicity, n to be a power of two, and denote with αi,jthe number of faults that are detected by S-Merge on the j-th recursive problem which operateson input sequences of length 2i, with 0 ≤ i < log n and 0 ≤ j < n/2i. A fault injected in onesub-problem at level i may affect the parent problem at level i+1, but cannot affect sub-problemsat level i+ 2. Indeed, a key x corrupted during the sub-problem at level i may be out-of-order inthe output sequence. Key x is then recognized by the S-Merge at level i+ 1 as a fault, insertedin F by S-PurifyingMerge, and then positioned in the correct order in the output sequence byS-BucketSort (x will thus be consider as a faithful key in the parent problem at level i + 2).Another fault might cause key x to be stored out-of-order again in the output sequence at leveli + 1, but this fact is accounted to the new fault. Hence, we get∑logn−1i=0∑2i−1j=0 αi ≤ 2α. Bythe upper bound on the time of S-Merge in Lemma 3, we get that the running time of S-Sort isupper bounded byO\\uf8eb\\uf8edlogn−1∑i=02i−1∑j=0(n/2i + αi,j (δ/S + logS))\\uf8f6\\uf8f8 .The correctness of S-Sort follows by the correctness of S-Merge. \\x033. Resilient Priority QueueA resilient priority queue is a data structure which maintains a set of keys that can bemanaged and accessed through two main operations: Insert, which allows to add a key to thequeue; and Deletemin, which returns the minimum faithful key among those in the priority queueor an even smaller corrupted key and then removes it from the priority queue.In this section we present an implementation of the resilient priority queue that exploits asafe memory of size Θ (S). Let n denote the number of keys in the queue. Our implementationrequires O (log n+ δ/S) amortized time per operation, Θ (S) words in the safe memory and Θ (n)words in the faulty memory. Our resilient priority queue is based on the fault tolerant priorityqueue proposed in [10], which is in turn inspired by the cache-oblivious priority queue in [18]. The3The standard recursive mergesort algorithm requires a stack of length O (logn) which cannot be corrupted.However, it is easy to derive an iterative algorithm where a Θ (1) stack length suffices.1020, 1, 730, 32, 335, 26, 2815, 18, 1930,2, 95 152 3027, 29,323, 24, 2514, 17, 804, 6, 134 143 232, 80, 352 30 21Priority queue PI , [0, S]Up buffer U0, [0, s0/2]Immediate insertionbuffer I0, [0, log n+ δ/S]Down buffer D0, [s0/2, s0]Layer L0Buffers pointed by PI , [0, log n+ δ/S] eachPriority queue PU , [0, S]Priority queue PD, [0, S]8,3, 38 21, 31, 405,0, 70Pointer pUPointer pDFigure 2: Main support structures of the resilient priority queue (layers L1, . . . , Lk−1 are omitted). The figureshows the immediate insertion buffer I0, the priority queue PI and the respective pointed buffers, and layer L0.Layer L0 consists of the up buffer U0, the down buffer D0, the priority queues PU and PD, and the pointers pUand pD. The first δ + 1 entries of U0 and of D0 are organized in up to S sub-buffers of maximum size δ/S + 1.Each sub-buffer of U0 (resp., D0) is pointed by a node in PU (resp., PD). All priority queues are stored in thesafe memory, while buffers are contained in the faulty memory. Underlined keys are corrupted, while bold keysare used as priority in some queue. Each range [x, y] gives the minimum and maximum number of contained keysin a buffer or nodes in a priority queue.performance of the resilient priority queue is here improved by exploiting the safe memory andthe S-Merge and S-Sort algorithms, in place of the resilient merging and sorting algorithms in [6].It is important to point out that the Ω (log n+ δ) lower bound in [10] on the performance of theresilient priority queue does not apply to our data structure since the argument assumes that keysare not stored in safe memory between operations. The amortized time of each operation in ourimplementation matches the performance of classical optimal priority queues in the RAM modelwhen the number of tolerated corruptions is δ = O (S log n): this represents a Θ (S) improvementwith respect to the state of the art [10], where optimality is reached for δ = O (log n).The presentation is organized as follows: we first present in Section 3.1 the details of thepriority queue implementation, with particular emphasis on the role played by the safe memory;then we proceed in Section 3.2 to prove its correctness and complexity bounds.3.1. StructureThe structure of our resilient priority queue is similar to the one used in [10], however werequire some auxiliary structures and different constraints in order to exploit the safe memory.Specifically, the resilient priority queue presented in this paper contains the following structures(see Figure 2 for a graphical representation):• The immediate insertion buffer I0, which contains up to log n + δ/S keys. This buffer isstored in the faulty memory.• The priority queue PI , which contains up to S nodes. Each node contains a pointer to abuffer of size at most log n + δ/S and the priority key of the node is the smallest valuein the pointed buffer. Buffers are stored in the faulty memory, while the actual priority11queue PI and other structural information (e.g., buffer length, the position in the bufferof the smallest key) are stored in O (S) safe memory words. The purpose of PI is to actas a buffer between the newly inserted data in I0 and the main structure of the priorityqueue, that is layers L0, . . . , Lk−1 (see below). On the one hand it allows to rapidly accessthe newly inserted keys, while on the other hand it accumulates such keys so that thecomputational cost necessary for inserting all these keys in the main structure is amortizedover the insertion of at least S log n+ δ new values.• The layers L0, . . . , Lk−1, with k = O (log n). Each layer Li contains two faithfully orderedbuffers Ui and Di, named up buffer and down buffer, respectively. Up and down buffersare connected by a doubly linked list: for each 0 ≤ i < k, buffer Ui is linked to Di−1 andDi and vice versa. The layers are stored in the faulty memory, while the size and the linksto the neighbors of each buffer are reliably written (i.e., replicated 2δ + 1 times) in thefaulty memory using additional Θ (δ) space. For each layer, we define a threshold valuesi = 2i+1(S log2 n+ δ (logS + δ/S))which is used to determine whether an up buffer Uihas too many keys or a down buffer Di has too few. Specifically, we impose the followingorder and size invariants on all up and down buffers at any time:– (I1) All buffers are faithfully ordered;– (I2) For each 0 ≤ i < k − 1, the concatenations DiDi+1 and DiUi+1 are faithfullyordered;– (I3) For each 0 ≤ i < k − 1, si/2 ≤ |Di| ≤ si (this invariant may not hold for the lastlayer);– (I4) For each 0 ≤ i < k, |Ui| ≤ si/2.• The priority queues PU and PD and the pointers pU and pD, which are stored in the safememory. These queues are used to speed up the access to entries in U0 and D0. Weconsider the buffer U0 as the concatenation of two buffers UP0 and US0 . US0 contains keys inthe δ+1 smallest positions (if any) of U0, while UP0 contains all the remaining keys (if any)in U0. US0 itself is divided into up to S sub-buffers, each one with maximum size δ/S + 1and associated with one node of PU : each node maintains a pointer to the beginning ofa sub-buffer of US0 and its priority key is the smallest value in the respective sub-buffer.Each node also contains support information such as the size of the relative sub-bufferand the position of the smallest key in the sub-buffer. pU points to the first element ofUP0 . This structure ensures that the concatenation of a resiliently sorted US0 with UP0 isa faithful ordering of all the elements in U0 at all times. The priority queue PU can bebuilt by determining the minimum element of each sub-buffer in US0 and then by buildingthe priority queue in safe memory. The priority queue PD and pointer pD are analogouslyconstructed from buffer D0.12Since the priority queues PI , PU and PD are resiliently stored, we use any standard imple-mentation that supports the Peekmin operation, which is an operation that returns the minimumvalue in the priority queue without removing it. We note that buffer sizes (i.e., si) depend onn: As suggested in [10], a global rebuilding of the resilient priority queue is performed when thenumber of keys in it varies by Θ (n). The rebuilding is done by resiliently sorting all the keysand then distributing them among the down buffers starting from D0.The functioning and purpose of the auxiliary structures will be detailed in the description ofthe Insert and Deletemin operations in Section 3.1.1. We now provide an intuitive explanationof the functioning of our priority queue. Newly inserted keys are collected in the immediateinsertion buffer I0 and in the buffers pointed by nodes in PI , while the majority of the previouslyinserted values are maintained in the up and down buffers in the k layers Li. The role of thedown buffers is to contain small keys that are likely to be soon removed by Deletemin and thenshould move towards the lower levels (i.e., I0, PI or L0); on the other hand, up buffers store largekeys that will not be required in the short time (note that this fact is a consequence of invariant(I2)). Keys are moved among layers by means of the two fundamental primitives Push and Pull:these functions, which are described in Section 3.1.2, are invoked when the up and down buffersviolate the size invariants, and exploit the resilient merging algorithm S-Merge. The purpose ofthe support structures is to reduce the overhead necessary for the management of the priorityqueue in the presence of errors by reducing the number of invocations to the costly maintenancetasks (i.e., Push or Pull) and by amortizing their computational cost over multiple executionsof Insert or Deletemin. It will be evident in the subsequent section that PU and PD may causea discrepancy with respect to the order invariants (I1) and (I2) for the first O (δ) positions ofbuffers D0 and U0. However, we will see that this violation can be in general ignored and canbe quickly restored any time the algorithm needs to exploit the invariants on D0 and U0, that isany time Push and Pull are invoked.3.1.1. Insert and DeleteminThe implementation of Insert and Deletemin varies significantly with respect to the resilientpriority queue presented in [10]. In particular, the safe memory plays an important role in orderto obtain the desired performance.Insert. The newly inserted key is appended to the immediate insertion buffer I0. If after theinsertion I0 contains log n+ δ/S keys, some values in I0 are moved into other buffers as follows.Suppose that PI contains less than S nodes. A new buffer I′ is created in the faulty memoryand filled with the log n+ δ/S keys in I0, then a new node is inserted in PI with the minimumvalue in I ′ as key and a pointer to I ′; I0 is flushed at the end of this operation. Suppose nowthat PI contains S nodes. All keys in buffer I0, in the buffers pointed by all nodes of PI and inthe sub-buffers managed through PU (i.e., in buffer US0 ) are resiliently sorted using the S-Sortalgorithm. These values are then merged with those in UP0 (if any) using S-Merge, and finally13inserted into buffer U0. After the merge, the immediate insertion buffer, the priority queue PIand all its associated buffers are emptied. If the merge does not cause U0 to overflow, the priorityqueue PU is rebuilt from the new values in US0 by following the previously described procedure.On the contrary, if U0 overflows breaking the size invariant (I4), the Push primitive is invoked onU0, PU is deallocated (since Push removes all keys in U0) and PD is rebuilt following a proceduresimilar to the one for PU .Deletemin. To determine and remove the minimum key in the priority queue it is necessary toevaluate the minimum key among the at most log n+ δ/S keys in the immediate insertion bufferI0 and the minimum values in PI , PD and PU , which can be evaluated using Peekmin. Finally,the minimum key v among these four values is selected, removed from the appropriate buffer asdescribed below, and hence returned. The removal of v is performed as follows.• v is in I0. Value v is removed from I0 and the remaining keys in I0 are shifted in order toensure that keys are consecutively stored.• v is in PI . A Deletemin is performed on PI for removing the node with key v. Let I ′ bethe buffer pointed by this node. Then key v is removed from I ′ and the remaining keysin I ′ are shifted in order to ensure that keys are consecutively stored. We note that thevalue v may not be anymore available in I ′ since it has been corrupted by the adversary:however, since each node contains the position of v in I ′, the faithful value can be restored.Let cI′ be the new size of I′. If cI′ ≥ (log n+ δ/S)/2, a new node pointing to I ′ is insertedin PI using as priority key the new minimum value in I′. If cI′ < (log n + δ/S)/2 and I0is not empty, up to (log n+ δ/S)/2 keys are removed from the immediate insertion bufferand inserted in buffer I ′; then, a new node is inserted in PI pointing to I ′ and with prioritykey set to the new minimum value in I ′. Finally, if cI′ < (log n+ δ/S)/2 and I0 is empty,all values in I ′ are transferred in the immediate insertion buffer I0 and I ′ is deallocated.• v is in PU . A Deletemin is performed on PU for removing the node with key v. Let U ′denote the sub-buffer pointed by the removed node. The minimum key v is removed fromU ′ and its spot is filled with the value pointed by pU , which is then increased to point to thesubsequent value in UP0 (if any). If no key can moved to U′ (i.e., there are no keys in UP0 ),the empty spot is removed by compacting U ′ in order to ensure that keys are consecutivelystored and no further operations are performed. The new minimum value in U ′ is thenevaluated and inserted in PU with the associated pointer to U′ (no operation is done if U ′is empty).• v is in PD. Operations similar to the previous case are performed if the minimum key isextracted from PD. In this case, Deletemin may cause D0 to underflow breaking the sizeinvariant I3: if that happens, the Pull primitive is invoked on D0 and PD is rebuilt followinga procedure analogous to the one previously detailed for PU .14We observe that the use of the auxiliary structures PU and PD in Deletemin may cause adiscrepancy with respect to the order invariants (I1) and (I2) for buffers U0 and D0. We canhowever justify the waiver from (I1) by pointing out that this structure still ensures that thefaithful keys in US0 are smaller than or equal to those in UP0 . In particular the concatenation of aresiliently sorted US0 with UP0 is faithfully ordered (similarly in D0). Additionally, we can justifythe waiver from (I2) by observing that the faithful keys in D0 are still smaller than or equal tothose in D1 and U1. Furthermore, the invariants can be easily restored before any invocation ofPush and Pull by resiliently sorting US0 (resp., DS0 ) and linking it with UP0 (resp., DP0 ). Therefore,since D0 and U0 still behave consistently with the invariants for what pertains the relations withother buffers and the possibility of accessing the faithful keys maintained by them in the correctorder, we can assume with a slight (but harmless) “abuse of notation” that the invariants areverified for U0 and D0 as well.3.1.2. Push and Pull primitivesPush and Pull are the two fundamental primitives used to structure and maintain the resilientpriority queue. Their execution is triggered whenever one of the buffer violates a size invariant inorder to restore it without affecting the order invariants. The primitives operate by redistributingkeys among buffers by making use of S-Merge. The main idea is to move keys in the buffers inorder to have the smaller ones kept in the layers close to the insertion buffer so they can be quicklyretrieved by Deletemin operations, while moving the larger keys to the higher order layers. Ourimplementation of Push and Pull corresponds to the one in [10] with the difference that the S-Merge algorithm proposed in the previous section is used rather than the merge algorithm in [6].It is important to stress how this variation, while allowing a reduction of the running time of Pushand Pull, does not affect the correctness nor the functioning of the primitives since the mergealgorithm is used with a black-box approach. We remark that, due to the additional structureintroduced by using the auxiliary priority queues PU and PD, every time a primitive involvingeither U0 or D0 is invoked it is necessary to restore them to be faithfully ordered buffers. Thiscan be easily achieved by concatenating the resiliently sorted UP0 (resp., DS0 ), with US0 (resp.,DS0 ). We can exploit PU (resp., PD) to resiliently sort UP0 (resp., DS0 ) by successively extractingthe minimum values in the priority queue. For the sake of completeness, we describe now thePush and Pull primitives and we refer to [10] for further details.Push. The Push primitive is invoked whenever the size of an up buffer Ui grows over the thresholdvalue si/2, therefore breaking the size invariant (I4). The execution of Push(Ui) works as follows.If Li is the last layer, then a new empty layer Li+1 is created. Buffers Ui, Di and Ui+1 are mergedinto a sequence M using the S-Merge algorithm. Then the first |Di| − δ keys of M are placedin a new buffer D′i, the remaining |Ui+1| + |Ui| + δ keys are placed in a new buffer U ′i+1, andan empty U ′i buffer is created. Finally, the newly created buffers U′i , D′i and U′i+1 are used torespectively replace the old buffers Ui, Di and Ui+1, which are then deallocated. If Li is the15last layer, U ′i+1 replaces Di+1 instead of Ui+1. If the new buffer U′i+1 contains too many keys,breaking the size invariant (I4), the Push primitive is invoked on U ′i+1. Furthermore, since D′iis smaller than Di, it could violate the size invariant (I3). This violation is handled at the endof the sequence of Push invocations on up buffers of layers Li, Li+1, . . . , Lj , 0 ≤ i < j < k (wesuppose the i and j indexes to be stored in safe memory). After all the j− i+ 1 invocations, theaffected down buffers are analyzed by simply following the pointers among buffers starting fromUi, and by invoking the Pull primitive (see below) on the down buffer not satisfying the invariant(I3).Pull. The Pull primitive is invoked whenever the size of a down buffer Di goes below the thresholdvalue si/2, therefore breaking the size invariant (I3). Since this invariant does not hold for thelast layer, we must have that Li is not the last layer. During the execution of Pull(Di) buffersDi, Ui+1, and Di+1 are merged into a sequence M using the S-Merge algorithm. The first sikeys of M are placed in a new buffer D′i, the following |Di+1| − (si − |Di|)− δ keys are writtento D′i+1, while the remaining keys in M are placed in a new buffer U′i+1. The newly createdbuffers D′i, D′i+1 and U′i+1 are then used to respectively replace the old buffers Di, Di+1 andUi+1, which are then deallocated. If the down and up buffers in layer Li+1 are empty after thisoperation, then layer Li+1 is removed (this can happen only if Li+1 is the last layer). Resultingfrom this operation, D′i+1 may break the size invariant (I3), if this is the case Pull is invoked onD′i+1. Additionally, after the merge, U i+1 may break the size invariant (I4). This violation ishandled at the end of the sequence of Pull invocations on down buffers of layers Li, Li+1, . . . , Lj ,0 ≤ i < j < k (we suppose the i and j indexes to be stored in safe memory). After all thej − i + 1 invocations, all the affected up buffers are analyzed by simply following the pointersamong buffers starting from Di, and by invoking the Push primitive wherever invariant (I4) isnot satisfied.3.2. Correctness and complexity analysisIn order to prove the correctness of the proposed resilient priority queue we show thatDeletemin returns the minimum faithful key in the priority queue or an even smaller corruptedvalue. As a first step, it is necessary to ensure that the invocation of one of the primitives Pushor Pull, triggered by an up or down buffer violating a size invariant I3 or I4, does not cause theorder invariants to be broken. The Push and Pull primitives used in our priority queue coincidewith the ones presented for the maintenance of the resilient priority queue in [10]: despite thefact that in our implementation the threshold si is changed to 2i+1(S log2 +δ (logS + δ/S)), theproofs provided in [10] (Lemmas 1 and 3) concerning the correctness of Push and Pull still applyin our case. We report here the statements of the cited lemmas:Lemma 4 ([10, Lemma 1]). The Pull and Push primitives preserve the order invariants.16Lemma 5 ([10, Lemma 3]). If a size invariant is broken for a buffer in L0, invoking Pull orPush on that buffer restores the invariants. Furthermore, during this operation Pull and Pushare invoked on the same buffer at most once. No other invariants are broken before or after thisoperation.For the complete proofs of these lemmas we refer the reader to the original work in [10]. Itis important to remark that both proofs are independent of the value used as size threshold andhence these proofs hold for our implementation as well. We can therefore conclude that when asize invariant is broken for a buffer in Li the consequent invocation of Push or Pull does indeedrestore the size invariant while preserving the order invariants which are thus maintained at alltimes.Concerning the computational cost of the primitives, an analysis carried out using the poten-tial function method [19, Section 17.3] allows to conclude that the amortized time needed for theexecution of both Push and Pull is negligible. A proof of this fact can be obtained by plugging thecomplexity of the S-Merge algorithm and the threshold value si defined in our implementationin the proof proposed in [10, Lemma 5].Lemma 6. The amortized cost of the Push and Pull primitives is negligible.Proof: We now upper bound the amortized cost of a call to the Push function on the up buffer Uiand we ignore at the moment the subsequent chain of calls to Push and Pull(a similar argumentapplies to Pull). The cost is computed by exploiting the following potential function definedin [10]:Φ =k∑i=1(c1|Ui| (log n− i) + ic2|Di|) .When a Push operation on Ui is performed, first the Ui, Di and Ui+1 buffers are merged andthen the sorted values are distributed into new buffers such that |U ′i | = 0, |D′i| = |Di| − δ and|U ′i+1| = |Ui+1|+ |Ui|+ δ. This leads to the following change in potential ∆Φ:∆Φ = −c1|Ui| (log n− i)− ic2δ + c1 (|Ui|+ δ) (log n− (i+ 1))= −c1|Ui|+ δ (−ic2 + c1 log n− ic1 − c1) .Push is invoked when (I4) is not valid for Ui and therefore |Ui| > si/2 = 2i(S log2 n+ δ (logS + δ/S)).Then, standard computations show that, for some constant c′ > 0 independent of c1, we have∆Φ ≤ −c1|Ui|+ c1δ log n ≤ −c1c′|Ui|.The time required for the execution of Push, including the time needed to retrieve the reli-ably stored pointers of the up and down buffers, is dominated by the computational cost ofmerging Ui, Di and Ui+1 which, using the S-Merge algorithm, is upper bounded by Tm =O (|Ui|+ |Di|+ |Ui+1|+ α (logS + δ/S)). By the potential method [19, Section 17.3], the amor-tized cost of Push follows by adding the merging time Tm to the potential variation ∆Φ. Since17|Ui| ∈ Θ(2i(S log2 n+ δ (logS + δ/S))), we have Tm = Θ (|Ui|) = cm|Ui|, where cm is a suitableconstant that depends on S-Merge. The amortized cost of Push is (cm−c1c′)|Ui| and it can be ig-nored by conveniently tweaking c1 according to the values of cm and c′ so that (cm−c1c′)|Ui| < 0.In the particular case for which an invocation of Push involves the buffers U0 and D0, wehave that prior to the standard operations, it is necessary to restore the buffers to their faithfullysorted version by resiliently sorting US0 (resp., DS0 ) and linking it with UP0 (resp., DP0 ). Thetime required to accomplish these operation is dominated by the time necessary to faithfully sortUS0 and DS0 according to the previously described technique, which is O (δ (logS + δ/S)). Thisimplies that the time required for restructuring U0 and D0 is sill dominated by the time requiredby Push and is hence negligible.Since each Push and Pull function is invoked on the same buffer at most once (Lemma 5) andthe amortized cost is negative, we have that the chain of Push and Pull operations that can startafter the initial call is negligible as well. The lemma follows.\\x03The following theorem evaluates the amortized cost of Insert and Deletemin in our resilientimplementation of the priority queue.Theorem 2. In the proposed resilient priority queue implementation, the Deletemin operationreturns the minimum faithful key in the priority queue or an even smaller corrupted one anddeletes it. Both Deletemin and Insert operations require O (log n+ δ/S) amortized time. Thepriority queue uses Θ (S) safe memory words and Θ (n) faulty memory words.Proof: We first observe that the size and order invariants can be considered maintained at all timesthanks to the maintenance Push and Pull tasks (see Lemmas 4 and 5), with the aforementionedexception on the first δ + 1 keys in the up and down buffers in L0. Moreover, by Lemma 6, thecost of Push and Pull can be ignored in our argument.We now focus on the correctness and complexity of Deletemin. Let v1, v2, v3 and v4 bethe minimum values in I0, PI , PU and PD, respectively. Deletemin evaluates these four valuesby scanning all the values in I0 and by performing a Peekmin operation for PI , PU and PD,respectively. By construction, each value in PI is selected as the minimum among the keysstored in the associated buffers: since PI is maintained in the safe memory, v2 is smaller thanany faithful value in the associated buffers. Similarly, v3 is smaller than the faithful δ+1 entries inUS0 , and thus of the remaining faithful entries in UP0 and of all entries in the up and down buffersfor invariant (I1). Similarly, we also have that v4 is smaller than all faithful keys in D0. We canthen conclude that min{v1, v2, v3, v4} is either the minimum faithful key in the priority queue oran even smaller corrupted value. The time for determining the minimum key and removing it isO (log n+ δ/S).We now discuss the correctness and complexity of Insert. The correctness of the insertionis evident since the input key is inserted in some support buffer and can be only removed by18Deletemin. Inserting a key in the immediate insertion buffer requires constant time. If I0 isfull and a new node of the priority queue PI needs to be created, a total O (log n+ δ/S) timeis required in order to find the minimum among the keys in I0 and to insert the new node inPI . When PI itself is full (i.e., contains S nodes), we have that O(S log2 n+ δ (log n+ δ/S))time is required to faithfully sort all keys in I0 and in the buffers managed through PI and PU ,to faithfully merge them with US0 , and to rebuild PU and PS . However, it will be necessaryto perform these operations at most once every Θ (S log n+ δ) key insertions and therefore itsamortized cost is O (log n+ δ/S).We recall that the algorithm invokes a global rebuilding every time the number of keyschanges by a Θ (n) factor. Since the cost of the rebuilding is dominated by the cost of the S-Sortalgorithm, which is O (n log n+ δ(δ/S + logS)), the amortized cost is O (log n+ δ/S).By opportunely doubling or halving the space reserved for the immediate buffer I0, the spacerequired for I0 is always at most twice the number of keys actually in the buffer. Additionally,the space required for the buffers maintained by PI is at most double than the number of keysactually in the buffer itself. The space required for each layer L0, . . . , Lk−1 with k ∈ O (log n),including the reliably written structural information, is proportional to the number of storedkeys, and therefore Θ (n) faulty memory words are used to store all the layers. Finally, Θ (S)safe memory words are required to maintain the priority queues PI , PD and PU and for thecorrect execution of S-Merge and S-Sort. The theorem follows. \\x034. ConclusionIn this paper we have shown that, for the resilient sorting problem and the priority queuedata structure, the presence of a safe memory of size S can be exploited in order to reduce thecomputational overhead due to the presence of corrupted values by a factor Θ (S). As futureresearch, it would be interesting to investigate which other problems can benefit of a non constantsafe memory and propose tradeoffs highlighting the achievable performance with respect to thesize of the available safe memory. We observe that not all problems can in fact exploit an S-sizesafe memory: indeed the the Ω (log n+ δ) lower bound for searching derived in [5] applies even ifa safe memory of size S ≤ \\x0fn, for a suitable constant \\x0f ∈ (0, 1), is available. Finally, we remarkthat the analysis of tradeoffs between the safe memory size and the performance achievable byresilient algorithms may provide useful insights for designing hybrid systems mounting both cheapfaulty memory and expensive ECC memory, as recently studied in [17].AcknowledgementsThe authors would like to thank G. Brodal, I. Finocchi and an anonymous reviewer foruseful comments. This work was supported, in part, by University of Padova under projectsSTPD08JA32 and CPDA121378, and by MIUR of Italy under project AMANDA.19References[1] R. Baumann, Radiation-induced soft errors in advanced semiconductor technologies, IEEETrans. Devive and Materials Reliability 5 (3) (2005) 305–316. doi:10.1109/TDMR.2005.853449.[2] B. Schroeder, E. Pinheiro, W. D. Weber, DRAM errors in the wild: a large-scale field study,Communications of the ACM 54 (2) (2011) 100–107. doi:10.1145/1897816.1897844.[3] S. Govindavajhala, A. W. Appel, Using memory errors to attack a virtual machine, in: Proc.IEEE Symposium on Security and Privacy, 2003, pp. 154–165. doi:10.1109/SECPRI.2003.1199334.[4] I. Finocchi, F. Grandoni, G. F. Italiano, Designing reliable algorithms in unreliable memo-ries, Computer Science Review 1 (2) (2007) 77–87. doi:10.1016/j.cosrev.2007.10.001.[5] I. Finocchi, G. F. Italiano, Sorting and searching in faulty memories, Algorithmica 52 (3)(2008) 309–332. doi:10.1007/s00453-007-9088-4.[6] I. Finocchi, F. Grandoni, G. F. Italiano, Optimal resilient sorting and searching in thepresence of memory faults, Theoretical Computer Science 410 (44) (2009) 4457–4470. doi:10.1016/j.tcs.2009.07.026.[7] T. Kopelowitz, N. Talmon, Selection in the presence of memory faults, with applica-tions to in-place resilient sorting, in: Proc. 23rd International Symposium on Algo-rithms and Computation (ISAAC), Vol. 7676 of LNCS, 2012, pp. 558–567. doi:10.1007/978-3-642-35261-4\\\\_58.[8] S. Caminiti, I. Finocchi, E. G. Fusco, F. Silvestri, Dynamic programming in faulty memoryhierarchies (cache-obliviously), in: Proc. 31st Conference on Foundations of Software Tech-nology and Theoretical Computer Science (FSTTCS), Vol. 13 of LIPIcs, 2011, pp. 433–444.doi:10.4230/LIPIcs.FSTTCS.2011.433.[9] I. Finocchi, F. Grandoni, G. F. Italiano, Resilient dictionaries, ACM Transactions on Algo-rithms 6 (1) (2009) 1:1–1:19. doi:10.1145/1644015.1644016.[10] A. G. Jørgensen, G. Moruz, T. Mølhave, Priority queues resilient to memory faults, in: Proc.10th Workshop on Algorithms and Data Structures (WADS), Vol. 4619 of LNCS, 2007, pp.127–138. doi:10.1007/978-3-540-73951-7\\\\_12.[11] P. Rech, L. Pilla, F. Silvestri, P. Navaux, L. Carro, Neutron sensitivity and software harden-ing strategies for matrix multiplication and FFT on graphics processing units, in: Proc.3rd Workshop on Fault-tolerance for HPC at Extreme Scale (FTXS), 2013, pp. 13–20.doi:10.1145/2465813.2465816.20[12] F. Gieseke, G. Moruz, J. Vahrenhold, Resilient k-d trees: k-means in space revisited, Fron-tiers of Computer Science 6 (2) (2012) 166–178. doi:10.1007/s11704-012-2870-8.[13] P. Christiano, E. Demaine, S. Kishore, Lossless fault-tolerant data structures with additiveoverhead, in: Proc. 12th Workshop on Algorithms and Data Structures (WADS), Vol. 6844of LNCS, 2011, pp. 243–254. doi:10.1007/978-3-642-22300-6\\\\_21.[14] L. Pilla, P. Rech, F. Silvestri, C. Frost, P. Navaux, M. Sonza Reorda, L. Carro, Software-based hardening strategies for neutron sensitive FFT algorithms on GPUs, IEEE Transac-tions on Nuclear Science 61 (4) (2014) 1874–1880. doi:10.1109/TNS.2014.2301768.[15] U. F. Petrillo, F. Grandoni, G. F. Italiano, Data structures resilient to memory faults: Anexperimental study of dictionaries, ACM Journal of Experimental Algorithmics 18. doi:10.1145/2444016.2444022.[16] U. F. Petrillo, I. Finocchi, G. F. Italiano, Experimental study of resilient algorithms anddata structures, in: Proc. 9th International Symposium Experimental Algorithms (SEA),2010, pp. 1–12. doi:10.1007/978-3-642-13193-6\\\\_1.[17] D. Li, Z. Chen, P. Wu, J. S. Vetter, Rethinking algorithm-based fault tolerance witha cooperative software-hardware approach, in: Proc. International Conference on HighPerformance Computing, Networking, Storage and Analysis (SC), 2013, pp. 44:1–44:12.doi:10.1145/2503210.2503226.[18] L. Arge, M. A. Bender, E. D. Demaine, B. H. Minkley, J. I. Munro, An optimal cache-oblivious priority queue and its application to graph algorithms, SIAM Journal Computing36 (6) (2007) 1672–1695. doi:10.1137/S0097539703428324.[19] T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein, Introduction to Algorithms, 3rdEdition, The MIT Press, 2009.21'}\n",
      "{'_id': ObjectId('6815e61add259dc7e6e1cd31'), 'authors': 'Andreux M., Boscaini D., Bruna J., Choromanska A., Clevert D., Cosmo L., Dechter R., Erhan D., Glorot X., Goodfellow I., Gregor K., Han X., Hochreiter S., Jaderberg M., Kawaguchi K., Kingma D. P., Krizhevsky A., Lähner Z., Mallat S., Masci J., Mnih V., Radford A., Rodolà E., Rusinkiewicz S., Srivastava R. K., Srivastava R. K., Stollenga M., Wang S., Werbos P. J., Wu Z., Xu K., Yu F.', 'year': '2019', 'title': 'Geometric deep learning', 'full_text': 'Geometric deep learning on graphs and manifolds using mixture model CNNs\\nFederico Monti1∗ Davide Boscaini1∗ Jonathan Masci1,4\\nEmanuele Rodola`1 Jan Svoboda1 Michael M. Bronstein1,2,3\\n1USI Lugano 2Tel Aviv University 3Intel Perceptual Computing 4Nnaisense\\nAbstract\\nDeep learning has achieved a remarkable performance\\nbreakthrough in several fields, most notably in speech\\nrecognition, natural language processing, and computer vi-\\nsion. In particular, convolutional neural network (CNN) ar-\\nchitectures currently produce state-of-the-art performance\\non a variety of image analysis tasks such as object detec-\\ntion and recognition. Most of deep learning research has\\nso far focused on dealing with 1D, 2D, or 3D Euclidean-\\nstructured data such as acoustic signals, images, or videos.\\nRecently, there has been an increasing interest in geomet-\\nric deep learning, attempting to generalize deep learning\\nmethods to non-Euclidean structured data such as graphs\\nand manifolds, with a variety of applications from the do-\\nmains of network analysis, computational social science,\\nor computer graphics. In this paper, we propose a uni-\\nfied framework allowing to generalize CNN architectures to\\nnon-Euclidean domains (graphs and manifolds) and learn\\nlocal, stationary, and compositional task-specific features.\\nWe show that various non-Euclidean CNN methods previ-\\nously proposed in the literature can be considered as par-\\nticular instances of our framework. We test the proposed\\nmethod on standard tasks from the realms of image-, graph-\\nand 3D shape analysis and show that it consistently outper-\\nforms previous approaches.\\n1. Introduction\\nIn recent years, increasingly more fields have to deal\\nwith geometric non-Euclidean structured data such as man-\\nifolds or graphs. Social networks are perhaps the most\\nprominent example of such data; additional examples in-\\nclude transportation networks, sensor networks, functional\\nnetworks representing anatomical and functional structure\\nof the brain, and regulatory networks modeling gene expres-\\nsions. In computer graphics, 3D objects are traditionally\\nmodeled as Riemannian manifolds. The success of deep\\nlearning methods in many fields has recently provoked a\\n∗Equal contribution\\nkeen interest in geometric deep learning [11] attempting to\\ngeneralize such methods to non-Euclidean structure data.\\n1.1. Related works\\nImage processing. Classical deep learning algorithms\\nbuild on top of traditional signal processing that has been\\ndeveloped primarily for linear shift-invariant systems, natu-\\nrally arising when dealing with signals on Euclidean spaces.\\nIn this framework, basic filtering operations are represented\\nas convolutions. A significant paradigm shift in image pro-\\ncessing came with the pioneering work of Perona and Ma-\\nlik [35], suggesting the use of non-shift-invariant image fil-\\ntering preserving the edge structures. This work was the\\nprecursor of a whole new class of PDE-based methods for\\nimage processing. Sochen et al. [50] brought geometric\\nmodels into image processing, considering images as man-\\nifolds and employing tools from differential geometry for\\ntheir processing and analysis. More recent graph-based im-\\nage processing methods relying on spectral graph theory\\n[46, 40, 61, 29] can be traced back to these works.\\nManifold learning. A similar trend of employing geo-\\nmetric models can also be observed in the machine learn-\\ning community in the past decade. Modelling data as\\nlow-dimensional manifolds is the core of manifold learning\\ntechniques such as Laplacian eigenmaps [3] for non-linear\\ndimensionality reduction, spectral clustering [34] or spec-\\ntral hashing [57].\\nSignal processing on graphs. More recent works tried to\\ngeneralize signal processing methods to graph-based data\\n[47]. Spectral analysis techniques were extended to graphs\\nconsidering the orthogonal eigenfunctions of the Laplacian\\noperator as a generalization of the Fourier basis. Construc-\\ntions such as wavelets [14, 18, 21, 39, 45] or algorithms\\nsuch as dictionary learning [62], Lasso [9], PCA [43, 44],\\nor matrix completion [23] originally developed for the Eu-\\nclidean domain, were also applied to graph-structured data.\\nDeep learning on graphs. The earliest attempts to gener-\\nalize neural networks to graphs we are aware of are due to\\n1\\nar\\nX\\niv\\n:1\\n61\\n1.\\n08\\n40\\n2v\\n3 \\n [c\\ns.C\\nV]\\n  6\\n D\\nec\\n 20\\n16\\nScarselli et al. [19, 41]. This work remained practically un-\\nnoticed and has been rediscovered only recently [30, 52].\\nThe interest in non-Euclidean deep learning has recently\\nsurged in the computer vision and machine learning com-\\nmunities after the seminal work of Bruna et al. [12, 22], in\\nwhich the authors formulated CNN-like [28] deep neural ar-\\nchitectures on graphs in the spectral domain, employing the\\nanalogy between the classical Fourier transforms and pro-\\njections onto the eigenbasis of the graph Laplacian operator\\n[47]. In a follow-up work, Defferrard et al. [15] proposed\\nan efficient filtering scheme that does not require explicit\\ncomputation of the Laplacian eigenvectors by using recur-\\nrent Chebyshev polynomials. Kipf and Welling [26] further\\nsimplified this approach using simple filters operating on\\n1-hop neighborhoods of the graph. Similar methods were\\nproposed in [2] and [17]. Finally, in the network analysis\\ncommunity, several works constructed graph embeddings\\n[36, 53, 13, 20, 60] methods inspired by the Word2Vec tech-\\nnique [33].\\nA key criticism of spectral approaches such as [12, 22,\\n15] is the fact that the spectral definition of convolution\\nis dependent on the Fourier basis (Laplacian eigenbasis),\\nwhich, in turn is domain-dependent. It implies that a spec-\\ntral CNN model learned on one graph cannot be trivially\\ntransferred to another graph with a different Fourier basis,\\nas it would be expressed in a ‘different language’.\\nDeep learning on manifolds. In the computer graphics\\ncommunity, we can notice a parallel effort of generalizing\\ndeep learning architectures to 3D shapes modeled as mani-\\nfolds (surfaces). Masci et al. [32] proposed the first intrin-\\nsic version of convolutional neural networks on manifolds\\napplying filters to local patches represented in geodesic po-\\nlar coordinates [27]. Boscaini et al. [7] used anisotropic\\nheat kernels as an alternative way of extracting intrinsic\\npatches on manifolds. In [6], the same authors proposed a\\nCNN-type architecture in the spatio-frequency domain us-\\ning the windowed Fourier transform formalism [48]. Sinha\\net al. [49] used geometry images representation to obtain\\nEuclidean parametrization of 3D shapes on which standard\\nCNNs can be applied.\\nThe key advantage of spatial techniques is that they gen-\\neralize across different domains, which is a crucial property\\nin computer graphics applications (where a CNN model can\\nbe trained on one shape and applied to another one). How-\\never, while spatial constructions such as anisotropic heat\\nkernels have a clear geometric interpretation on manifolds,\\ntheir interpretation on general graphs is somewhat elusive.\\n1.2. Main contribution\\nIn this paper, we present mixture model networks\\n(MoNet), a general framework allowing to design convo-\\nlutional deep architectures on non-Euclidean domains such\\nas graphs and manifolds. Our approach follows the general\\nphilosophy of spatial-domain methods such as [32, 7, 2],\\nformulating convolution-like operations as template match-\\ning with local intrinsic ‘patches’ on graphs or manifolds.\\nThe key novelty is in the way in which the patch is ex-\\ntracted: while previous approaches used fixed patches, e.g.\\nin geodesic or diffusion coordinates, we use a parametric\\nconstruction. In particular, we show that patch operators\\ncan be constructed as a function of local graph or mani-\\nfolds pseudo-coordinates, and study a family of functions\\nrepresented as a mixture of Gaussian kernels. Such a con-\\nstruction allows to formulate previously proposed Geodesic\\nCNN (GCNN) [32] and Anisotropic CNN (ACNN) [7] on\\nmanifolds or GCN [26] and DCNN [2] on graphs as partic-\\nular instances of our approach.\\nAmong applications on which we exemplify our ap-\\nproach are classical problems from the realms of image-,\\ngraph- and 3D- shape analysis. In the first class of prob-\\nlems, the task is to classify images, treated as adjacency\\ngraphs of superpixels. In the second class of problems, we\\nperform vertex-wise classification on a graph representing\\na citation network of scientific papers. Finally, we consider\\nthe problem of finding dense intrinsic correspondence be-\\ntween 3D shapes, treated as manifolds. In all the above\\nproblems, we show that our approach consistently outper-\\nforms previously proposed non-Euclidean deep learning\\nmethods.\\n2. Deep learning on graphs\\nLet G = ({1, . . . , n}, E ,W) be an undirected weighted\\ngraph, represented by the adjacency matrix W = (wij),\\nwhere wij = wji, wij = 0 if (i, j) /∈ E and wij > 0\\nif (i, j) ∈ E . The (unnormalized) graph Laplacian is an\\nn×n symmetric positive-semidefinite matrix ∆ = D−W,\\nwhere D = diag\\n(∑\\nj 6=i wij\\n)\\nis the degree matrix.\\nThe Laplacian has an eigendecomposition ∆ = ΦΛΦ>,\\nwhere Φ = (φ1, . . .φn) are the orthonormal eigenvectors\\nand Λ = diag(λ1, . . . , λn) is the diagonal matrix of cor-\\nresponding eigenvalues. The eigenvectors play the role of\\nFourier atoms in classical harmonic analysis and the eigen-\\nvalues can be interpreted as frequencies. Given a signal\\nf = (f1, . . . , fn)\\n> on the vertices of graph G, its graph\\nFourier transform is given by fˆ = Φ>f . Given two signals\\nf ,g on the graph, their spectral convolution can be defined\\nas the element-wise product of the Fourier transforms,\\nf ? g = Φ(Φ>f) ◦ (Φ>g) = Φ diag(gˆ1, . . . , gˆn)fˆ , (1)\\nwhich corresponds to the property referred to as the Convo-\\nlution Theorem in the Euclidean case.\\nSpectral CNN. Bruna et al. [12] used the spectral defini-\\ntion of convolution (1) to generalize CNNs on graphs, with\\na spectral convolutional layer of the form\\nfoutl = ξ\\n(\\np∑\\nl′=1\\nΦkGˆl,l′Φ\\n>\\nk f\\nin\\nl′\\n)\\n. (2)\\nHere the n × p and n × q matrices Fin = (f in1 , . . . , f inp )\\nand Fout = (fout1 , . . . , f\\nout\\nq ) represent respectively the p-\\nand q-dimensional input and output signals on the vertices\\nof the graph, Φ = (φ1, . . . ,φk) is an n × k matrix of\\nthe first eigenvectors, Gˆl,l′ = diag(gˆl,l′,1, . . . , gˆl,l′,k) is a\\nk × k diagonal matrix of spectral multipliers representing\\na learnable filter in the frequency domain, and ξ is a non-\\nlinearity (e.g. ReLU) applied on the vertex-wise function\\nvalues. The analogy of pooling in this framework is a graph\\ncoarsening procedure, which, given a graph with n vertices,\\nproduces a graph with n′ < n vertices and transfers signals\\nfrom the vertices of the fine graph to those of the coarse one.\\nWhile conceptually important, this framework has sev-\\neral major drawbacks. First, the spectral filter coefficients\\nare basis dependent, and consequently, a spectral CNN\\nmodel learned on one graph cannot be applied to another\\ngraph. Second, the computation of the forward and inverse\\ngraph Fourier transform incurs expensive O(n2) multipli-\\ncation by the matrices Φ,Φ>, as there is no FFT-like algo-\\nrithms on general graphs. Third, there is no guarantee that\\nthe filters represented in the spectral domain are localized in\\nthe spatial domain; assuming k = O(n) eigenvectors of the\\nLaplacian are used, a spectral convolutional layer requires\\npqk = O(n) parameters to train.\\nSmooth Spectral CNN. In a follow-up work, Henaff et\\nal. [22] argued that smooth spectral filter coefficients result\\nin spatially-localized filters and used parametric filters of\\nthe form\\ngˆi =\\nr∑\\nj=1\\nαjβj(λi), (3)\\nwhere β1(λ), . . . , βr(λ) are some fixed interpolation ker-\\nnels, and α = (α1, . . . , αr) are the interpolation co-\\nefficients. In matrix notation, the filter is expressed as\\ndiag(Gˆ) = Bα, where B = (bij) = (βj(λi)) is a k×r ma-\\ntrix. Such a parametrization results in filters with a number\\nof parameters constant in the input size n.\\nChebyshev Spectral CNN (ChebNet). In order to allevi-\\nate the cost of explicitly computing the graph Fourier trans-\\nform, Defferrard et al. [15] used an explicit expansion in the\\nChebyshev polynomial basis to represent the spectral filters\\ngα(∆) =\\nr−1∑\\nj=0\\nαjTj(∆˜) =\\nr−1∑\\nj=0\\nαjΦTj(Λ˜)Φ\\n>, (4)\\nwhere ∆˜ = 2λ−1n ∆− I is the rescaled Laplacian such that\\nits eigenvalues Λ˜ = 2λ−1n Λ − I are in the interval [−1, 1],\\nα is the r-dimensional vector of polynomial coefficients\\nparametrizing the filter, and\\nTj(λ) = 2λTj−1(λ)− Tj−2(λ), (5)\\ndenotes the Chebyshev polynomial of degree j defined in a\\nrecursive manner with T1(λ) = λ and T0(λ) = 1.\\nSuch an approach has several important advantages.\\nFirst, it does not require an explicit computation of the\\nLaplacian eigenvectors. Due to the recursive definition of\\nthe Chebyshev polynomials, the computation of the filter\\ngα(∆)f entails applying the Laplacian r times, resulting\\nin O(rn) operations. Second, since the Laplacian is a lo-\\ncal operator affecting only 1-hop neighbors of a vertex and\\naccordingly its (r − 1)st power affects the r-hop neighbor-\\nhood, the resulting filters are localized.\\nGraph convolutional network (GCN). Kipf and Welling\\n[26] considered the construction of [15] with r = 2,\\nwhich, under the additional assumption of λn ≈ 2, and\\nα = α0 = −α1 yields single-parametric filters of the form\\ngα(f) = α(I + D\\n−1/2WD−1/2)f . Such a filter is numeri-\\ncally unstable since the maximum eigenvalue of the matrix\\nI + D−1/2WD−1/2 is 2; a renormalization\\ngα(f) = αD˜\\n−1/2W˜D˜−1/2f , (6)\\nwith W˜ = W + I and D˜ = diag(\\n∑\\nj 6=i w˜ij) is introduced\\nby the authors in order to cure such problem and allow mul-\\ntiple convolutional levels to be casted one after the other.\\nDiffusion CNN (DCNN). A different spatial-domain\\nmethod was proposed by Atwood and Towsley [2], who\\nconsidered a diffusion (random walk) process on the graph.\\nThe transition probability of a random walk on a graph\\nis given by P = D−1W. Different features are pro-\\nduced by applying diffusion of different length (the powers\\nP0, . . . ,Pr−1),\\nfoutl,j = ξ(wljP\\njf inl ),\\nwhere the n × p and n × pr matrices Fin = (f in1 , . . . , f inp )\\nand Fout = (fout1,1 , . . . , f\\nout\\np,r ) represent the p- and pr-\\ndimensional input and output signals on the vertices of the\\ngraph and W = (wlj) is the p× r matrix of weights.\\n3. Deep learning on manifolds\\nLet X be a d-dimensional differentiable manifold, possi-\\nbly with boundary ∂X . Around point x ∈ X , the manifold\\nis homeomorphic to a d-dimensional Euclidean space re-\\nferred to as the tangent space and denoted by TxX . An\\ninner product 〈·, ·〉TxX : TxX × TxX → R depending\\nsmoothly on x is called the Riemannian metric. In the fol-\\nlowing, we denote by f : X → R smooth real functions\\n(scalar fields) on the manifold. In shape analysis, 3D shapes\\nare modeled as 2-dimensional manifolds (surfaces), repre-\\nsenting the boundaries of 3D volumes.\\nTable 1. Several CNN-type geometric deep learning methods on graphs and manifolds can be obtained as a particular setting of the\\nproposed framework with an appropriate choice of the pseudo-coordinates and weight functions in the definition of the patch operator. x\\ndenotes the reference point (center of the patch) and y a point within the patch. x denotes the Euclidean coordinates on a regular grid.\\nα¯, σ¯ρ, σ¯θ and u¯j , θ¯j , j = 1, . . . , J denote fixed parameters of the weight functions.\\nMethod Pseudo-coordinates u(x, y) Weight function wj(u), j = 1, . . . , J\\nCNN [28] Local Euclidean x(x, y) = x(y)− x(x) δ(u− u¯j)\\nGCNN [32] Local polar geodesic ρ(x, y), θ(x, y) exp(− 12 (u− u¯j)>\\n(\\nσ¯2ρ\\nσ¯2θ\\n)−1\\n(u− u¯j))\\nACNN [7] Local polar geodesic ρ(x, y), θ(x, y) exp(− 12u>Rθ¯j ( α¯ 1 ) R>¯θju)\\nGCN [26] Vertex degree deg(x), deg(y)\\n(\\n1− |1− 1√u1 |\\n)(\\n1− |1− 1√u2 |\\n)\\nDCNN [2] Transition probability in r hops p0(x, y), . . . , pr−1(x, y) id(uj)\\nGeodesic CNN (GCNN). Masci et al. [32] introduced a\\ngeneralization of CNNs on 2-dimensional manifolds, based\\non the definition of a local charting procedure in geodesic\\npolar coordinates [27]. Such a construction, named the\\npatch operator\\n(D(x)f)(ρ, θ) =\\n∫\\nX\\nwρ,θ(x, y)f(y)dy\\nmaps the values of the function f at a neighborhood of the\\npoint x ∈ X into the local polar coordinates ρ, θ. Here dy\\ndenotes the area element induced by the Riemannian metric,\\nand wρ,θ(x, y) is a weighting function localized around ρ, θ\\n(see examples in Figure 1). D(x)f can be regarded as a\\npatch on the manifold; the geodesic convolution\\n(f?g)(x) = max\\n∆θ∈[0,2pi)\\n∫ 2pi\\n0\\n∫ ρmax\\n0\\ng(ρ, θ+∆θ)(D(x)f)(ρ, θ)dρdθ,\\ncan be thought of as matching a template g(ρ, θ) with the\\nextracted patch at each point, where the maximum is taken\\nover all possible rotations of the template in order to re-\\nsolve the origin ambiguity in the angular coordinate. The\\ngeodesic convolution is used to define an analogy of a tra-\\nditional convolutional layer in GCNN, where the templates\\ng are learned.\\nAnisotropic CNN (ACNN). Boscaini et al. [7] consid-\\nered the anisotropic diffusion equation on the manifold\\nft(x, t) = −divX (A(x)∇X f(x, t)) , (7)\\nwhere ∇X and divX denote the intrinsic gradient and di-\\nvergence, respectively, f(x, t) is the temperature at point\\nx and time t, and the conductivity tensor A(x) (operating\\non the gradient vectors in the tangent space TxX ) allows to\\nmodel heat flow that is position- and direction-dependent.\\nIn particular, they used the 2× 2 tensor\\nAαθ(x) = Rθ(x)\\n(\\nα\\n1\\n)\\nR>θ (x) , (8)\\nwhere matrix Rθ is a rotation by θ in the tangent plane w.r.t.\\nthe maximal curvature direction, and the parameter α > 0\\ncontrols the degree of anisotropy (isotropic diffusion is ob-\\ntained for α = 1). Using as initial condition f(x, 0) a point\\nsource of heat at x, the solution to the heat equation (7) is\\ngiven by the anisotropic heat kernel hαθt(x, y), represent-\\ning the amount of heat that is transferred from point x to\\npoint y at time t. By varying the parameters α, θ and t (con-\\ntrolling respectively the elongation, orientation, and scale\\nof the kernel) one obtains a collection of kernels that can be\\nused as weighting functions in the construction of the patch\\noperator (see examples in Figure 1). This gives rise to an\\nalternative charting to the geodesic patches of GCNN, more\\nrobust to geometric noise, and more efficient to compute.\\nBoth GCNN and ACNN operate in the spatial domain\\nand thus do not suffer from the inherent inability of spec-\\ntral methods to generalize across different domains. These\\nmethods were shown to outperform all the known hand-\\ncrafted approaches for finding intrinsic correspondence be-\\ntween deformable shapes [32, 7], a notoriously hard prob-\\nlem in computer graphics.\\n4. Our approach\\nThe main contribution of this paper is a generic spatial-\\ndomain framework for deep learning on non-Euclidean do-\\nmains such as graphs and manifolds. We use x to de-\\nnote, depending on context, a point on a manifold or a\\nvertex of a graph, and consider points y ∈ N (x) in the\\nneighborhood of x. With each such y, we associate a\\nd-dimensional vector of pseudo-coordinates u(x, y). In\\nthese coordinates, we define a weighting function (kernel)\\nwΘ(u) = (w1(u), . . . , wJ(u)), which is parametrized by\\nsome learnable parameters Θ. The patch operator can there-\\nfore be written in the following general form\\nDj(x)f =\\n∑\\ny∈N (x)\\nwj(u(x, y))f(y), j = 1, . . . , J, (9)\\nwhere the summation should be interpreted as an integral\\nin the case we deal with a continuous manifold, and J rep-\\nresents the dimensionality of the extracted patch. A spa-\\ntial generalization of the convolution on non-Euclidean do-\\nmains is then given by a template-matching procedure of\\nPolar coordinates ρ, θ 00.24681\\n0\\npi/63pi/24pi/65\\npi\\n7pi/64 3pi/25pi/311pi/6 GCNN\\n00.24681\\n0\\npi/63pi/24pi/65\\npi\\n7pi/64 3pi/25pi/311pi/6 ACNN\\n00.24681\\n0\\npi/63pi/24pi/65\\npi\\n7pi/64 3pi/25pi/311pi/6 MoNet\\nFigure 1. Left: intrinsic local polar coordinates ρ, θ on manifold around a point marked in white. Right: patch operator weighting functions\\nwi(ρ, θ) used in different generalizations of convolution on the manifold (hand-crafted in GCNN and ACNN and learned in MoNet). All\\nkernels are L∞-normalized; red curves represent the 0.5 level set.\\nthe form\\n(f ? g)(x) =\\nJ∑\\nj=1\\ngj Dj(x)f. (10)\\nThe two key choices in our construction are the pseudo-\\ncoordinates u and the weight functions w(u). Table 3\\nshows that other deep learning methods (including the clas-\\nsical CNN on Euclidean domains, DCN and DCNN on\\ngraphs, and GCNN and ACNN on manifolds) can be ob-\\ntained as particular settings of our framework with appro-\\npriate definition of u and w(u). For example, GCNN and\\nACNN boil down to using Gaussian kernels on local po-\\nlar geodesic coordinates ρ, θ on a manifold, and GCN can\\nbe interpreted as applying a triangular kernel on pseudo-\\ncoordinates given by the degree of the graph vertices.\\nIn this paper, rather than using fixed handcrafted weight\\nfunctions we consider parametric kernels with learnable pa-\\nrameters. In particular, a convenient choice is\\nwj(u) = exp(− 12 (u− µj)>Σ−1j (u− µj)), (11)\\nwhere Σj and µj are learnable d× d and d× 1 covariance\\nmatrix and mean vector of a Gaussian kernel, respectively.\\nFormulae (9–10) can thus be interpreted as a gaussian mix-\\nture model (GMM). We further restrict the covariances to\\nhave diagonal form, resulting in 2d parameters per kernel,\\nand a total of 2Jd parameters for the patch operator.\\nWhile extremely simple, we show in the next section that\\nthese additional degrees of freedom afford our architecture\\nsufficient complexity allowing it to outperform existing ap-\\nproaches. More complex versions of the weighting func-\\ntions could include additional non-linear transformation of\\nthe pseudo-coordinates u before feeding them to the Gaus-\\nsian kernel, or even more general network-in-a-network ar-\\nchitectures [31].\\n5. Results\\n5.1. Images\\nIn our first experiment, we applied the proposed method\\non a classical task of handwritten digit classification in the\\nMNIST dataset [28]. While almost trivial by todays stan-\\ndards, we nevertheless use this example to visualize an\\nimportant advantage of our approach over spectral graph\\nCNN methods. Our experimental setup followed [15]. The\\n28× 28 images were represented as graphs, where vertices\\ncorrespond to (super)pixels and edges represent their spatial\\nrelations. We considered two constructions: all images rep-\\nresented on the same graph (regular grid) and each image\\nrepresented as a different graph (Figure 2 left and right, re-\\nspectively). Furthermore, we varied the graph size: the full\\nand 14 grids contained 728 and 196 vertices, respectively,\\nwhile the superpixel-based graphs contained 300, 150, and\\n75 vertices.\\nThree methods were compared: classical CNN LeNet5\\narchitecture [28] (containing two convolutional, two max-\\npooling, and one fully-connected layer, applied on regular\\ngrids only), spectral ChebNet[15] and the proposed MoNet.\\nWe used a standard splitting of the MNIST dataset into\\ntraining-, testing-, and validation sets of sizes 55K, 10K,\\nand 5K images, respectively. LeNet used 2×2 max-pooling;\\nin ChebNet and MoNet we used three convolutional lay-\\ners, interleaved with pooling layers based on the Graclus\\nmethod [16] to coarsen the graph by a factor of four.\\nFor MoNet, we used polar coordinates u = (ρ, θ) of pix-\\nels (respectively, of superpixel barycenters) to produce the\\npatch operator; as the weighting functions of the patch op-\\nerator, 25 Gaussian kernels (initialized with random means\\nand variances) were used. Training was done with 350K it-\\nerations of Adam method [25], initial learning rate 10−4,\\nregularization factor 10−4, dropout probability 0.5, and\\nbatch size of 10.\\nTable 2 summarizes the performance of different algo-\\nrithms. On regular grids, all the methods perform approx-\\nimately equally well. However, when applying ChebNet\\non superpixel-based representations, the performance drops\\ndramatically (by up to almost 25%). The reason lies in the\\nkey drawback of spectral CNN models, wherein the def-\\ninition of the filters is basis- and thus domain-dependent.\\nSince in this case each image is represented as a different\\nRegular grid Superpixels\\nFigure 2. Representation of images as graphs. Left: regular grid\\n(the graph is fixed for all images). Right: graph of superpixel\\nadjacency (different for each image). Vertices are shown as red\\ncircles, edges as red lines.\\ngraph, the model fails to generalize well. The effect is most\\npronounced on smaller graphs (150 and 75 superpixels) that\\nvary strongly among each other. In contrast, the proposed\\nMoNet approach manifests consistently high accuracy, and\\nonly a light performance degradation is observed when the\\nimage presentation is too coarse (75 superpixels).\\nTable 2. Classification accuracy of classical Euclidean CNN\\n(LeNet5), spectral CNN (ChebNet) and the proposed approach\\n(MoNet) on different versions of the MNIST dataset. The setting\\nof all the input images sharing the same graph is marked with *.\\nDataset LeNet5 [28] ChebNet [15] MoNet\\n*Full grid 99.33% 99.14% 99.19%\\n* 14 grid 98.59% 97.70% 98.16%\\n300 Superpixels - 88.05% 97.30%\\n150 Superpixels - 80.94% 96.75%\\n75 Superpixels - 75.62% 91.11%\\n5.2. Graphs\\nIn the second experiment, we address the problem of\\nvertex classification on generic graphs. We used the pop-\\nular Cora and PubMed [42] citation graphs as our datasets.\\nIn each dataset, a vertex represents a scientific publication\\n(2708 vertices in Cora and 19717 in PubMed, respectively),\\nand an undirected unweighted edge represents a citation\\n(5429 and 44338 edges in Cora and PubMed). For each\\nvertex, a feature vector representing the content of the pa-\\nTable 3. Learning configuration used for Cora and PubMed exper-\\niments.\\nCora PubMed\\nLearning Algorithm Adam Adam\\nNumber of epochs 3000 1000\\nValidation frequency 0.01 0.04\\nLearning rate 0.1 0.1\\nDecay rate 10−1 -\\nDecay epochs 1500, 2500 -\\nEarly stopping No No\\nper is given (1433-dimensional binary feature vectors in\\nCora, and 500-dimensional tf-idf weighted word vectors in\\nPubMed). The task is to classify each vertex into one of the\\ngroundtruth classes (7 in Cora and 3 in PubMed).\\nWe followed verbatim the experimental settings pre-\\nsented in [60, 26]. The training sets consisted of 20 sam-\\nples per class; the validation and test sets consisted of 500\\nand 1000 disjoint vertices. The validation set was chosen\\nin order to reflect the probability distribution of the various\\nclasses over the entire dataset. We compared our approach\\nto all the methods compared in [26].\\nFor MoNet, we used the degrees of the nodes as the input\\npseudo-coordinates u(x, y) = ( 1√\\ndeg(x)\\n, 1√\\ndeg(y)\\n)>; these\\ncoordinates underwent an additional transformation in the\\nform of a fully-connected neural network layer u˜(x, y) =\\ntanh(Au(x, y) + b), where the r × 2 matrix A and r × 1\\nvector b were also learned (we used r = 2 for Cora and\\nr = 3 for PubMed). The Gaussian kernels were applied on\\ncoordinates u˜(x, y) yielding patch operators of the form\\nDj(x)fl =\\n∑\\ny∈N (x)\\ne−\\n1\\n2 (u˜(x,y)−µj)\\n>Σ−1j (u˜(x,y)−µj)fl(y),\\nwhere Σj , µj , j = 1, . . . , J are the r × r and r × 1 covari-\\nance matrices and mean vectors of the Gaussian kernels,\\nrespectively. DCNN, GCN and MoNet were trained in the\\nsame way in order to give a fair comparison (see training de-\\ntails in Table 3). The L2-regularization weights for MoNet\\nwere γ = 10−2 and 5×10−2 for Cora and PubMed, respec-\\ntively; for DCNN and GCN we used the values suggested\\nby the authors in [2] and [26].\\nThe vertex classification results of different methods are\\nsummarized in Table 4 and visualized in Figure 3. MoNet\\ncompares favorably to other approaches. The tuning of\\nthe network hyper-parameters has been fundamental in this\\ncase for avoiding overfitting, due to a very small size of the\\ntraining set. Being more general, our architecture is more\\ncomplex compared to GCN and DCNN and requires an ap-\\npropriate regularization to be used in such settings. At the\\nsame time, the greater complexity of our framework might\\nprove advantageous when applied to larger and more com-\\nplex data.\\nFigure 3. Predictions obtained applying MoNet over the Cora dataset. Marker fill color represents the predicted class; marker outline color\\nrepresents the groundtruth class.\\n5.3. Manifolds\\nThe last application we consider is learning dense intrin-\\nsic correspondence between collections of 3D shapes repre-\\nsented as discrete manifolds. For this purpose, correspon-\\ndence is cast as a labelling problem, where one tries to label\\neach vertex of a given query shape X with the index of a\\ncorresponding point on some reference shape Y [38, 32, 7].\\nLet n and m denote the number of vertices in X and Y , re-\\nspectively. For a point x on a query shape, the last layer of\\nthe network is soft-max, producing an m-dimensional out-\\nput f(x) that is interpreted as a probability distribution on\\nY (the probability of x mapped to y). Learning is done by\\nminimizing the standard logistic regression cost [7].\\nMeshes. We reproduced verbatim the experiments of [32,\\n7] on the FAUST humans dataset [5], comparing to the\\nmethods reported therein. The dataset consisted of 100 wa-\\ntertight meshes representing 10 different poses for 10 differ-\\nent subjects with exact ground-truth correspondence. Each\\nshape was represented as a mesh with 6890 vertices; the\\nfirst subject in first pose was used as the reference. For all\\nthe shapes, point-wise 544-dimensional SHOT descriptors\\n(local histogram of normal vectors) [54] were used as input\\ndata. We used MoNet architecture with 3 convolutional lay-\\ners, replicating the architectures of [32, 7]. First 80 subjects\\nTable 4. Vertex classification accuracy on the Cora and PubMed\\ndatasets following the splitting suggested in [60]. Learning meth-\\nods (DCNN, GCNN and MoNet) were trained and tested fifty\\ntimes for showing their average behavior with different initializa-\\ntions.\\nMethod Cora PubMed\\nManiReg [4] 59.5% 70.7%\\nSemiEmb [58] 59.0% 71.1%\\nLP [63] 68.0% 63.0%\\nDeepWalk [36] 67.2% 65.3%\\nPlanetoid [60] 75.7% 77.2%\\nDCNN [2] 76.80 ± 0.60% 73.00 ± 0.52%\\nGCN [26] 81.59 ± 0.42% 78.72 ± 0.25%\\nMoNet 81.69 ± 0.48% 78.81 ± 0.44%\\nin all the poses were used for training (800 shapes in total);\\nthe remaining 20 subjects were used for testing. The output\\nof the network was refined using the intrinsic Bayesian filter\\n[55] in order to remove some local outliers.\\nCorrespondence quality was evaluated using the Prince-\\nton benchmark [24], plotting the percentage of matches that\\nare at most r-geodesically distant from the groundtruth cor-\\nrespondence on the reference shape. For comparison, we re-\\nport the performance of blended maps [24], random forests\\n[38], GCNN [32], ADD [8], and ACNN [7].\\nFigure 1 shows the weighting functions of the patch op-\\nerator that are fixed in GCNN and ACNN architectures, and\\npart of the learnable parameters in the proposed MoNet.\\nThe patch operators of GCNN and ACNN can be obtained\\nas a particular configuration of MoNet, implying that if\\ntrained correctly, the new model can only improve w.r.t.\\nthe previous ones. Figure 4 depicts the evaluation results,\\nshowing that MoNet significantly outperforms the compet-\\ning approaches. In particular, close to 90% of points have\\nzero error, and for 99% of the points the error is below 4cm.\\nFigure 6 shows the point-wise geodesic correspondence er-\\nror of our method, and Figure 7 visualizes the obtained cor-\\nrespondence using texture transfer.\\nRange maps. Finally, we repeated the shape correspon-\\ndence experiment on range maps synthetically generated\\nfrom FAUST meshes. For each subject and pose, we pro-\\nduced 10 rangemaps in 100×180 resolution, covering shape\\nrotations around the z-axis with increments of 36 degrees\\n(total of 1000 range maps), keeping the groundtruth cor-\\nrespondence. We used MoNet architecture with 3 convo-\\nlutional layers and local SHOT descriptors as input data.\\nTraining and testing set splitting was done as previously.\\nFigure 5 shows the quality of correspondence computed\\nusing the Princeton protocol. For comparison, we show the\\nperformance of a standard Euclidean CNN in equivalent ar-\\nchitecture (3 convolutional layers) applied on raw depth val-\\nues and on SHOT descriptors. Our approach clearly shows\\n0 4 8 12 16 20\\nGeodesic error (cm)\\n0 0.02 0.04 0.06 0.08 0.1\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nGeodesic error (% diameter)\\n%\\nco\\nrr\\nes\\npo\\nnd\\nen\\nce\\ns\\nBIM\\nRF\\nADD\\nGCNN\\nACNN\\nMoNet\\nFigure 4. Shape correspondence quality obtained by different\\nmethods on the FAUST humans dataset. The raw performance\\nof MoNet is shown in dotted curve.\\n0 10 20 30 40\\nGeodesic error (cm)\\n0 0.05 0.1 0.15 0.2\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nGeodesic error (% diameter)\\n%\\nco\\nrr\\nes\\npo\\nnd\\nen\\nce\\ns\\nCNN on depth\\nCNN on SHOT\\nMoNet\\nFigure 5. Shape correspondence quality obtained by different\\nmethods on FAUST range maps. For comparison, we show the\\nperformance of a Euclidean CNN with a comparable 3-layer ar-\\nchitecture. The raw performance is shown as dotted curve.\\na superior performance. Figure 8 shows the point-wise\\ngeodesic correspondence error. Figure 9 shows a qualitative\\nvisualization of correspondence using similar color code for\\ncorresponding vertices. We also show correspondence on\\nshapes from SCAPE [1] and TOSCA [10] datasets.\\n6. Conclusions\\nWe proposed a spatial-domain model for deep learning\\non non-Euclidean domains such as manifolds and graphs.\\nOur approach generalizes several previous techniques that\\ncan be obtained as particular instances thereof. Extensive\\nexperimental results show that our model is applicable to\\ndifferent geometric deep learning tasks, achieving state-of-\\nthe-art results. In deformable 3D shape analysis applica-\\ntions, the key advantage of our approach is that it is intrinsic\\nand thus deformation-invariant by construction, as opposed\\nto Euclidean models [51, 59, 56, 37] that in general require\\nsignificantly higher complexity and huge training sets to\\nlearn the deformation invariance. In future works, we will\\nstudy additional promising applications of our model, for\\nexample in the domain of computational social sciences.\\nAcknowledgments\\nThis research was supported in part by the ERC Starting\\nGrant No. 307047 (COMET), a Google Faculty Research\\nAward, and Nvidia equipment grant.\\nReferences\\n[1] D. Anguelov, P. Srinivasan, D. Koller, S. Thrun, J. Rodgers,\\nand J. Davis. SCAPE: shape completion and animation of\\npeople. In TOG, volume 24, pages 408–416, 2005.\\n[2] J. Atwood and D. Towsley. Diffusion-convolutional neural\\nnetworks. arXiv:1511.02136v2, 2016.\\n[3] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimen-\\nsionality reduction and data representation. Neural Compu-\\ntation, 15(6):1373–1396, 2003.\\n[4] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regular-\\nization: A geometric framework for learning from labeled\\nand unlabeled examples. JMLR, 7:2399–2434, 2006.\\n[5] F. Bogo, J. Romero, M. Loper, and M. J. Black. FAUST:\\nDataset and evaluation for 3D mesh registration. In Proc.\\nCVPR, 2014.\\n[6] D. Boscaini, J. Masci, S. Melzi, M. M. Bronstein, U. Castel-\\nlani, and P. Vandergheynst. Learning class-specific descrip-\\ntors for deformable shapes using localized spectral convolu-\\ntional networks. Computer Graphics Forum, 34(5):13–23,\\n2015.\\n[7] D. Boscaini, J. Masci, E. Rodola`, and M. M. Bronstein.\\nLearning shape correspondence with anisotropic convolu-\\ntional neural networks. In Proc. NIPS, 2016.\\n[8] D. Boscaini, J. Masci, E. Rodola`, M. M. Bronstein, and\\nD. Cremers. Anisotropic diffusion descriptors. Computer\\nGraphics Forum, 35(2):431–441, 2016.\\n[9] X. Bresson, T. Laurent, and J. von Brecht. Enhanced lasso\\nrecovery on graph. In Proc. EUSIPCO, 2015.\\n[10] A. M. Bronstein, M. M. Bronstein, and R. Kimmel. Numer-\\nical geometry of non-rigid shapes. Springer, 2008.\\n[11] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Van-\\ndergheynst. Geometric deep learning: going beyond eu-\\nclidean data. 2016. preprint.\\n[12] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral\\nnetworks and locally connected networks on graphs. Proc.\\nICLR, 2013.\\n[13] S. Cao, W. Lu, and Q. Xu. GraRep: Learning graph repre-\\nsentations with global structural information. In Proc. IKM,\\n2015.\\n[14] R. R. Coifman and M. Maggioni. Diffusion wavelets. Ap-\\nplied and Computational Harmonic Analysis, 21(1):53–94,\\n2006.\\n[15] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolu-\\ntional neural networks on graphs with fast localized spectral\\nfiltering. In Proc. NIPS, 2016.\\n[16] I. S. Dhillon, Y. Guan, and B. Kulis. Weighted graph\\ncuts without eigenvectors: a multilevel approach. PAMI,\\n29(11):1944–1957, 2007.\\n[17] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bom-\\nbarell, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. Con-\\nvolutional networks on graphs for learning molecular finger-\\nprints. In Proc. NIPS, 2015.\\n[18] M. Gavish, B. Nadler, and R. R. Coifman. Multiscale\\nwavelets on trees, graphs and high dimensional data: The-\\nory and applications to semi supervised learning. In Proc.\\nICML, 2010.\\n[19] M. Gori, G. Monfardini, and F. Scarselli. A new model for\\nlearning in graph domains. In Proc. IJCNN, 2005.\\n[20] A. Grover and J. Leskovec. node2vec: Scalable feature\\nlearning for networks. In Proc. KDD, 2016.\\n[21] D. K. Hammond, P. Vandergheynst, and R. Gribonval.\\nWavelets on graphs via spectral graph theory. Applied and\\nComp. Harmonic Analysis, 30(2):129–150, 2011.\\n[22] M. Henaff, J. Bruna, and Y. LeCun. Deep convolutional net-\\nworks on graph-structured data. arXiv:1506.05163, 2015.\\n[23] V. Kalofolias, X. Bresson, M. M. Bronstein, and P. Van-\\ndergheynst. Matrix completion on graphs. arXiv:1408.1717,\\n2014.\\n[24] V. Kim, Y. Lipman, and T. Funkhouser. Blended intrinsic\\nmaps. ACM Trans. Graphics, 30(4):79, 2011.\\n[25] D. P. Kingma and J. Ba. Adam: A method for stochastic\\noptimization. arXiv:1412.6980, 2014.\\n[26] T. N. Kipf and M. Welling. Semi-supervised classifica-\\ntion with graph convolutional networks. arXiv:1609.02907,\\n2016.\\n[27] I. Kokkinos, M. Bronstein, R. Litman, and A. Bronstein. In-\\ntrinsic shape context descriptors for deformable shapes. In\\nProc. CVPR, 2012.\\n[28] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-\\nbased learning applied to document recognition. Proc. IEEE,\\n86(11):2278–2324, 1998.\\n[29] O. Le´zoray and L. Grady. Image processing and analysis\\nwith graphs: theory and practice. CRC Press, 2012.\\n[30] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated\\ngraph sequence neural networks. arXiv:1511.05493, 2015.\\n[31] M. Lin, Q. Chen, and S. Yan. Network in network. CoRR,\\nabs/1312.4400, 2013.\\n[32] J. Masci, D. Boscaini, M. M. Bronstein, and P. Van-\\ndergheynst. Geodesic convolutional neural networks on Rie-\\nmannian manifolds. In Proc. 3DRR, 2015.\\n[33] T. Mikolov and J. Dean. Distributed representations of words\\nand phrases and their compositionality. Proc. NIPS, 2013.\\n[34] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering:\\nAnalysis and an algorithm. In Proc. NIPS, 2002.\\n[35] P. Perona and J. Malik. Scale-space and edge detection using\\nanisotropic diffusion. Trans. PAMI, 12(7):629–639, 1990.\\n[36] B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: Online\\nlearning of social representations. In Proc. KDD, 2014.\\n[37] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. J.\\nGuibas. Volumetric and multi-view CNNs for object clas-\\nsification on 3D data. In Proc. CVPR, 2016.\\n[38] E. Rodola`, S. Rota Bulo`, T. Windheuser, M. Vestner, and\\nD. Cremers. Dense non-rigid shape correspondence using\\nrandom forests. In Proc. CVPR, 2014.\\n[39] R. Rustamov and L. J. Guibas. Wavelets on graphs via deep\\nlearning. In Advances in Neural Information Processing Sys-\\ntems, pages 998–1006, 2013.\\n[40] A. Sanfeliu et al. Graph-based representations and tech-\\nniques for image processing and image analysis. Pattern\\nRecognition, 35(3):639–650, 2002.\\n[41] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and\\nG. Monfardini. The graph neural network model. IEEE\\nTrans. Neural Networks, 20(1):61–80, 2009.\\n[42] P. Sen, G. M. Namata, M. Bilgic, L. Getoor, B. Gallagher,\\nand T. Eliassi-Rad. Collective classification in network data.\\nAI Magazine, 29(3):93–106, 2008.\\n[43] N. Shahid, V. Kalofolias, X. Bresson, M. M. Bronstein, and\\nP. Vandergheynst. Robust principal component analysis on\\ngraphs. In Proc. ICCV, 2015.\\n[44] N. Shahid, N. Perraudin, V. Kalofolias, G. Puy, and P. Van-\\ndergheynst. Fast robust PCA on graphs. IEEE J. Selected\\nTopics in Signal Processing, 10(4):740–756, 2016.\\n[45] N. Sharon and Y. Shkolnisky. A class of Laplacian mul-\\ntiwavelets bases for high-dimensional data. Applied and\\nComp. Harmonic Analysis, 38(3):420–451, 2015.\\n[46] J. Shi and J. Malik. Normalized cuts and image segmenta-\\ntion. Trans. PAMI, 22(8):888–905, 2000.\\n[47] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and\\nP. Vandergheynst. The emerging field of signal processing\\non graphs: Extending high-dimensional data analysis to net-\\nworks and other irregular domains. IEEE Sig. Proc. Maga-\\nzine, 30(3):83–98, 2013.\\n[48] D. I. Shuman, B. Ricaud, and P. Vandergheynst. Vertex-\\nfrequency analysis on graphs. App. and Comp. Harmonic\\nAnalysis, 40(2):260–291, 2016.\\n[49] A. Sinha, J. Bai, and K. Ramani. Deep learning 3D shape\\nsurfaces using geometry images. In Proc. ECCV, 2016.\\n[50] N. Sochen, R. Kimmel, and R. Malladi. A general framework\\nfor low level vision. Trans. Image Processing, 7(3):310–318,\\n1998.\\n[51] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller. Multi-\\nview convolutional neural networks for 3D shape recogni-\\ntion. In Proc. ICCV, 2015.\\n[52] S. Sukhbaatar, A. Szlam, and R. Fergus. Learning multiagent\\ncommunication with backpropagation. arXiv:1605.07736,\\n2016.\\n[53] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei.\\nLINE: Large-scale information network embedding. In Proc.\\nWWW, 2015.\\n[54] F. Tombari, S. Salti, and L. Di Stefano. Unique signatures\\nof histograms for local surface description. In Proc. ECCV,\\n2010.\\n[55] M. Vestner, R. Litman, A. Bronstein, E. Rodola`, and D. Cre-\\nmers. Bayesian inference of bijective non-rigid shape corre-\\nspondence. arXiv:1607.03425, 2016.\\n[56] L. Wei, Q. Huang, D. Ceylan, E. Vouga, and H. Li. Dense\\nhuman body correspondences using convolutional networks.\\nIn Proc. CVPR, 2016.\\n[57] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In\\nProc. NIPS, 2009.\\n[58] J. Weston, F. Ratle, H. Mobahi, and R. Collobert. Deep learn-\\ning via semi-supervised embedding. In Neural Networks:\\nTricks of the Trade, pages 639–655. 2012.\\n[59] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and\\nJ. Xiao. 3D shapenets: A deep representation for volumetric\\nshapes. In Proc. CVPR, 2015.\\n[60] Z. Yang, W. Cohen, and R. Salakhutdinov. Revis-\\niting semi-supervised learning with graph embeddings.\\narXiv:1603.08861, 2016.\\n[61] F. Zhang and E. R. Hancock. Graph spectral image smooth-\\ning using the heat kernel. Pattern Recognition, 41(11):3328–\\n3342, 2008.\\n[62] X. Zhang, X. Dong, and P. Frossard. Learning of structured\\ngraph dictionaries. In Proc. ICASSP, 2012.\\n[63] X. Zhu, Z. Ghahramani, J. Lafferty, et al. Semi-supervised\\nlearning using gaussian fields and harmonic functions. In\\nProc. ICML, 2003.\\n07.5%\\nBlended intrinsic maps\\n0\\n7.5%\\nAnisotropic Diffusion Descriptors\\n0\\n7.5%\\nGeodesic CNN\\n0\\n7.5%\\nAnisotropic CNN\\n0\\n7.5%\\nMoNet\\nFigure 6. Pointwise error (geodesic distance from groundtruth) of different correspondence methods on the FAUST humans dataset. For\\nvisualization clarity, the error values are saturated at 7.5% of the geodesic diameter, which corresponds to approximately 15 cm. Hot colors\\nrepresent large errors.\\nFigure 7. Examples of correspondence on the FAUST humans dataset obtained by the proposed MoNet method. Shown is the texture\\ntransferred from the leftmost reference shape to different subjects in different poses by means of our correspondence.\\n0\\n7.5%\\nEuclidean CNN\\n0\\n7.5%\\nMoNet\\nFigure 8. Pointwise error (geodesic distance from groundtruth) of different methods on FAUST range maps. For visualization clarity, the\\nerror values are saturated at 7.5% of the geodesic diameter, which corresponds to approximately 15 cm. Hot colors represent large errors.\\nFigure 9. Visualization of correspondence on FAUST range maps as color code (corresponding points are shown in the same color). Full\\nreference shape is shown on the left. Bottom row show examples of additional shapes from SCAPE and TOSCA datasets.\\n'}\n",
      "{'_id': ObjectId('6815e61add259dc7e6e1cd32'), 'authors': 'Chen, Yiping, Li, Jonathan, Wang, Cheng, Yang, Chenhui, Zeng, Hongbin, Zhang, Zongliang', 'year': '2019', 'title': 'Geometric Multi-Model Fitting by Deep Reinforcement Learning', 'full_text': 'Geometric Multi-Model Fitting by Deep Reinforcement Learning\\nZongliang Zhang, Hongbin Zeng, Jonathan Li*, Yiping Chen, Chenhui Yang, Cheng Wang\\nFujian Key Laboratory of Sensing and Computing for Smart Cities, School of Information Science and Engineering\\nXiamen University, Xiamen, Fujian 361005, China (*Corresponding author)\\n{zhangzongliang, hbzeng}@stu.xmu.edu.cn, {junli, chenyiping, chyang, cwang}@xmu.edu.cn\\nAbstract\\nThis paper deals with the geometric multi-model fitting from\\nnoisy, unstructured point set data (e.g., laser scanned point\\nclouds). We formulate multi-model fitting problem as a se-\\nquential decision making process. We then use a deep rein-\\nforcement learning algorithm to learn the optimal decisions\\ntowards the best fitting result. In this paper, we have com-\\npared our method against the state-of-the-art on simulated\\ndata. The results demonstrated that our approach significantly\\nreduced the number of fitting iterations.\\nIntroduction\\nGeometric model fitting aims to reconstruct underlying\\nmodels (e.g., lines, circles, characters, and buildings) from\\ngiven data (e.g., images or laser scanning point clouds).\\nWith the reconstructed rich model information (e.g., shape,\\nscale, rotation, and location), the data can be comprehen-\\nsively understood. With such merit, model fitting has con-\\nstantly attracted research interests for a long time. However,\\nthe model fitting problem is far from being solved, at least\\nin terms of computational speed, because of increasing com-\\nplexity of encountered data and thus models. A common\\ncase of complex data is that data conceive multiple models.\\nFor example, a CAPTCHA image usually contains multiple\\ncharacters (George et al. 2017). A multi-model fitting tech-\\nnique is needed to handle such data.\\nA recent trend for addressing model fitting problem is to\\nformulate it as an optimization problem (Lake, Salakhutdi-\\nnov, and Tenenbaum 2015), such that it can be conveniently\\ntackled by an existing optimization algorithm. Our previous\\nmethod (Zhang et al. 2019) uses the cuckoo search (CS)\\nalgorithm (Yang and Deb 2010) to solve the optimization\\nproblem in model fitting, and notably achieves the new state-\\nof-the-art in the challenging few-shot character recognition\\ntasks (George et al. 2017). CS can approach the optimum\\nwith high precision. However, it usually takes many itera-\\ntions to converge to the optimum, especially when the fit-\\nting involves a large number of variables, which is the case\\nof multi-model fitting. The number of variables involved in\\nn-model fitting is as large as n times that in single-model\\nfitting. In other words, it is time-consuming to use CS to\\nperform multi-model fitting.\\nCopyright c© 2019, Association for the Advancement of Artificial\\nIntelligence (www.aaai.org). All rights reserved.\\nIn this paper, we propose a reinforcement learning ap-\\nproach for optimization in multi-model fitting. Our insight\\nis as follows. The selection of variable values for a model\\ncan be seen as a decision. The fitting of multiple models\\nis a process that consists of a sequence of decisions. Such\\ndecision making process can be efficiently optimized by re-\\ninforcement learning.\\nThe work similar to ours can be found in (Teboul et\\nal. 2013), which uses a traditional reinforcement learning\\nmethod to control binary split shape grammar for parsing\\nfacade images. Their method works under the assumption\\nthat the split grammar has discrete variables. However, the\\nvariables involved in model fitting usually are continuous,\\nwhich are challenging for a traditional reinforcement learn-\\ning method to handle (Lillicrap et al. 2015). In contrast,\\nour work is based on recently developed deep reinforcement\\nlearning (DRL), which has made remarkable progress for\\na number of challenging tasks including continuous control\\n(Lillicrap et al. 2015).\\nMethod\\nA geometric model M is a k-dimensional point set, i.e.,\\nM ⊂ Rk. In this paper, k = 3. Give a data point set\\nD ⊂ Rk, the goal of model fitting is to find a model M\\nthat is most similar to D. For multi-model fitting, M is the\\nunion set of multiple models, i.e., M =\\n⋃n\\ni=1M\\nθi\\ng , where\\nn is the number of models, and Mθg is the model defined by\\na given parametric rule g which is parameterized by vari-\\nable θ. Formally, multi-model fitting can be formulated as\\nthe following maximization problem:\\nmax\\n(θ1,θ2,··· ,θn)\\nf(θ1, θ2, · · · , θn) = s(\\n⋃n\\ni=1\\nMθig , D), (1)\\nwhere s(·, ·) is the geometric similarity estimator defined\\nin (Zhang et al. 2019). The pseudo-code of our method is\\nshown in Algorithm 1, where the DRL actor q is based on\\n(Lillicrap et al. 2015), and Ni is exploration noise (Lilli-\\ncrap et al. 2015). Our method follows a hypothesis and ver-\\nify paradigm to solve the maximization problem Eq. (1). In\\neach iteration, for each model, a hypothesis value θi is pro-\\nposed according to the actor and exploration noise. Then the\\nhypothesis is verified through computing the reward in order\\nto update the actor.\\nar\\nX\\niv\\n:1\\n80\\n9.\\n08\\n39\\n7v\\n2 \\n [c\\ns.C\\nV]\\n  2\\n7 D\\nec\\n 20\\n18\\nAlgorithm 1 The proposed method\\ninput: a data point setD, a parametric rule g with variable\\nθ, the number of models n, the DRL actor q, the verify\\nfunction f\\noutput: (θ∗1 , θ∗2 , · · · , θ∗n) that maximizes Eq. (1)\\nRandomly initialize (θ∗1 , θ\\n∗\\n2 , · · · , θ∗n)\\nfor iteration j = 1 to jmax do\\nfor i = 1 to n do\\nSelect variable θi = q(i) +Ni\\nObserve reward:\\nri = f(θ1, θ2, · · · , θi)− f(θ1, θ2, · · · , θi−1)\\nUpdate q according to ri\\nend for\\nif f(q(1), q(2), · · · , q(n)) > f(θ∗1 , θ∗2 , · · · , θ∗n) then\\n(θ∗1 , θ\\n∗\\n2 , · · · , θ∗n) = (q(1), q(2), · · · , q(n))\\nend if\\nend for\\nWe now present the computation cost of the proposed\\nDRL based n-model fitting method. In each iteration, the\\ncomputational time cost of a hypothesis and verify algorithm\\nis composed of two parts: the hypothesis part tHDRL and the\\nverify part tVDRL. Therefore, the total computational cost is\\ntDRL = jDRL(t\\nH\\nDRL + t\\nV\\nDRL), where jDRL is the total number\\nof iterations. For n-model fitting, it is needed to calculate\\nthe verify function f for n times in each iteration. Let tf\\nbe the cost to calculate f one time, then tVDRL = ntf , and\\ntDRL = jDRL(t\\nH\\nDRL + ntf ). In contrast, the CS based method\\n(Zhang et al. 2019) only needs to calculate f one time in\\none iteration. Consequently, the total computational cost of\\nthe CS based method is tCS = jCS(tHCS + tf ). It can be con-\\ncluded that, when tf \\x1d tHDRL and jCS > njDRL, DRL is more\\nefficient than CS. Note that tf is determined by the data and\\nthe model sizes (Zhang et al. 2019). Therefore, tf \\x1d tHDRL\\nholds for many applications in which data and model sizes\\nare large. For example, a laser scanning point cloud is usu-\\nally large in size as containing millions of points.\\nExperiments\\nIn this abstract, we preliminarily evaluate our method by fit-\\nting line segments to the data D2 shown in Fig. 1b. Specifi-\\ncally, the parametric rule g input to our method is a vertical\\nline segment rule with only one variable θ ∈ R that deter-\\nmines the horizontal location of the line segment. We also\\nfix the number of models n = 4.\\nAs shown in Figs. 1c and 1d, DRL fits the data well after\\nonly 100 iterations, whereas CS cannot well fit the data even\\nafter 1000 iterations. The evolutions of similarity during fit-\\nting are shown in Fig. 2, where each line represents the mean\\nof similarity values and the patches around each line repre-\\nsents the standard deviations. The mean values and standard\\ndeviations are computed from 5 times of fitting. The results\\nclearly indicates that DRL is tens of times more efficient\\nthan CS in terms of the numbers of fitting iterations.\\n(a) (b) (c) (d)\\nFigure 1: Data point sets and fitted models. (a) Clean data\\nD1. (b) Corrupted data D2, which is generated by adding\\nsome outliers to D1. (c) The model fitted by CS after 1000\\niterations. (d) The model fitted by DRL after 100 iterations.\\n500 1000 1500 2000 2500 3000\\nNumber of iterations\\n0\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\n3500\\n4000\\nS\\nim\\nil\\nar\\nit\\ny\\nDRL\\nCS\\nFigure 2: Fitting similarities at different iterations of DRL\\nand CS.\\nReferences\\n[George et al. 2017] George, D.; Lehrach, W.; Kansky, K.;\\nLazaro-Gredilla, M.; Laan, C.; Marthi, B.; Lou, X.; Meng,\\nZ.; Liu, Y.; Wang, H.; et al. 2017. A Generative Vision\\nModel That Trains with High Data Efficiency and Breaks\\nText-Based CAPTCHAs. Science 358(6368):eaag2612.\\n[Lake, Salakhutdinov, and Tenenbaum 2015] Lake, B. M.;\\nSalakhutdinov, R.; and Tenenbaum, J. B. 2015. Human-\\nLevel Concept Learning Through Probabilistic Program In-\\nduction. Science 350(6266):1332–1338.\\n[Lillicrap et al. 2015] Lillicrap, T. P.; Hunt, J. J.; Pritzel, A.;\\nHeess, N.; Erez, T.; Tassa, Y.; Silver, D.; and Wierstra, D.\\n2015. Continuous Control with Deep Reinforcement Learn-\\ning. arXiv preprint arXiv:1509.02971.\\n[Teboul et al. 2013] Teboul, O.; Kokkinos, I.; Simon, L.;\\nKoutsourakis, P.; and Paragios, N. 2013. Parsing Facades\\nwith Shape Grammars and Reinforcement Learning. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\n35(7):1744–1756.\\n[Yang and Deb 2010] Yang, X. S., and Deb, S. 2010. Engi-\\nneering Optimisation by Cuckoo Search. International Jour-\\nnal of Mathematical Modelling and Numerical Optimisation\\n1(4):330–343.\\n[Zhang et al. 2019] Zhang, Z.; Li, J.; Guo, Y.; Li, X.; Lin, Y.;\\nXiao, G.; and Wang, C. 2019. Robust Procedural Model\\nFitting with a New Geometric Similarity Estimator. Pattern\\nRecognition 85:120 – 131.\\n'}\n",
      "{'_id': ObjectId('6815e61add259dc7e6e1cd33'), 'authors': 'Chen, Chen, Del Bue, Alessio, Murino, Vittorio, Zhang, Baochang', 'year': '2017', 'title': 'Manifold Constrained Low-Rank Decomposition', 'full_text': 'Manifold Constrained Low-Rank Decomposition\\nChen Chen2, Baochang Zhang1,3,∗, Alessio Del Bue4, Vittorio Murino4\\n1State Key Laboratory of Satellite Navigation System and Equipment Technology, Shijiazhuang, China\\n2Center for Research in Computer Vision (CRCV), University of Central Florida (UCF)\\n3School of Automation Science and Electrical Engineering, Beihang University, Beijing, China\\n4 Istituto Italiano di Tecnologia, Genova, Italy\\nchenchen870713@gmail.com, alessio.delbue@iit.it, bczhang@buaa.edu.cn, vittorio.murino@iit.it ∗\\nAbstract\\nLow-rank decomposition (LRD) is a state-of-the-art\\nmethod for visual data reconstruction and modelling. How-\\never, it is a very challenging problem when the image data\\ncontains significant occlusion, noise, illumination varia-\\ntion, and misalignment from rotation or viewpoint changes.\\nWe leverage the specific structure of data in order to im-\\nprove the performance of LRD when the data are not ideal.\\nTo this end, we propose a new framework that embeds\\nmanifold priors into LRD. To implement the framework,\\nwe design an alternating direction method of multipliers\\n(ADMM) method which efficiently integrates the manifold\\nconstraints during the optimization process. The proposed\\napproach is successfully used to calculate low-rank models\\nfrom face images, hand-written digits and planar surface\\nimages. The results show a consistent increase of perfor-\\nmance when compared to the state-of-the-art over a wide\\nrange of realistic image misalignments and corruptions.\\n1. Introduction\\nWith the increasing number of images and videos pro-\\nduced everyday, it becomes more problematic when exist-\\ning algorithms have to deal with realistic data containing\\nsevere occlusion, misalignment, noise, significant illumina-\\ntion variation, and viewpoint changes [13, 14, 11]. Low-\\nrank decomposition (LRD) techniques have been an impor-\\ntant tool in batch data analysis in the past decade, which ef-\\nfectively converts high dimensional raw data into a compact\\nand low-dimensional representation. It has been success-\\nfully used in a variety of applications such as subspace seg-\\nmentation [22], visual tracking, image clustering [29] and\\nvideo background foreground separation [2]. However, this\\ntechnique works properly when the data is captured in an\\nideal situation or it is manually aligned. The performance\\n∗Baochang Zhang is the corresponding author.\\nof the algorithm degrades significantly in case of rotation,\\ncorruption, occlusion and misalignment in the data. In such\\nsituations, low-rank matrices cannot be accurately recov-\\nered from the data because geometrical distortions are diffi-\\ncult to grasp with a linear subspace model.\\nTo make LRD based methods applicable in more real-\\nistic scenarios, various solutions have been developed. For\\ninstance, a sophisticated measure of image similarity is used\\nin [23, 12] to address the batch image alignment problem.\\nAlternatively, Learned-Miller’s congealing algorithm [12]\\nseeks an alignment that minimizes the sum of entropy of\\npixel value at each pixel location in the batch of aligned im-\\nages. Instead of using the entropy, the least squares congeal-\\ning procedure [8] minimizes the sum of squared distances\\nbetween pairs of images, and therefore requires the columns\\nto be nearly constant. In [6], Vedaldi et al. choose to min-\\nimize a log-determinant measure that can be viewed as a\\nsmooth surrogate for the rank function. The Robust Princi-\\npal Component Analysis (RPCA) algorithm fits a low-rank\\nmodel, and uses a fitting function to reduce the influence of\\ncorruptions and occlusions.\\nDifferently, the Robust Alignment by Sparse and Low-\\nrank Decomposition (RASL) [17] has shown the potential\\nto solve realistic problems with misalignments and corrup-\\ntions by using a nuclear-norm minimization based on the\\nalternating direction method of multipliers (ADMM). The\\ncore idea of the method is to find an optimal set of transfor-\\nmations such that the matrix of transformed images can be\\ndecomposed as a low-rank matrix of recovered aligned im-\\nages and a sparse matrix of errors. The algorithm is subject\\nto a set of linear equality constraints, which impose a linear\\nrelationship with the input data. However, the fact that in-\\nput data has generally a nonlinear structure, i.e. distributed\\nover a manifold, is not fully investigated in the optimization\\nprocess.\\nIn this paper, we provide new insights into the nuclear-\\nnorm minimization method, in particular a relevant intuition\\nthat was neglected in previous work. That is, data often\\nar\\nX\\niv\\n:1\\n70\\n8.\\n01\\n84\\n6v\\n1 \\n [c\\ns.C\\nV]\\n  6\\n A\\nug\\n 20\\n17\\nTable 1. A brief description of variables used in the paper.\\nVd: input data matrix (samples in rows) Vr: low-rank matrix calculated from Vd\\nτ : geometric transformation ∆τ : to calculate new τ\\nVm: calculated by ∆τ and Vd V ′m: embedding of Vm\\nE: the error matrix Sα[x]: soft-thresholding function\\nlies on specific manifolds [21], especially when the data\\ncomes from a well-defined object from a given set of sam-\\nples (e.g. faces, digits, etc.). From the optimization perspec-\\ntive, assuming that the solution of the optimization prob-\\nlem is always data related, the constraints derived from the\\ndata structure can make the algorithm immune to the vari-\\nations in the testing data [3]. Consequently, it is important\\nto incorporate the data structure prior in the learning pro-\\ncedure. In this paper, we show that there is a solution with\\nhigh practicability that can include manifold constraints in\\nADMM, which is applied to solve the LRD problem. Tech-\\nnically, we avoid complex nonlinear optimization over the\\nmanifold by recasting the problem as a simpler matrix pro-\\njection over the same manifold. Different from other works\\n[1], the manifold is not given but actually estimated from\\nthe data, leading to a solution complying with the intrinsic\\ndata distribution.\\nIn summary, the contributions of this paper are twofold.\\n• We propose to incorporate the manifold constraints\\nin low-rank decomposition methods, achieving much bet-\\nter results than the prior art.\\n• We present a manifold embedding based ADMM\\n(MeADMM) framework, where the manifold constraint is\\ntranslated into a matrix projection operation computed by\\na neighbor-preserving embedding process, which greatly\\nsimplifies the optimization.\\nFor clarity, we summarize all the variables in Table 1.\\nThe matrix Vd is the data matrix containing the data sam-\\nples at each row. The matrix Vr is the low-rank matrix cal-\\nculated from Vd. The geometric transformation functions\\nτ and ∆τ are used to calculate Vm from Vr. Using τ and\\n∆τ , we register the data in a way that the rank properties\\nare preserved. Finally, V ′m is a manifold embedding of the\\nregistered input data Vm.\\n2. Related work\\nOur work is related to the RASL framework [17] and\\nmanifold methods. Therefore, the literature overview fo-\\ncuses on RASL methodology as well as relevant manifold\\napproaches. RASL. The misalignment problem is one of\\nthe most difficult problems in computer vision. By formu-\\nlating the batch image alignment as searching for a set of\\ntransformations that minimize the rank of the transformed\\nimages, RASL investigates the linearly correlated relation-\\nship among the input images, which is shown in Problem 1\\n(P1):\\nVˆr, τˆ = arg min rank(Vr) + λ ∗ ||E||1,\\nsubject to Vd ◦ τ = Vr + E, τ ∈ G\\n(P1)\\nAs a practical example, each row of Vd can correspond to\\nan M × N image frame of a video with B frames while\\nVr contain a compact low-rank description of the video. In\\nparticular, Vd can be a collection of images with variations\\nincluding rotation, illumination changes, occlusion and ge-\\nometric transformations given by τ ∈ G where the operator\\nG is defined as a 3 × 3 matrix [17]. To efficiently solve the\\nproblem, a linearization process is used such that:\\nVd ◦ (τ + ∆τ) = Vd ◦ τ +\\nB∑\\ni\\nJi∆τi\\x0fi, (1)\\nwhere (τ + ∆τ) gives at each step the new τ during it-\\nerations while the increment ∆τ is derived as detailed in\\n[17]. Ji is the Jacobian matrix of the ith image with re-\\nspect to the transformation parameters and \\x0fi denotes the\\nstandard bases. The above linearisation process only holds\\nlocally. Therefore, linearisation of current estimates is re-\\npeated by solving a sequence of convex problems. After\\nthe linearisation, a semi-definite programming problem is\\nsolved in thousands or millions of variables. Thanks to\\nrecent works on high-dimensional nuclear norm minimiza-\\ntion, such problems are well within the capability of a stan-\\ndard PC [17].\\nManifolds. Manifolds are popular in machine learning,\\nbecause they allow to describe the intrinsic distribution of\\ndata in the Euclidean space. Most of the existing works\\nrelated to manifolds focus on modeling the nonlinearity of\\ndata. To represent high-dimensional data, manifold learn-\\ning [19] projects the original data onto lower dimensions\\nsuch that its inherent structure can be preserved. As an-\\nother application of manifold learning, an embedding of a\\nsample can be obtained by projecting onto a well-designed\\nmanifold [18]. To exploit the geometry of the marginal dis-\\ntribution, a semi-supervised framework based on manifold\\nregularization is used to learn from both labeled and un-\\nlabeled data in the form of a multiple kernel learning. In\\n[9], by representing the covariance matrix as a point on a\\nmanifold, a new metric is learned for that manifold. Differ-\\nently, [1] imposes the manifold constraints in an augmented\\nLagrange multipliers (ALM) strategy by using a matrix pro-\\njection as a constraint for the optimized variable, which effi-\\nciently computes the solution over several given manifolds.\\nINPUT Manifold Embedding\\nVm\\nVm\\nLRD\\nFigure 1. The framework of the proposed manifold constrained\\nlow-rank decomposition.\\nThe work leads to a new framework using given manifolds\\nto solve the optimization problem. However, it fails to ex-\\nplain why the variable should stop on a manifold.\\nUnlike the existing works, we present a new method that\\nexploits learned manifold constraints in the ADMM frame-\\nwork. Instead of empirically adding manifold constraints on\\na variable, we introduce a manifold based ADMM approach\\nto regulate the optimization problem for LRD.\\n3. Low-rank decomposition based on manifold\\nconstraints\\nA constrained learning model allows to incorporate\\ndomain-specific knowledge to balance the learned model\\nbased on the implicit structure of the data [4]. From a ma-\\nchine learning perspective, it is important to simplify the\\nlearning stage while improving the accuracy of the solu-\\ntion. In this section, we present how manifold constraints\\ncan be embedded into an optimization problem. We formu-\\nlate the LRD optimization problem in terms of MeADMM,\\nresulting in a relaxed and more efficient solution to the new\\nproblem defined as P2.\\nOur idea is intuitively illustrated in Fig. 1, where the in-\\nput images are first embedded into a manifold and the low-\\nrank results are obtained afterwards by LRD.\\n3.1. LRD reformulation based on MeADMM\\nTo efficiently calculate low-rank from data with a non-\\nlinear structure, MeADMM reformulate the manifold con-\\nstraint in the optimization process. We first introduce a new\\nvariable Vm such that Vm = Vd ◦ (τ + ∆τ). the matrix Vm\\nreplaces Vd ◦ τ , which is a new variable and linearly corre-\\nlated to Vr in the new problem. LRD is then reformulated\\nas:\\nVˆr, Eˆ, τˆ = arg min{rank(Vr) + λ ∗ ||E||1},\\nsubject to Vm = Vr + E, Vr,i ∈M, τ ∈ G\\n(P2)\\nNormally, if Vm contains images, only a small fraction of\\npixels will be affected by partial occlusions or corruptions,\\nthus E is considered to be sparse. Supposed that the in-\\nput data Vm is generally of nonlinear structure, the samples\\nof Vr are reasonably considered to be from a manifoldM.\\nThat is, Vr,i ∈ M. We propose to solve the problem in\\nthree steps. We first exploit the ALM framework in this sub-\\nsection to solve the problem without taking manifold con-\\nstraints into account. In the second step, we introduce the\\nmanifold constraint into the objective function in Sec. 3.2.\\nFinally, we solve all variables in Algorithms 1 and 2 in Sec.\\n3.4.\\nThe idea of ALM is searching for a saddle point of the\\naugmented Lagrangian function instead of directly solving\\nthe constrained optimization problem. Given P1, we define\\nf(Vr, E,∆τ) = f(Vr, Vm, E,∆τ) = (Vr + E)\\n−(Vd ◦ τ +\\n∑\\nJi∆τi\\x0fi) = (Vr + E)− Vm.\\n(2)\\nThen we have:\\nLµ(Vr, E,∆τ, Y ) = ||Vr||∗ + λ ∗ ||E||1\\n− < Y, f(Vr, E,∆τ) > +µ\\n2\\n||f(Vr, E,∆τ)||2,\\n(3)\\nwhere Y ∈ <M×N is a Lagrange multiplier matrix, µ is a\\npositive scalar and < ., . > denotes the matrix inner prod-\\nuct. For an appropriate choice of the Lagrange multiplier\\nmatrix Y and sufficiently large constant µ, it can be shown\\nthat ALM has the same minimizer as that of the original\\nconstrained optimization problem.\\n3.2. MeADMM\\nMeADMM is proposed to solve our new problem (P2)\\nwith the data lying over a manifold. Specifically, we pro-\\npose to consider Vr as an unknown variable of the optimiza-\\ntion by performing variable cloning i.e. Vr,i → V ′r,i ∈ M\\nand enforcing manifold constraints over the cloned vari-\\nables V ′r. This introduces explicitly the manifold con-\\nstraints at the expenses of replicating a set of variables. The\\nvariable cloning V ′r,i = Vr,i is used to add the manifold con-\\nstraint to replace Vr,i ∈ M in a set of equations. Now, the\\nproblem P2 can be rewritten as:\\nVˆr, Eˆ, τˆ = arg minLµ(Vr, E,∆τ, Y ),\\nsubject to V ′r,i = Vr,i, V\\n′\\nr,i ∈M\\n(P3)\\nTo solve this problem (P3), we can derive with ADMM the\\nfollowing objective cost function:\\nLµ,1(Vr, E,∆τ, Y ) = ||Vr||∗ + λ ∗ ||E||1\\n− < Y, f(Vr, E,∆τ) > +µ\\n2\\n||f(Vr, E,∆τ)||2\\n+\\nB∑\\ni\\nσi\\n2\\n||Vr,i − V ′r,i||2\\n(4)\\nwhere σi is a positive value. However, the above objective\\nis still too complicated to be solved, as Lµ,1 is the combi-\\nnation of Lµ and another function related to Vr as shown in\\nEq. 4. In this case, the calculation of Vr is more compli-\\ncated than RASL, which is based on Lµ only. Assuming a\\nlinear constraint on Vm and Vr, i.e. Vm = Vr + E, we ne-\\nglect the error matrix E and obtain the following objective:\\nLµ,2(Vr, Vm, E,∆τ, Y ) = ||Vr||∗ + λ ∗ ||E||1\\n− < Y, f(Vr, Vm, E,∆τ) > +µ\\n2\\n||f(Vr, Vm, E,∆τ)||2\\n+\\nB∑\\ni\\nσi\\n2\\n|˙|Vm,i − V ′m,i||2,\\n(5)\\nwith V ′m,i = Vm,i and V\\n′\\nm,i ∈ M as beforehand. The\\nabove expression requires a minimization over V ′m,i ∈ M\\nwith i = 1, . . . , F . In [1], the manifold constraints are\\nenforced in an ALM strategy by using a matrix projection\\nwhich computes the solution over several given manifolds\\n(e.g. Stiefel and unit sphere). Differently, in our method\\ndata is embedded into a manifold not known a priori but\\nlearned from the very same data.\\nNow we formalize the manifold by introducing a\\nneighbor-preserving embedding [20, 5], which aims to find\\nan estimation of the manifold. Such a formalization is sim-\\nilar to [5] which calculates the weights in the process of\\ndimension reduction by LLE [18]. In particular, the embed-\\nding generated by [5] is exactly based on a manifold given\\nby a small set of samples. To do so, we find a projection in\\na manifold based on the “true” neighbors of input data mea-\\nsured by the Geodesic distance information, thus avoiding\\nthe perturbation of samples that are far from the input data.\\n3.3. Embedding for MeADMM\\nLetM be the sample set representing a manifold and x\\nbe the embedding ofM via a mapping function Φ(·).\\nDefinition 1 The mapping function Φ : x → M in\\nthe neighbour-preserving embedding method is conducted\\nbased on the Geodesic distance, which is defined as follows:\\n1. First we defineM1 =\\n∑K\\nj=1 (1 −Wj)Mj where Wj\\nis the Geodesic distance of the sample x and the jth\\nsample in a setM.\\n2. We define E =M−M1, and have:\\nΦα,\\x0f(x,M) = xα,\\x0f′ = M1 + \\x0f′ · Sα[E ]; Sα[x] =\\nsign(x) ·max{|x| − α, 0}\\nwhere α and \\x0f′ are used to represent the shrinkage fac-\\ntor and the scaler for reconstruction error respectively.\\n3. For a given point projected onto the manifold, larger\\nweights are reasonably assigned to its nearest points\\nin the recovery process.\\nFrom Definition 1, the input sample can be projected\\nonto a manifold via an embedding function by fully exploit-\\ning the neighbor structure information [5]. As shown in\\nLLE [21], a local point on a manifold can be represented by\\na small and compact set of K nearest neighbors to approxi-\\nmate ISOMAP. In [20], it has been shown that the Geodesic\\ndistance used in ISOMAP is another effective way to locate\\nthe neighbors for a linear embedding. We first follow the\\nidea in [15] to estimate the manifold dimension by PCA.\\nNext, we propose the mapping function Φ to generate an\\nembedding sample that lies on a given manifold. Our idea is\\nsimilar to [5] but the difference lies in its simplicity and fea-\\nsibility to solve the problem at hand. Note that MeADMM\\nadds an extra computational cost to our problem because\\nvariables are added by the cloning mechanism.\\nThe soft-thresholding function for the scalar values [17]\\nis defined as:\\nSα[x] = sign(x).max(|x| − α, 0),\\nwhere α ≥ 0. When applied to vectors and matrices, the\\nshrinkage operator acts element-wise. Based on Definition\\n1, MeADMM can be alternatively used to solve our problem\\nand in the following we give details about the optimization\\nprocedure.\\nAlgorithm 1: Main algorithm to solve LRD based on\\nMeADMM\\n1: INPUT:\\n1)Vd ◦ τ = [vec(I1), ..., vec(IB ]), where Ii with i = 1, ..., B represent\\nB input images;\\n2) Initialise with (V 0d , E\\n0,∆τ0).\\n2: repeat\\n3: compute Jacobian matrices w.r.t transformations:\\nJi ← ∂∂ζ\\n(\\nvec(Ii◦ζ)\\n‖vec(Ii◦ζ)‖\\n)∣∣∣\\nζ=τ\\n;\\n4: warp and normalize the images:\\nVd ◦ τ = [ vec(I1◦ζ)‖vec(I1◦ζ)‖ ,\\nvec(I2◦ζ)\\n‖vec(I2◦ζ)‖ , ...,\\nvec(IB◦ζ)\\n‖vec(IB◦ζ)‖ )\\n5: solve the manifold constraint on the transformation process on\\nVd ◦ τ +\\n∑\\nJi∆τi\\x0fi\\nthe details of V ′m and Vr are shown in the Alg. 2 and Eq. 8. (inner loop)\\n6: update the transformation: τ = τ + ∆τ\\n7: until some stopping criterion\\n8: OUTPUT: the solution (V ∗r , E\\n∗,∆τ∗) in our optimization framework.\\n3.4. The MeADMM algorithm\\nThe main algorithm to solve LRD based on MeADMM\\nis shown in Algorithm 1. In the outer loop, we solve τ ,\\nwhile other variables such as Vr and Vm are solved in the\\ninner loop (MeADMM). A separable structure based on\\nLµ,2(, ) can be exploited by ADMM, which is:\\n(V [k+1]r , E\\n[k+1],∆τ [k+1], Y [k+1]) =\\narg\\nVr,E,∆τ\\nminLµ,2(V\\n[k]\\nm , E\\n[k],∆τ [k], Y [k]).\\n(6)\\nDetails on the solution of Eq. 6 are shown in Algorithm 2.\\nDifferent from the original objective function in [17], the\\nmatrix V [k]m needs to be estimated first in order to perform\\nSVD decomposition. Considering the constraint V [k]m =\\nV\\n′[k]\\nm , the matrix V\\n′[k]\\nm can be used to replace the original\\nV\\n[k]\\nm as shown in Algorithm 2. V\\n′[k+1]\\nm is actually used to\\napproximate V [k+1]m that lies on the manifold. Now we have\\na new objective as:\\nLµ,2(Vr, Vm, E,∆τ, Y ) = ||Vr||∗ + λ ∗ ||E||1\\n− < Y, ((Vr + E)− V ′m) > +µ\\n2\\n||(Vr + E)− V ′m||2\\n+\\nB∑\\ni\\nσi\\n2\\n|˙|Vm,i − V ′m,i||2, and V ′m,i ∈M\\n(7)\\nAlgorithm 2: Variable solution based on the\\nMeADMM algorithm\\n1: INPUT: V\\n′[k]\\nm calculated in Def. 1.\\n2: compute (U,Σ,V) = SVD(V ′[k]m + Y [k]/µ[k] − E[k])\\n3: compute V [k+1]r = US 1\\nµ[k]\\n|Σ|VT\\n4: compute\\nE[k+1] = S 1\\nµ[k]\\n[Vd ◦ τ [k] +\\n∑\\nJi∆τ\\n[k]\\ni \\x0fi\\x0f\\nT\\ni + Y\\n[k]/µ[k] − V [k+1]r ]\\n∆τ [k+1] =\\n∑\\ni\\nJi(V\\n[k+1]\\nr +E\\n[k+1] − Vd ◦ τ [k] − 1/(µ[k])Y [k])\\x0fi\\x0fTi\\nY [k+1] = Y [k] + µ[k]Lu(V\\n[k+1]\\nr , E\\n[k+1],∆τ [k+1], Y [k])\\nµ[k+1] = max(0.9µ[k], µ˜)\\n5: compute V\\n′[k+1]\\nm based on Def. 1.\\n6: OUTPUT: the solution (V ∗r , E∗,∆τ∗, Y ∗) to the recovery process\\nin our optimization framework.\\nFrom Eq. 7, the unknowns Y [k+1], E[k+1] and ∆[k+1]\\nare not directly related to\\n∑B\\ni\\nσi\\n2 |˙|Vm,i−V ′m,i||2. So we can\\nsolve (Algorithm 2) in a similar method as that of [17]. Now\\nonly the matrix V\\n′[k+1]\\nm remains unsolved. Given V\\n[k+1]\\nm =\\nV\\n[k+1]\\nr + E[k+1], based on the derivative of Eq. 7, V\\n′[k+1]\\nm\\nis solved as:\\nV\\n′[k+1]\\nm = Tm · V [k+1]m . (8)\\nAs shown in Eq. 8, Tm 1 is a unknown projection ma-\\ntrix on V [k+1]m . Based on the manifold embedding, the\\nprojection is solved in an efficient way, i.e. V\\n′[k+1]\\nm,i =\\nΦα,\\x0f(V\\n[k+1]\\nm,i , V\\n[k+1]\\nm ). The embedding performs well for a\\nsmall set of nearest samples, which leads to the robustness\\nagainst severe illumination and corruption.\\n1Without considering the manifold constraint, we have Tm = (Y +\\n(σ∗ + µ) · I)−1 · (σ∗ + 2µ), σi is the diagonal element of σ∗, and I is\\nthe identity matrix.\\nTable 2. Alignment errors in eye centers, calculated as the dis-\\ntances from the estimated eye centers to their ground truth.\\nMethods Mean error (pixel) Error std. Max error\\nInitial 1.69 0.428 2.23\\nRASL 0.16 0.36 1.0\\nMeADMM 0.14 0.35 1.0\\n4. Experiments\\nWe evaluate MeADMM on four datasets including Ex-\\ntended Yale face database B [7], AR face [16], USPS digits\\n[10] and planar surfaces (window images) [17]. The images\\nin those databases suffer from rotation, occlusion and light-\\ning variations. The Geodesic distance is calculated based\\non Vr with a number of the neighbors (K) set to 7.We also\\nempirically set α = 0.05 and \\x0f′ = 0.85 in all our experi-\\nments.\\nWe compare the performance of the proposed\\nMeADMM with RASL [17], which is the state-of-\\nthe-art algorithm for LRD. An improved algorithm based\\non RASL is proposed in [28]. Due to lack of imple-\\nmentation, we implemented their algorithm by ourselves,\\nbut cannot reproduce the results reported in their paper.\\nTherefore, to facilitate a fair comparison, in this paper\\nwe show comparison results of MeADMM and RASL\\nevaluated on each dataset. It is also worth noting that the\\nparameter settings of our method are the same as those\\nused by RASL.\\nExtended Yale-B. In the Extended Yale Face Database\\nB, each subject image contains 64 different illumination\\nconditions and the images are resized to 42 × 45. The 64\\nimages of a subject in a given pose were acquired at 30\\nframes/second in about 2 seconds, so there is only a small\\nchange in head pose and facial expressions in the images.\\nTo increase the difficulty of the LRD problem, we ran-\\ndomly rotate and shift the face images. The qualitative\\nresults are illustrated in Fig. 2.\\nWith respect to the alignment, both methods achieve\\nmore or less the same performance. For the average faces\\nafter alignment, both methods can well solve the misalign-\\nment problem. We also present the quantitative results in\\nterms of alignment errors in eye centers on this dataset (see\\nTable 2). As evident in this table, both approaches achieve\\nsmall misalignment errors for the faces with rotation, light-\\ning variations and shifting. Different from RASL that fo-\\ncuses on the misalignment performance, we pay more atten-\\ntion to the recovery effectiveness. Fig. 2 shows MeADMM\\nachieving much better performance than RASL in terms of\\nreconstruction quality. Especially for the results on the last\\nthree rows, MeADMM significantly eliminates the illumi-\\nnation variations from the original images, even in the pres-\\nence of severe illumination changes and misalignment.\\nAR Face Database. We next test MeADMM on the\\nFigure 2. Results of RASL and MeADMM on the Extended Yale-B database. In the first row, the averages of input, alignment, and low-rank\\nresults are shown for RASL and MeADMM. In the second row, the first and third columns results are obtained by RASL and MeADMM,\\nrespectively. (D: Input; A: Low-rank component)\\nAR face database which contains 126 persons with different\\nfacial expressions, illumination conditions, and occlusions\\n(such as sun glasses and scarf). The pictures were taken\\nwith no restrictions on participants’ appearance (clothes,\\nglasses, etc.), make-up, hair style, etc. in two sessions, sep-\\narated by two weeks time. For each person, we choose 26\\nimages (64 × 64) from Session 1 to validate both methods.\\nDifferent from Yale database B, the faces are severely oc-\\ncluded by glasses and scarf. It can be observed from Fig. 3\\nthat MeADMM achieves much better low-rank images than\\nRASL, especially for the subjects wearing scarf and having\\nlarge expression variations. The eyes and mouths are almost\\nrecovered from the input images as shown in the last row,\\ndemonstrating the superiority of MeADMM on image re-\\ncovery. This is also beneficial for other vision applications\\nsuch as face recognition and human re-identification.\\nUSPS Database. In the USPS handwritten digit\\ndatabase, we use a standard subset containing 10-class digit\\nimages, and perform MeADMM and RASL on the train-\\ning set. The low-rank decomposition results are illustrated\\nin Fig. 3. MeADMM successfully recovers all the im-\\nages of digit 1, but RASL fails on three images. Similarly,\\nMeADMM works well on digit 4, whereas RASL fails to\\nrecover one low-rank image. The experiments on the digit\\nimages again show the advantages of our method with varia-\\ntions such as rotation, shift and affine transformations. Due\\nto lack of ground truth, we can only show the qualitative\\nresults for digits analysis and window image analysis fol-\\nlowing the evaluation protocol of [17].\\nPlanar Surfaces. Moreover, we demonstrate that\\nMeADMM can be used to align images affected by planar\\nhomography transformations. To better demonstrate the ro-\\nRASL\\nMeADMM\\nInput\\nFigure 3. Low-rank decomposition results by RASL and MeADMM on the AR database. The top row shows the input images. The second\\nand third rows show the results by RASL and MeADMM, respectively.\\nbustness of MeADMM, we manipulate the input images by\\ncropping patches or changing illuminations, as in Fig. 4\\n(input). MeADMM achieves better results than RASL (first\\nand third window images). MeADMM not only aligns the\\nwindows and removes the occluded tree branches, but also\\nfaithfully inpaints the missing areas. Together with con-\\nsistent results obtained from faces and digits datasets, we\\ncould draw a conclusion that MeADMM is more effective\\nfor low-rank calculation, when the data includes rotation,\\nocclusion and illumination variations.The performance im-\\nprovement of MeADMM is attribute to the manifold con-\\nstraint implicitly learned from the data.\\nSpeed of MeADMM. Regarding the computational cost,\\nMeADMM is not as fast as RASL due to solving additional\\nvariables. Running time for window images is 220ms and\\n102ms for MeADMM and RASL, respectively on a PC with\\nIntel i5 CPU and 4G RAM. The source code of the proposed\\nMeADMM algorithm will be released to public to facilitate\\nfurther development.\\n5. Conclusion\\nThis paper presents a new insight into low-rank de-\\ncomposition using manifold constraint and we propose the\\nMeADMM method based on a neighbour-preserving em-\\nbedding approach. We solve manifold constraint with a\\nprojection, which is efficiently calculated during the em-\\nbedding process. The proposed approach is successfully\\napplied to faces, digits and planar surfaces, showing a con-\\nsistent increase on image alignment and recovery perfor-\\nmance as compared to the state-of-the-art. In future work,\\nwe will investigate MeADMM in other applications such as\\nimage inpainting, video frame prediction, background sub-\\nstraction, tracking [24, 25, 26, 27].\\nAcknowledgment\\nThe work was supported in part by the Natural Sci-\\nence Foundation of China under Contract 61672079 and\\n61473086. The work of B. Zhang was supported in part\\nby the Program for New Century Excellent Talents Univer-\\nsity within the Ministry of Education, China, and in part\\nby the Beijing Municipal Science and Technology Commis-\\nsion under Grant Z161100001616005. Baochang Zhang is\\nthe corresponding author.\\nReferences\\n[1] L. Agapito, J. Xavier, A. D. Bue, and M. Paladini. Bilinear\\nmodeling via augmented lagrange multipliers (balm). IEEE\\nTransactions on Pattern Analysis & Machine Intelligence,\\n34(8):1496–1508, 2012.\\n[2] S. D. Babacan, M. Luessi, R. Molina, and A. K. Katsagge-\\nlos. Sparse bayesian methods for low-rank matrix estima-\\nFigure 4. low-rank decomposition results of MeADMM (second column) and RASL (third column) on the USPS database.\\nInput\\nFigure 5. Low-rank decomposition results of RASL and MeADMM on planar surfaces.\\ntion. Signal Processing IEEE Transactions on, 60(8):3964 –\\n3977, 2011.\\n[3] G. Cabanes and Y. Bennani. Learning topological constraints\\nin self-organizing map. In Neural Information Process-\\ning. MODELS and Applications - International Conference,\\nICONIP 2010, Sydney, Australia, November 22-25, 2010,\\nProceedings, pages 367–374, 2010.\\n[4] M. W. Chang, L. A. Ratinov, and R. Dan. Guiding semi-\\nsupervision with constraint-driven learning. In ACL 2007,\\nProceedings of the Meeting of the Association for Computa-\\ntional Linguistics, June 23-30, 2007, Prague, Czech Repub-\\nlic, pages 224–227, 2007.\\n[5] J. Chen, R. Wang, S. Yan, and S. Shan. Enhancing human\\nface detection by resampling examples through manifolds.\\nIEEE Transactions on Systems Man and Cybernetics - Part\\nA Systems and Humans, 37(6):1017–1028, 2007.\\n[6] M. Cox, S. Sridharan, S. Lucey, and J. Cohn. Least squares\\ncongealing for unsupervised alignment of images. In Com-\\nputer Vision and Pattern Recognition, 2008. CVPR 2008.\\nIEEE Conference on, pages 1–8. IEEE, 2008.\\n[7] A. S. Georghiades, P. N. Belhumeur, and D. J. Kriegman.\\nFrom few to many: Illumination cone models for face recog-\\nnition under variable lighting and pose. IEEE transactions on\\npattern analysis and machine intelligence, 23(6):643–660,\\n2001.\\n[8] G. B. Huang, V. Jain, and E. Learned-Miller. Unsupervised\\njoint alignment of complex images. In IEEE International\\nConference on Computer Vision, pages 1–8, 2007.\\n[9] Z. Huang, R. Wang, S. Shan, and X. Chen. Hybrid euclidean-\\nand-riemannian metric learning for image set classification.\\nIn Asian Conference on Computer Vision, pages 562–577.\\nSpringer, 2014.\\n[10] J. J. Hull. A database for handwritten text recognition\\nresearch. Pattern Analysis & Machine Intelligence IEEE\\nTransactions on, 16(5):550–554, 1994.\\n[11] J. Jiang, C. Chen, J. Ma, Z. Wang, Z. Wang, and R. Hu.\\nSrlsp: A face image super-resolution algorithm using smooth\\nregression with local structure prior. IEEE Transactions on\\nMultimedia, 19(1):27–40, Jan 2017.\\n[12] E. G. Learnedmiller. Data driven image models through con-\\ntinuous joint alignment. Pattern Analysis & Machine Intelli-\\ngence IEEE Transactions on, 28(2):236–250, 2006.\\n[13] C. Li, L. Lin, W. Zuo, W. Wang, and J. Tang. An approach\\nto streaming video segmentation with sub-optimal low-rank\\ndecomposition. IEEE Transactions on Image Processing,\\n25(5):1947–1960, 2016.\\n[14] J. Liang, Z. Hou, C. Chen, and X. Xu. Supervised bi-\\nlateral two-dimensional locality preserving projection algo-\\nrithm based on gabor wavelet. Signal, Image and Video Pro-\\ncessing, 10(8):1441–1448, 2016.\\n[15] T. Lin and H. Zha. Riemannian manifold learning. IEEE\\nTransactions on Pattern Analysis & Machine Intelligence,\\n30(5):796–809, 2007.\\n[16] A. M. Martinez. The ar face database. Cvc Technical Report,\\n24, 1998.\\n[17] Y. Peng, A. Ganesh, J. Wright, W. Xu, and Y. Ma. Rasl:\\nrobust alignment by sparse and low-rank decomposition for\\nlinearly correlated images. IEEE Transactions on Pattern\\nAnalysis & Machine Intelligence, 34(11):2233–46, 2012.\\n[18] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduc-\\ntion by locally linear embedding. Science, 290(5500):2323–\\n6, 2000.\\n[19] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global\\ngeometric framework for nonlinear dimensionality reduc-\\ntion. science, 290(5500):2319–2323, 2000.\\n[20] C. Varini, A. Degenhard, and T. Nattkemper. Isolle: Locally\\nlinear embedding with geodesic distance. In European Con-\\nference on Principles of Data Mining and Knowledge Dis-\\ncovery, pages 331–342. Springer, 2005.\\n[21] R. Wang, S. Shan, X. Chen, Q. Dai, and W. Gao. Manifold–\\nmanifold distance and its application to face recognition\\nwith image sets. IEEE Transactions on Image Processing,\\n21(10):4466–4479, 2012.\\n[22] M. Yin, S. Cai, and J. Gao. Robust face recognition via dou-\\nble low-rank matrix recovery for feature extraction. In 2013\\nIEEE International Conference on Image Processing, pages\\n3770–3774. IEEE, 2013.\\n[23] M. Yin, J. Gao, and Z. Lin. Laplacian regularized low-rank\\nrepresentation and its applications. IEEE Transactions on\\nPattern Analysis & Machine Intelligence, PP(99):504–517,\\n2016.\\n[24] B. Zhang, Z. Li, X. Cao, Q. Ye, C. Chen, L. Shen, A. Pe-\\nrina, and R. Jill. Output constraint transfer for kernelized\\ncorrelation filter in tracking. IEEE Trans. Systems, Man, and\\nCybernetics: Systems, 47(4):693–703, 2017.\\n[25] B. Zhang, Z. Li, A. Perina, A. D. Bue, V. Murino, and J. Liu.\\nAdaptive local movement modeling for robust object track-\\ning. IEEE Trans. Circuits Syst. Video Techn., 27(7):1515–\\n1526, 2017.\\n[26] B. Zhang, A. Perina, Z. Li, J. L. V. Murino, and R. Ji.\\nBounding multiple gaussians uncertainty with application to\\nobject tracking. International Journal of Computer Vision,\\n118(3):364–379, 2017.\\n[27] B. Zhang, A. Perina, V. Murino, and A. D. Bue. Sparse rep-\\nresentation classification with manifold constraints transfer.\\nIn IEEE Conference on Computer Vision and Pattern Recog-\\nnition (CVPR), pages 4557–4565, June 2015.\\n[28] D. W. Z. Z. Zhang, Xiaoqin and Y. Ma. Simultaneous recti-\\nfication and alignment via robust recovery of low-rank ten-\\nsors. In Advances in Neural Information Processing Systems,\\npages 1637–1645, 2013.\\n[29] Z. Zhang and K. Zhao. Low-rank matrix approximation with\\nmanifold regularization. IEEE Transactions on Pattern Anal-\\nysis & Machine Intelligence, 35(7):1717–1729, 2013.\\n'}\n",
      "{'_id': ObjectId('6815e61add259dc7e6e1cd34'), 'authors': 'Chang, J. Morris, Chen, Keyu, Zhuang, Di', 'year': '2021', 'title': 'Discriminative Adversarial Domain Generalization with Meta-learning\\n  based Cross-domain Validation', 'full_text': 'Discriminative Adversarial Domain Generalization with Meta-learning based\\nCross-domain Validation\\nKeyu Chen, Di Zhuang, J. Morris Chang\\nDepartment of Electrical Engineering\\nUniversity of South Florida, Tampa, FL 33620\\n{keyu, dizhuang, chang5}@usf.edu\\nAbstract\\nThe generalization capability of machine learning mod-\\nels, which refers to generalizing the knowledge for an “un-\\nseen” domain via learning from one or multiple seen do-\\nmain(s), is of great importance to develop and deploy ma-\\nchine learning applications in the real-world conditions.\\nDomain Generalization (DG) techniques aim to enhance\\nsuch generalization capability of machine learning models,\\nwhere the learnt feature representation and the classifier are\\ntwo crucial factors to improve generalization and make de-\\ncisions. In this paper, we propose Discriminative Adver-\\nsarial Domain Generalization (DADG) with meta-learning-\\nbased cross-domain validation. Our proposed framework\\ncontains two main components that work synergistically to\\nbuild a domain-generalized DNN model: (i) discrimina-\\ntive adversarial learning, which proactively learns a gen-\\neralized feature representation on multiple “seen” domains,\\nand (ii) meta-learning based cross domain validation, which\\nsimulates train/test domain shift via applying meta-learning\\ntechniques in the training process. In the experimental eval-\\nuation, a comprehensive comparison has been made among\\nour proposed approach and other existing approaches on\\nthree benchmark datasets. The results shown that DADG\\nconsistently outperforms a strong baseline DeepAll, and\\noutperforms the other existing DG algorithms in most of\\nthe evaluation cases.\\n1 Introduction\\nMachine Learning (ML) and Deep Learning (DL) have\\nachieved great success in numerous applications, such as\\nskin lesion analysis [26, 5], human activity recognition\\n[30, 40], active authentication [37], facial recognition [6,\\n24, 42], botnet detection [22, 38, 39] and community detec-\\ntion [29, 41]. Most of the ML/DL applications are under-\\nlying the assumption that the training and testing data are\\ndrawn from the same distribution (i.e., domain). However,\\nin practice, it is more common that the data are from vari-\\nous domains (i.e., domain shift [18]). For instance, the im-\\nage data for the medical diagnosis application might be col-\\nlected from different hospitals, by different types of devices,\\nor using different data preprocessing protocols. The domain\\nshift issue results in a rapid performance degradation, where\\nthe machine learning applications is trained on “seen” do-\\nmains and tested on other “unseen” domains. Even well-\\nknown strong learners such as deep neural networks are\\nsensitive to domain shifts [12]. It is crucial to enhance the\\ngeneralization capability of machine learning models in the\\nreal-world applications. Because, on one hand, it is costly\\nto re-collect/label the data and re-train the model for such\\n“unseen” domains. On the other hand, we can never enu-\\nmerate all the “unseen” domains in advances.\\nDomain Generalization (DG), as illustrated in Figure 1,\\nwhich aims to learn a domain-invariant feature representa-\\ntion from multiple given domains and expecting good per-\\nformance on the “unseen” domains. It is one of the tech-\\nniques that aiming to enhance the generalization capability\\nof machine learning models. However, designing an effec-\\ntive domain generalization approach is challenging. First, a\\nwell-designed DG approach should be model-agnostic. Do-\\nmain shift is a general problem in the designing of ML/DL\\nmodels, such that the approach should not be designed for a\\nspecific network architecture. Second, an effective DG ap-\\nproach should not be data-dependent. There exists different\\ntypes of domain shift, such as different art forms or dif-\\nferent centric-images. A data-dependent approach can lead\\npromising results on some datasets. However, the approach\\ncan be overfitting to the particular domain shift and might\\nnot have comparable performance on the other datasets.\\nHence, it is a challenging task to design an effective DG\\napproach.\\nTo date, a few algorithms have been proposed to enhance\\nthe generalization capability of ML/DL models. For in-\\nstance, D-SAM [7] designs a domain-specific aggregation\\nmodule for each “seen” domain, and plugs it on a partic-\\nar\\nX\\niv\\n:2\\n01\\n1.\\n00\\n44\\n4v\\n1 \\n [\\ncs\\n.L\\nG\\n] \\n 1\\n N\\nov\\n 2\\n02\\n0\\ncartoon\\nart-painting\\nphoto\\nsketch\\n \\nML/DL Model\\nTrain\\nTest\\nFigure 1. Multi-source Domain Generalization: training\\na model on one or multiple seen source domains and test on\\ncertain “unseen” target domain.\\nular network architecture to eliminate the domain specific\\ninformation. However, it is a model-based approach, be-\\ncause the aggregation module is designed for a particular\\nmodel, and additional implementation of aggregation mod-\\nule is required when the model changed. Hex [36] is pro-\\nposed to learn robust representations cross various domains\\nvia reducing the model dependence on high-frequency tex-\\ntural information. The original supervised model is trained\\nwith an explicit objective to ignore the superficial statistics,\\nwhich only presents in certain datasets. Its representation\\nlearning is fully unsupervised, and performs good on certain\\nimage datasets. However, due to the assumption of domain\\nshift and the unsupervised natural, Hex might not have the\\npromising performance on the other image datasets. Ap-\\nproaches that leveraging the idea of meta-learning for do-\\nmain generalization have been also proposed [18, 21, 2].\\nFor instance, MLDG [18] was inspired by MAML [10]\\nto simulate the domain-shift and optimize meta-train and\\nmeta-test together during the training phase. However, it\\nonly focuses on the classifier optimization, and lacks of\\neffective guidance on the feature representation learning,\\nwhere the better feature representation can benefit the clas-\\nsifier to make decisions.\\nIn this paper, we present a novel DG approach, Discrim-\\ninative Adversarial Domain Generalization (DADG). Our\\nDADG contains two main components, discriminative ad-\\nversarial learning (DAL) and meta-learning based cross do-\\nmain validation (Meta-CDV). We adopt the DAL to learn\\nthe set of features, which provides domain-invariant repre-\\nsentation for the following classification task, and apply the\\nMeta-CDV to further enhance the robustness of the classi-\\nfier. Specifically, on one hand, we consider the DAL con-\\nponent as a discriminator that trains a domain-invariant fea-\\nture extractor by distinguishing the source domain of cor-\\nresponding training data. On the other hand, we employ\\nmeta-learning optimization strategy to “boost” the objective\\ntask classifier by validating it on previously “unseen” do-\\nmain data in each iteration. The two components guide each\\nother from both feature representation and object recogni-\\ntion level via a model-agnostic process over iterations to\\nbuild a domain-generalization model. Note that our DADG\\nmakes no assumption on the datasets, and it is a model-\\nagnostic approach, which can be applied to any network ar-\\nchitectures.\\nIn the experimental evaluation, a comprehensive com-\\nparison has been made among our proposed approach (i.e.,\\nDADG) and other 8 existing DG algorithms, including\\nDeepAll (i.e., the baseline that simply used pre-trained net-\\nwork, without applying any DG techniques), TF [17], Hex\\n[36], D-SAM [7], MMD-AAE [19], MLDG [18], Feature-\\nCritic (FC) [21], and JiGen [3]. We conduct the compari-\\nson and the evaluation of our approach on three well-known\\nDG benchmark datasets: PACS [17], VLCS [32] and Office-\\nHome [35], utilizing two deep neural network architectures,\\nAlexNet and ResNet-18. Our experimental result shows\\nthat our approach performs well at cross domain recogni-\\ntion tasks. Specifically, we achieve the best performance on\\n2 datasets (VLCS and Office-Home) and performs 2nd best\\non PACS. For instance, on VLCS dataset, we improve on\\nthe strong baseline DeepAll by 2.6% (AlexNet) and 3.11%\\n(ResNet-18). Moreover, an ablation study also conducted\\nto evaluate the influence of each component in DADG.\\nTo summarize, our work has the following contributions:\\n• We present a novel, effective and model-agnostic\\nframework, Discriminative Adversarial Domain General-\\nization (DADG) to tackle the DG problem. Our ap-\\nproach adopts discriminative adversarial learning to learn\\nthe domain-invariant feature extractor and utilizes meta-\\nlearning optimization strategy to enhance the robustness of\\nthe classifier.\\n• To the best of our knowledge, DADG is the first work\\nthat uses meta-learning optimization to regularize the fea-\\nture learning of discriminative adversarial learning in do-\\nmain generalization.\\n• A comprehensive comparison among our algorithm\\nand the state-of-the-art algorithms has been conducted (Sec-\\ntion 4). For the sake of reproducibility and convenience\\nof future studies about domain generalization, we have re-\\nleased our prototype implementation of DADG. 1\\nThe rest of this paper is organized as follows: Section 2\\npresents the related literature review. Section 3 presents the\\nnotations in common domain generalization problem, and\\ndescribes our proposed algorithm. Section 4 presents the\\nexperimental evaluation. Section 5 presents the conclusion.\\n1https://github.com/keyu07/DADG\\n2\\n2 Related Work\\n2.1 Generative Adversarial Nets (GAN)\\nGenerative Adversarial Nets (GAN) [14] aims to approx-\\nimate the distribution Pd of a dataset via a generative model.\\nGAN simultaneously trains two components generator G\\nand discriminator D. The two components, generator and\\ndiscriminator can be built from neural networks (e.g., con-\\nvolutional layers and fully connected layers). The input of\\nG is sampled from a prior distribution Pz(z) through which\\nG generates fake samples similar to the real samples. Mean-\\nwhile, D is trained to differentiate between fake samples\\nand real samples, and sends feedback to G for improve-\\nment. GAN can be formed as a two-player minimax game\\nwith value function V (G,D):\\nmin\\nG\\nmax\\nD\\nV (G,D) = Ex∼Pd [log(D(x))] +\\nEz∼Pz [log(1−D(G(z)))]\\n(1)\\nGAN-based discriminative adversarial learning is able to\\nlearn a latent space from multiple different domains, where\\nthe latent space is similar to the given domains. It has been\\nused in some domain adaptation works, which we will dis-\\ncuss below.\\n2.2 Domain Adaptation\\nDomain adaptation (DA) is one of the closely related\\nwork to domain generalization. The main difference be-\\ntween DA and DG is that DA assumes unlabeled target data\\nis available during the training phase, but DG has no ac-\\ncess to the target data. Many domain adaptation algorithms\\n[34, 12, 11, 33] are designed via mapping the source and the\\ntarget domain into a domain-invariant feature space. GAN\\nor GAN-based discriminative adversarial techniques have\\nbeen utilized in many such domain adaptation works. For\\ninstance, ADDA [34] maps the data from the target domain\\nto the source domain through training a domain discrimina-\\ntor. DANN [12] is proposed to train a “domain classifier” to\\nlearn the latent representations of the source and the target\\ndomains. Tzeng et al. [33] proposes to use multiple adver-\\nsarial discriminators to apply on the data of different avail-\\nable source domains. Discriminative adaversarial learning\\nsuccessfully learns the domain-invariant feature representa-\\ntion, which considered as a latent space that similar to all\\nsource domains. This success motivates us to optimize the\\nfeature learning of domain generalization.\\n2.3 Domain Generalization\\nIn contrast to domain adaptation, domain generalization\\nis a more challenging problem, because it requires no prior\\nknowledge about the target domain. Given a ML/DL appli-\\ncation that has multiple “seen” or/and “unseen” domains,\\nwe observe that each domain has two elements: the pri-\\nvate element and the global element. The private element\\ncontains the specific representation/information of each do-\\nmain, while the global element holds the invariant features\\nacross different domains. Most of the recent domain gener-\\nalization works aim to improve the learnt feature by using\\none of the two strategies: (i) Eliminating the influence of\\nthe private elements or (ii) Extracting the global elements.\\nOther than the two main strategies, there are other alterna-\\ntive studies, such as a data augmentation based method [31]\\nand a recent self-supervised learning method JiGen [3]. Ji-\\nGen [3] uses a jigsaw-puzzle classifier to guide the feature\\nextractor to capture the most informative part of the images,\\nand it achieves current state-of-the-art results on three do-\\nmain generalization benchmark datasets. We include JiGen\\n[3] in all our evaluations.\\nMany model-enhancement based studies are proposed\\nunder the first strategy. For instance, Li et al. [17] de-\\nvelops a low-rank parameterized network to decrease the\\nsize of parameters. D’Innocente et al. [7] proposes to\\nbuild domain-specific aggregation modules and stack on the\\nbackbone network to merge specific and generic informa-\\ntion. However, it is a model based approach. Because one\\nset of aggregation modules can only apply on one particular\\nbackbone network. Additional implementation is required\\nwhen we change the network architecture. Hex [36] is pro-\\nposed to learn robust representations cross various domains\\nvia reducing the model dependence on high-frequency tex-\\ntural information. The original supervised model is trained\\nwith an explicit objective to ignore the so called superficial\\nstatistics, which is presented in the training set but may not\\nbe present in future testing sets. Its representation learn-\\ning is fully unsupervised, and performs good on certain\\nimage datasets. However, because the assumption of do-\\nmain shift and the unsupervised natural of Hex, it might not\\nhave the comparable good performance on the other image\\ndatasets. However, designing an approach to weaken certain\\ntypes of domain-specific elements may suffer from overfit-\\nting on such domain elements. Though some outstanding\\nresults have been shown by this kind of approaches on cer-\\ntain datasets, while may not be able to be generalized to\\nmany more “unseen” domains. For instance, the different\\ndomain types are considered as different art forms or differ-\\nent centric-images.\\nFor the second strategy, most of the previous works\\nare focusing on learning domain-invariant representation,\\nwhich is able to capture the important similar information\\namong multiple different domains and have the capability\\nof generalizing to more “unseen” domains. As such, these\\nworks are more similar to the work of domain adaptation.\\nFor intance, Ghifary et al. [13] proposes to learn domain-\\n3\\ninvariant features via a multi-domain reconstruction auto-\\nencoder. However, the effectiveness for reconstruction the\\nauto-encoder is limited while applying to more complex\\ndatasets [17]. Motiian et al.[23] employs maximum mean\\ndiscrepancy (MMD) and proposes to learn a latent space\\nthat minimizes the distance among images that have the\\nsame class label but different domains. Li et al. [19] pro-\\nposes to align source domains to learn a domain-agnostic\\nrepresentation using adversarial autoencoders with MMD\\nconstraints, and uses adversarial learning to match the dis-\\ntribution of generated data with a prior distribution.\\nOur approach also belongs to the second strategy. We\\nuse the discriminative adversarial learning to learn a latent\\ndistribution among the source domains. By doing so, we\\nachieve a domain-invariant feature representation that dif-\\nferent domains are indistinguishable. Beyond the domain-\\ninvariant feature representation, in order to improve the rel-\\nevant classification task, we also propose a more robust\\nclassifier, by using meta-learning based optimization, which\\nleads more competitive classification results. To the best\\nof our knowledge, this is the first work that uses meta-\\nlearning optimization to regularize the discriminative adver-\\nsarial learning in domain generalization.\\n2.4 Meta-Learning\\nMeta-learning introduces a concept “learning-to-learn”\\nand recently receives great interests with applications in-\\ncluding few-shot learning [10, 25, 27] and learning opti-\\nmizations [20, 1]. It learns from various tasks during train-\\ning and such that the model can be quickly generalized to\\nnew tasks. MAML [10] is typical in those works. It uti-\\nlizes sampled episodes during training, where each episode\\nis designed to simulate the few-shot tasks in a train-test split\\nmanner. Recently, a few works have applied this episodic\\nmeta-learning optimization method in domain generaliza-\\ntion [18, 2, 21]. For instance, MLDG [18] borrows the idea\\nof [10] to optimize the classifier, by simulating the train-test\\ndomain shift during training phase. MetaReg [2] proposes\\nto learn a regularization function for the network classifier.\\nLi et al. [21] proposes to simultaneously learn an auxiliary\\nloss and measure whether the performance of validation set\\nhas been improved. However, MLDG [18] and MetaReg\\n[2] only focus on classifier optimization, and are lacking of\\ndetails addressing the learning of a domain-invariant fea-\\nture space. The success of meta-learning method on the en-\\nhancement of classifier robustness motivates us to optimize\\nthe network classifier for domain generalization. To sum-\\nmarize, in order to address the challenging domain general-\\nization problem, we apply discriminative adversarial learn-\\ning and meta-learning, where the discriminative adversarial\\nlearning extracts domain-invariant feature representation,\\nand meta-learning enhances the classifier robustness.\\n3 Methodology\\nThe design of DADG is based on our assumption\\nthat there exists a domain-invariant feature representation,\\nwhich contains the common information for both the “seen”\\nand “unseen” domains. It should satisfy the following prop-\\nerties: (i) The feature representation should be invariant in\\nterms of data distributions (i.e., domains). Since ML/DL\\nmodels are designed to transfer the knowledge from seen\\ndomains to unseen domains, they could fail if the distri-\\nbutions differ a lot. (ii) It should keep the variance be-\\ntween different objects (i.e., classes). This helps the model\\nto capture the unique information of different objects (i.e.,\\nclasses) and to make precise decisions (i.e., classifications).\\nWe use two key components in DADG to address the above\\ntwo properties: discriminative adversarial learning (DAL)\\nand meta-learning based cross domain validation (Meta-\\nCDV). DAL aims to learn a domain-invariant feature repre-\\nsentation where different data distributions (i.e., domains)\\nare indistinguishable. Therefore, the domain variance will\\nbe minimized. Meta-CDV brings the learnt features to su-\\npervised learning by training a classifier in a meta-learning\\nmanner. It evaluates the validation performance of previous\\nunseen domains within each training iteration.\\nTo better understand our idea, we take a look at the Fig-\\nure 2. The goal of our DADG is to find the optimized feature\\nrepresentation point, which satisfies the above two prop-\\nerties. A, B and C present the different domains. DAL\\nand Meta-CDV address DG in two aspects: (i) A domain-\\ninvariant feature representation is learnt by making differ-\\nent distributions indistinguishable. As shown by the orange\\nlines, the dash lines are the potential gradient decent di-\\nrection when tackling feature learning on different domains\\n∇DA and ∇DB , where the solid line is the actual “learnt”\\ndirection. (ii) A robust classifier is trained by evaluating\\nthe performance of cross domains on the learnt feature rep-\\nresentation, where the cross domains are the “unseen” do-\\nmains in each iteration. As shown by the blue lines, the\\ndash lines indicate the potential gradient decent direction\\nwhen solving certain tasks ∇TA and ∇TB , and the solid\\nline denotes the “learnt” direction and further with a cross\\ndomain validation (∇TC) optimization.\\nIn the rest of this section, we denote the input data space\\nas x ∈ X , the class label space as y ∈ Y and the domain\\nlabel (i.e., belonging to which distribution) space as yd ∈\\nY d. The source domains are described as Di ∈ S, and\\nthe target domains as T . Also, please note that in the rest\\nof this section, the superscript of each parameter indicates\\ndifferent updating stages within one iteration, denoted asm,\\nwhile the subscript indicates different iterations, denoted as\\nn. We introduce our two main components in the remaining\\nsections: DAL in 3.1 and Meta-CDV in 3.2. Finally we\\nsummarize the two components together in 3.3.\\n4\\n∇𝐷𝐴\\n∇𝐷𝐵\\n∇𝑇𝐴\\n∇𝑇𝐶\\n∇𝑇𝐵\\nFigure 2. Illustration of our idea. Better view in colors.\\n3.1 Discriminative Adversarial Learning\\nAs described above, the goal of this component is to\\nlearn a domain classification model, which aims to classify\\ndata from different domains. We consider our DAL con-\\ntaining two parts: (i) a feature extractor fθ with parameter\\nθ, and (ii) a discriminator dψ with parameter ψ. Both the\\nfeature extractor and the discriminator are deep neural net-\\nworks, convolutional layers and fully connected layers.\\nIn our approach, we first randomly divide the source\\ndomains S into two mutually exclusive sets: Sd for DAL\\nand Sc for Meta-CDV. The discriminator acts as a classifier,\\nwhich takes the learnt sample features fθ(xj) of each arbi-\\ntrary input xj and tries to discriminate its domain label yd.\\nThus, we need to learn the parameters (ψ) that minimize the\\ndiscriminative loss, which as follows:\\nLdisc(dψmn (fθmn (xj)), y\\nd\\nj ) (2)\\nThe loss function of DAL is presented as follows:\\nF (·) =\\n∑\\nDi∈S′\\n∑\\nxj∈Di\\nLdisc(dψmn (fθmn (xj)), y\\nd\\nj ) (3)\\nThe objective of the feature extractor is to maximize\\nthe discriminative loss, to achieve indistinguishable of the\\nlearnt feature representation. Following the design of GAN\\n[14], the objective function of our discriminative adversarial\\nlearning can be written as the following minimax optimiza-\\ntion:\\nargmin\\nψmn\\nmax\\nθmn\\nF (·) (4)\\nSuch minimax parameter updating can be achieved by\\ngradient reversal layer (GRL) [11], which placed between\\nthe feature extractor and discriminator. During forward\\npropagation, GRL keeps the parameters the same. During\\nback propagation, it multiply the gradient by−λ and pass it\\nto the preceding layer.\\nTo summarize, we update the parameters of feature ex-\\ntractor and discriminator as follows:\\nθm+1n ← θmn − α · ∇(−λ · F (·)) (5)\\nψmn+1 ← ψmn − α · ∇F (·) (6)\\nwhere the step size α is a fixed hyperparameter. Thereafter,\\nthe parameter θm+1n will be shared in further training within\\nthe same iteration (as we illustrated in Figure 3 step 1©), and\\nϕmn+1 will be used in the next iteration.\\n3.2 Meta-learning based Cross Domain Valida-\\ntion\\nAfter the feature extractor has been trained to minimize\\nthe domain variance, we adopt meta-learning based cross\\ndomain validation (Meta-CDV) to address the enhancement\\nof the classifier robustness. Robust classifier is able to help\\nthe feature extractor to keep the discriminant power be-\\ntween various classes. This is accomplished by training the\\nclassification model on 2 seen domains Sd in DAL and val-\\nidating the performance on cross domains Sc.\\nTo train the model on seen domains Sd, we consider a\\nclassification model is composed of a feature extractor fθ\\nand a classifier cϕ with parameters ϕ, and the training loss\\nis defined as follows:\\nLtrain(cϕmn (fθm+1n (xj)), yj) (7)\\nwhere xj is an arbitrary input and yj is the corresponding\\noutput label.\\nThe loss function of classification training on seen do-\\nmains is presented as follows (as illustrated in Figure 3 step\\n2©):\\nG(·) =\\n∑\\nDi∈Sd\\n∑\\nxj∈Di\\nLtrain(cϕmn (fθm+1n (xj)), yj) (8)\\nNote that the training is performed over the updated fea-\\nture extractor parameters θm+1 in DAL. As such, the pa-\\nrameters are updated as follows:\\nθm+2n ← θm+1n − β · ∇ G(·) (9)\\nϕm+1n ← ϕmn − β · ∇ G(·) (10)\\nwhere the β is the step size. Here the updated parameter\\nθm+1n is involved in the calculation of training loss. It also\\n5\\nmeans that we need the second derivative with respect to θ,\\nwhile minimizing the loss function 8.\\nAfter finishing the classification task on seen domains,\\nwe evaluate the performance on cross domains Sc to boost\\nthe objective classification model. This process simulates\\nthe virtual train/test settings. The evaluation is performed\\non the updated parameters θ and ϕ (as illustrated in Figure\\n3 step 3©). More concretely, this evaluation come up with\\nthe cross domain validation loss:\\nLval(cϕm+1n (fθm+2n (xj)), yj) (11)\\nThe loss function of cross domain validation is as fol-\\nlows:\\nH(·) =\\n∑\\nDi∈Sc\\n∑\\nxj∈Di\\nLval(cϕm+1n (fθm+2n (xj)), yj) (12)\\nFinally, as illustrated in Figure 3 step 2©, 4© and 5©, we\\nupdate our classification model by adding the training loss\\nLtrain and cross domain validation loss Lval at the end of\\neach iteration:\\nθmn+1 ← θm+1n − γ · ∇H(·) (13)\\nϕmn+1 ← ϕmn − γ · ∇H(·) (14)\\nwhere γ presents the step size of cross domain valida-\\ntion. Note that the parameter updating on seen domains\\nclassification is performed over the parameter θm+1n and\\nϕmn , whereas the cross domain validation is evaluated over\\nparameter θm+2n and ϕ\\nm+1\\nn . In other words, the optimiza-\\ntion of our classification model is involved in third deriva-\\ntive with respect to θ and second derivative with respect to\\nϕ.\\n3.3 Summary of DADG\\nAs illustrated in Figure 3, the DAL and Meta-CDV\\noptimize the model by addressing different aspects of\\ndomain generalization, and work synergistically within\\none iteration. In each iteration, we randomly split the\\ntrain/validation (Sd/Sc) domains. DAL learns a domain-\\ninvariant feature extractor (fθ) under discriminator’s (dψ)\\nassistance by maximizing the discriminative loss. Then, our\\napproach learns a robust classification model via updating\\non (θ, ϕ) by adopting a simple classification training and\\ncross domain validation, which optimized in meta-learning\\nbased manner. For the whole process, our objective function\\ncan be introduced as:\\nargmin\\nψmn\\nmax\\nθmn\\nF (·) + argmin\\nθm+1n , ϕmn\\n(G(·) +H(·)) (15)\\nOnce Equation 15 is optimized to converge on the source\\ndomains, we test the classification model on the unseen tar-\\nget domains.\\nFeature \\nextractor \\n( )\\nGRL Discriminator (𝑑\\n𝜓\\n𝑚) ℒdisc\\nFeature \\nextractor \\n( )\\nClassifier (𝑐𝜑𝑚) ℒtrain\\nFeature \\nextractor \\n( )\\nℒvalClassifier (𝑐𝜑𝑚+1)\\n∇(ℒtrain + ℒval)\\n1\\n3 3\\n2\\n4\\n5\\nFigure 3. The training flow of DADG.\\n4 Experimental Evaluation\\nTo evaluate our we conduct experiments on 3 benchmark\\ndatasets (PACS [17], VLCS [32] and Office-Home [35]) and\\n2 deep neural network architectures with pretrained parame-\\nters (AlexNet [16] and ResNet-18 [15]) to evaluate the gen-\\neralization capability of our proposed approach. A com-\\nprehensive comparison has been made among our approach\\nand other baseline approaches. The presented results are\\nshown that our DADG performs consistently comparable in\\nall the evaluations, and achieves the state-of-the-art results\\nin two datasets. The effectiveness of each component in our\\napproach also discussed. All the details are described in\\nfollowing.\\n4.1 Baseline Approaches\\nWe compare our proposed approach performance with\\nfollowing baseline DG approaches.\\n• DeepAll is the baseline that simply use deep learning\\nnetwork to train the aggregation of all source domains\\nand test the unseen domain. It is a strong baseline that\\nsurpasses many previous DG works [17].\\n• TF [17] introduces a low-rank parameter network to\\ndecrease the size of parameters. This work also shows\\nthat the DeepAll can surpass many previous studies\\nand first provides PACS dataset.\\n• Hex [36] attempts to reduce the sensitivity of a model\\non high frequency texture information, and thus to in-\\ncrease model domain-robustness.\\n6\\n• MMD-AAE [19] is based on adversarial autoencoder.\\nIt aligns different domain distributions to an arbitrary\\nprior via MMD regularization, to learn an invariant\\nfeature representation.\\n• Feature-Critic(FC) [21] aims to train a robust feature\\nextractor. It uses meta-learning approach, along with\\nan auxiliary loss to measure whether the updated pa-\\nrameter has improved the performance on the valida-\\ntion set.\\n• MLDG [18] is the first work that addresses domain\\ngeneralization using meta-learning. It is inspired by\\nMAML [10] and proposed visual cross domain clas-\\nsification task by splitting source domains into meta-\\ntrain and meta-test.\\n• D-SAM [7] plugs parallel domain-specific aggregation\\nmodules on a given network architecture to neglect do-\\nmain specific information.\\n• JiGen [3] is the first work that addresses DG by self-\\nsupervised learning. It divides each image into small\\npatches and shuffle the order. Then, trains an ob-\\nject classifier and a jigsaw order classifier simultane-\\nously. It achieves the state-of-the-art results on the\\nthree datasets VLCS [32], PACS [17] and Office-Home\\n[35].\\n4.2 Experiment Datasets\\nWe utilize three well-known domain generalization\\nbenchmark datasets.\\n• VLCS [32] is composed of 10,729 images with resolu-\\ntion 227 × 227, taken from 4 different datasets (i.e.,\\ndomains): PASCAL VOC2007 [8], LabelMe [28],\\nCaltech101 [9] and Sun09 [4]. It depicts 5 categories\\n(i.e., classes): bird, car, chair, dog and person.\\n• PACS [17] contains more severe domain shifts than\\nVLCS. PACS aggregates 9,991 images in 7 different\\nclasses: dog, elephant, giraffe, guitar, house, horse and\\nperson. It shared by 4 different domains: Photo, Art,\\nCartoon and Sketch.\\n• Office-Home [35] was created to evaluate DA and\\nDG algorithms for object recognition in deep learn-\\ning. There are 15,592 images from 4 different do-\\nmains: Art, Clipart, Product and real-world images,\\neach domain includes 65 classes.\\n4.3 Experimental Setting\\nAs described above, all three benchmark datasets con-\\ntain the data of four different domains. We first hold one\\nVLCS VOC LabelMe Caltech Sun Avg.\\nAlexNet\\nTF [17] 69.99 63.49 93.63 61.32 72.11\\nHEX∗ [36] 68.51 63.67 89.63 62.12 70.98\\nMMD-AAE [19] 67.70 62.60 94.40 64.40 72.28\\nFC∗ [21] 66.79 61.48 95.68 63.13 71.77\\nMLDG∗ [18] 70.01 61.06 95.68 65.08 72.96\\nD-SAM [7] 63.75 54.81 94.96 64.56 69.52\\nJiGen [3] 70.62 60.90 96.93 64.30 73.19\\nDeepAll 68.11 61.30 94.44 63.58 71.86\\nDADG 70.77 63.44 96.80 66.81 74.46\\nResNet-18\\nMLDG∗ [18] 74.41 63.45 96.75 69.35 75.99\\nD-SAM∗ [7] 70.42 58.70 88.90 71.36 72.35\\nJiGen∗ [3] 74.91 63.00 98.39 69.37 76.42\\nDeepAll 73.84 62.17 97.10 67.28 75.10\\nDADG 76.17 67.22 98.50 70.95 78.21\\nTable 1. Cross domain classification accuracy (in %) on\\nVLCS dataset when using network architecture AlexNet\\nand ResNet-18. The results of our implementation were the\\naverage over 20 repetitions. Each column name indicates\\nthe target domain. Best performance in bold.\\ndomain (i.e., the target domain) for testing and the rest three\\nfor training. Then, in the training phase, we randomly se-\\nlect two domains to apply discriminative adversarial learn-\\ning (DAL), and select one domain to boost our classifier by\\nmeta-learning based cross domain validation (Meta-CDV).\\nOur discriminator consists of two fully connected layer with\\n1024 neurons and one output layer with 1 neuron. The batch\\nsize of each step were 64 for DAL and 32 for Meta-CDV.\\nWe updated the deep neural networks with stochastic gra-\\ndient descent(SGD) in 2000 iterations with the following\\nhyperparameters: β = 5 × 10−4, γ = 5 × 10−4, momen-\\ntum = 0.9, weight decay = 5× 10−5 and α = 5× 10−5. We\\nfollowed the setting in DANN [12] to assign λ = 1 in all\\nthe cases of the gradient reversal layer. The model-agnostic\\ncan be achieved by simply changing the backbone network\\narchitectures without additional implementation. All of our\\nexperiments are implemented using PyTorch, on a server\\nwith GTX 1080Ti 11 GB GPU.\\n4.4 Effectiveness Analysis\\nIn this section, we discuss the performance of our pro-\\nposed approach and the baseline approaches in terms of\\nclassification accuracy. Table 4.3 - Table 4.4 show the re-\\nsults of datasets VLCS [32], PACS [17] and Office-Home\\n[35]. To make a more comprehensive comparison, we im-\\nplement MLDG [18] our own, because only demo code is\\nprovided by the author. Besides, we implement Hex [36],\\n7\\nPACS Photo Art-paint Cartoon Sketch Avg.\\nAlexNet\\nTF [17] 89.50 62.86 66.97 57.51 69.21\\nHEX [36] 87.90 66.80 69.70 56.30 70.18\\nFC [21] 90.10 64.40 68.60 58.40 70.38\\nMLDG[18] 88.00 66.23 66.88 58.96 70.02\\nD-SAM [7] 85.55 63.87 70.70 64.66 71.20\\nJiGen [3] 89.00 67.63 71.71 65.18 73.38\\nDeepAll 88.65 63.12 66.16 60.27 69.55\\nDADG 89.76 66.21 70.28 62.18 72.11\\nResNet-18\\nMLDG∗ [18] 94.03 76.42 73.03 68.15 77.91\\nD-SAM [7] 95.30 77.33 72.43 77.83 80.72\\nJiGen [3] 96.03 79.42 75.25 71.35 80.51\\nDeepAll 93.06 75.60 72.30 68.10 77.27\\nDADG 94.86 79.89 76.25 70.51 80.38\\nTable 2. Cross domain classification accuracy (in %) on\\nPACS dataset when using network architecture AlexNet and\\nResNet-18. The results of our implementation were the av-\\nerage over 20 repetitions. Each column name indicates the\\ntarget domain. Best performance in bold.\\nFeature-Critic [21], D-SAM [7] and JiGen [3] by using the\\ncode that are provided by the authors. All the implemen-\\ntations are evaluated on the datasets or network architec-\\ntures they did not report. Our results of these approaches\\nare highlighted in the three tables with *. The details of\\neach dataset are presented below:\\nVLCS: We follow the standard protocol of [13] to ran-\\ndomly divide the data of each source domain into training\\n(70%) and testing (30%) sets. Finally we test on all the im-\\nages in target domain. The upper and bottom part of Table\\n4.3 show the results when using different network archi-\\ntectures AlexNet and ResNet-18, respectively. From table\\n4.3, we can observe that (i) The baseline DeepAll performs\\ncompetitively and surpasses many previous DG works on\\noverall performance, such as HEX, Feature-Critic and D-\\nSAM. But our approach outperforms DeepAll in all target\\ndomain cases and on different network architectures. (ii) On\\nAlexNet, our DADG performs better than DeepAll by 2.6%\\nand better than Jigen [3] by 1.27%, such that we achieve the\\nnew state-of-the-art result on VLCS dataset. More specifi-\\ncally, DADG provides the best results in two (i.e., VOC and\\nSUN respectively) out of four target cases. (iii) On ResNet-\\n18, DADG surpasses the previous SOTA result Jigen [3] in\\naverage performance and performs the best in three out of\\nfour target domain cases.\\nPACS: We follow the training protocol of [17], consider-\\ning three domains as source domains and the remaining one\\nas target. The evaluation results are shown in Table 4.3, we\\ncan see that: (i) On AlexNet, although we do not achieve the\\nOffice-Home Art Clipart Product Real-World Avg.\\nResNet-18\\nMLDG∗ [18] 52.88 45.72 69.90 72.68 60.30\\nD-SAM [7] 58.03 44.37 69.22 71.45 60.77\\nJiGen [3] 53.04 47.51 71.47 72.79 61.20\\nDeepAll 54.31 41.41 70.31 73.03 59.77\\nDADG 55.57 48.71 70.90 73.70 62.22\\nTable 3. Cross domain classification accuracy (in %) on\\nOffice-Home dataset when using ResNet-18. The results of\\nour implementation were the average over 20 repetitions.\\nEach column name indicates the target domain. Best per-\\nformance in bold.\\nbest performance on any target domain cases, our DADG\\nprovides consistently comparable results, and performs the\\n2nd best in average results. (ii) On ResNet-18, we have two\\nbest results on Art-paint (79.89%) and Cartoon (76.25%),\\nand only slight worse (0.34%) than the best JiGen [3] in\\naverage performance.\\nOffice-Home: We follow the protocol of [7], also con-\\nsidering three as source domains and the rest one as target.\\nThe results are shown in Table 4.4, and we can observe that:\\n(i) The advantage of of D-SAM [7] in average results orig-\\ninates from its results on Art and Clipart, but the rest two\\nwere lower than DeepAll. (ii) Our DADG achieves the best\\nin two target cases and the best in average results, and im-\\nproves the previous SOTA result Jigen [3] by 1.02%.\\nSummary of the Experimental Evaluation: From the ex-\\nperimental evaluation analyzed above, we conclude that:\\n(i) DeepAll exceeds many previous approaches in different\\ndatasets. In general, only MLDG, JiGen and our DADG\\ncan outperform DeepAll in all three datasets. (ii) As we\\nmentioned in Section 2.3. The approaches that aim to ne-\\nglect particular domain-specific information, may assist the\\nmodel in some datasets but fail in others. For instance, HEX\\n[36] and D-SAM [7] are better than DeepAll on PACS, but\\nworse than DeepAll on VLCS. (iii) our DADG has consis-\\ntently comparable results in all the datasets and achieves the\\nSOTA results on VLCS [32] and Office-Home [35], also the\\nsecond best on PACS [17]. On VLCS [32] and Office-Home\\n[35], DADG outperforms the previous SOTA JiGen [3] all\\nover 1%.\\n4.5 Impact of Different DADG Components\\nIn this section, we conduct an extended study using\\nPACS dataset with network architecture AlexNet [16] to in-\\nvestigate the impact of the two key components (i.e., DAL\\nand Meta-CDV) in our proposed approach DADG. Specifi-\\ncally, we test the performance in terms of classification ac-\\ncuracy by excluding each component in our approach re-\\n8\\nPACS Photo Art-paint Cartoon Sketch Avg.\\nAlexNet\\nDeepAll 88.65 63.12 66.16 60.27 69.55\\nDADG-DAL 89.51 65.43 69.19 61.70 71.46\\nDADG-CDV 89.10 64.22 68.24 60.60 70.54\\nDADG 89.76 66.21 70.28 62.18 72.11\\nTable 4. Cross domain classification accuracy (in %) on\\nPACS dataset using AlexNet. The results of our imple-\\nmentation were the average over 20 repetitions. Each col-\\numn name indicates the target domain. Best performance in\\nbold.\\nspectively. DADG-DAL only contained the discriminative\\nadversarial learning (DAL) component and trained the clas-\\nsification model conventionally instead of in meta-learning\\nmanner. While DADG-CDV meant that we removed the\\nDAL component and only updated the classification model\\nparameters in meta-learning manner.\\nFrom the results in Table 4.5, we can see that DADG-\\nDAL and DADG-CDV consistently perform better than\\nDeepAll, and our full version DADG surpasses both base-\\nline models in average performance and in every target do-\\nmain cases. The results in Table 4.5 shown that: (i) Employ-\\ning discriminative adversarial learning is able to effectively\\nguide the feature extractor to learn the invariant features\\namong multiple source domains. (ii) Since the only dif-\\nference between DeepAll and DADG-CDV is the updating\\nmanner. Thus applying meta-learning based cross domain\\nvalidation can make the classification model more robust.\\n(iii) The full version DADG consistently performs the best\\nin every single case, which has shown that combining do-\\nmain invariant representation and robust classifier together\\nhelped the model to enhance generalization.\\n5 Conclusion\\nIn this paper, we proposed DADG, a novel domain gen-\\neralization approach, that contains two main components,\\ndiscriminative adversarial learning and meta-learning based\\ncross domain validation. The discriminative adversarial\\nlearning component learns a domain-invariant feature ex-\\ntractor, while the meta-learning based cross domain vali-\\ndation component trains a robust classifier for the objec-\\ntive task (i.e., classification task). Extensive experiments\\nhave been conducted to show that our feature extractor and\\nclassifier could achieve good generalization performance\\non three domain generalization benchmark datasets. Ex-\\nperiments indicate that the feature extractor and classi-\\nfier achieve good generalization on three benchmark do-\\nmain generalization datasets. The experiment results also\\nshow that our approach consistently beat the strong base-\\nline DeepAll. For instance, while using PACS dataset, our\\napproach performs better than DeepAll by 1.56% (AlexNet)\\nand 3.11% (ResNet-18). Notably, we also reach the state-\\nof-the-art performance on VLCS and Office-Home datasets,\\nand improve the average accuracy by over 1% in each case.\\nIn the current stage, we assume the domains all share the\\nsame label space. However, the enhancement of model gen-\\neralization capability should not be limited by this assump-\\ntion. In our future work, we plan to design approaches that\\ncan further improve the model generalization to tackle the\\n“unseen” labels in the unseen domains.\\nAcknowledgments\\nEffort sponsored in part by United States Special Op-\\nerations Command (USSOCOM), under Partnership In-\\ntermediary Agreement No. H92222-15-3-0001-01. The\\nU.S. Government is authorized to reproduce and distribute\\nreprints for Government purposes notwithstanding any\\ncopyright notation thereon. 1\\nReferences\\n[1] M. Andrychowicz, M. Denil, S. Gomez, M. W.\\nHoffman, D. Pfau, T. Schaul, B. Shillingford, and\\nN. De Freitas. Learning to learn by gradient descent\\nby gradient descent. In Advances in neural informa-\\ntion processing systems, pages 3981–3989, 2016.\\n[2] Y. Balaji, S. Sankaranarayanan, and R. Chellappa.\\nMetareg: Towards domain generalization using meta-\\nregularization. In Advances in Neural Information\\nProcessing Systems, pages 998–1008, 2018.\\n[3] F. M. Carlucci, A. D’Innocente, S. Bucci, B. Caputo,\\nand T. Tommasi. Domain generalization by solving\\njigsaw puzzles. In Proceedings of the IEEE Con-\\nference on Computer Vision and Pattern Recognition,\\npages 2229–2238, 2019.\\n[4] M. J. Choi, J. J. Lim, A. Torralba, and A. S. Willsky.\\nExploiting hierarchical context on a large database of\\nobject categories. In 2010 IEEE Computer Society\\nConference on Computer Vision and Pattern Recog-\\nnition, pages 129–136. IEEE, 2010.\\n[5] N. N. Di Zhuang, K. Chen, and J. M. Chang. Saia:\\nSplit artificial intelligence architecture for mobile\\nhealthcare systems. arXiv preprint arXiv:2004.12059,\\n2020.\\n1The views and conclusions contained herein are those of the authors\\nand should not be interpreted as necessarily representing the official poli-\\ncies or endorsements, either expressed or implied, of the United States\\nSpecial Operations Command.\\n9\\n[6] C. Ding and D. Tao. Trunk-branch ensemble convo-\\nlutional neural networks for video-based face recogni-\\ntion. IEEE transactions on pattern analysis and ma-\\nchine intelligence, 40(4):1002–1014, 2017.\\n[7] A. D’Innocente and B. Caputo. Domain generalization\\nwith domain-specific aggregation modules. In Ger-\\nman Conference on Pattern Recognition, pages 187–\\n198. Springer, 2018.\\n[8] M. Everingham, L. Van Gool, C. K. Williams, J. Winn,\\nand A. Zisserman. The pascal visual object classes\\n(voc) challenge. International journal of computer vi-\\nsion, 88(2):303–338, 2010.\\n[9] L. Fei-Fei, R. Fergus, and P. Perona. Learning genera-\\ntive visual models from few training examples: An in-\\ncremental bayesian approach tested on 101 object cat-\\negories. In 2004 conference on computer vision and\\npattern recognition workshop, pages 178–178. IEEE,\\n2004.\\n[10] C. Finn, P. Abbeel, and S. Levine. Model-agnostic\\nmeta-learning for fast adaptation of deep networks.\\nIn Proceedings of the 34th International Conference\\non Machine Learning-Volume 70, pages 1126–1135.\\nJMLR. org, 2017.\\n[11] Y. Ganin and V. Lempitsky. Unsupervised domain\\nadaptation by backpropagation. In International\\nconference on machine learning, pages 1180–1189.\\nPMLR, 2015.\\n[12] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain,\\nH. Larochelle, F. Laviolette, M. Marchand, and\\nV. Lempitsky. Domain-adversarial training of neural\\nnetworks. In Domain Adaptation in Computer Vision\\nApplications, pages 189–209. Springer, 2017.\\n[13] M. Ghifary, W. Bastiaan Kleijn, M. Zhang, and\\nD. Balduzzi. Domain generalization for object recog-\\nnition with multi-task autoencoders. In Proceedings\\nof the IEEE international conference on computer vi-\\nsion, pages 2551–2559, 2015.\\n[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Ben-\\ngio. Generative adversarial nets. In Advances in neu-\\nral information processing systems, pages 2672–2680,\\n2014.\\n[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep resid-\\nual learning for image recognition. In Proceedings of\\nthe IEEE conference on computer vision and pattern\\nrecognition, pages 770–778, 2016.\\n[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Im-\\nagenet classification with deep convolutional neural\\nnetworks. In Advances in neural information process-\\ning systems, pages 1097–1105, 2012.\\n[17] D. Li, Y. Yang, Y.-Z. Song, and T. M. Hospedales.\\nDeeper, broader and artier domain generalization. In\\nProceedings of the IEEE International Conference on\\nComputer Vision, pages 5542–5550, 2017.\\n[18] D. Li, Y. Yang, Y.-Z. Song, and T. M. Hospedales.\\nLearning to generalize: Meta-learning for domain\\ngeneralization. In Thirty-Second AAAI Conference on\\nArtificial Intelligence, 2018.\\n[19] H. Li, S. Jialin Pan, S. Wang, and A. C. Kot. Do-\\nmain generalization with adversarial feature learning.\\nIn Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, pages 5400–5409,\\n2018.\\n[20] K. Li and J. Malik. Learning to optimize neural nets.\\narXiv preprint arXiv:1703.00441, 2017.\\n[21] Y. Li, Y. Yang, W. Zhou, and T. Hospedales. Feature-\\ncritic networks for heterogeneous domain generalisa-\\ntion. In The Thirty-sixth International Conference on\\nMachine Learning, 2019.\\n[22] L. Mai and D. K. Noh. Cluster ensemble with link-\\nbased approach for botnet detection. Journal of Net-\\nwork and Systems Management, 26(3):616–639, 2018.\\n[23] S. Motiian, M. Piccirilli, D. A. Adjeroh, and\\nG. Doretto. Unified deep supervised domain adap-\\ntation and generalization. In The IEEE International\\nConference on Computer Vision (ICCV), Oct 2017.\\n[24] H. Nguyen, D. Zhuang, P.-Y. Wu, and M. Chang.\\nAutogan-based dimension reduction for privacy\\npreservation. Neurocomputing, 2019.\\n[25] A. Nichol and J. Schulman. Reptile: a scal-\\nable metalearning algorithm. arXiv preprint\\narXiv:1803.02999, 2, 2018.\\n[26] F. Perez, S. Avila, and E. Valle. Solo or ensem-\\nble? choosing a cnn architecture for melanoma clas-\\nsification. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition Workshops,\\npages 0–0, 2019.\\n[27] A. Rajeswaran, C. Finn, S. M. Kakade, and S. Levine.\\nMeta-learning with implicit gradients. In Advances in\\nNeural Information Processing Systems, pages 113–\\n124, 2019.\\n10\\n[28] B. C. Russell, A. Torralba, K. P. Murphy, and W. T.\\nFreeman. Labelme: a database and web-based tool for\\nimage annotation. International journal of computer\\nvision, 77(1-3):157–173, 2008.\\n[29] A. Tagarelli, A. Amelio, and F. Gullo. Ensemble-\\nbased community detection in multilayer networks.\\nData Mining and Knowledge Discovery, 31(5):1506–\\n1543, 2017.\\n[30] D. Tao, L. Jin, Y. Yuan, and Y. Xue. Ensemble mani-\\nfold rank preserving for acceleration-based human ac-\\ntivity recognition. IEEE transactions on neural net-\\nworks and learning systems, 27(6):1392–1404, 2014.\\n[31] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba,\\nand P. Abbeel. Domain randomization for transfer-\\nring deep neural networks from simulation to the real\\nworld. In 2017 IEEE/RSJ International Conference on\\nIntelligent Robots and Systems (IROS), pages 23–30.\\nIEEE, 2017.\\n[32] A. Torralba, A. A. Efros, et al. Unbiased look at\\ndataset bias. In CVPR, volume 1, page 7. Citeseer,\\n2011.\\n[33] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Si-\\nmultaneous deep transfer across domains and tasks. In\\nProceedings of the IEEE International Conference on\\nComputer Vision, pages 4068–4076, 2015.\\n[34] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Ad-\\nversarial discriminative domain adaptation. In Pro-\\nceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 7167–7176, 2017.\\n[35] H. Venkateswara, J. Eusebio, S. Chakraborty, and\\nS. Panchanathan. Deep hashing network for unsuper-\\nvised domain adaptation. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recogni-\\ntion, pages 5018–5027, 2017.\\n[36] H. Wang, Z. He, Z. L. Lipton, and E. P. Xing. Learning\\nrobust representations by projecting superficial statis-\\ntics out. In International Conference on Learning Rep-\\nresentations, 2019.\\n[37] P.-Y. Wu, C.-C. Fang, J. M. Chang, and S.-Y. Kung.\\nCost-effective kernel ridge regression implementa-\\ntion for keystroke-based active authentication system.\\nIEEE transactions on cybernetics, 47(11):3916–3927,\\n2016.\\n[38] D. Zhuang and J. M. Chang. Peerhunter: Detect-\\ning peer-to-peer botnets through community behavior\\nanalysis. In 2017 IEEE Conference on Dependable\\nand Secure Computing, pages 493–500. IEEE, 2017.\\n[39] D. Zhuang and J. M. Chang. Enhanced peerhunter:\\nDetecting peer-to-peer botnets through network-\\nflow level community behavior analysis. IEEE\\nTransactions on Information Forensics and Security,\\n14(6):1485–1500, 2018.\\n[40] D. Zhuang and J. M. Chang. Utility-aware\\nprivacy-preserving data releasing. arXiv preprint\\narXiv:2005.04369, 2020.\\n[41] D. Zhuang, M. J. Chang, and M. Li. Dynamo: Dy-\\nnamic community detection by incrementally maxi-\\nmizing modularity. IEEE Transactions on Knowledge\\nand Data Engineering, 2019.\\n[42] D. Zhuang, S. Wang, and J. M. Chang. Fripal: Face\\nrecognition in privacy abstraction layer. In 2017 IEEE\\nConference on Dependable and Secure Computing,\\npages 441–448. IEEE, 2017.\\n11\\n'}\n",
      "{'_id': ObjectId('6815e61add259dc7e6e1cd35'), 'authors': 'Ermon, Stefano, Gao, Ruiqi, Ho, Jonathan, Kingma, Diederik P., Meng, Chenlin, Salimans, Tim', 'year': '2022', 'title': 'On Distillation of Guided Diffusion Models', 'full_text': 'ON DISTILLATION OF GUIDED DIFFUSION MODELSChenlin Meng∗Stanford Universitychenlin@cs.stanford.eduRuiqi GaoGoogle Research, Brain Teamruiqig@google.comDiederik P. KingmaGoogle Research, Brain Teamdurk@google.comStefano ErmonStanford Universityermon@cs.stanford.eduJonathan HoGoogle Research, Brain Teamjonathanho@google.comTim SalimansGoogle Research, Brain Teamsalimans@google.comABSTRACTClassifier-free guided diffusion models have recently been shown to be highlyeffective at high-resolution image generation, and they have been widely usedin large-scale diffusion frameworks including DALL·E 2, GLIDE and Imagen.However, a downside of classifier-free guided diffusion models is that they arecomputationally expensive at inference time since they require evaluating twodiffusion models, a class-conditional model and an unconditional model, hundredsof times. To deal with this limitation, we propose an approach to distilling classifier-free guided diffusion models into models that are fast to sample from: Given apre-trained classifier-free guided model, we first learn a single model to matchthe output of the combined conditional and unconditional models, and then weprogressively distill that model to a diffusion model that requires much fewersampling steps. On ImageNet 64x64 and CIFAR-10, our approach is able togenerate images visually comparable to that of the original model using as few as4 sampling steps, achieving FID/IS scores comparable to that of the original modelwhile being up to 256 times faster to sample from.1 INTRODUCTIONDenoising diffusion probabilistic models (DDPMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020;Song & Ermon, 2019; Song et al., 2021b) have achieved state-of-the-art performance on imagegeneration (Nichol & Dhariwal, 2021; Rombach et al., 2022; Ramesh et al., 2021; 2022; Sahariaet al., 2022), audio synthesis (Kong et al., 2021), molecular generation (Xu et al., 2022), andlikelihood estimation (Kingma et al., 2021). Classifier-free guidance (Ho & Salimans, 2022) furtherimproves the sample quality of diffusion models and has been widely used in large-scale diffusionmodel frameworks including GLIDE (Nichol et al., 2021), DALL·E 2 (Ramesh et al., 2022), andImagen (Saharia et al., 2022). However, one key limitation of classifier-free guidance is its lowsampling efficiency—it requires evaluating two diffusion models hundreds of times to generate onesample. This limitation has hindered the application of classifier-free guidance models in real-worldsettings. Although distillation approaches have been proposed for diffusion models (Salimans &Ho, 2022; Song et al., 2021a), these approaches are currently not applicable to classifier-free guideddiffusion models. To deal with this issue, we propose a two-step distillation approach to improve thesampling efficiency of classifier-free guided models. In the first step, we introduce a single studentmodel to match the combined output of the two diffusion models of the teacher. In the second step,we progressively distill the model learned from the first step to a fewer-step model use the approachintroduced in (Salimans & Ho, 2022). Using our approach, a single distilled model is able to handle awide range of different guidance strengths, allowing for the trade-off between sample quality and∗Work done during an internship at Google1arXiv:2210.03142v1  [cs.CV]  6 Oct 2022diversity efficiently. To sample from our model, we consider existing deterministic sampler in theliterature (Song et al., 2021a; Salimans & Ho, 2022) and further propose a stochastic samplingprocess. Our experiments on ImageNet 64x64 and CIFAR-10 show that the proposed distilled modelcan generate samples visually comparable to that of the teacher using only 4 steps and is able toachieve comparable FID/IS scores as the teacher model using as few as 8 to 16 steps on a wide rangeof guidance strengths (see Fig. 1). Additional experiments on ImageNet 64x64 also demonstrate thepotential of the proposed framework in style-transfer applications (Su et al., 2022).! = 0Deterministic8-step1-step! = 1 ! = 2 ! = 4Figure 1: Class-conditional samples from our two-step (deterministic) approach on ImageNet 64x64.By varying the guidance weight w, our distilled model is able to trade-off between sample diversityand quality, while achieving visually pleasant results using as few as one sampling step.2 BACKGROUND ON DIFFUSION MODELSGiven samples x from a data distribution pdata(x), noise scheduling functions αt and σt, we train adiffusion model x̂θ, with parameter θ, via minimizing the weighted mean squared errorθ∗ = argminθEt∼U [0,1],x∼pdata(x),zt∼q(zt|x)[ω(λt)||x̂θ(zt)− x||22], (1)where λt = log[α2t /σ2t ] is a signal-to-noise ratio (Kingma et al., 2021), q(zt|x) = N (zt;αtx, σ2t I)and ω(λt) is a pre-specified weighting function (Kingma et al., 2021).Once the diffusion model x̂θ is trained, one can use discrete-time DDIM sampler (Song et al., 2021a)to sample from the model. Specifically, the DDIM sampler starts with z1 ∼ N (0, I) and updates asfollowszs = αsx̂θ(zt) + σszt − αtx̂θ(zt)σt, s = t− 1/N (2)with N the total number of sampling steps. The final sample will then be generated using x̂θ(z0).Classifier-free guidance Classifier-free guidance (Ho & Salimans, 2022) is an effective approachshown to significantly improve the sample quality of class-conditioned diffusion models, and hasbeen widely used in large-scale diffusion models including GLIDE (Nichol et al., 2021), DALL·E2 (Ramesh et al., 2022) and Imagen (Saharia et al., 2022). Specifically, it introduces a guidanceweight parameter w ∈ R≥0 to trade-off between sample quality and diversity. To generate a sample,classifier-free guidance evaluates both a conditional diffusion model x̂c,θ and a jointly trainedunconditional diffusion model x̂θ at each update step, using x̂wθ = (1 + w)x̂c,θ − wx̂θ as the modelprediction in Eq. (2). As each sampling update requires evaluating two diffusion models, samplingwith classifier-free guidance is often expensive (Ho & Salimans, 2022).Progressive distillation Our approach is based on progressive distillation (Salimans & Ho, 2022),an effective method for improving the sampling speed of diffusion models by repeated distillation.Until now, this method could not be directly applied to distillation of guided models or to samplersother than the deterministic DDIM sampler (Song et al., 2021a). In this paper we resolve theseshortcomings.23 DISTILLING A CLASSIFIER-FREE GUIDED DIFFUSION MODELIn the following, we discuss our approach for distilling a classifier-free guided diffusion model (Ho &Salimans, 2022). Given a trained guided model [x̂c,θ, x̂θ] (teacher), our approach can be decomposedinto two steps. Step one In the first step, we introduce a continuous-time student model x̂η1(zt, w),with learnable parameter η1, to match the output of the teacher at any time-step t ∈ [0, 1]. Given arange of guidance strengths [wmin, wmax] we are interested in, we optimize the student model usingthe following objectiveη∗1 = argminη1Ew∼U [wmin,wmax],t∼U [0,1],x∼pdata(x),zt∼q(zt|x)[ω(λt)‖x̂η1(zt, w)− x̂wθ (zt)‖22], (3)where x̂wθ (zt) = (1 + w)x̂c,θ(zt)− wx̂θ(zt). To incorporate the guidance weight w, we introducea w-conditioned model, where w is fed as an input to the student model. To better capture the feature,we apply Fourier embedding to w, which is then incorporated into the diffusion model backbonein a way similar to how the time-step was incorporated in Kingma et al. (2021); Salimans & Ho(2022). As initialization plays a key role in the performance, we initialize the student model with thesame parameters as the conditional model of the teacher, except for the newly introduced parametersrelated to w-conditioning. We provide the detailed algorithm for Step-one training in Appendix C.Step two In the second step, we consider a discrete time-step scenario and progressively distillthe learned model from the first step x̂η1(zt, w) into an fewer-step student model x̂η2(zt, w) withlearnable parameter η2, by halving the number of sampling steps each time. Letting N denote thenumber of sampling steps, given w ∼ U [wmin, wmax] and t ∈ {1, ..., N}, we train the student modelto match the output of two-step DDIM sampling of the teacher (i.e., from t/N to t − 0.5/N andfrom t − 0.5/N to t − 1/N ) in one step, following the approach of Salimans & Ho (2022). Afterdistilling the 2N steps in the teacher model to N steps in the student model, we can use the N -stepstudent model as the new teacher model, repeat the same procedure, and distill the teacher model intoa N/2-step student model. At each step, we initialize the student model with the parameters of theteacher. More details are provided in Appendix C.N -step deterministic and stochastic sampling Once the model x̂η2 is trained, given a specifiedw ∈ [wmin, wmax], we can perform sampling via the DDIM update rule in Eq. (2). We note that giventhe distilled model x̂η2 , this sampling procedure is deterministic given the initialization zw1 .In fact, we can also perform N -step stochastic sampling: We apply one deterministic sampling stepwith two-times the original step-length (i.e., the same as a N/2-step deterministic sampler) andthen perform one stochastic step backward (i.e., perturb with noise) using the original step-length, aprocess inspired by Karras et al. (2022). With zw1 ∼ N (0, I), we use the following update rule whent > 1/Nzwk = αkx̂η2(zwt ) + σkzwt − αtx̂wη2(zt)σt, zws = (αs/αk)zwk + σs|kε, ε ∼ N (0, I) (4)zwh = αhx̂η2(zws ) + σhzws − αsx̂wη2(zws )σs, zwk = (αk/αh)zwh + σk|hε, ε ∼ N (0, I), (5)where h = t− 3/N , k = t− 2/N , s = t− 1/N and σ2a|b = (1− eλa−λb)σ2a. When t = 1/N , weuse deterministic update Eq. (2) to obtain zw0 from zw1/N . We note that compared to the deterministicsampler, performing stochastic sampling requires evaluating the model at slightly different time-steps,and would require small modifications to the training algorithm for the edge cases. We provide moredetails in Appendix C.Other distillation approaches A direct application of progressive distillation (Salimans & Ho,2022) to guided models is to follow the structure of the teacher model and directly distill the studentmodel into one jointly-trained conditional and unconditional model. We explore this option andobserve that this approach does not work well. We provide more details and analysis in Appendix C.8.4 EXPERIMENTIn this section, we evaluate the performance of our distillation approach. We observe that ourapproach is able to achieve competitive FID/IS scores while using as few as 4 steps. We provide extraexperimental details in Appendix C and extra samples in Appendix D.3Distillation for classifier-free guided models We focus on ImageNet 64x64 (Russakovsky et al.,2015) and CIFAR-10 in this experiment. We explore different ranges for the guidance weightand observe that all ranges work comparably and therefore use [wmin, wmax] = [0, 4] for theexperiments. We train the step-one model using SNR loss, and the step-two model using SNRloss with truncation (Salimans & Ho, 2022). The baselines we consider include DDPM ancestralsampling (Ho et al., 2020) and DDIM (Song et al., 2021a). To better understand how the guidanceweight w should be incorporated, we also include models trained using a single fixed w as a baseline.We use the same pre-trained teacher model for all the methods for fair comparisons. Following Hoet al. (2020); Ho & Salimans (2022), we use a U-Net (Ronneberger et al., 2015) architecture for thebaselines, and the same U-Net backbone with the introduced w-embedding for our two-step studentmodels (see Section 3). We report the performance for all approaches on ImageNet 64x64 in Fig. 2and Table 1. We provide the results on CIFAR-10 and extended results on ImageNet 64x64 in Table 2(see Appendix C.6). We provide samples for both datasets in Appendix D.w = 0 w = 0.3 w = 1 w = 4Method FID (↓) IS (↑) FID (↓) IS (↑) FID (↓) IS (↑) FID (↓) IS (↑)Ours 1-step (D/S) 22.74 / 26.91 25.51 / 23.55 14.85 / 18.48 37.09 / 33.30 7.54 / 8.92 75.19 / 67.80 18.72 / 17.85 157.46 / 148.97Ours 4-step (D/S) 4.14 / 3.91 46.64 / 48.92 2.17 / 2.24 69.64 / 73.73 7.95 / 8.51 128.98 / 135.36 26.45 / 27.33 207.45 / 216.56Ours 8-step (D/S) 2.79 / 2.44 50.72 / 55.03 2.05 / 2.31 76.01 / 83.00 9.33 / 10.56 136.47 / 147.39 26.62 / 27.84 203.47 / 219.89Ours 16-step (D/S) 2.44 / 2.10 52.53 / 57.81 2.20 / 2.56 79.47 / 87.50 9.99 / 11.63 139.11 / 153.17 26.53 / 27.69 204.13 / 218.70Single-w 1-step 19.61 24.00 11.70 36.95 6.64 74.41 19.857 170.69Single-w 4-step 4.79 38.77 2.34 62.08 8.23 118.52 27.75 219.64Single-w 8-step 3.39 42.13 2.32 68.76 9.69 125.20 27.67 218.08Single-w 16-step 2.97 43.63 2.56 70.97 10.34 127.70 27.40 216.52DDIM 16x2-step 7.68 37.60 5.33 60.83 9.53 112.75 21.56 195.17DDIM 32x2-step 5.03 40.93 7.47 9.33 9.26 126.22 23.03 213.23DDIM 64x2-step 3.74 43.16 5.52 9.51 9.53 133.17 23.64 217.88Teacher (DDIM 1024x2-step) 2.92 44.81 2.36 74.83 9.84 139.50 23.94 224.74Table 1: ImageNet 64x64 distillation results (w = 0 refers to non-guided models). For our method,D and S stand for deterministic and stochastic sampler respectively. We observe that training themodel conditioned on a guidance interval w ∈ [0, 4] performs comparably with training a model on afixed w (see Single-w). Our approach significantly outperforms DDIM when using fewer steps, andis able to match the teacher performance using as few as 8 to 16 steps.! = 0 ! = 1 ! = 2 ! = 4i64Figure 2: ImageNet 64x64 sample quality evaluated by FID and IS scores. Our distilled modelsignificantly outperform the DDPM and DDIM baselines, and is able to match the performance ofthe teacher using as few as 8 to 16 steps. By varying w, a single distilled model is able to capture thetrade-off between sample diversity and quality.Progressive distillation for encoding In this experiment, we explore distilling the encoding processfor the teacher model and perform experiments on style-transfer in a setting similar to Su et al. (2022).Specifically, to perform style-transfer between two domains A and B, we encode the image fromdomain-A using a diffusion model trained on domain-A, and then decode with a diffusion modeltrained on domain-B. As the encoding process can be understood as reversing the DDIM samplingprocess, we perform distillation for both the encoder and decoder with classifier-free guidance, andcompare with a DDIM encoder and decoder in Fig. 3. We also explore how modifying the guidancestrength w can impact the performance in Fig. 4 and Fig. 12. We provide more details in Appendix C.4Input (orange) Ours (lemon) Ours (dough)DDIM (lemon) DDIM (dough)Figure 3: Style transfer comparison on ImageNet 64x64. For our approach, we use a distilled encoderand decoder. For the baseline, we encode and decode using DDIM. We use w = 0 and 16 samplingsteps for both the encoder and decoder. We observe that our method achieves more realistic outputs.! = 0 ! = 1 ! = 2 ! = 4InputFigure 4: Style transfer on ImageNet 64x64 (orange to bell pepper). We use a distilled 16-stepencoder and decoder. We fix the encoder guidance strength to be 0 and vary the decoder guidancestrength from 0 to 4. As we increase w, we notice a trade-off between sample diversity and sharpness.REFERENCESJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,2022.Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances inNeural Information Processing Systems, pp. 6840–6851, 2020.Alexia Jolicoeur-Martineau, Ke Li, Rémi Piché-Taillefer, Tal Kachman, and Ioannis Mitliagkas.Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080,2021.Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. arXiv preprint arXiv:2206.00364, 2022.Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. arXivpreprint arXiv:2107.00630, 2021.Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. DiffWave: A VersatileDiffusion Model for Audio Synthesis. International Conference on Learning Representations,2021.Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. InternationalConference on Machine Learning, 2021.Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing withtext-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on MachineLearning, pp. 8821–8831. PMLR, 2021.Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.5Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022.Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedicalimage segmentation. In International Conference on Medical image computing and computer-assisted intervention, pp. 234–241. Springer, 2015.Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognitionchallenge. International Journal of Computer Vision, 115(3):211–252, 2015.Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, SeyedKamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al.Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprintarXiv:2205.11487, 2022.Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXivpreprint arXiv:2202.00512, 2022.Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervisedlearning using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, March 2015.Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. InternationalConference on Learning Representations, 2021a.Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.In Advances in Neural Information Processing Systems, pp. 11895–11907, 2019.Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and BenPoole. Score-based generative modeling through stochastic differential equations. InternationalConference on Learning Representations, 2021b.Xuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon. Dual diffusion implicit bridges forimage-to-image translation. arXiv preprint arXiv:2203.08382, 2022.Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometricdiffusion model for molecular conformation generation. arXiv preprint arXiv:2203.02923, 2022.6APPENDIXA RELATED WORKOur approach is related to existing works on improving the sampling speed of diffusion models.For instance, denoising diffusion implicit model (DDIM Song et al. (2021a)), probability flowsampler (Song et al., 2021b), fast SDE integrators (Jolicoeur-Martineau et al., 2021) have beenproposed to improve the sampling speed of diffusion models (Sohl-Dickstein et al., 2015; Ho et al.,2020). Progressive distillation (Salimans & Ho, 2022) is perhaps the most relevant work. Specifically,it proposes to progressively distill a pre-trained diffusion model into a fewer-step student model withthe same model architecture. However, none of the above approaches have been applied to distillingclassifier-free guided models. At the same time, these approaches (Song et al., 2021a;b; Salimans& Ho, 2022) only consider deterministic sampling schemes to improve the sampling speed. In thiswork, we propose an approach to distill classifier-free guided diffusion models and further develop aneffective stochastic sampling approach to sample from the distilled models.B CONCLUSIONIn this paper, we propose a distillation approach for guided diffusion models (Ho & Salimans, 2022)and further propose a stochastic sampler to sample from the distilled model. Empirically, our approachis able to achieve visually decent samples using as few as one step and obtain a comparable FID/ISscore as the teacher using only 8 to 16 steps, reducing the sampling steps by up to 256 times.C EXTRA DETAILS ON EXPERIMENTSC.1 TEACHER MODELThe model architecture we use is a U-Net model similar to the ones used in Ho & Salimans (2022).The model is parameterized to predict v as discussed in Salimans & Ho (2022). We use the sametraining setting as Ho & Salimans (2022).C.2 STEP-ONE DISTILLATIONThe model architecture we use is a U-Net model similar to the ones used in Ho & Salimans (2022).We use the same number of channels and attention as used in Ho & Salimans (2022) for bothImageNet 64x64 and CIFAR-10. As mentioned in Section 3, we also make the model take w as input.Specifically, we apply Fourier embedding to w before combining with the model backbone. The waywe incorporate w is the same as how time-step is incorporated to the model as used in Kingma et al.(2021); Salimans & Ho (2022). We parameterize the model to predict v as discussed in Salimans& Ho (2022). We train the distilled model using Algorithm 1. We train the model using SNRloss Kingma et al. (2021); Salimans & Ho (2022). For ImageNet 64x64, we use learning rate 3e− 4,with EMA decay 0.9999; for CIFAR-10, we use learning rate 1e− 3, with EMA decay 0.9999. Weinitialize the student model with parameters from the teacher model except for the parameters relatedto w-embedding.C.3 STEP-TWO DISTILLATION FOR DETERMINISTIC SAMPLERWe use the same model architectures as the ones used in Step-one (see Appendix C.2). We train thedistilled model using Algorithm 2. We first use the student model from Step-one as the teacher model.We start from 1024 DDIM sampling steps and progressively distill the student model from Step-oneto a one step model. We train the student model for 50,000 parameter updates, except for samplingstep equals to one or two where we train the model for 100,000 parameter updates, before the numberof sampling step is halved and the student model becomes the new teacher model. At each samplingstep, we initialize the student model with the parameters from the teacher model. We train the modelusing SNR truncation loss (Kingma et al., 2021; Salimans & Ho, 2022). For each step, we linearlyanneal the learning rate from 1e− 4 to 0 during each parameter update. We do not use EMA decayfor training. Our training setting follows the setting in Salimans & Ho (2022) closely.7Algorithm 1 Step-one distillationRequire: Trained classifier-free guidance teacher model [x̂c,θ, x̂θ]Require: Data set DRequire: Loss weight function ω()while not converged dox ∼ D . Sample datat ∼ U [0, 1] . Sample timew ∼ U [wmin, wmax] . Sample guidanceε ∼ N(0, I) . Sample noisezt = αtx+ σtε . Add noise to dataλt = log[α2t /σ2t ] . log-SNRx̂wθ (zt) = (1 + w)x̂c,θ(zt)− wx̂θ(zt) . Compute targetLη1 = ω(λt)‖x̂wθ (zt)− x̂η1(zt, w)‖22 . Lossη1 ← η1 − γ∇η1Lη1 . Optimizationend whileAlgorithm 2 Step-two distillation for deterministic samplerRequire: Trained teacher model x̂η(zt, w)Require: Data set DRequire: Loss weight function ω()Require: Student sampling steps Nfor K iterations doη2 ← η . Init student from teacherwhile not converged dox ∼ Dt = i/N, i ∼ Cat[1, 2, . . . , N ]w ∼ U [wmin, wmax] . Sample guidanceε ∼ N(0, I)zt = αtx+ σtε# 2 steps of DDIM with teachert′ = t− 0.5/N , t′′ = t− 1/Nzwt′ = αt′ x̂η(zt, w) +σt′σt(zt − αtx̂η(zt, w))zwt′′ = αt′′ x̂η(zwt′ , w) +σt′′σt′(zwt′ − αt′ x̂η(zwt′ , w))x̃w =zwt′′−(σt′′/σt)ztαt′′−(σt′′/σt)αt. Teacher x̂ targetλt = log[α2t /σ2t ]Lη2 = ω(λt)‖x̃w − x̂η2(zt, w)‖22η2 ← η2 − γ∇η2Lη2end whileη ← η2 . Student becomes next teacherN ← N/2 . Halve number of sampling stepsend forC.4 STEP-TWO DISTILLATION FOR STOCHASTIC SAMPLINGWe train the distilled model using Algorithm 3. We use the same model architecture and trainingsetting as Step-two distillation described in Appendix C.3 for both ImageNet 64x64 and CIFAR-10:The main difference here is that our distillation target corresponds to taking a sampling step that istwice as large as for the deterministic sampler. We provide visualization for samples with varyingguidance strengths w in Fig. 5.C.5 BASELINE SAMPLESWe provide extra samples for the DDIM baseline in Fig. 6 and Fig. 7.8! = 0stochastic8-step1-step! = 1 ! = 2 ! = 4Figure 5: Class-conditional samples from our two-step (stochastic) approach on ImageNet 64x64. Byvarying the guidance weight w, our distilled model is able to trade-off between sample diversity andquality, while achieving visually pleasant results using as few as one sampling step.Ddim 8 steps (baseline comparison)! = 0 ! = 1 ! = 2 ! = 4Figure 6: ImageNet 64x64 class-conditional generation using DDIM (baseline) 8 sampling steps. Weobserve clear artifacts when w = 0.Ddim 16 steps (baseline comparison)! = 0 ! = 1 ! = 2 ! = 4Figure 7: ImageNet 64x64 class-conditional generation using DDIM (baseline) 16 sampling steps.C.6 EXTRA DISTILLATION RESULTSWe provide the FID and IS results for our method and the baselines on ImageNet 64x64 and CIFAR-10 in Fig. 8, Fig. 10 and Table 2. We also visualize the FID and IS trade-off curves for both datasetsin Fig. 9 and Fig. 11, where we select guidance strength w = {0, 0.3, 1, 2, 4} for ImageNet 64x64and w = {0, 0.1, 0.2, 0.3, 0.5, 0.7, 1, 2, 4} for CIFAR-10.9Algorithm 3 Step-two distillation for stochastic samplerRequire: Trained teacher model x̂η(zt, w)Require: Data set DRequire: Loss weight function ω()Require: Student sampling steps Nfor K iterations doη2 ← η . Init student from teacherwhile not converged dox ∼ Dt = i/N, i ∼ Cat[1, 2, . . . , N ]w ∼ U [wmin, wmax] . Sample guidanceε ∼ N(0, I)zt = αtx+ σtεif t > 1/N then# 2 steps of DDIM with teachert′ = t− 1/N , t′′ = t− 2/Nzwt′ = αt′ x̂η(zt, w) +σt′σt(zt − αtx̂η(zt, w))zwt′′ = αt′′ x̂η(zwt′ , w) +σt′′σt′(zwt′ − αt′ x̂η(zwt′ , w))x̃w =zwt′′−(σt′′/σt)ztαt′′−(σt′′/σt)αt. Teacher x̂ targetelse . Edge case# 1 step of DDIM with teachert′ = t− 1/Nzwt′ = αt′ x̂η(zt, w) +σt′σt(zt − αtx̂η(zt, w))x̃w =zwt′−(σt′/σt)ztαt′−(σt′/σt)αt. Teacher x̂ targetend ifλt = log[α2t /σ2t ]Lη2 = ω(λt)‖x̃w − x̂η2(zt, w)‖22η2 ← η2 − γ∇η2Lη2end whileη ← η2 . Student becomes next teacherN ← N/2 . Halve number of sampling stepsend for10ImageNet 64x64 CIFAR-10Guidance w Model FID (↓) IS (↑) FID (↓) IS (↑)w = 0.0 Ours 1-step (D/S) 22.74 / 26.91 25.51 / 23.55 8.34 / 10.65 8.63 / 8.42Ours 2-step (D/S) 9.75 /10.67 36.69 / 37.12 4.48 / 4.81 9.23 / 9.30Ours 4-step (D/S) 4.14 / 3.91 46.64 / 48.92 3.18 / 3.28 9.50 / 9.60Ours 8-step (D/S) 2.79 / 2.44 50.72 / 55.03 2.86 / 3.11 9.68 / 9.74Ours 16-step (D/S) 2.44 / 2.10 52.53 / 57.81 2.78/3.12 9.67 / 9.76Single-w 1-step 19.61 24.00 6.64 8.88Single-w 4-step 4.79 38.77 3.14 9.47Single-w 8-step 3.39 42.13 2.86 9.67Single-w 16-step 2.97 43.63 2.75 9.65DDIM 16-step 7.68 37.60 10.11 8.81DDIM 32-step 5.03 40.93 6.67 9.17DDIM 64-step 3.74 43.16 4.64 9.32Target (DDIM 1024-step) 2.92 44.81 2.73 9.66w = 0.3 Ours 1-step (D/S) 14.85 / 18.48 37.09 / 33.30 7.34 / 9.38 8.90 / 8.67Ours 2-step (D/S) 5.052 / 5.81 54.44 / 54.37 4.23 / 4.74 9.45 / 9.45Ours 4-step (D/S) 2.17 / 2.24 69.64 / 73.73 3.58 / 3.95 9.73 / 9.77Ours 8-step (D/S) 2.05 / 2.31 76.01 / 83.00 3.54 / 3.96 9.87 / 9.90Ours 16-step (D/S) 2.20 / 2.56 79.47 / 87.50 3.57 / 4.17 9.89 / 9.97Single-w 1-step 11.70 36.95 5.98 9.13Single-w 4-step 2.34 62.08 3.58 9.75Single-w 8-step 2.32 68.76 3.57 9.85Single-w 16-step 2.56 70.97 3.61 9.88DDIM 16-step 5.33 60.83 10.83 8.96DDIM 32-step 3.45 68.03 7.47 9.33DDIM 64-step 2.80 72.55 5.52 9.51Target (DDIM 1024-step) 2.36 74.83 3.65 9.83w = 1.0 Ours 1-step (D/S) 7.54 / 8.92 75.19 / 67.80 8.62 / 10.27 9.21 / 8.97Ours 2-step (D/S) 5.77 /5.83 109.97 / 108.38 6.88 / 7.52 9.64 / 9.55Ours 4-step (D/S) 7.95 / 8.51 128.98 / 135.36 7.39 / 7.64 9.86 / 9.87Ours 8-step (D/S) 9.33 / 10.56 136.47 / 147.39 7.81 / 7.85 9.9 / 10.05Ours 16-step (D/S) 9.99 / 11.63 139.11 / 153.17 7.97 / 8.34 10.00 / 10.05Single-w 1-step 6.64 74.41 8.18 9.32Single-w 4-step 8.23 118.52 7.66 9.88Single-w 8-step 9.69 125.20 8.09 9.89Single-w 16-step 10.34 127.70 8.30 9.95DDIM 16-step 9.53 112.75 14.81 8.98DDIM 32-step 9.26 126.22 11.44 9.36DDIM 64-step 9.53 133.17 9.79 9.64Target (DDIM 1024-step) 9.84 139.50 7.80 9.96w = 2.0 Ours 1-step (D/S) 10.71 / 10.55 118.55 / 108.37 13.23 / 14.33 9.23 / 9.02Ours 2-step (D/S) 14.08 / 14.18 160.04/ 161.43 12.58 / 12.57 9.51 / 9.48Ours 4-step (D/S) 17.61 / 18.23 178.29 / 184.45 13.83 / 13.24 9.70 / 9.77Ours 8-step (D/S) 18.80 / 20.25 181.53 / 193.49 14.41 / 13.67 9.77 / 9.87Ours 16-step (D/S) 19.25 / 21.11 183.17 / 197.71 14.80 / 14.28 9.79 / 9.84Single-w 1-step 11.12 120.74 13.31 9.23Single-w 4-step 18.14 172.74 14.04 9.70Single-w 8-step 19.24 176.74 14.67 9.77Single-w 16-step 19.81 177.69 15.04 9.79DDIM 16-step 15.92 157.67 20.25 8.97DDIM 32-step 16.85 175.72 17.27 9.29DDIM 64-step 17.53 182.11 15.66 9.48Target (DDIM 1024-step) 17.97 190.56 13.60 9.81w = 4.0 Ours 1-step (D/S) 18.72 / 17.85 157.46 / 148.97 23.20 / 23.79 8.88 / 8.70Ours 2-step (D/S) 23.74 / 24.34 196.05 / 200.11 23.41 / 22.75 9.16 / 9.11Ours 4-step (D/S) 26.45 / 27.33 207.45 / 216.56 25.11 / 23.62 9.23 / 9.33Ours 8-step (D/S) 26.62 / 27.84 203.47 / 219.89 25.94 / 23.98 9.26 / 9.55Ours 16-step (D/S) 26.53 / 27.69 204.13 / 218.70 26.01 / 24.40 9.33 / 9.50Single-w 1-step 19.857 170.69 23.17 8.93Single-w 4-step 27.75 219.64 24.45 9.32Single-w 8-step 27.67 218.08 24.83 9.38Single-w 16-step 27.40 216.52 25.11 9.37DDIM 16-step 21.56 195.17 27.99 8.71DDIM 32-step 23.03 213.23 25.07 9.07DDIM 64-step 23.64 217.88 23.41 9.17Target (DDIM 1024-step) 23.94 224.74 21.28 9.54Table 2: Distillation results on ImageNet 64x64 and CIFAR-10 (w = 0 refers to non-guided models).For our method, D and S stand for deterministic and stochastic sampler respectively. We observethat training the model conditioned on an guidance interval w ∈ [0, 4] performs comparably withtraining a model on a fixed w (see Single-w). Our approach significantly outperforms DDIM whenusing fewer steps, and is able to match the teacher performance using as few as 8 to 16 steps.11! = 0 ! = 0.3! = 1 ! = 2 ! = 4i64Figure 8: ImageNet 64x64 sample quality evaluated by FID and IS scores. Our distilled modelsignificantly outperform the DDPM and DDIM baselines, and is able to match the performance ofthe teacher using as few as 8 steps. By varying w, our distilled model is able to capture the trade-offbetween sample diversity and quality.12i641-step 2-step4-step 8-step 16-step32-step 64-step 128-step256-step 512-stepFigure 9: FID and IS score trade-off on ImageNet 64x64. We plot the results using guidance strengthw = {0, 0.3, 1, 2, 4}. For the 1-step plot, the curves of DDIM and DDPM are too far away to bevisualized.13! = 0 ! = 0.3! = 1 ! = 2 ! = 4cifar10Figure 10: CIFAR-10 sample quality evaluated by FID and IS scores. Our distilled model significantlyoutperform the DDPM and DDIM baselines, and is able to match the performance of the teacherusing as few as 8 steps. By varying w, our distilled model is able to capture the trade-off betweensample diversity and quality.14cifar101-step 2-step4-step 8-step 16-step32-step 64-step 128-step256-step 512-stepFigure 11: FID and IS score trade-off on CIFAR-10. We plot the results using guidance strengthw = {0, 0.1, 0.2, 0.3, 0.5, 0.7, 1, 2, 4}. For the 1-step and 2-step plots, the curves of DDIM andDDPM are too far away to be visualized. For the 4-step plot, the curve of DDIM is too far away tobe visualized.15C.7 STYLE TRANSFERWe focus on ImageNet 64x64 for this experiment. As discussed in Su et al. (2022), one can performstyle-transfer between domain A and B by encoding (performing reverse DDIM) an image using adiffusion model train on domain A and then decoding using DDIM with a diffusion model trained ondomain B. We train the model using Algorithm 4. We use the same w-conditioned model architectureand training setting as discussed in Appendix C.3.! = 0 ! = 1 ! = 2 ! = 4InputFigure 12: Style transfer on ImageNet 64x64 (orange to acorn squash). We use a distilled16-step encoder and decoder. We fix the encoder guidance strength to be 0 and vary the decoderguidance strength from 0 to 4. As we increase the guidance strength w, we notice a trade-off betweensample diversity and sharpness.Algorithm 4 Encoder distillationRequire: Trained teacher model x̂η(zt, w)Require: Data set DRequire: Loss weight function ω()Require: Student sampling steps Nfor K iterations doη2 ← η . Init student from teacherwhile not converged dox ∼ Dt = i/N, i ∼ Cat[0, 1, . . . , N − 1]w ∼ U [wmin, wmax] . Sample guidanceε ∼ N(0, I)zt = αtx+ σtε# 2 steps of reversed DDIM with teachert′ = t+ 0.5/N , t′′ = t+ 1/Nzwt′ = αt′ x̂η(zt, w) +σt′σt(zt − αtx̂η(zt, w))zwt′′ = αt′′ x̂η(zwt′ , w) +σt′′σt′(zwt′ − αt′ x̂η(zwt′ , w))x̃w =zwt′′−(σt′′/σt)ztαt′′−(σt′′/σt)αt. Teacher x̂ targetλt = log[α2t /σ2t ]Lη2 = ω(λt)‖x̃w − x̂η2(zt, w)‖22η2 ← η2 − γ∇η2Lη2end whileη ← η2 . Student becomes next teacherN ← N/2 . Halve number of sampling stepsend for16C.8 NAIVE PROGRESSIVE DISTILLATIONA natural approach to apply progressive distillation Salimans & Ho (2022) to a guided model is touse a student model that follows the same structure as the teacher—that is with a jointly trainedconditional and unconditional diffusion component. Denote the pre-trained teacher model [x̂c,θ, x̂θ]and the student model [x̂c,η, x̂η], we provide the training algorithm in Algorithm 5. To sample fromthe trained model, we can use DDIM deterministic sampler Song et al. (2021a) or the proposedstochastic sampler (see Eq. (4)). We follow the training setting in Appendix C.3, use a w-conditionedmodel and train the model to condition on the guidance strength [0, 4]. We observe that the modeldistilled with Algorithm 5 is not able to generate reasonable samples when the number of samplingis small. We provide the generated samples on CIFAR-10 with DDIM sampler in Fig. 13, and theFID/IS scores in Table 3.Guidance w Number of step FID (↓) IS (↑)w = 0.0 1 212.20 3.6616 42.02 7.9564 35.37 8.47128 29.74 8.87256 20.14 9.50w = 0.3 1 213.07 3.6216 48.74 7.70128 34.28 8.57256 24.54 9.21w = 1.0 1 214.88 3.5416 64.92 7.2164 48.54 7.62128 42.56 8.00256 32.20 8.81w = 2.0 1 217.37 3.4816 87.19 6.5064 57.15 7.22128 50.30 7.53256 39.76 8.26w = 4.0 1 220.11 3.4516 115.57 6.1664 71.45 6.78128 61.75 7.02256 49.21 7.69Table 3: Naive progressive distillation results on CIFAR-10. We observe that the naive distillationapproach is not able to achieve strong performance.(a) 256-step (b) 64-step (c) 16-step (d) 1-stepFigure 13: A naive application of progressive distillation Salimans & Ho (2022) to guided distillationmodels. The model is trained with guidance strength w ∈ [0, 4] on CIFAR-10. The samples aregenerated with DDIM (deterministic) sampler at w = 0. We observe clear artifacts when the numberof sampling step is small.17Algorithm 5 Two-student progressive distillationRequire: Trained classifier-free guidance teacher model [x̂c,θ, x̂θ]Require: Data set DRequire: Loss weight function ω()Require: Student sampling steps Nfor K iterations doη ← θ . Init student from teacherwhile not converged dox ∼ Dt = i/N, i ∼ Cat[1, 2, . . . , N ]w ∼ U [wmin, wmax] . Sample guidanceε ∼ N(0, I)zt = αtx+ σtεx̂wθ (zt) = (1 + w)x̂c,θ(zt)− wx̂θ(zt) . Compute target# 2 steps of DDIM with teachert′ = t− 0.5/N , t′′ = t− 1/Nzwt′ = αt′ x̂wθ (zt) +σt′σt(zt − αtx̂wθ (zt))zwc,t′′ = αt′′ x̂c,θ(zwt′ ) +σt′′σt′(zwt′ − αt′ x̂c,θ(zwt′ ))x̃wc =zwc,t′′−(σt′′/σt)ztαt′′−(σt′′/σt)αt. Conditional teacher x̂ targetzwt′′ = αt′′ x̂θ(zwt′ ) +σt′′σt′(zwt′ − αt′ x̂θ(zwt′ ))x̃w =zwt′′−(σt′′/σt)ztαt′′−(σt′′/σt)αt. Unconditional teacher x̂ targetλt = log[α2t /σ2t ]Lη = ω(λt)(‖x̃wc − x̂c,η(zt, w)‖22 + ‖x̃w − x̂η(zt, w)‖22)η ← η − γ∇ηLηend whileθ ← η . Student becomes next teacherN ← N/2 . Halve number of sampling stepsend for18D EXTRA SAMPLESD.1 CIFAR-10 256-STEP SAMPLES(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 14: Ours (deterministic). Distilled 256 sampling steps.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 15: Ours (stochastic). Distilled 256 sampling steps.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 16: Ours (deterministic). Distilled 256 sampling steps. Class-conditioned samples.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 17: Ours (stochastic). Distilled 256 sampling steps. Class-conditioned samples.19D.2 CIFAR-10 4-STEP SAMPLES(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 18: Ours (deterministic). Distilled 4 sampling steps.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 19: Ours (stochastic). Distilled 4 sampling steps.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 20: Ours (deterministic). Distilled 4 sampling steps. Class-conditioned samples.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 21: Ours (stochastic). Distilled 4 sampling steps. Class-conditioned samples.20D.3 CIFAR-10 2-STEP SAMPLES(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 22: Ours (deterministic). Distilled 2 sampling steps.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 23: Ours (stochastic). Distilled 2 sampling steps.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 24: Ours (deterministic). Distilled 2 sampling steps. Class-conditioned samples.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 25: Ours (stochastic). Distilled 2 sampling steps. Class-conditioned samples.21D.4 CIFAR-10 1-STEP SAMPLES(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 26: Ours (deterministic). Distilled 1 sampling step.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 27: Ours (stochastic). Distilled 1 sampling step.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 28: Ours (deterministic). Distilled 1 sampling step. Class-conditioned samples.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 29: Ours (stochastic). Distilled 1 sampling step. Class-conditioned samples.22D.5 IMAGENET 64X64 256-STEP SAMPLES(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 30: Ours (deterministic). Distilled 256 sampling steps.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 31: Ours (stochastic). Distilled 256 sampling steps.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 32: Ours (deterministic). Distilled 256 sampling steps. Class-conditioned samples.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 33: Ours (stochastic). Distilled 256 sampling steps. Class-conditioned samples.23D.6 IMAGENET 64X64 8-STEP SAMPLES(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 34: Ours (deterministic). Distilled 8 sampling step.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 35: Ours (stochastic). Distilled 8 sampling step.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 36: Ours (deterministic). Distilled 8 sampling step. Class-conditioned samples.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 37: Ours (stochastic). Distilled 8 sampling step. Class-conditioned samples.24D.7 IMAGENET 64X64 2-STEP SAMPLES(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 38: Ours (deterministic). Distilled 2 sampling steps.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 39: Ours (stochastic). Distilled 2 sampling steps.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 40: Ours (deterministic). Distilled 2 sampling steps. Class-conditioned samples.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 41: Ours (stochastic). Distilled 2 sampling steps. Class-conditioned samples.25D.8 IMAGENET 64X64 1-STEP SAMPLES(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 42: Ours (deterministic). Distilled 1 sampling step.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 43: Ours (stochastic). Distilled 1 sampling step.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 44: Ours (deterministic). Distilled 1 sampling step. Class-conditioned samples.(a) w = 0 (b) w = 1 (c) w = 2 (d) w = 4Figure 45: Ours (stochastic). Distilled 1 sampling step. Class-conditioned samples.26'}\n",
      "{'_id': ObjectId('6815e61add259dc7e6e1cd36'), 'authors': 'Abrevaya, Victoria Fernandez, Andreou, Nefeli, Cani, Marie-Paule, Dufour, Nicolas, Kalogeiton, Vicky, Picard, David, Wang, Xi', 'year': '2024', 'title': 'Analysis of Classifier-Free Guidance Weight Schedulers', 'full_text': 'Published in Transactions on Machine Learning Research (12/2024)Analysis of Classifier-Free Guidance Weight SchedulersXi Wang1, Nicolas Dufour1,2, Nefeli Andreou3†, Marie-Paule Cani1, Victoria FernándezAbrevaya4, David Picard2∗, Vicky Kalogeiton1∗1LIX, École Polytechnique, CNRS, IPP firstname.lastname@polytechnique.edu2LIGM, École des Ponts, Univ Gustave Eiffel, CNRS firstname.lastname@enpc.fr3University of Cyprus nefeliandreou@outlook.com4Max Planck Institute, Tübingen victoria.abrevaya@tuebingen.mpg.de† Work done during an internship at LIX, prior to joining Amazon. ∗ Denotes equal supervisionReviewed on OpenReview: https://openreview.net/forum?id=SUMtDJqicdAbstractClassifier-Free Guidance (CFG) enhances the quality and condition adherence of text-to-image diffusion models. It operates by combining the conditional and unconditional pre-dictions using a fixed weight. However, recent works vary the weights throughout the dif-fusion process, reporting superior results but without providing any rationale or analysis.By conducting comprehensive experiments, this paper provides insights into CFG weightschedulers. Our findings suggest that simple, monotonically increasing weight schedulersconsistently lead to improved performances, requiring merely a single line of code. In addi-tion, more complex parametrized schedulers can be optimized for further improvement, butdo not generalize across different models and tasks.1 IntroductionDiffusion models have demonstrated prominent generative capabilities in various domains e.g. images (Hoet al., 2020), videos (Luo et al., 2023), acoustic signals (Kang et al., 2023b), or 3D avatars (Chen et al.,2023). Conditional generation with diffusion (e.g. text-conditioned image generation) has been explored innumerous works (Saharia et al., 2022; Ruiz et al., 2023; Balaji et al., 2022), and is achieved in its simplestform by adding an extra condition input to the model (Nichol & Dhariwal, 2021). To increase the influence ofthe condition on the generation process, Classifier Guidance (Dhariwal & Nichol, 2021) proposes to linearlycombine the gradients of a separately trained image classifier with those of a diffusion model. Alternatively,Classifier-Free Guidance (CFG) (Ho & Salimans, 2021) simultaneously trains conditional and unconditionalmodels, and exploits a Bayesian implicit classifier to condition the generation without an external classifier.In both cases, a weighting parameter ω controls the importance of the generative and guidance terms andis directly applied at all timesteps. Varying ω is a trade-off between fidelity and condition reliance, as anincrease in condition reliance often results in a decline in both fidelity and diversity. In some recent literature,the concept of dynamic guidance instead of constant one has been mentioned: MUSE (Chang et al., 2023)observed that a linearly increasing guidance weight could enhance performance and potentially increasediversity. This approach has been adopted in subsequent works, such as in Stable Video Diffusion (Blattmannet al., 2023), and further mentioned in Gao et al. (2023) through an exhaustive search for a parameterizedcosine-based curve (pcs4) that performs very well on a specific pair of model and task. Intriguingly, despitethe recent appearance of this topic in the literature, none of the referenced studies has conducted anyempirical experiments or analyses to substantiate the use of a guidance weight scheduler. For instance, theconcept of linear guidance is briefly mentioned in MUSE (Chang et al., 2023), around Eq. 1: \"we reducethe hit to diversity by linearly increasing the guidance scale t [...] allowing early tokens to be sampled morefreely\". Similarly, the pcs4 approach Gao et al. (2023) is only briefly discussed in the appendix, withoutany detailed ablation or comparison to static guidance baselines. Thus, to the best of our knowledge, acomprehensive guide to dynamic guidance weight schedulers does not exist at the moment.1arXiv:2404.13040v2  [cs.CV]  4 Dec 2024Published in Transactions on Machine Learning Research (12/2024)In this paper, we bridge this gap by delving into the behavior of guidance and systematically examining itsinfluence on the generation, discussing the mechanism behind dynamic schedulers and the rationale for theirenhancement. We explore various heuristic dynamic schedulers and present a comprehensive benchmark ofboth heuristic and parameterized dynamic schedulers across different tasks, focusing on fidelity, diversity,and textual adherence. Our analysis is supported by quantitative, and qualitative results and user studies.Low static guidance:w = 2.0for t in range(1, T):eps_c = model(x, T-t, c)eps_u = model(x, T-t, 0)eps = (w+1)*eps_c - w*eps_ux = denoise(x, eps, T-t)✗ Fuzzy images, but many details and texturesHigh static guidance:w = 14.0for t in range(1, T):eps_c = model(x, T-t, c)eps_u = model(x, T-t, 0)eps = (w+1)*eps_c - w*eps_ux = denoise(x, eps, T-t)✗ Sharp images, but lack of details and solidcolorsDynamic guidance:w0 = 14.0for t in range(1, T):eps_c = model(x, T-t, c)eps_u = model(x, T-t, 0)# clamp-linear schedulerw = max(1, w0*2*t/T)eps = (w+1)*eps_c - w*eps_ux = denoise(x, eps, T-t)✓ Sharp images with many details and textures,without extra cost.“full body, a cat dressed as a Viking, with weapons in his paws, on a Viking ship, battle coloring, glow hyper-detail, hyper-realism, cinematic, trending on artstation”Figure 1: Classifier-Free Guidance introduces a trade-off between detailed but fuzzy images (low guidance,top) and sharp but simplistic images (high guidance, middle). Using a guidance scheduler (bottom) is simpleyet very effective in improving this trade-off.Our findings are the following: First, we show that too much guidance at the beginning of the denoisingprocess is harmful and that monotonically increasing guidance schedulers are performing the best. Second,we show that a simple linearly increasing scheduler always improves the results over the basic static guidance,while costing no additional computational cost, requiring no additional tuning, and being extremely simpleto implement. Third, a parameterized scheduler, like clamping a linear scheduler below a carefully chosenthreshold (Figure 1), can significantly further improve the results, but the choice of the optimal parameterdoes not generalize across models and tasks and has thus to be carefully tuned for the target model andtask. All our findings are guides to CFG schedulers that will benefit and improve all works relying on CFG.2 Related WorkGenerative and Diffusion Models. Before the advent of diffusion models, several generative modelswere developed to create new data that mimics a given dataset, either unconditionally or with conditionalguidance. Notable achievements include Variational AutoEncoders (VAEs) (Kingma & Welling, 2014) and2Published in Transactions on Machine Learning Research (12/2024)Figure 2: Examples of all heuristics on SDXL. Increasing ones (linear and cosine) enhance fidelity,textual adherence and diversity.Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), which have recorded significant progressin various generative tasks (Brock et al., 2018; Kang et al., 2023a; Dufour et al., 2022; Donahue et al.,2018). Recently, diffusion models have demonstrated a remarkable capacity to produce high-quality anddiverse samples. They have achieved state-of-the-art results in several generation tasks, notably in imagesynthesis (Song et al., 2020; Ho et al., 2020), text-to-image applications (Dhariwal & Nichol, 2021; Rombachet al., 2022; Podell et al., 2023; Pernias et al., 2023) and text-to-motion (Chen et al., 2023).Guidance in Diffusion and Text-to-Image. Making generative models controllable and capable ofproducing user-aligned outputs requires making the generation conditional on a given input. Conditioneddiffusion models have been vastly explored (Saharia et al., 2022; Ruiz et al., 2023; Balaji et al., 2022). Thecondition is achieved in its simplest form by adding extra input, typically with residual connections (Nichol& Dhariwal, 2021). To reinforce the model’s fidelity to specific conditions, two main approaches prevail:Classifier Guidance (CG) (Dhariwal & Nichol, 2021), which involves training an image classifier externally,and Classifier-Free Guidance (CFG) (Ho & Salimans, 2021), that relies on an implicit classifier through jointtraining of conditional and unconditional models (using dropout on the condition).Particularly, CFG has catalyzed advancements in text-conditional generation, a domain where traininga noisy text classifier is less convenient and performs worse. This approach breathed new life into thetext-to-image application, initially proposed in several works such as (Reed et al., 2016; Mansimov et al.,2015). Numerous works (Rombach et al., 2022; Ramesh et al., 2022; Nichol et al., 2022; Avrahami et al.,2022) have leveraged text-to-image generation with CFG diffusion models conditioned on text encoderslike CLIP (Radford et al., 2021), showcasing significant progress in the field, e.g. the Latent DiffusionModel (Dhariwal & Nichol, 2021) and Stable Diffusion (Rombach et al., 2022) employ VAE latent spacediffusion with CFG with CLIP encoder. SDXL, an enhanced version, leverages a larger model and anadditional text encoder for high-resolution synthesis.Improvements on Diffusion Guidance. Noticed that in Classifier Guidance (CG), the classifier’s gradi-ent tends to vanish towards the early and final stages due to overconfidence, Zheng et al. (2022) leverages theentropy of the output distribution as an indication of vanishing gradient and rescales the gradient accord-ingly. To prevent such adversarial behaviours, Dinh et al. (2023b) explored using multiple class conditions,guiding the image generation from a noise state towards an average of image classes before focusing on the3Published in Transactions on Machine Learning Research (12/2024)Figure 3: Qualitative results of fidelity for different guidance schedulers compared with static baseline.linear and cosine schedulers show better image details (flower petal, figurine engraving), more natural colour(pink corridor), and better textual adherence (bad weather for the two birds image, key-chain of the figurine).desired class with an empirical scheduler. Subsequently, Dinh et al. (2023a) identified and quantified gradientconflicts emerging from the guidance and suggested gradient projection as a solution.In Classifier-Free Guidance (CFG), Li et al. (2023) used CFG to recover a zero-shot classifier by samplingacross timesteps and averaging the guidance magnitude for different labels, with the lowest magnitude corre-sponding to the most probable label. However, they observed a discrepancy in performance across timestepswith early stages yielding lower accuracy than intermediate ones. Chang et al. (2023) observed that a linearincrease in guidance scale enhances diversity. Similarly, Gao et al. (2023) developed a parameterized power-cosine-like curve, optimizing a specific parameter for their dataset and method. However, these linear andpower-cosine schedulers have been suggested as improvements over constant static guidance without rigorousanalysis or testing. To this end, we provide an extensive study of dynamic guidance for both heuristic andparametrized schedulers across several tasks. Concurrently, Kynkäänniemi et al. (2024) proposes empiricallyremoving the initial and final timesteps of the classifier-free guidance (CFG) for improved generation. Simi-larly, Zhang et al. (2024); Castillo et al. (2023) observes that the conditional and unconditional responses ofsome models may converge to similar behaviours at certain timesteps, particularly towards the ending stage.3 Background on GuidanceFollowing DDPM (Ho et al., 2020), diffusion consists in training a network ϵθ to denoise a noisy input torecover the original data at different noise levels. More formally, the goal is to recover x0, the originalimage from xt=√γ(t)x0+√1−γ(t)ϵ, where γ(t)∈[0, 1] is a noise scheduler of the timestep t and applied to astandard Gaussian noise ϵ∼N (0, 1). In practice, Ho et al. (2020) find that predicting the noise ϵθ instead ofx0 yielded better performance leading to the training loss: Lsimple=Ex0∼pdata,ϵ∼N (0,1),t∼U [0,1] [∥ϵθ(xt) − ϵ∥]based on the target image distribution pdata with U uniform distributions.Once the network is trained, we can sample from pdata by setting xT =ϵ ∼ N (0, 1) (with γ(T )=0), andgradually denoising to reach the data point x0∼pdata with different types of samplers e.g., DDPM (Ho et al.,2020) or DDIM (Song et al., 2020). To leverage a condition c and instead sample from p(xt|c), Dhariwal &Nichol (2021) propose Classifier Guidance (CG) that uses a pretrained classifier p(c|xt), forming:∇xt log p(xt|c)=∇xt log p(xt)+∇xt log p(c|xt) , (1)according to Bayes rule. This leads to the following Classifier Guidance equation, with a scalar ω>0 con-trolling the amount of guidance towards the condition c:ϵ̂θ(xt, c) = ϵθ(xt) + (ω + 1)∇xt log p(c|xt) . (2)4Published in Transactions on Machine Learning Research (12/2024)Figure 4: Qualitative results of diversity of different guidance schedulers compared with static baseline.Heuristic schedulers show better diversity: more composition and richer background types for the teddy bearexample, as well as the gesture, lighting, colour and compositions in the astronaut image.However, this requires training a noise-dependent classifier externally, which can be cumbersome and im-practical for novel classes or more complex conditions e.g. textual prompts. For this reason, with an implicitclassifier from Bayes rule ∇xt log p(c|xt)=∇xt log p(xt, c)−∇xt log p(xt), Ho & Salimans (2021) propose totrain a diffusion network on the joint distribution of data and condition by replacing ϵθ(xt) with ϵθ(xt, c) inLsimple. By dropping the condition during training, they employ a single network for both ∇xt log p(xt, c)and ∇xt log p(xt). This gives the Classifier-Free Guidance (CFG), also controlled by ω:ϵ̂θ(xt, c) = ϵθ(xt, c) + ω (ϵθ(xt, c) − ϵθ(xt)) . (3)We can reformulate the above two equations into two terms: a generation term ϵθ(xt)∝∇xt log p(xt) and aguidance term ∇xt log p(c|xt). The guidance term can be derived either from a pre-trained classifier or animplicit one, with ω balancing between generation and guidance.4 Towards dynamic guidance: Should guidance be constant?Our initial experiments show that removing guidance at certain timesteps can improve performance. Thisis in line with concurrent work (Kynkäänniemi et al., 2024). To further investigate this, we conducted anegative perturbation analysis experiment to determine the impact of the guidance across all timesteps.Negative Perturbation Analysis. This analysis is on the CIFAR-10 dataset: a 60,000 images datasetwith a 32 × 32 resolution, distributed across 10 classes. We choose the original DDPM method (Ho et al.,2020) denoising on pixel space as the backbone and class-conditioning guidance.To investigate the importance of guidance across different timestep intervals, we first employ static guidanceof ω = 1.15, then independently set the guidance to zero across different 50-timestep intervals (20 intervals intotal spanning all timesteps), and compute the FID for each of these piece-wise zeroed guidance schedulersof 50,000 generated images. If we mathematically model the removal method, it can approximate a family5Published in Transactions on Machine Learning Research (12/2024)of parameterized gate/inverse gate functions with two parameters defining the starting point s and size ofthe kernel d: g(t) = 1 − (H(t − s) − H(t − (s + d))), where H is Heaviside step function.The results are illustrated in Figure 6b. For example, the second data point on the left of the curve representsthe FID performance when guidance is removed only in the interval t = [50, 100] while maintaining static atothers. We observe multiple phenomena from the results: (1) non-constant guidance curve can outperformstatic guidance in terms of FID; (2) zeroing the guidance at earlier stages improves the FID performance;(3) zeroing the guidance at later stages significantly degrades it.However, this removal scheme is not practical for real usage: (i) grid searching two parameters requiresgenerating a prohibitively costly number of images; (ii) as shown in Section 6, parameterized methods oftenfail to generalize; (iii) further investigation, detailed in Appendix Section B, demonstrates that instead ofcompletely removing CFG from some timesteps, keeping it with lower values increases the performance.0 250 500 750 1000Timesteps0.00.20.40.6Percentage(i) Conflict Ratio (Percentage)SD [35]linear0 250 500 750 1000Timesteps−0.020.000.020.04(ii) Avg. Cosine Similarity0 250 500 750 1000Timesteps−0.05−0.04−0.03−0.02Similarity(iii) Avg. Conflict MagnitudeFigure 5: Visualization of Conflicted Terms from SD1.5 Rombach et al. (2022) shows that static guidancepresents conflicts, while a guidance scheduler reduces the conflict between generation and guidance terms.Conflicted terms. Our assumption is that the guidance and generation terms (see Eq. 3) may be adversarialduring inference. Following (Dinh et al., 2023a), Figure 5 quantifies the conflict by measuring (a) the ratio ofnegative cosine similarity; (b) average cosine similarity (i.e. directional conflict, -1 and 1 for maximum andminimum conflicts); and (c) conflict magnitude (Dinh et al., 2023a), defined as:Φ (ϵ1, ϵ2) = −2|ϵ1|2|ϵ2|2|ϵ1|22+|ϵ2|22 whereϵ is each term at each timestep, with Φ resulting −1 and 0 indicates zero and maximum conflict. We evaluate1000 generation from COCO prompts (Lin et al., 2014) with SD1.5 and show in Figure 5(i) that SD (orange)exhibits ∼ 50% conflict ratio along the generation with high magnitude conflict (see Figure 5(iii)(right) withcurves closer to zero). When the guidance is lowered at the beginning (e.g. linearly increasing as shown inSection 5), less conflict for both magnitude and directional metrics is shown in all subfigures blue curves.Dynamic guidance. Having observed that removing the guidance at certain timesteps improves the per-formance over using a static weight ω for CFG like in Ho & Salimans (2021); Dhariwal & Nichol (2021)and reducing the guidance at beginning linearly can reduce the conflict, we ask the question of whether wecan replace static guidance with other options. Therefore, we investigate dynamic guidance scheduler thatevolves throughout the generation process, which is also in line with some empirical schemes mentioned inrecent literature (Blattmann et al., 2023; Chang et al., 2023; Donahue et al., 2018). In that case, the CFGEquation 3 is rewritten as follows:ϵ̂θ(xt, c) = ϵθ(xt, c)+ω(t) (ϵθ(xt, c) − ϵθ(xt)) . (4)To identify an effective dynamic scheduler ω(t), we analyse two types of function in subsequent sections:parameter-free heuristic schedulers in Section 5 and single-parameter parameterized ones in Section 6.5 Dynamic Guidance: Heuristic SchedulersWe first use six simple heuristic schedulers as dynamic guidance ω(t), split into three groups depending onthe shape of their curve: (a) increasing functions (linear, cosine); (a) decreasing functions (inverse linear,6Published in Transactions on Machine Learning Research (12/2024)9.6 9.7 9.8Inception Score2.62.83.03.23.43.63.84.0FIDHeuristics FID vs ISbaseline-staticlinearinvlinearcosinesine⇤-shapeV-shape0 200 400 600 800 1000Timesteps0.00.51.01.52.02.53.0WeightsHeursitcs Weights vs Timestepsbaseline-staticlinearinvlinearcosinesine⇤-shapeV-shape(a) Experiment on Different Heuristics0 200 400 600 800Timestep2.93.03.13.23.3FIDNegative Test on CIFAR-10baselinenegative pertubation(b) Negative PerturbationFigure 6: Preliminary Analysis on CIFAR-10 (a) Various heuristic curves with their FID vs. ISperformances. (b) Negative perturbation by setting the guidance scale to 0 across distinct intervals whilepreserving static guidance to the rest. By eliminating the weight at the initial stage (e.g., T = 800), thelowered FID shows an enhancement, whereas removing guidance at higher timesteps leads to worse FID.sine); (c) non-monotonic functions (linear V-shape, linear Λ-shape), defined as:linear: ω(t) = 1 − t/T, invlinear: ω(t) = t/T,cosine: ω(t) = cos (πt/T ) + 1, sine: ω(t) = sin (πt/T − π/2) + 1,V-shape: ω(t) = invlinear(t) if t < T/2, Λ-shape: ω(t) = linear(t) if t < T/2,linear(t) else, invlinear(t) else.To allow for a direct comparison between the effect of these schedulers and the static guidance ω, wenormalize each scheduler by the area under the curve. This ensures that the same amount of total guidanceis applied over the entire denoising process, and allows users to rescale the scheduler to obtain a behaviorsimilar to that of increasing ω in static guidance. More formally, this corresponds to the following constraint:∫ T0 ω(t)dt = ωT . For example, this normalization leads to the corresponding normalized linear schedulerω(t) = 2(1 − t/T )ω. We show in Figure 6a (left) the different normalized curves of the 6 schedulers.5.1 Class-conditional image generation with heuristic schedulersHeuristic Schedulers Analysis. We first study the 6 previously defined heuristic schedulers ω(t) on theCIFAR-10-DDPM setting for class-conditional synthesis same as in the Section 4. To assess the performance,we use the Frechet Inception Distance (FID) and Inception Score (IS) metrics, over 50, 000 inference from50-step DDIM (Song et al., 2020). In this experiment, we evaluate the impact of a range of different guidancetotal weight: [1.1, 1.15, 1.2, 1.25, 1.3, 1.35], to study its influence over the image quality vs class adherencetrade-off. We show the results in Figure 6a, right panel and observe that both increasing schedulers (linearand cosine) significantly improve over the static baseline, whereas decreasing schedulers (invlinear and sine)are significantly worse than the static. The V-shape and Λ-shape schedulers perform respectively better andworse than the static baseline, but only marginally.Preliminary Conclusion. Combining with the observation from Section 4 that removing the beginningstage improves the performance, they point to the same conclusion: monotonically increasing guidanceschedulers achieve improved performances, revealing that the static CFG primarily may overshoot theguidance in the initial stages. In the remainder of this work, we only consider monotonically increasingschedulers, as we consider these findings sufficient to avoid examining all other schedulers on other tasks.(more details in Appx. and Figure 2 shows a qualitative results in SDXL)Experiments on ImageNet. On ImageNet, we explore the linear and cosine schedulers that performedbest on CIFAR-10. In Figure 7d, we observe that the linear and cosine schedulers lead to a significant7Published in Transactions on Machine Learning Research (12/2024)improvement over the baseline, especially at higher guidance weights, enabling a better FID/Inception Scoretrade-off. More experiments in Appx. lead to a similar conclusion.5.2 Text-to-image generation with heuristic schedulersWe study the linear and the cosine scheduler on text-to-image generation. The results for all proposedheuristics are in Appx. Tables 12 and 14, where we observe a similar trend as before: heuristic functionswith increasing shape report the largest gains on both SD1.5 and SDXL.Dataset and Metrics. We use text-to-image models pre-trained on LAION (Schuhmann et al., 2022), whichcontains 5B high-quality images with paired textual descriptions. For evaluation, we use the COCO (Linet al., 2014) val set with 30, 000 text-image paired data.We use three metrics: (i) Fréchet inception distance (FID) for the fidelity of generated images; (ii) CLIP-Score (CS) (Radford et al., 2021) to assess the alignments between the images and their corresponding textprompts; (iii) Diversity (Div) to measure the model’s capacity to yield varied content. For this, we computethe standard deviation of image embeddings via Dino-v2 (Oquab et al., 2023) from multiple generations ofthe same prompt (more details for Diversity in Appendix).We compute FID and CS for a sample set of 10, 000 images against the COCO dataset in a zero-shotfashion (Rombach et al., 2022; Saharia et al., 2022). For diversity, we resort to two text description subsetsfrom COCO: 1000 longest captions and shortest captions each (-L and -S in Figure 7a) to represent varyingdescriptiveness levels; longer captions provide more specific conditions than shorter ones, presumably leadingto less diversity. We produce 10 images for each prompt using varied sampling noise.Model. We experiment with two models: (1) Stable Diffusion (SD) (Rombach et al., 2022), which usesthe CLIP (Radford et al., 2021) text encoder to transform text inputs to embeddings. We use the publiccheckpoint of SD v1.5 1 and employ DDIM sampler with 50 steps. (2) SDXL (Podell et al., 2023), which is alarger, advanced version of SD (Rombach et al., 2022), generating images with resolutions up to 1024 pixels.It leverages LDM (Dhariwal & Nichol, 2021) with larger U-Net architectures, an additional text-encoder(OpenCLIP ViT-bigG), and other conditioning enhancements. We use the SDXL-base-1.02 (SDXL) versionwithout refiner, sampling with DPM-Solver++ (Lu et al., 2022b) of 25 steps.Results. We show the FID vs. CS curves in Figure 7a, 7c for SD and SDXL respectively (more tables inAppx. Section G). We expect an optimal balance of a high CS and a low FID (i.e., the right-down corner).Analysis on SD (Figure 7a). For FID vs CS, the baseline (Rombach et al., 2022) yields inferior resultscompared to the linear and cosine heuristics with linear recording lower FID. The baseline regresses FIDfast when CS is high, but generates the best FID when CS is low, i.e., low condition level. This, however,is usually not used for real applications, e.g., the recommended ω value is 7.5 for SD1.5, highlighted bythe dotted line in Figure 7a with the black solid arrow representing the gain of heuristic schedulers on FIDand CS respectively. For Div vs CS, heuristic schedulers outperform the baseline (Rombach et al., 2022) onboth short (S) and long (L) captions at different guidance scales. Also, cosine shows superiority across themajority of the CS range. Overall, heuristic schedulers achieve improved performances in FID and Diversity,recording 2.71(17%) gain on FID and 0.004(16%) gain (of max CS-min CS of baseline) on CS over ω=7.5default guidance in SD. Note, this gain is achieved without hyperparameter tuning or retraining.Analysis on SDXL (Figure 7c). In FID, both the linear and cosine schedulers achieve better FID-CSthan the baseline (Podell et al., 2023). In Diversity, linear is slightly lower than cosine, and they are bothbetter than static baseline. Additionally, unlike the baseline (blue curves) where higher guidance typicallyresults in compromised FID, heuristic schedulers counter this.User study. We present users with a pair of mosaics of 9 generated images and ask them to vote for thebest in terms of realism, diversity and text-image alignment. Each pair compares static baseline generationsagainst cosine and linear schedulers. Figure 7b reports the results. We observe that over 60% of users considerscheduler-generated images more realistic and better aligned with the text prompt, while approximately1https://huggingface.co/runwayml/stable-diffusion-v1-52https://github.com/Stability-AI/generative-models8Published in Transactions on Machine Learning Research (12/2024)0.255 0.260 0.265 0.270 0.275 0.280CLIP-Score101214161820FIDbaselinelinearcosinew=7.5 SD1.50.26 0.27 0.28CLIP-Score1.01.11.2Diversity-S-L0.250 0.2752040(a) SD1.5: FID and Div-CS25%50%75% Realism SchedulerBaseline25%50%75%PreferenceRateDiversitylinear cosine25%50%75% Text alignment(b) SD1.5:User Study0.2650 0.2675 0.2700 0.2725 0.2750 0.2775 0.2800 0.2825 0.2850CLIP-Score242628303234FIDSDXL FID vs. CSbaselinelinearcosinew=8 for SDXL0.27 0.28CLIP-Score0.91.01.1DiversitySDXL Div vs. CS-S-L0.26 0.282550(c) SDXL: FID-CS200 250 300Inception Score468CIN-256baselinelinearcosine(d) CIN-256:FID-ISFigure 7: Class-conditioned and text-to-image generation results of monotonically-increasingheuristic schedulers (linear and cosine). (a) FID and Div vs. CS for SD1.5 (Rombach et al., 2022).We highlight the gain of FID and CS compared with the default ω=7.5 with black arrows, diversity on theright shows that the heuristic guidance performs better than static baseline guidance; (b) our user studyalso reveals that images generated with schedulers are consistently preferred than the baseline in realism,diversity and text alignment; (c) results for SDXL (Podell et al., 2023) on FID and Div vs. CS withsimilar setup to (a); (d) CIN-256 LDM (Dhariwal & Nichol, 2021) are assessed with FID vs. IS. Heuristicschedulers outperform the baseline static guidance on fidelity and diversity across multiple models.Figure 8: Failure cases of parameter-free and parameterized approaches: monotonically increasing guidancemay mute the guidance at the beginning (especially when overall guidance is low), causing structural errors;and incorrectly chosen parameters can lead to fuzzy details and low saturation problems.80% find guidance schedulers results more diverse. This corroborates our hypothesis that static weighting isperceptually inferior to dynamic weighting. More details in Appx.Qualitative results. Figure 3 depicts the fidelity of various sets of text-to-image generations from SD andSDXL. Heuristic schedulers (linear and cosine) enhance the image fidelity: better details in petals and leavesof the flower images, as well as the texture of bird features. In the arches example, we observe more naturalcolour shading as well as more detailed figurines with reflective effects. Figure 4 showcases the diversity ofoutputs in terms of composition, color palette, art style and image quality by refining shades and enrichingtextures. Notably, the teddy bear shows various compositions and better-coloured results than the baseline,9Published in Transactions on Machine Learning Research (12/2024)Figure 9: Qualitative comparison of baseline, linear and clamp-linear on SDXL. Both dynamic schedulersare better than the baseline, clamp-linear with c=4 outperforms all with better details and higher fidelity.which collapsed into similar compositions. Similarly, in the astronaut example, the baseline generates similarimages while heuristic schedulers reach more diverse character gestures, lighting and compositions.5.3 Findings with heuristic schedulersIn summary, we make the following observations: monotonically increasing heuristic schedulers (e.g., linearand cosine) (a) improve generation performances (IS/CS vs. FID) over static baseline on different models;(b) improve image fidelity (texture, details), diversity (composition, style) and quality (lighting, gestures).We note that this gain is achieved without hyperparameter tuning, retraining or extra computational cost.Failure cases. For the failure cases involving monotonically increasing guidance, we observe that under-shooting the guidance scale during the initial stages can compromise the structural integrity of the generatedoutputs. This often results in anatomical and geometric errors, such as the appearance of a third leg inhumans, a fifth leg in quadruped animals, or incorrect spatial relationships, as illustrated in Figure 8.6 Dynamic Guidance: Parametrized SchedulersWe investigate two parameterized schedulers with a tunable parameter to maximize performance: a power-cosine curve family (introduced in MDT (Gao et al., 2023)) and two clamping families (linear and cosine).The parameterized family of powered-cosine curves (pcs) and clamping (clamp) is defined by the controllableparameter s and c respectively:wt =1 − cos π(T −tT)s2 w (pcs) (5)wt = max(c, wt) (clamp) (6)In our work, we use clamp-linear but this family can be extended to other schedulers (more in sup. mat.).Our motivation lies in our observation that excessive muting of guidance weights at the initial stages cancompromise the structural integrity of prominent features. This contributes to bad FID at lower values of ωin Figure 7a, suggesting a trade-off between model guidance and image quality. However, reducing guidance10Published in Transactions on Machine Learning Research (12/2024)9.55 9.60 9.65 9.70 9.75 9.80 9.85 9.90Inception Score2.83.03.2FIDFID vs ISbaseline (static)linearclamp-linear (c=1.005)clamp-linear (c=1.01)clamp-linear (c=1.015)pcs (s=0.1)pcs (s=1)pcs (s=2)pcs (s=4)(a) CIFAR-10-DDPM150 175 200 225 250 275 300 325Inception Score468FIDFID vs ISbaseline (static)linearclamp-linear (c=1.005)clamp-linear (c=1.1)clamp-linear (c=1.3)pcs (s=0.1)pcs (s=1)pcs (s=2)pcs (s=4)(b) CIN-256-LDMFigure 10: Class-conditioned generation results of parameterized clamp-linear and pcs on (a) CIFAR-10-DDPM and (b) CIN-256-LDM. Optimising parameters improves performances but these parameters donot generalize across models and datasets.intensity early in the diffusion process is also the origin of enhanced performances, as shown in Section 5.This family represents a trade-off between diversity and fidelity while giving users precise control.0 250 500 750 1000Timestep51015scaledω(t)(constantω=7.5) constant ω = 7.5clamp-linear (c=1)clamp-linear (c=2)clamp-linear (c=3)clamp-linear (c=4)(a) ω(t): clamp-linear0.260 0.265 0.270 0.275 0.280CLIP-Score1214161820FIDbaselinelinearclamp-linear (c=1)clamp-linear (c=2)clamp-linear (c=3)clamp-linear (c=4)w=7.5 SD1.5(b) SD1.5: clamp-linear0.265 0.270 0.275 0.280 0.285CLIP-Score20304050FIDbaselinelinearclamp-linear (c=2)clamp-linear (c=4)clamp-linear (c=6)w=8 for SDXL(c) SDXL: clamp-linear0 250 500 750 1000Timestep010203040scaledω(t)(constantω=7.5) constant ω = 7.5pcs (s=0.1)pcs (s=1)pcs (s=2)pcs (s=4)(d) ω(t): pcs0.25 0.26 0.27 0.28CLIP-Score12.515.017.520.022.5FIDbaselinepcs (s=1)pcs (s=4)pcs (s=2)pcs (s=0.1)w=7.5 SD1.5(e) SD1.5: pcs0.24 0.25 0.26 0.27 0.28CLIP-Score20304050FIDbaselinepcs (s=1)pcs (s=4)pcs (s=2)pcs (s=0.1)w=8 for SDXL(f) SDXL: pcsFigure 11: Text-to-image performance for two parameterized schedulers: clamp-linear and pcs.For clamp-linear, (a) shows the guidance curves for different parameters and (b,c) displays the FID vs. CSfor SD1.5 and SDXL, respectively. For pcs, (d) shows the guidance curves and (e,f) depicts the FID vs. CS.Optimal parameters for either clamp or pcs outperform the static baseline for both SD1.5 and SDXL.6.1 Class-conditional image generation with parametrized schedulersWe experiment with two parameterized schedulers: clamp-linear and pcs on CIFAR10-DDPM (Figure 10a)and ImageNet(CIN)256-LDM (Figure 10b). We observe that, for both families, tuning parameters improvesresults over baseline and heuristic schedulers. The optimal parameters are c=1.01 for clamp-linear and s=4for pcs on CIFAR10-DDPM, vs c=1.1 for clamp-linear and s=2 for pcs on CIN-256. Overall, parameterizedschedulers improve performances; however, the optimal parameters do not apply across datasets and models.11Published in Transactions on Machine Learning Research (12/2024)6.2 Text-to-image generation with parametrized schedulersFigure 12: Qualitative results for parametrized schedulers clamp-linear and pcs on SDXL Podell et al.(2023). Overall, c=4 for clamp-linear gives the most visually pleasing results.We experiment with two parameterized schedulers: clamp-linear (clamp-cosine in sup. mat.) and pcs, withtheir guidance curves in Figures 11a,11d, respectively.For SD1.5 (Rombach et al., 2022), the FID vs. CS results are depicted in Figures 11b and 11e. The pcs familystruggles to achieve low FID, except when s = 1. Conversely, the clamp family exhibits optimal performancearound c=2, achieving the best FID and CLIP-score balance while outperforming all pcs values.For SDXL (Podell et al., 2023), the FID vs. CS results are depicted in Figures 11c and 11f. The pcs showsthe best performance at s = 0.1. Clamp-linear achieves optimum at c = 4 (FID 18.2), largely improving FIDacross the entire CS range compared to the baseline (FID 24.9, about 30% gain) and the linear scheduler.The optimal parameters of clamp-linear (resp. pcs) are not the same for both models, i.e. c=2 for SD1.5and c=4 for SDXL (resp. s=1 and s=0.1 for pcs). This reveals the lack of generalizability of this family.Qualitative results. The results of Figure 9 further underscore the significance of choosing the rightclamping parameter. This choice markedly enhances generation performance, as evidenced by improvedfidelity (dog and squirrel image), textual comprehension (Fries Big Ben), and details (sunglasses).Figure 12 compares two parameterized families: (i) clamp and pcs (Gao et al., 2023), where the clampperforms best at c = 4 and the pcs at s = 1. We observe that the clamp-linear c = 4 demonstrates betterdetails (e.g., mug, alien), realism (e.g., car, storm), and more textured backgrounds (e.g., mug, car). Althoughs = 4 for pcs leads to the best results for class-conditioned generation, we see that the pcs in text-to-imagetask tends to over-simplify content, produce fuzzy images (e.g., mug) and deconstructed composition. Thishighlights our argument that optimal parameters do not generalize across datasets or tasks.12Published in Transactions on Machine Learning Research (12/2024)Rectified flow model. Recent advancements in Rectified Flow (RF) (Liu et al., 2022) improve generativemodels with straighter and shorter generation trajectories. In this section, we show the performance of guid-ance schedulers on RF-based methods e.g. Stable Diffusion 3 (SD3). Our experiments involved calculatingthe FID and CS on the COCO dataset, similar to all previous experiments. The results, shown in Figure 13a,reveal three key findings: (1) dynamically increasing schedulers can further enhance the generation capabil-ities of RF methods, e.g. SD3 (∼ 6 FID (gain more than ∼ 20%) at the same CS); (2) applying clampingleads to additional improvements; and (3) the optimal hyperparameters (c = 0.5) does not generalize toother models (SD1.5, SDXl), which is in line with our central hypothesis. The qualitative results presented inFigure 13b and in appendix Figure 24 also demonstrate that the guidance scheduler and clamping methodsenhance the details (castle, ship), tone (night and sea), and composition (fishes) of the generation.Text-to-text Image Translation To show the generalization of schedulers other than text- or label-conditioned tasks, we experiment with image-to-image translation on SD1.5 (Rombach et al., 2022), withdetails in Appendix A. Results show improved FID-CS balance for linear schedulers, with no clamping beingoptimal. This differs from the SD1.5 T2I task, showing that the optimal parameter does not generalize.0.265 0.270 0.275CLIP-Score1820222426283032FIDbaselinelinearclamp-linear (c=0.5)clamp-linear (c=1)w=7 for SD3(a) SD3: clamp-linear (b) SD3: Qualitative ResultsFigure 13: Text-to-image performance and qualitative results on SD3. We show that (a) both linearand clamp-linear guidance schedulers enhance the balance between FID and CLIP score (CS), and (b) thegenerated images exhibit improved detail and higher fidelity.6.3 Findings with parametrized schedulersOur observations are: (a) tuning the parametrized functions improves the performance for both generationtasks, (b) tuning clamps seems easier than pcs family, as its performance shows fewer variations, and (c) theoptimal parameters for one method does not generalize across different settings. Thus, specialized tuning isrequired for each model and task, leading to extensive grid searches and increased computational load.Failure cases. The main risk with parameterized functions arises from ill-chosen parameters. As shownin Figures 8, 12 20, poorly chosen parameters, particularly for pcs family, often lead to overshooting at thelater stages, resulting in fuzzy detail, broken structure and unnatural colour in the generated images.7 ConclusionWe analyzed dynamic schedulers for the weight parameter in Classifier-Free Guidance by systematicallycomparing heuristic and parameterized schedulers. We experiment on two tasks (class-conditioned generationand text-to-image generation), several models (DDPM, SD1.5 and SDXL) and various datasets.13Published in Transactions on Machine Learning Research (12/2024)Discussion. Our findings are: (1) a simple monotonically increasing scheduler systematically improves theperformance compared to a constant static guidance, at no extra computational cost and with no hyper-parameter search. (2) parameterized schedulers with tuned parameters per task, model and dataset, improvethe results. They, however, do not generalize well to other models and datasets as there is no universalparameter that suits all tasks.For practitioners who target state-of-the-art performances, we recommend searching or optimizing for thebest clamping parameter. For those not willing to manually tune parameters per case, we suggest usingheuristics, specifically linear or cosine.8 AcknowledgmentThis work was supported by ANR APATE ANR-22-CE39-0016, ANR TOSAI ANR-20-IADJ-0009, Hi!Parisgrant and fellowship, and by the European Union’s Horizon 2020 research and innovation programme underthe Marie Curie grant agreement No 860768 (CLIPE project). It was granted access to the High-PerformanceComputing (HPC) resources of IDRIS under the allocations 2024-AD011014300R1 made by GENCI.ReferencesOmri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images.In IEEE Conf. Comput. Vis. Pattern Recog., 2022.Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, TimoAila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble ofexpert denoisers. arXiv preprint arXiv:2211.01324, 2022.Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz,Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent videodiffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural imagesynthesis. In Int. Conf. Learn. Represent., 2018.Angela Castillo, Jonas Kohler, Juan C Pérez, Juan Pablo Pérez, Albert Pumarola, Bernard Ghanem, PabloArbeláez, and Ali Thabet. Adaptive guidance: Training-free acceleration of conditional diffusion models.arXiv preprint arXiv:2312.12487, 2023.Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, José Lezama, Lu Jiang, Ming-Hsuan Yang, KevinMurphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via maskedgenerative transformers. In Int. Conf. on Machine Learning, 2023.Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commandsvia motion diffusion in latent space. In IEEE Conf. Comput. Vis. Pattern Recog., 2023.Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Adv. Neural Inform.Process. Syst., 2021.Anh-Dung Dinh, Daochang Liu, and Chang Xu. Pixelasparam: A gradient view on diffusion sampling withguidance. In Int. Conf. on Machine Learning. PMLR, 2023a.Anh-Dung Dinh, Daochang Liu, and Chang Xu. Rethinking conditional diffusion sampling with progressiveguidance. In Adv. Neural Inform. Process. Syst., 2023b.Chris Donahue, Julian McAuley, and Miller Puckette. Adversarial audio synthesis. In Int. Conf. Learn.Represent., 2018.Nicolas Dufour, David Picard, and Vicky Kalogeiton. Scam! transferring humans between images withsemantic cross attention modulation. In Eur. Conf. Comput. Vis. Springer, 2022.14Published in Transactions on Machine Learning Research (12/2024)Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strongimage synthesizer. In Int. Conf. Comput. Vis., 2023.Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, AaronCourville, and Yoshua Bengio. Generative adversarial nets. In Adv. Neural Inform. Process. Syst., 2014.Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS-W on Deep GenerativeModels and Downstream Applications, 2021.Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Adv. Neural Inform.Process. Syst., 2020.Nandakishore Kambhatla and Todd K Leen. Dimension reduction by local principal component analysis.Neural computation, 1997.Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park.Scaling up gans for text-to-image synthesis. In IEEE Conf. Comput. Vis. Pattern Recog., 2023a.Minki Kang, Dongchan Min, and Sung Ju Hwang. Grad-stylespeech: Any-speaker adaptive text-to-speechsynthesis with diffusion models. In IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 2023b.Diederik P Kingma and Max Welling. Auto-encoding variational bayes. stat, 2014.Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applyingguidance in a limited interval improves sample and distribution quality in diffusion models. arXiv preprintarXiv:2404.07724, 2024.Alexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion modelis secretly a zero-shot classifier. In Int. Conf. Comput. Vis., 2023.Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Eur. Conf. on Comput. Vis.Springer, 2014.Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transferdata with rectified flow. arXiv preprint arXiv:2209.03003, 2022.Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solverfor diffusion probabilistic model sampling in around 10 steps. Adv. Neural Inform. Process. Syst., 2022a.Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solverfor guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022b.Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, JingrenZhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. InIEEE Conf. Comput. Vis. Pattern Recog., 2023.Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Generating images fromcaptions with attention. arXiv preprint arXiv:1511.02793, 2015.Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In Int.Conf. on Machine Learning, 2021.Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew,Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In Int. Conf. on Machine Learning. PMLR, 2022.Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, PierreFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visualfeatures without supervision. arXiv preprint arXiv:2304.07193, 2023.15Published in Transactions on Machine Learning Research (12/2024)Pablo Pernias, Dominic Rampas, and Marc Aubreville. Wuerstchen: Efficient pretraining of text-to-imagemodels. arXiv preprint arXiv:2306.00637, 2023.Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna,and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXivpreprint arXiv:2307.01952, 2023.Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In Int. Conf. on Machine Learning. PMLR, 2021.Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Pranav Shyam, Pamela Mishkin, Bob McGrew, andIlya Sutskever. Hierarchical text-conditional image generation with clip latents. arXiv preprintarXiv:2204.06125, 2022.Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generativeadversarial text to image synthesis. In Int. Conf. on Machine Learning. PMLR, 2016.Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolutionimage synthesis with latent diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., 2022.Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In IEEE Conf. Comput.Vis. Pattern Recog., 2023.Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, KamyarGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Adv. Neural Inform. Process. Syst., 2022.Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scaledataset for training next generation image-text models. Adv. Neural Inform. Process. Syst., 2022.Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Int. Conf. Learn.Represent., 2020.Wentian Zhang, Haozhe Liu, Jinheng Xie, Francesco Faccio, Mike Zheng Shou, and Jürgen Schmidhu-ber. Cross-attention makes inference cumbersome in text-to-image diffusion models. arXiv preprintarXiv:2404.02747, 2024.Guangcong Zheng, Shengming Li, Hui Wang, Taiping Yao, Yang Chen, Shouhong Ding, and Xi Li. Entropy-driven sampling and training scheme for conditional diffusion generation. In Eur. Conf. on Comput. Vis.Springer, 2022.AppendixIn this appendix, we provide additional content covering: (A) an extra experiment about the task of image-to-image translation; (B) an ablation study to demonstrate the necessity of CFG at all time intervals;(C) a toy example to explain the mechanism and rationale of the dynamic weighted scheduler; (D) anadditional comparison of parameterized function-based dynamic schedulers; (E) more qualitative results;(F) ablation experiments on different aspects of dynamic weighting schedulers; (G) a list of tables of allresults demonstrated; (H) detailed design of user study. Following is the table of contents(A) Image-to-image Translation Task(B) The necessity of CFG at all time interval(C) A toy example of fidelity vs condition adherence16Published in Transactions on Machine Learning Research (12/2024)(D) Comparison of Parameterized Schedulers(E) Qualitative Results(F) Ablation on Robustness and Generalization(G) Detailed Table of Experiments(H) User StudyA Image-to-image Translation TaskIn addition to the image generation task, we also evaluated the image-to-image translation task to demon-strate the generalization capabilities of the dynamic guidance scheduler across multiple conditioning scenar-ios. The experimental setup closely follows the one outlined in the main manuscript (Section 6), using theSD1.5 backbone model (Rombach et al., 2022) and images and their correspondent prompts from COCOdataset (Lin et al., 2014) to achieve image-to-image translation task. As shown in Figure 14a, the lineardynamic guidance scheduler significantly improves the FID vs. CLIP-Score trade-off (∼ 2 FID at the sameCLIP-Score) in image-to-image translation tasks. However, the optimal parameter for the parameterizedscheduler was found to involve clamping at guidance level c = 0, which differs from the optimal parametersidentified in the generation task (c = 2). This further supports our primary claim that a generalized param-eterized scheduler does not exist across different tasks. The qualitative results in Figure 14b also showedbetter structure (car), details (bird background) and prompt understanding (graffiti in the bin).0.268 0.270 0.272 0.274 0.276CLIP-Score101112131415161718FIDbaselineclamp-linear (c=0)clamp-linear (c=2)clamp-linear (c=4)w=8 for I2I_SD1.5(a) Image-to-image: clamp-linear (b) Image-to-image: Qualitative ResultsFigure 14: Image-to-image performance and qualitative results on SD1.5. We show that (a) bothlinear and clamp-linear guidance schedulers enhance the balance between FID and CLIP score (CS) of theimage-to-image translation task, and (b) the generated images exhibit improved detail and higher fidelity.B The necessity of CFG at all time intervalRecent concurrent work Kynkäänniemi et al. (2024); Zhang et al. (2024); Castillo et al. (2023) has suggestedthat partially removing the classifier-free guidance (CFG) (beginning or ending) could enhance generationperformance or achieve the acceleration with minimal performance influence. For instance, Kynkäänniemiet al. (2024) proposes removing the initial and final timesteps of CFG, retaining only a middle interval forthe guidance process. In this section, we conduct two ablation studies on SD1.5 to confirm that the CFGcan be required across all time intervals.17Published in Transactions on Machine Learning Research (12/2024)0 500 1000Timestep0.02.55.07.510.012.515.0clamp-linearω(t)(constantω=8) clamp-linear (c=1)clamp-linear (c=2)clamp-linear (c=3)clamp-linear (c=4)0 500 1000Timestep0.02.55.07.510.012.515.0remove-linearω(t)(constantω=8) remove-linear (z=1)remove-linear (z=1)remove-linear (z=1)remove-linear (z=1)0.255 0.260 0.265 0.270 0.275 0.280CLIP-Score12141618FIDlinearbaselineclamp-linear (c=1)clamp-linear (c=2)clamp-linear (c=3)remove-linear (z=1)remove-linear (z=2)remove-linear (z=3)Figure 15: Comparison between guidance removal (labelled as z) and clamping (labelled as c)on SD1.5. We see that CFG removal scheme shows improvement compared to the static guidance baseline(black) but is worse than the linear guidance scheduler (blue solid) and clamping schemes (red dotted).The necessity of CFG at the beginning stage As demonstrated in Figure 6b, we explore the impactof negative perturbation and ablation of all heuristic functions in Figure 12. Generally, employing a lowerguidance level in the initial stages can enhance performance compared to static guidance. To analyse theeffectiveness of guidance removal (setting to zero), static guidance, the linear scheduler, and the clampingscheme, we conducted experiments on SD1.5. Guidance is removed at the same timestep as the clampingtransition point; rather than clamping guidance to a hyperparameter constant, we reduce it completely tozero (Figure 15 left two panels). The results, depicted in Figure 15 right panel, show that while guidanceremoval at the beginning stage (red dotted curve) indeed improves performance compared to the staticbaseline (black solid curve), both the linear scheduler (blue solid curve) and clamping schemes (green dottedlines) achieve better balances of FID vs. CLIP-Score (CS).The necessity of CFG at the ending stage Zhang et al. (2024); Castillo et al. (2023) suggest thatremoving the final stage of guidance could accelerate generation by directly replacing the CFG with condi-tional or unconditional outputs. However, as shown in Figure 6b(b), our analysis indicates that removingthis stage can reduce performance for specific tasks. Despite this, the possibility of safely removing theending stage guidance does not contradict our argument that enhancing the end could improve per-formance. To further confirm this, we conducted an ablation experiment comparing the effects of removingversus boosting the final guidance intervals. In this experiment, 10%, 20%, and 30% of the ending guidancewere either removed or increased by a factor of 1.5. The results, presented in Table 1, reveal: (i) removingor boosting the ending guidance has a marginal impact on the CLIP-Score; (ii) elimination of guidance canlead to a regression in performance; and (iii) boosting the guidance can significantly enhance FID, with gainsof 0.54 and 0.8 in FID when boosting the final 30% of guidance by 1.5×.In conclusion, based on the results from two previous ablation studies, we confirm that an adequatelevel of guidance is necessary at all intervals of the generation process. While removing parts ofthe guidance can accelerate the process, it results in underperformance when compared to our analyzedheuristic monotonically increasing guidance scheduler, such as linear, and also when compared to well-tunedparameterized functions, such as the clamping method.C A toy example of fidelity vs condition adherenceKnowing the equation of CFG can be written as a combination between a generation term and a guidanceterm, with the second term controlled by guidance weight ω:ϵ̂θ(xt, c) = ϵθ(xt, c) + ω (ϵθ(xt, c) − ϵθ(xt)) . (7)To better understand the problems in diffusion guidance, we present a toy example, where we first train adiffusion model on a synthetic dataset of 50, 000 images (32 × 32) from two distinct Gaussian distributions:18Published in Transactions on Machine Learning Research (12/2024)Table 1: Impact of removing/boosting CFG at the end with SD1.5.Guidance scale ω=8 ω=11Method Clip-Score FID Clip-Score FIDstatic 0.2775 16.78 0.2792 19.03remove final 10% 0.2777 17.61 0.2792 19.94remove final 20% 0.2777 18.33 0.2792 20.68remove final 30% 0.2777 19.18 0.2792 21.65boost (1.5×) final 10% 0.2773 16.75 0.2789 18.39boost (1.5×) final 20% 0.2772 16.51 0.2787 18.96boost (1.5×) final 30% 0.2772 16.24 0.2790 18.23Figure 16: Two-Gaussians Example. We employ DDPM with CFG to fit two Gaussian distributions, abright one (red) and a darker one (blue). The middle panel showcases samples of generation trajectories atdifferent guidance scales ω, using PCA visualization. Increasing guidance scale ω raises two issues: repeatedtrajectory: when ω=50 the generation diverges from its expected direction before converging again, andshaky motion: when ω=100 some trajectories wander aimlessly.one sampled with low values of intensity (dark noisy images in the bottom-left of Figure 16), and the otherwith high-intensities (bright noisy images). The top-left part in Figure 16 shows the PCA (Kambhatla &Leen, 1997)-visualised distribution of the two sets, and the bottom-left part shows some ground-truth images.To fit these two labelled distributions, we employ DDPM (Ho et al., 2020) with CFG (Ho & Salimans, 2021)conditioned on intensity labels.Upon completion of the training, we can adjust the guidance scale ω to balance between the sample fidelityand condition adherence, illustrated in the right part of Figure 16. The first row depicts the variations ingenerated distributions on different ω (from 0 to 100), visualized by the same PCA parameters. The secondrow shows the entire diffusion trajectory for sampled data points (same seeds across different ω): progressingfrom a random sample (i.e., standard Gaussian) when t = T to the generated data (blue or red in Figure 16)when t = 0.Emerging issues and explainable factors. As ω increases, the two generated distributions diverge dueto guidance term in Eq. 7 shifting the generation towards different labels at a fidelity cost (see Figure 16first row).As shown in Figure 16 (second row), two issues arise: (i) repeated trajectories that diverge from the expectedconvergence path before redirecting to it; and (ii) shaky motions that wander along the trajectory.19Published in Transactions on Machine Learning Research (12/2024)9.6 9.7 9.8 9.9Inception Score2.82.93.03.13.2FIDbaseline (static)linearclamp-linear (c=1.005)clamp-linear (c=1.01)clamp-linear (c=1.015)9.6 9.7 9.8 9.9Inception Score2.82.93.03.13.2FIDFID vs IS CIFARbaseline (static)cosineclamp-cosine (c=1.005)clamp-cosine (c=1.01)clamp-cosine (c=1.015)9.6 9.7 9.8 9.9Inception Score2.72.82.93.03.13.23.3FIDbaseline (static)pcs (s=0.1)pcs (s=1)pcs (s=2)pcs (s=4)200 250 300Inception Score3456789FIDbaseline (static)linearclamp-linear (c=1.005)clamp-linear (c=1.1)clamp-linear (c=1.3)200 250 300Inception Score3456789FIDFID vs IS CIN-256baseline (static)cosineclamp-cos (c=1.005)clamp-cos (c=1.1)clamp-cos (c=1.3)150 200 250 300Inception Score3456789FIDbaseline (static)pcs (s=0.1)pcs (s=1)pcs (s=2)pcs (s=4)Figure 17: Class-conditioned image generation results of two parameterized families (clamp-linear, clamp-cosine and pcs) on CIFAR-10 and CIN-256. Optimising parameters of guidanceresults in performance gains, however, these parameters do not generalize across models and datasets.These two issues can be attributed to two factors: (1) incorrect classification prediction, and (2) the conflictsbetween guidance and generation terms in Eq. 7. For the former, optimal guidance requires a flawlessclassifier, whether explicit for CG or implicit for CFG. In reality, discerning between two noisy data ischallenging and incorrect classification may steer the generation in the wrong direction, generating shakytrajectories. A similar observation is reported in Zheng et al. (2022); Dinh et al. (2023b) for CG and in Liet al. (2023) for CFG. For the latter, due to the strong incentive of the classifier to increase the distance withrespect to the other classes, trajectories often show a U-turn before gravitating to convergence (repeatedtrajectory in Figure 16). We argue that this anomaly is due to the conflict between guidance and generationterms in Eq. 7.In conclusion, along the generation, the guidance can steer suboptimally (especially when t → T ), and evenimpede generation. We argue that these erratic behaviours contribute to the performance dichotomybetween fidelity and condition adherence (Ho & Salimans, 2021; Dhariwal & Nichol, 2021).D Comparison of Parameterized SchedulersD.1 Parameterized Comparison on Class-Conditioned GenerationFor CIFAR-10-DDPM, we show in Figure 17 upper panels (see all data in Table 5, 6, 7) the comparison oftwo parameterized functions families: (i) clamp family on linear and cosine and (ii) pcs family mentioned inGao et al. (2023).The ImageNet-256 and Latent Diffusion Model (LDM) results are presented in Figure 17 lower panels and(data in Table 9, 10, 11).20Published in Transactions on Machine Learning Research (12/2024)0 250 500 750 1000Timestep51015scaledω(t)(constantω=7.5)0.260 0.265 0.270 0.275 0.280CLIP-Score1214161820FIDbaselinelinearclamp-linear (c=1)clamp-linear (c=2)clamp-linear (c=3)clamp-linear (c=4)w=7.5 SD1.50.26 0.27 0.28CLIP-Score1.001.051.101.151.201.25Diversity(a) clamp-linear0 250 500 750 1000Timestep51015scaledω(t)(constantω=7.5)0.255 0.260 0.265 0.270 0.275 0.280CLIP-Score1214161820FIDbaseline [38]cosclamp-cosine (c=1)clamp-cosine (c=2)clamp-cosine (c=3)clamp-cosine (c=4)w=7.5 SD1.50.26 0.27 0.28CLIP-Score1.001.051.101.151.201.25Diversity(b) clamp-cosine0 250 500 750 1000Timestep010203040scaledω(t)(constantω=7.5)0.245 0.250 0.255 0.260 0.265 0.270 0.275 0.280CLIP-Score12.515.017.520.022.5FIDbaselinepcs (s=1)pcs (s=4)pcs (s=2)pcs (s=0.1)w=7.5 SD1.50.25 0.26 0.27 0.28CLIP-Score1.01.11.21.3Diversity(c) pcs FamilyFigure 18: Text-to-image generation FID and diversity of all two parameterized families (clampwith clamp-linear, clamp-cosine and pcs) on SD1.5 (left to right): (a) parameterized schedulercurves; (b) FID vs. CS of SD1.5 and (c) FID vs. Div. of SD1.5. We show that in terms of diversity,the clamp family still achieves more diverse results than the baseline, though it reduces along the clampingparameter, as the beginning stage of the diffusion is muted.The conclusion of these parts is as follows: (i) optimising both groups of parameterized function helpsimprove the performance of FID-CS; (ii) the optimal parameters for different models are very different andfail to generalize across models and datasets.D.2 Parameterized Comparison on Text-to-image GenerationWe then show the FID vs. CS and Diversity vs. CS performance of the parameterized method in Figure 18.The conclusion is coherent with the main paper: all parameterized functions can enhance performance onboth FID and diversity, provided that the parameters are well-selected. Moreover, for the clamp family,it appears that the clamp parameter also adjusts the degree of diversity of the generated images; loweringthe clamp parameter increases the diversity. We recommend that users tune this parameter according tothe specific model and task. For SDXL, the clamp-cosine is shown in Figure 19, and also reaches a similarconclusion.21Published in Transactions on Machine Learning Research (12/2024)0.265 0.270 0.275 0.280 0.285CLIP-Score20253035404550FIDFID vs CS clamp-linearlinearbaselineclamp linear (c=2)clamp linear (c=4)clamp linear (c=6)w=8 for SDXL0.265 0.270 0.275 0.280 0.285CLIP-Score20253035404550FIDFID vs CS clamp-cosinecosinebaselineclamp cos (c=2)clamp cos (c=4)clamp cos (c=6)w=8 for SDXL0.24 0.25 0.26 0.27 0.28CLIP-Score20253035404550FIDFID vs CS pcs familypcs (s=1)baselinepcs (s=4)pcs (s=2)pcs (s=0.1)w=8 for SDXLFigure 19: Text-to-image generation results of two parameterized families (clamp-linear, clamp-cosine and pcs) on SDXL. Both clamps reach their best FID-CS at c = 4 vs s = 0.1 for pcs, which differfrom the optimal parameters for SD1.5.E Qualitative ResultsMore Results of Parameterized Functions on SDXL In Figure 20, we show more examples of differentparameterized functions. It appears that carefully selecting the parameter (c = 4), especially for the clamp-linear method, achieves improvement in image quality in terms of composition (e.g., template), detail (e.g.,cat), and realism (e.g., dog statue). However, for SDXL, this method shows only marginal improvementswith the pcs family, which tends to produce images with incorrect structures and compositions, leading tofuzzy images.Figure 20: Qualitative comparison clamp vs. pcs family, we see clearly that clamping at c = 4 gives the bestvisual qualitative results.Stable Diffusion v1.5. Figure 21 shows qualitative results of using increasing shaped methods: linear,cosine compared against the baseline. It shows clearly that the increasingly shaped heuristic guidancegenerates more diversity and the baseline suffers from a collapsing problem, i.e., different sampling of thesame prompt seems only to generate similar results. In some figures, e.g., Figure 21 with an example of22Published in Transactions on Machine Learning Research (12/2024)Table 2: Ablation on sampling steps DDIM. Experiment on CIN-256 and Latent Diffusion Modelbaseline (static) linear cosinesteps FID↓ IS↑ FID↓ IS↑ FID↓ IS↑50 3.393 220.6 3.090 225.0 2.985 252.4100 3.216 229.8 2.817 225.2 2.818 255.3200 3.222 229.5 2.791 223.2 2.801 254.3the mailbox, we can see that the baseline ignores graffiti and increasing heuristic guidance methods cancorrectly retrieve this information and illustrate it in the generated images. We also see in M&M’s thatheuristic guidance methods show more diversity in terms of colour and materials. with much richer varianceand image composition. However some negative examples can also be found in Figure 21, in particular, thefoot of horses in the prompt: a person riding a horse while the sun sets. We posit the reason for theseartefacts is due to the overmuting of the initial stage and overshooting the final stage during the generation,which can be rectified by the clamping method.SDXL. The SDXL (Podell et al., 2023) shows better diversity and image quality comparing to its precedent.Whereas some repetitive concepts are still present in the generated results: see Figure 22, that first row \"Asingle horse leaning against a wooden fence\" the baseline method generate only brown horses whereas allheuristic methods give a variety of horse colours. A similar repetitive concept can also be found in the \"Aperson stands on water skies in the water\" with the color of the character. For the spatial combinationdiversity, please refer to the example in Figure 23: \"A cobble stone courtyard surrounded by buildings andclock tower.\" where we see that heuristic methods yield more view angle and spatial composition. Similarbehaviour can be found in the example of \"bowl shelf\" in Figure 22 and \"teddy bear\" in Figure 22.SD3. In SD3 (see Figure 24), we demonstrate that the schedulers enhance the details (e.g., books, street,lake surface), improve tone and chromatic performance (e.g., street and sunset), and lead to a better under-standing of the prompt (e.g., giant clock).F Ablation on Robustness and GeneralizationDifferent DDIM steps. DDIM sampler allows for accelerated sampling (e.g., 50 steps as opposed to1000) with only a marginal compromise in generation performance. In this ablation study, we evaluate theeffectiveness of our dynamic weighting schedulers across different sampling steps. We use the CIN256-LDMcodebase, with the same configuration as our prior experiments of class-conditioned generation. We conducttests with 50, 100, and 200 steps, for baseline and two heuristics (linear and cosine), all operating at theiroptimal guidance scale in Tab 8. The results, FID vs. IS for each sampling step, are presented in Tab. 2.We observe that the performance of dynamic weighting schedulers remains stable across different timesteps.Different Solvers. To validate the generalizability of our proposed method beyond the DDIM (Songet al., 2020) sampler used in the experiment Section, we further evaluated its performance using the moreadvanced DPM-Solver (Lu et al., 2022a) sampler (3rd order). This sampler is capable of facilitating diffusiongeneration with fewer steps and enhanced efficiency compared to DDIM. The experiment setup is similar tothe text-to-image generation approach using Stable Diffusion (Rombach et al., 2022) v1.5. The results ofthis experiment are reported in Table 3 and visually illustrated in Figure 25.As depicted in Figure 25: our proposed methods continue to outperform the baseline (static guidance)approach. Substantial improvements are seen in both FID and CLIP-Score metrics, compared to baseline(w=7.5) for example. Notably, these gains become more pronounced as the guidance weight increases, atrend that remains consistent with all other experiments observed across the paper.Diversity Diversity plays a pivotal role in textual-based generation tasks. Given similar text-image match-ing levels (usually indicated by CLIP-Score), higher diversity gives users more choices of generated content.Most applications require higher diversity to prevent the undesirable phenomenon of content collapsing,23Published in Transactions on Machine Learning Research (12/2024)Figure 21: Qualitative SD1.524Published in Transactions on Machine Learning Research (12/2024)Figure 22: Qualitative SDXL (1)25Published in Transactions on Machine Learning Research (12/2024)Figure 23: Qualitative SDXL (2)26Published in Transactions on Machine Learning Research (12/2024)Figure 24: Qualitative SD327Published in Transactions on Machine Learning Research (12/2024)0.255 0.260 0.265 0.270 0.275 0.280 0.285CLIP-Score101214161820222426FIDFID vs CSbaselinecosinelinearw=7.5 for SD1.5, Equal ClipScore Linew=7.5 for SD1.5, Equal FID lineFigure 25: FID vs. CLIP-Score generated by SD1.5 (Rombach et al., 2022) with DPM-Solver (Lu et al.,2022a)Table 3: Experiment of FID and CLIP-Score generated by Stable Diffusion v1.5 with DPM-Solver Lu et al.(2022a)w 1 3 5 7 9 11 13 15 20clip-score 0.2287 0.2692 0.2746 0.2767 0.2782 0.2791 0.2797 0.2802 0.2805baseline(static) FID 28.188 10.843 13.696 16.232 17.933 19.136 19.930 20.538 21.709clip-score 0.2287 0.2646 0.2713 0.2743 0.2762 0.2774 0.2785 0.2792 0.2813linear FID 28.188 13.032 11.826 12.181 12.830 13.461 13.984 14.541 15.943clip-score 0.2287 0.2643 0.2712 0.2741 0.2762 0.2778 0.2789 0.2797 0.2812cosine FID 28.188 12.587 11.810 12.400 13.197 13.968 14.717 15.366 16.90128Published in Transactions on Machine Learning Research (12/2024)where multiple samplings of the same prompt yield nearly identical or very similar results. We utilize thestandard deviation within the image embedding space as a measure of diversity. This metric can be derivedusing models such as Dino-v2 Oquab et al. (2023) or CLIP Radford et al. (2021). Figure 26 provides a side-by-side comparison of diversities computed using both Dino-v2 and CLIP, numerical results are also reportedin Table. 16. It is evident that Dino-v2 yields more discriminative results compared to the CLIP embedding.While both embeddings exhibit similar trends, we notice that CLIP occasionally produces a narrower gapbetween long captions (-L) and short captions (-S). In some instances, as depicted in Figure 26, CLIP evenreverses the order, an observation not apparent with the Dino-v2 model. In both cases, our methods areconsistently outperforming the baseline on both metrics.0.255 0.260 0.265 0.270 0.275 0.280CLIP score0.260.280.300.32FIDCLIP score vs CLIP-based Diversity for SDbaseline (static)-Sbaseline (static)-Llinear-Slinear-Lcos-Scos-L0.255 0.260 0.265 0.270 0.275 0.280CLIP score1.01.11.2DivCLIP score vs Dinov2-based Diversity for SDFigure 26: Experiment on Stable Diffusion on two types of diversity. Zero-shot COCO 10k CLIP-Score vs. Diversity computed by CLIP and Dino-v2 respectively.G Detailed Table of ExperimentsIn this section, we show detailed tables of all experiments relevant to the paper:• CIFAR-10-DDPM: results of different shapes of heuristics (Table 4), results of parameterizedmethods (Table 5, Table 6, Table 7)• CIN (ImageNet) 256-LDM: results of different shapes of heuristics (Table 8) and results ofparameterized methods (Table 9, Table 9, Table 11)• Stable Diffusion 1.5: results of different shapes of heuristics in Table 12 and results of parame-terized methods in Table 13.• Stable Diffusion XL: results of different shapes of heuristics in Table 14 and results of parame-terized methods in Table 15.Table 4: Experiment of different Heuristics on CIFAR-10 DDPM. We evaluate the FID and ISresults for the baseline, all heuristic methods (green for increasing, red for decreasing and purple for non-linear) of 50K images. Best FID and IS are highlighted. We see clearly that the increasing shapes outperformall the others.baseline (static) linear cos invlinear sin Λ-shape V-shapeGuidance Scale FID IS FID IS FID IS FID IS FID IS FID IS FID IS1.10 2.966 9.564 2.893 9.595 2.875 9.606 3.033 9.554 3.068 9.550 3.017 9.615 3.005 9.5501.15 2.947 9.645 2.853 9.666 2.824 9.670 3.050 9.628 3.086 9.612 3.040 9.698 2.954 9.5961.20 2.971 9.690 2.854 9.729 2.813 9.726 3.106 9.643 3.149 9.645 3.119 9.738 2.928 9.6441.25 3.025 9.733 2.897 9.799 2.850 9.794 3.192 9.675 3.261 9.660 3.251 9.746 2.930 9.6771.30 3.111 9.764 2.968 9.833 2.933 9.838 3.311 9.689 3.389 9.664 3.407 9.774 2.951 9.7251.35 3.233 9.787 3.062 9.872 3.026 9.882 3.460 9.700 3.543 9.678 3.606 9.804 2.985 9.763H User StudyIn this section, we elaborate on the specifics of our user study setup corresponding to Figure 3. (b) in ourmain manuscript.29Published in Transactions on Machine Learning Research (12/2024)Table 5: Experiment of clamp-linear on CIFAR-10 DDPM. We evaluate the FID and IS results forthe baseline, parameterized method as clamp-linear of 50K images FID. Best FID and IS are highlighted,the optimal parameter seems at c = 1.1.baseline (static) linear linear (c=1.05) linear (c=1.1) linear (c=1.15)Guidance Scale FID IS FID IS FID IS FID IS FID IS1.10 2.966 9.564 2.893 9.595 2.852 9.622 2.856 9.638 2.876 9.6471.15 2.947 9.645 2.853 9.666 2.816 9.693 2.793 9.696 2.832 9.6931.20 2.971 9.690 2.854 9.729 2.822 9.757 2.820 9.755 2.834 9.7501.25 3.025 9.733 2.897 9.799 2.863 9.809 2.863 9.809 2.863 9.8091.30 3.111 9.764 2.968 9.833 2.929 9.870 2.922 9.863 2.929 9.8671.35 3.233 9.787 3.062 9.872 3.025 9.913 3.021 9.910 3.018 9.908Table 6: Experiment of clamp-cosine on CIFAR-10 DDPM. We evaluate the FID and IS results forthe baseline, parameterized method as clamping on the cosine increasing heuristic (clamp-cosine) of 50Kimages. Best FID and IS are highlighted. It sees the optimising clamping parameter helps to improve theFID-IS performance, the optimal parameter seems at c = 1.05.baseline (static) cos cos (c=1.05) cos (c=1.1) cos (c=1.15)Guidance Scale FID IS FID IS FID IS FID IS FID IS1.10 2.966 9.564 2.875 9.606 2.824 9.632 2.839 9.651 2.963 9.6331.15 2.947 9.645 2.824 9.670 2.781 9.712 2.794 9.710 2.917 9.6891.20 2.971 9.690 2.813 9.726 2.771 9.781 2.786 9.774 2.901 9.7531.25 3.025 9.733 2.850 9.794 2.810 9.828 2.819 9.823 2.913 9.8211.30 3.111 9.764 2.933 9.838 2.880 9.884 2.888 9.885 2.976 9.8651.35 3.233 9.787 3.026 9.882 2.963 9.933 2.969 9.941 3.052 9.923Table 7: Experiment of pcs family on CIFAR-10 DDPM. We evaluate the FID and IS results forthe baseline, parameterized pcs method of 50K image FID. Best FID and IS are highlighted. It sees theoptimising clamping parameter helps to improve the FID-IS performance, the optimal parameter seems ats = 4.baseline (static) pcs (s=4) pcs (s=2) pcs (s=1) pcs (s=0.1)Guidance Scale FID IS FID IS FID IS FID IS FID IS1.10 2.966 9.564 2.920 9.600 2.969 9.614 2.875 9.606 3.010 9.5721.15 2.947 9.645 2.818 9.663 2.886 9.670 2.824 9.670 2.983 9.6571.20 2.971 9.690 2.748 9.726 2.844 9.729 2.813 9.726 3.010 9.7061.25 3.025 9.733 2.714 9.782 2.839 9.782 2.850 9.794 3.065 9.7331.30 3.111 9.764 2.700 9.834 2.858 9.847 2.933 9.838 3.157 9.7701.35 3.233 9.787 2.711 9.885 2.902 9.889 3.026 9.882 3.276 9.786Table 8: Experiment of different Heuristics on CIN-256-LDM. We evaluate the FID and IS resultsfor the baseline, all heuristic methods (green for increasing, red for decreasing and purple for non-linear) of50K images FID. Best FID and IS are highlighted. We see clearly that the increasing shapes outperform allthe others.baseline linear cos invlinear sin Λ-shape V-shapeguidance FID IS FID IS FID IS FID IS FID IS FID IS FID IS1.4 4.117 181.2 4.136 178.3 4.311 175.4 4.323 180.7 4.405 180.2 3.444 207.8 6.118 146.21.6 3.393 225.0 3.090 220.6 3.083 216.2 3.974 222.7 4.176 221.7 3.694 256.5 4.450 176.81.8 3.940 260.8 3.143 257.5 2.985 252.4 4.797 257.3 5.087 254.8 4.922 294.9 3.763 206.12.0 5.072 291.4 3.858 288.9 3.459 283.3 6.085 284.2 6.398 281.2 6.517 324.8 3.806 232.22.2 6.404 315.8 4.888 315.1 4.256 310.1 7.517 306.9 7.835 303.4 8.164 346.2 4.293 255.72.4 8.950 335.9 6.032 336.5 5.215 331.2 8.978 325.5 9.291 321.3 9.664 362.9 5.051 277.0Table 9: Experiment of clamp-linear family on CIN-256-LDM. We evaluate the FID and IS resultsfor the baseline, parameterized clamp-linear on 50K images FID. Best FID and IS are highlighted. It sees theoptimising parameter helps to improve the FID-IS performance, the optimal parameter seems at c = 1.005.baseline linear linear (c=1.005) linear (c=1.1) linear (c=1.3)guidance FID IS FID IS FID IS FID IS FID IS1.4 4.12 181.2 4.14 178.3 4.16 177.8 4.18 178.1 3.95 184.61.6 3.39 225.0 3.09 220.6 3.06 219.6 3.13 219.2 3.14 222.71.8 3.94 260.8 3.14 257.5 3.18 255.9 3.18 257.2 3.24 259.02.0 5.07 291.4 3.86 288.9 3.88 287.0 3.86 288.7 3.92 289.62.2 6.40 315.8 4.89 315.1 4.91 312.4 4.87 313.8 4.92 314.92.4 8.95 335.9 6.03 336.5 6.00 334.3 5.97 336.8 6.01 337.230Published in Transactions on Machine Learning Research (12/2024)Table 10: Experiment of clamp-cosine family on CIN-256-LDM. We evaluate the FID and IS resultsfor the baseline, parameterized method of clamp-cosine method on 50K images. Best FID and IS are high-lighted. It sees the optimising parameter helps to improve the FID-IS performance, the optimal parameterseems at c = 1.005.baseline cosine cosine (c=1.005) cosine (c=1.1) cosine (c=1.3)guidance FID IS FID IS FID IS FID IS FID IS1.4 4.12 181.24 4.31 175.4 4.24 176.0 4.24 177.1 3.82 188.21.6 3.39 224.96 3.08 216.2 3.06 217.0 3.08 217.1 3.09 224.61.8 3.94 260.85 2.98 252.4 2.91 251.8 3.01 253.2 3.13 258.42.0 5.07 291.37 3.46 283.3 3.47 282.5 3.48 284.1 3.67 288.22.2 6.40 315.84 4.26 310.1 4.27 307.9 4.28 310.5 4.49 313.12.4 8.95 335.86 5.22 331.2 5.23 329.7 5.24 331.3 5.44 334.1Table 11: Experiment of pcs family on CIN-256-LDM. We evaluate the FID and IS results for thebaseline, parameterized method of the pcs family of 50K images. Best FID and IS are highlighted. It sees theoptimising parameter helps to improve the FID-IS performance, the optimal parameter seems at s = 2 forFID. Interestingly, the pcs family presents a worse IS metric, than baseline and clamp-linear/cosine methods.baseline pcs (s=4) pcs (s=2) pcs (s=1) pcs (s=0.1)guidance FID IS FID IS FID IS FID IS FID IS1.4 4.12 181.24 6.94 144.98 6.10 150.49 4.31 175.40 4.09 181.001.6 3.39 224.96 5.69 162.99 4.27 180.52 3.08 216.21 3.43 225.311.8 3.94 260.85 4.80 179.71 3.29 208.86 2.98 252.37 3.96 264.032.0 5.07 291.37 4.18 195.75 2.88 234.09 3.46 283.32 5.08 294.772.2 6.40 315.84 3.73 210.60 2.81 257.22 4.26 310.14 6.44 319.972.4 8.95 335.86 3.457 224.4 2.98 278.14 5.22 331.17 7.85 339.05Table 12: Different Heuristic Modes of SD1.5, we show FID vs. CLIP-score of 10K images. we highlightdifferent range of clip-score by low (∼ 0.272), mid (∼ 0.277) and high (∼ 0.280) by pink, orange and bluecolors. We see that increasing modes demonstrate the best performance at high w, whereas decreasing modesregress on the performance. non-linear modes, especially Λ-shape also demonstrate improved performanceto baseline but worse than increasing shapes.w 2 4 6 8 10 12 14clip-score 0.2593 0.2719 0.2757 0.2775 0.2790 0.2796 0.2803baseline FID 11.745 11.887 14.639 16.777 18.419 19.528 20.462clip-score 0.2565 0.2697 0.2741 0.2763 0.2780 0.2788 0.2799linear FID 14.649 11.260 12.056 13.147 14.179 15.032 15.663clip-score 0.2553 0.2686 0.2728 0.2751 0.2770 0.2782 0.2793cos FID 15.725 11.846 12.009 12.796 13.629 14.282 15.058clip-score 0.261 0.272 0.2754 0.2773 0.2780 0.2787 0.2793sin FID 10.619 14.618 18.323 20.829 22.380 23.534 24.561clip-score 0.2608 0.2723 0.2757 0.2773 0.2781 0.2789 0.2793invlinear FID 10.649 14.192 17.810 20.206 21.877 22.962 24.128clip-score 0.2603 0.2719 0.2756 0.2774 0.2785 0.2794 0.2802Λ-shape FID 11.940 12.106 14.183 16.100 17.530 18.663 19.723clip-score 0.2569 0.2706 0.2747 0.2764 0.2773 0.2783 0.2789V-shap FID 11.790 12.407 15.912 18.220 19.796 20.992 21.90531Published in Transactions on Machine Learning Research (12/2024)Table 13: Different parameterized functions of SD1.5, we show FID vs. CLIP-score of 10K images.we highlight different range of clip-score by low (∼ 0.272), mid (∼ 0.277) and high (∼ 0.280) by pink, orangeand blue colors. We see that for the pcs family the optimal parameter is at s = 1, whereas for clamp-linearand clamp-cosine methods, they are at c = 2.w 2 4 6 8 10 12 14clip-score 0.2593 0.2719 0.2757 0.2775 0.2790 0.2796 0.2803baseline FID 11.745 11.887 14.639 16.777 18.419 19.528 20.462clip-score 0.2453 0.2582 0.2637 0.2668 0.2691 0.2706 0.2720pcs (s=4) FID 23.875 19.734 19.167 19.627 20.513 22.022 23.585clip-score 0.2591 0.2642 0.2691 0.2720 0.2740 0.2754 0.2766pcs (s=2) FID 18.026 14.414 13.503 13.652 14.175 14.806 15.480clip-score 0.2553 0.2686 0.2728 0.2751 0.2770 0.2782 0.2793pcs (s=1) FID 15.725 11.846 12.009 12.796 13.629 14.282 15.058clip-score 0.2507 0.2642 0.2755 0.2772 0.2785 0.2796 0.2800pcs (s=0.1) FID 19.532 14.414 14.770 16.901 18.312 19.349 20.271clip-score 0.2613 0.2705 0.2745 0.2766 0.2781 0.2790 0.2798linear (c=1) FID 11.4448 11.011 12.130 13.211 14.219 15.129 15.888clip-score 0.2679 0.2717 0.2751 0.2769 0.2783 0.2795 0.2800linear (c=2) FID 10.7382 11.169 12.168 13.211 14.166 14.946 16.041clip-score 0.2719 0.2732 0.2756 0.2771 0.2783 0.2798 0.2800linear (c=3) FID 12.1284 12.328 13.019 13.916 14.701 16.109 16.420clip-score 0.2742 0.2746 0.2761 0.2775 0.2786 0.2794 0.2802linear (c=4) FID 13.768 13.813 14.213 14.765 15.311 15.834 16.422clip-score 0.2618 0.2703 0.2740 0.2762 0.2775 0.2787 0.2795cos (c=1) FID 11.386 10.986 11.732 12.608 13.460 14.288 14.978clip-score 0.2682 0.2722 0.2751 0.2769 0.2780 0.2789 0.2800cos (c=2) FID 10.816 11.309 12.055 12.908 13.602 14.326 15.008clip-score 0.2719 0.2736 0.2757 0.2772 0.2792 0.2792 0.2800cos (c=3) FID 12.121 12.363 12.956 13.631 14.263 14.869 15.385clip-score 0.2742 0.2748 0.2764 0.2776 0.2786 0.2795 0.2802cos (c=4) FID 13.734 13.827 14.222 14.690 15.090 15.560 15.916Table 14: Different Heuristic Modes of SDXL, we show FID vs. CLIP-score of 10K images. we highlightdifferent range of clip-score by low (∼ 0.2770), mid (∼ 0.280) and high (∼ 0.2830) by pink, orange and bluecolors. We see that increasing modes demonstrate the best performance at high w, whereas decreasingmodes regress on the performance. non-linear modes, especially Λ-shape demonstrate improved performanceagainst baseline but regress fast when the ω is high.w 1 3 5 7 9 11 13 15 20clip-score 0.2248 0.2712 0.2767 0.2791 0.2806 0.2817 0.2826 0.2832 0.2836baseline FID 59.2480 24.3634 24.9296 25.7080 26.1654 27.2308 27.4628 28.0538 29.6868clip-score 0.2248 0.2653 0.2732 0.2773 0.2798 0.2810 0.2821 0.2828 0.2840linear FID 59.2480 29.0917 25.0276 24.4500 24.6705 25.1286 25.5488 25.8457 26.5993clip-score 0.2248 0.2621 0.2708 0.2751 0.2776 0.2794 0.2803 0.2817 0.2830cosine FID 59.2480 32.8264 27.0004 25.5468 25.4331 25.5244 25.7375 25.8758 26.8427clip-score 0.2248 0.2739 0.2783 0.2800 0.2814 0.2826 0.2823 0.2807 0.2730invlinear FID 59.2480 23.8196 25.4335 26.1458 27.8969 29.6194 31.8970 35.2600 47.8467clip-score 0.2248 0.2741 0.2786 0.2803 0.2816 0.2823 0.2816 0.2794 0.2713sin FID 59.2480 23.9147 25.4203 26.3137 28.1756 29.3571 30.5314 36.3049 51.6672clip-score 0.2248 0.2721 0.2782 0.2809 0.2826 0.2831 0.2837 0.2846 0.2849Λ-shape FID 59.2480 22.3927 24.0785 25.6845 26.7019 27.5095 28.2058 32.1870 34.9896clip-score 0.2248 0.2688 0.2747 0.2770 0.2785 0.2793 0.2795 0.2786 0.2736V-shape FID 59.2480 21.6560 22.7042 23.6659 24.0550 25.4073 26.2993 27.6580 35.293532Published in Transactions on Machine Learning Research (12/2024)Table 15: Different parameterized results in SDXL, we show FID vs. CLIP-Score of pcs family andclamp family of 10K images: pcs family records best performance at s = 0.1, clamp-linear and clamp-cosinestrategies all record best performance at c = 4.w 1 3 5 7 9 11 13 15 20clip-score 0.2248 0.2712 0.2767 0.2791 0.2806 0.2817 0.2826 0.2832 0.2836baseline FID 59.2480 24.3634 24.9296 25.7080 26.1654 27.2308 27.4628 28.0538 29.6868clip-score 0.2248 0.2336 0.2396 0.2440 0.2470 0.2494 0.2513 0.2527 0.2549pcs (s = 4) FID 59.2480 55.2402 52.0731 50.3335 48.9980 48.4516 48.0146 47.7025 48.9481clip-score 0.2248 0.2486 0.2581 0.2638 0.2673 0.2704 0.2722 0.2738 0.2765pcs (s = 2) FID 59.2480 35.2002 28.7500 24.8120 22.8518 21.7098 22.1061 23.0833 23.5282clip-score 0.2248 0.2621 0.2708 0.2751 0.2776 0.2794 0.2803 0.2817 0.2830pcs (s = 1) FID 59.2480 32.8264 27.0004 25.5468 25.4331 25.5244 25.7375 25.8758 26.8427clip-score 0.2248 0.2710 0.2769 0.2798 0.2812 0.2823 0.2830 0.2836 0.2844pcs (s = 0.1) FID 59.2480 18.5894 18.8975 19.8658 20.5433 21.1257 21.6248 21.9118 23.7671clip-score 0.2248 0.2717 0.2752 0.2781 0.2798 0.2810 0.2822 0.2830 0.2840linear (c = 2) FID 59.2480 24.3084 23.8361 24.0241 24.4806 24.6759 24.9336 25.6498 26.6398clip-score 0.2248 0.2773 0.2778 0.2792 0.2805 0.2818 0.2827 0.2831 0.2845linear (c = 4) FID 59.2480 18.2321 18.2517 18.2678 18.3675 18.5902 18.8356 19.1395 19.9400clip-score 0.2248 0.2798 0.2799 0.2803 0.2811 0.2819 0.2825 0.2832 0.2846linear (c = 6) FID 59.2480 19.3309 19.3295 19.2716 19.2801 19.2955 19.4298 19.5635 20.1577clip-score 0.2248 0.2720 0.2748 0.2775 0.2794 0.2806 0.2816 0.2822 0.2836cosine (c = 2) FID 59.2480 24.2768 23.9367 23.8442 24.1493 24.3516 24.6917 25.0779 25.8126clip-score 0.2248 0.2773 0.2780 0.2793 0.2806 0.2816 0.2825 0.2832 0.2843cosine (c = 4) FID 59.2480 18.2321 18.2336 18.2764 18.2364 18.3372 18.5678 18.8925 19.6065clip-score 0.2248 0.2798 0.2799 0.2805 0.2813 0.2821 0.2826 0.2830 0.2843cosine (c = 6) FID 59.2480 19.2943 19.2701 19.2261 19.2656 19.2711 19.2743 19.2670 19.7355Table 16: Experiment on SD1.5 with Diversity measures of 10K images, comparison between thebaseline and two increasing heuristic shapes, linear and cosine.w 2 4 6 8 10 12 14 20 25baselineclip-score 0.2593 0.2719 0.2757 0.2775 0.2790 0.2796 0.2803 0.2813 0.2817FID 11.745 11.887 14.639 16.777 18.419 19.528 20.462 22.463 23.810Div-CLIP-L 0.315 0.289 0.275 0.267 0.260 0.257 0.254 0.250 0.251Div-Dinov2-L 1.188 1.083 1.033 1.007 0.987 0.976 0.967 0.951 0.948Div-CLIP-S 0.317 0.288 0.273 0.263 0.256 0.252 0.249 0.246 0.246Div-Dinov2-S 1.241 1.131 1.082 1.051 1.031 1.019 1.006 0.992 0.986linearclip-score 0.2565 0.2697 0.2741 0.2763 0.2780 0.2788 0.2799 0.2817 0.2826FID 14.649 11.260 12.056 13.147 14.179 15.032 15.663 17.478 18.718Div-CLIP-L 0.320 0.300 0.289 0.281 0.275 0.271 0.268 0.262 0.259Div-Dinov2-L 1.209 1.119 1.076 1.048 1.030 1.016 1.006 0.986 0.979Div-CLIP-S 0.324 0.302 0.291 0.282 0.277 0.271 0.270 0.263 0.261Div-Dinov2-S 1.262 1.172 1.129 1.099 1.082 1.060 1.057 1.038 1.027cosclip-score 0.2553 0.2686 0.2728 0.2751 0.2770 0.2782 0.2793 0.2812 0.2821FID 15.725 11.846 12.009 12.796 13.629 14.282 15.058 16.901 18.448Div-CLIP-L 0.322 0.304 0.293 0.287 0.282 0.278 0.275 0.268 0.265Div-Dinov2-L 1.215 1.134 1.092 1.068 1.051 1.039 1.030 1.008 1.001Div-CLIP-S 0.326 0.307 0.296 0.290 0.285 0.282 0.278 0.272 0.269Div-Dinov2-S 1.266 1.186 1.145 1.120 1.104 1.093 1.081 1.063 1.054Table 17: Experiment on SDXL with Diversity., we present FID vs. CLIP-Score (CS) for SDXL of10K images, and we see the similar trending to Table 16 that the heuristic methods outperform the baseline,both on FID and Diversity.w 3 5 7 8 9 11 13 15 20clip-score 0.2712 0.2767 0.2791 0.2799 0.2806 0.2817 0.2826 0.2832 0.2836FID 24.36 24.93 25.71 26.06 26.17 27.23 27.46 28.05 29.69Div-Dinov2-L 0.951 0.886 0.857 0.850 0.841 0.831 0.827 0.829 0.853baselineDiv-Dinov2-S 1.052 0.985 0.950 0.940 0.934 0.920 0.916 0.912 0.927clip-score 0.2653 0.2732 0.2773 0.2789 0.2798 0.2810 0.2821 0.2828 0.2840FID 29.09 25.03 24.45 24.52 24.67 25.13 25.55 25.85 26.60Div-Dinov2-L 0.999 0.949 0.916 0.904 0.897 0.881 0.873 0.863 0.854linearDiv-Dinov2-S 1.123 1.064 1.030 1.018 1.007 0.989 0.980 0.973 0.956clip-score 0.2621 0.2708 0.2751 0.2764 0.2776 0.2794 0.2803 0.2817 0.2830FID 32.83 27.00 25.55 25.41 25.43 25.52 25.74 25.88 26.84Div-Dinov2-L 1.017 0.969 0.941 0.932 0.922 0.908 0.899 0.893 0.879cosineDiv-Dinov2-S 1.143 1.095 1.066 1.056 1.045 1.031 1.020 1.008 0.99433Published in Transactions on Machine Learning Research (12/2024)For the evaluation, each participant was presented with a total of 10 image sets. Each set comprised 9images. Within each set, three pairwise comparisons were made: linear vs. baseline, and cosine vs. baseline.Throughout the study, two distinct image sets (20 images for each method) were utilized. We carried outtwo tests for results generated with stable diffusion v1.5 and each image are generated to make sure thattheir CLIP-Score are similar.Subsequently, participants were prompted with three questions for each comparison:1. Which set of images is more realistic or visually appealing?2. Which set of images is more diverse?3. Which set of images aligns better with the provided text description?In total, we recorded 54 participants with each participant responding to 90 questions. We analyzed theresults by examining responses to each question individually, summarizing the collective feedback.34'}\n",
      "{'_id': ObjectId('6815e61add259dc7e6e1cd37'), 'authors': 'Bhakat, Soumendranath, Bowman, Gregory R, Meller, Artur, Solieva, Shahlo', 'year': '2023', 'title': 'Accelerating cryptic pocket discovery using AlphaFold', 'full_text': 'Washington University School of Medicine Digital Commons@Becker 2020-Current year OA Pubs Open Access Publications 7-25-2023 Accelerating cryptic pocket discovery using AlphaFold Artur Meller Soumendranath Bhakat Shahlo Solieva Gregory R Bowman Follow this and additional works at: https://digitalcommons.wustl.edu/oa_4  Part of the Medicine and Health Sciences Commons Please let us know how this document benefits you. Accelerating Cryptic Pocket Discovery Using AlphaFoldArtur Meller, Soumendranath Bhakat,* Shahlo Solieva, and Gregory R. Bowman*Cite This: J. Chem. Theory Comput. 2023, 19, 4355−4363 Read OnlineACCESS Metrics & More Article Recommendations *sı Supporting InformationABSTRACT: Cryptic pockets, or pockets absent in ligand-free,experimentally determined structures, hold great potential as drugtargets. However, cryptic pocket openings are often beyond thereach of conventional biomolecular simulations because certaincryptic pocket openings involve slow motions. Here, we investigatewhether AlphaFold can be used to accelerate cryptic pocketdiscovery either by generating structures with open pockets directlyor generating structures with partially open pockets that can beused as starting points for simulations. We use AlphaFold togenerate ensembles for 10 known cryptic pocket examples,including five that were deposited after AlphaFold’s training datawere extracted from the PDB. We find that in 6 out of 10 casesAlphaFold samples the open state. For plasmepsin II, an asparticprotease from the causative agent of malaria, AlphaFold only captures a partial pocket opening. As a result, we ran simulations froman ensemble of AlphaFold-generated structures and show that this strategy samples cryptic pocket opening, even though anequivalent amount of simulations launched from a ligand-free experimental structure fails to do so. Markov state models (MSMs)constructed from the AlphaFold-seeded simulations quickly yield a free energy landscape of cryptic pocket opening that is in goodagreement with the same landscape generated with well-tempered metadynamics. Taken together, our results demonstrate thatAlphaFold has a useful role to play in cryptic pocket discovery but that many cryptic pockets may remain difficult to sample usingAlphaFold alone.■ INTRODUCTIONCryptic pockets, or pockets absent in ligand-free experimentalstructures, are a promising means to expand the scope of drugdiscovery. By one estimate, almost half of all structureddomains lack obvious pockets in their experimental structures.1These proteins have often been considered “undruggable”.However, as proteins fluctuate in solution, they may adoptexcited structural states that contain cryptic pockets. Thus,cryptic pockets may provide a means to target these“undruggable” proteins.2 Furthermore, many cryptic pocketsare distant from active sites, suggesting that targeting themmay lead to the discovery of allosteric activators3 or morespecific modulators given the high sequence conservation ofmany active sites.4The discovery of cryptic pockets using experimental andcomputational methods remains difficult in many cases. Mostcryptic pockets are discovered serendipitously when exper-imental structures of a ligand bound to a protein reveal a novelbinding site that is closed in ligand-free structures of the sameprotein.5 While this process has revealed cryptic pockets, itrequires knowledge of a ligand a priori. Molecular dynamicssimulations can reveal excited states with cryptic pockets thatcan then be used for structure-based drug design.2,6 However,in certain cases, cryptic pockets may not be discovered bysimulations because cryptic pocket opening motions may beslow (e.g., Niemann-Pick C2 Protein in Meller et al.1). Twoclasses of slow motions include side chain ring flipping7 eventsand secondary structure rearrangements8 which can both occuron microsecond and slower time scales.Here, we explore the possibility of using AlphaFold9 toaccelerate cryptic pocket discovery. Previous work has shownthat stochastic sampling of AlphaFolds’s input multiplesequence alignment can generate diverse conformations ofmembrane and globular proteins.10,11 We hypothesized that asimilar strategy can be applied to discover cryptic pockets.Even if AlphaFold can only capture partial openings, wereasoned that starting molecular dynamics simulations fromthese structures may capture full openings far more quicklythan starting simulations from completely closed structures(Figure 1).We test our strategy of launching simulations fromAlphaFold-generated starting structures with plasmepsin II(PM II), a well-studied protease from the causative agent ofSpecial Issue: Machine Learning for Molecular Simu-lationReceived: November 23, 2022Published: March 22, 2023Articlepubs.acs.org/JCTC© 2023 The Authors. Published byAmerican Chemical Society4355https://doi.org/10.1021/acs.jctc.2c01189J. Chem. Theory Comput. 2023, 19, 4355−4363Downloaded via WASHINGTON UNIV on October 27, 2023 at 21:44:30 (UTC).See https://pubs.acs.org/sharingguidelines for options on how to legitimately share published articles.malaria.12−14 PM II is one of many aspartic proteases that playan important role in the lifecycle of Plasmodium falciparum. Itis found in digestive vacuoles where it is used by the parasite todigest hemoglobin. Though functional redundancy in digestivevacuoles may limit the utility of narrow PM II inhibitors, PM IImay play a role in antimalarial drug resistance15 and provideinsight into developing inhibitors of other aspartic proteasesthat are essential in the Plasmodium lifecycle. Notably, PM IIcontains a cryptic pocket adjacent to its active site, which wasrevealed in several experimental structures capturing PM IIbound to different classes of inhibitors. Given that previoussimulation studies of PM II have failed to sample crypticpocket opening,13 here we explore if increasing aggregatesimulation time is sufficient to open this pocket or if AlphaFoldcan accelerate cryptic pocket discovery.■ METHODSEnsemble Generation Using AlphaFold. To generateensembles of structures from a sequence rather than a singlestructure, we use two modifications to the original AlphaFoldimplementation. First, we stochastically subsample the multiplesequence alignment (MSA) to a maximum of 32 clustercenters and 64 extra sequences. Each time we generate astructure prediction, a different random seed is used forsequence clustering, so that the input MSA passed toAlphaFold is slightly modified. Second, we also enable dropoutduring the forward pass through the model.We generated ensembles for each of the proteins studiedusing ColabFold,16 a fast and user-friendly implementation ofthe AlphaFold algorithm. Specifically, we used the GoogleCollaboratory notebook. We generated initial MSAs using thejackhammer method with prefiltering that enforced a minimum50% coverage and 20% sequence identity with the query. Wethen limited the depth of the input MSA by setting themax_msa_clusters variable to 32 and max_extra_msa to 64. Wegenerated an ensemble of 32 or 160 structures by settingnum_models to 1 or 5, respectively, and num_samples to 32.We enabled dropout by setting is_training to True. We alsoFigure 1. To efficiently sample cryptic pocket openings, we propose launching molecular dynamics simulations from diverse AlphaFold-generatedstarting conformations. Starting with an multiple sequence alignment (MSA) of a query sequence (top left), the MSA can be stochasticallyclustered to create input MSAs of lower depth that are then fed to AlphaFold. Through this procedure, we can generate an ensemble of structuresof the same protein (top right shows snapshots of Plasmodium falciparum’s plasmepsin II). These structures may adopt different conformations atknown cryptic pockets (bottom right inset highlights different conformations of the plasmepsin II cryptic pocket). To generate free energylandscapes of cryptic pocket opening, we can launch molecular dynamics simulations from these different conformations and then stitch thesesimulations together with a Markov state model.Journal of Chemical Theory and Computation pubs.acs.org/JCTC Articlehttps://doi.org/10.1021/acs.jctc.2c01189J. Chem. Theory Comput. 2023, 19, 4355−43634356enabled use_ptm, set num_ensembles to 1, set tol to 0, and setmax_recycles to 3.The link to the Google Collaboratory notebook is here(https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/beta/AlphaFold2_advanced.ipynb).Molecular Dynamics Simulations. We prepared molec-ular dynamics simulations using the tleap module integratedwith Amber 202017 with the workflow described here. Proteinswere parametrized using the AMBER FF14SB18 force field andsolvated in a truncated octahedron box with TIP3P19 waters.Each system was neutralized by 17 Na+ ions. For each system,the box was extended 1.0 nm from protein atoms in alldirections. Minimization was performed in two steps: (a)initial minimization where the protein was constrained with arestrained potential of 100 kcal/mol−1 Å2 to minimize only thewater and ions (200 steps of steepest descent followed by 200steps of conjugate gradients) followed by (b) 500 steps ofunrestrained minimization of the whole system.We equilibrated protein systems and performed productionruns using Gromacs 2021.20 Following minimization in Amber,we converted Amber topologies to Gromacs format usingAcpype.21 Initially, we heated each system (from 0 to 300 K)using the NVT ensemble for 500 ps with harmonic restraints of500 kJ mol−1nm−2 applied to backbone heavy atoms. Next,each system was equilibrated at 300 K in an NPT ensemble for200 ps without any restraints using the Parrinello−Rahmanbarostat22 to maintain the pressure at 1 bar and the v-rescalethermostat for temperature control. Production runs werecarried out in the NPT ensemble at 300 K and 1 bar using theleapfrog integrator and Parrinello−Rahman thermostat with a2 fs time step. Nonbonded interactions were cut off at 1.0 nm,and long-range electrostatic potentials were treated using theParticle Mesh Ewald (PME) method23 with a grid spacing of0.16 nm. The LINCS algorithm24 was used to constrain H-bonds during MD simulations.We performed 640 independent MD simulations in total togenerate apo-seeded and AF-seeded ensembles each with 32 μsof sampling. We used 32 different AlphaFold-generatedstarting structures for plasmepsin II with 10 independent(i.e., starting from different initial velocities) simulationslaunched for each structure. Each simulation was 100 ns inlength. For the apo-seeded ensemble, we ran 320 independentsimulations 100 ns in length starting from a single startingstructure from the PDB (1LF425). We note that while we usedan ensemble of 160 structures to evaluate the conformationaldiversity of structures produced by AF (Figure 2A), wegenerated an ensemble of 32 structures of plasmpesin II forsimulations separately.Markov State Modeling. To construct MSMs,26−28 wefirst defined a subset of features that were relevant to PM IIcryptic pocket opening. We focused on the set of residues thatwere within 0.5 nm of the cryptic extension of the A1T ligandin the holo crystal structure (PDB: 2IGX29). Specifically, welocated all residues that were within 0.5 nm of the followingA1T atoms: C48, C46, C43, C40, C38, C36, C33, C34, C30,N29, and C26. We then used backbone (phi, psi) and sidechain dihedrals for those residues to define an initial feature setrelevant for cryptic pocket opening. We removed any χ-2angles that included symmetrically equivalent atoms (e.g., χ-2for tyrosine residues).To perform clustering in a kinetically relevant space, weapplied time−structure independent component analysis30(tICA) to these features. Specifically, we used a tICA lagtime of 10 ns and retained the top n tICs that accounted forFigure 2. Stochastic clustering of its input multiple sequence alignment allows AlphaFold to generate structures with open or partially open crypticpockets across multiple systems. (A) In 6 out of 10 examples, AlphaFold samples the open state of a known cryptic pocket. The box-and-whiskerplots show cryptic pocket root-mean-square deviations (RMSD) to a holo crystal structure (defined by heavy atoms within 5 Å of the ligand thatbinds at the cryptic pocket). For the top five examples, the holo structure was part of the training data set for AlphaFold, but the bottom fiveexamples had their holo crystal structures deposited after AlphaFold was trained. The red line indicates 1.2 Å RMSD, a proposed cutoff for samplingthe open state. (B) Structural overlay of an AlphaFold-generated structure with the holo structure of Neimann-Pick C2 Protein (NPC2) shows thatAlphaFold samples the open state. The ligand which binds in the cryptic pocket is shown in magenta, the apo structure in gray, the holo structure inblue, and the AF structure in orange. Residues that change rotamer state between apo and holo experimental structures are shown in sticks. (C)Structural overlay of an AlphaFold-generated structure of plasmepsin II with a holo structure containing a cryptic pocket shows that AlphaFoldpartially samples cryptic pocket openings. Select residues that change rotamer state between apo and holo experimental structures show thatAlphaFold samples holo-like tryptophan orientations in the plasmepsin II cryptic pocket. As in B, the ligand which binds in the cryptic pocket isshown in magenta, the apo structure in gray, the holo structure in blue, and the AF structure in orange.Journal of Chemical Theory and Computation pubs.acs.org/JCTC Articlehttps://doi.org/10.1021/acs.jctc.2c01189J. Chem. Theory Comput. 2023, 19, 4355−4363435790% of kinetic variance using commute mapping. We foundthat the choice of tICA lag time (between 5 and 20 ns) did notaffect which slow collective motions were identified by tICA.To determine the appropriate number of microstates forclustering, we used a cross-validation scheme where trajectorieswere partitioned into training and test sets. Clustering into kmicrostates was performed using only the training set, and thetest set trajectories were assigned to these k microstates basedon their Euclidean proximity in tICA space to eachmicrostate’s centroid. Using the test set only, an MSM wasfit using maximum likelihood estimation (MLE), and thequality of the MSM was assessed with the rank-10 VAMP-2score of the transition matrix. We found that 25 microstateshad the highest VAMP-2 score on average across 10 trials onthe test set for the AF-seeded ensemble (Figure S17). Forconsistency, we used the same number of microstates for theapo-seeded MSM.Finally, MSMs of the PM II cryptic pocket were fit for theapo-seeded and AF-seeded ensembles separately using MLE.Lag times were chosen by the logarithmic convergence of theimplied time scales test (Figures S18, S19). Lag times of 12.5ns were used for both the apo-seeded and AF-seeded MSMs.MSM construction was performed using the PyEMMA31software package.Metadynamics. We performed well-tempered metady-namics32,33 (WTMeta) simulations to sample the conforma-tional landscape associated with Trp41 ring flipping, one of themotions necessary for plasmepsin II cryptic pocket opening.For each residue, we performed two-dimensional WTMeta at300 K using χ-1 and χ-2 angles as collective variables.Gaussians were deposited every 500 time steps with a widthand height of 0.05 radians and 1.2 kJ/mol, respectively, and abias factor of 20. Unbiased free energy surfaces along differentcollective variables were extracted from WTMeta using thereweighting protocol described by Tiwary and Parrinello.34We also used WTMeta simulations to study unbinding ofsmall molecules from two holo conformations (PBD: 2BJU,354AY836). Small molecules were parametrized using the GeneralAmber Force Field37 (GAFF), and the protein was para-metrized using the Amber14SB force field. The complexeswere neutralized using sodium ions and immersed into atruncated octahedral box such that the distance from protein tothe edge of the box was at least 1 nm. Equilibration andproduction runs were performed using the protocol describedin Bhakat and Söderhjelm.13 To estimate the apparent freeenergy profile of ligand unbinding, we performed multipleindependent WTMeta simulations using the distance betweenthe center of mass of the active side residues and the ligand ascollective variables. All unbinding WTMeta simulations wereperformed at 300 K with a bias factor of 10 using a Gaussianwidth and height of 0.011 nm and 1.2 kJ/mol, respectively.■ RESULTSAlphaFold Predicts Some but Not All Known CrypticPocket Openings. We reasoned that AlphaFold (AF) couldproduce conformations with open cryptic pockets throughstochastic sampling of its input multiple sequence alignment.Previous studies have shown that AlphaFold samples diverseconformations of transporters and receptors when its inputMSA is stochastically subsampled to only include 16sequences.10,11 Additionally, AF ensembles of a set of proteinswhere ligand binding is associated with conformationalrearrangements (though not necessarily at the ligand bindingsite) often included holo-like conformations.38 However, it wasnot known if AlphaFold samples open structures for proteinsknown to form cryptic pockets when bound to drug-likemolecules (e.g., not ions).We generated AlphaFold ensembles for 10 known crypticpocket examples, including a subset that was deposited to thePDB after AlphaFold was trained. These examples includeseveral different types of conformational rearrangements: loopmotions, secondary structure motions, and interdomainmotions. To ensure that the network was not “memorizing”particular conformations in its training data set, we alsofocused on five cryptic pocket examples that were deposited tothe PDB after April 2018, the date when the AlphaFoldtraining set was pulled. We used ColabFold’s implementationof AlphaFold to generate 160 conformers for each inputsequence because it offered a massive speed up and supportedstochastic clustering of the input MSA (see Methods). We alsoused dropout in the forward pass through the network toamplify structural diversity.We find that AlphaFold samples many but not all crypticpocket openings (Figure 2). Among proteins that were in thetraining data set, AlphaFold recapitulates known crypticpockets in three out of five examples. In those cases, AFpredicts a structure with less than 1.2 Å root-mean-squaredeviation (RMSD) to the holo structure in the cryptic site (i.e.,using all heavy atoms within 5 Å of where the cryptic ligandbinds for the RMSD calculation). Interestingly, AlphaFoldgenerates open states of the Niemann-Pick C2 Protein thatwere not discovered in 2 μs of adaptive sampling simulations(Figure 2B).1 However, AlphaFold’s ensemble of TEM β-lactamase structures does not include any open states wherethe Horn39 or omega6 pockets are open (Figure S1). Amongproteins that were not in the training data set, AlphaFoldrecapitulates three of the five cryptic pockets (i.e., using pocketRMSD of 1.2 Å as the cutoff again). There appears to be acorrelation between the size of the rearrangement (i.e., RMSDbetween apo and holo structures) and the ability of AF tosample cryptic pockets (Figures S2−S12). For example, acryptic pocket opening in fascin requires a large interdomainmotion (0.47 pocket RMSD between apo and holo) and is notcaptured in the AF ensemble.Interestingly, for plasmepsin II (PM II), AlphaFold onlysamples partial cryptic pocket opening, capturing a ring flipthat is necessary but not sufficient for pocket opening. In anAF-generated ensemble of 32 structures, there are severaldifferent Trp41 orientations (Figure S13A). Notably, ligand-free PM II structures have only ever been observed in a singleTrp41 orientation that blocks access to the cryptic site (FigureS14). In contrast, the AF ensemble contains a Trp41orientation that has only been experimentally observed inholo PM II structures with an open cryptic pocket (Figure 2C,PDB: 2BJU,35 2IGX, 2IGY29). Similarly, AF-generatedstructures sample the Tyr77 conformation seen in holo PMII structures (Figure S15). Despite this progress towardobserving pocket opening, there are still significant differencesin the position of the flap domain in the AF ensemble ascompared to the holo crystal structures. In the AF ensemble,the flap domain has not moved away from the active site,sterically blocking known cryptic pocket binders. Wewondered if simulations launched from the AF ensemblewould sample cryptic pocket openings.PM II’s Cryptic Pocket Opening Is Not Captured withConventional MD Simulations.We wanted to set a baselineJournal of Chemical Theory and Computation pubs.acs.org/JCTC Articlehttps://doi.org/10.1021/acs.jctc.2c01189J. Chem. Theory Comput. 2023, 19, 4355−43634358to determine if AlphaFold accelerates cryptic pocket openings.Given recent success in using molecular dynamics to revealcryptic pockets,40−44 we wondered if simulations launchedfrom a ligand-free PM II structure would sample cryptic pocketopenings. Out of the 10 known cryptic pocket examples wetested with AF, we decided to focus on PM II because it wasthe only one where AF sampled partial cryptic pocket openings(i.e., its ensemble included several structures with pocketRMSD to holo between 1.2 and 2 Å but no structures below1.2 Å RMSD). Additionally, a previous simulation study of PMII did not observe cryptic pocket opening in ∼2 μs ofsampling.13 We hypothesized that increasing the aggregatesimulation time might be sufficient to observe cryptic pocketopenings. Hence, we launched 320 100 ns-long independentsimulations from an apo crystal structure of PM II (PDB:1LF425).To our surprise, we find that 32 μs of MD simulations donot reveal cryptic pocket opening in PM II. For the PM IIcryptic pocket to open, three separate events must occur:Trp41 must change its side chain orientation, Tyr77 must flipalong χ-1, and the “flap” domain must move away from theactive site. Our apo-seeded simulations sample both Tyr77flipping and flap domain movement. However, we do notsample the change in Trp41 side chain orientation (thedistance between Trp41’s side chain and the C-alpha of K72remains large as seen in Figure 3C). Hence, we conclude thatPM II’s cryptic pocket opening is not captured withconventional MD simulations, though it is possible that largeincreases in the amount of sampling could enable us to observeTrp41 ring flipping that is necessary for cryptic pocketopening.Seeding with AlphaFold Accelerates Exploration ofthe Free Energy Landscape of PM II’s Cryptic Pocket.Given that the AF ensemble of PM II included diverse partiallyopen structures (Figure 3B), we wondered if launchingsimulations from these structures would accelerate samplingof full cryptic pocket opening. The AF ensemble containsstructures with different Trp41 orientations, including one withthe Trp41 in the same orientation as holo crystal structures(Figure S13A). Given that flap domain movement was sampledin the simulations initiated from the crystal structure, wehypothesized that we would observe open states in oursimulations. We launched 10 independent simulations of 100ns in length for each of the 32 AlphaFold-generated startingstructures (32 μs of aggregate simulation time). We alsoperformed metadynamics simulations to generate a free energylandscape of Trp41 side chain orientations using an orthogonaltechnique that could be compared against our unperturbedsimulations.We find that simulations launched from the AF ensemblesample cryptic pocket opening. Unlike in single-seededsimulations, we sample all three events required for crypticpocket opening when simulations are launched from the AFensemble (Figure 3D). The Trp41 adopts a holo-likeorientation while the distance between Trp41 and Tyr77 islarge, creating a cavity for ligands to bind. Furthermore, we canbuild Markov state models26,41,45 (MSMs) of the crypticpocket ensemble to measure the probability of cryptic pocketopening. MSMs are network models of free energy landscapesFigure 3. Launching simulations from AlphaFold-generated structures improves sampling of cryptic pocket opening in plasmepsin II. (A) Structureof PM II’s flap domain showing key residues involved in PM II’s cryptic pocket. Trp41 and Tyr77, part of the flap domain, are shown in sticks. Weuse the distances indicated in dotted lines to capture pocket opening. Specifically, the cryptic pocket is open when the minimum distance betweenY77 and W41 is large (indicated with blue line), and the distance between the W41 side chain (either atom CZ3 or CH2 depending on which iscloser) and a reference residue in the 6th β-strand (K72) is small (indicated with red line). (B) Pocket distances for a set of 32 AlphaFold-generatedconformers (gray dots) and holo crystal structures (black triangles) show that the AlphaFold ensemble includes partially open states for PM II.Trp41 is in its holo orientation in one of the AlphaFold structures, but the distance between Trp41 and Tyr77 is smaller than it is in holo crystalstructures. (C) A free energy surface from a Markov state model from apo-seeded simulations shows that these simulations do not sample crypticpocket openings. Though the flap dissociates as indicated by large Trp41−Tyr77 distances, Trp41 does not adopt the holo orientation, despite 32μs of sampling. (D) A free energy surface from a Markov state model generated from AlphaFold-seeded simulations shows robust sampling of theopen state. Both requirements for cryptic pocket opening are fulfilled as indicated by the overlay of holo crystal structures (black triangles) on thefree energy surface.Journal of Chemical Theory and Computation pubs.acs.org/JCTC Articlehttps://doi.org/10.1021/acs.jctc.2c01189J. Chem. Theory Comput. 2023, 19, 4355−43634359composed of many conformational states and the probabilitiesof transitioning between these states. Specifically, weconstructed a MSM using a time−structure independentcomponent analysis (tICA) projection of the backbone and χ-1dihedrals within the cryptic pocket (see Methods). Despitestarting from different starting structures, we find that ourmodel is fully connected in this feature space, and we predictthat the probability of cryptic pocket opening is 0.07,indicating that open states are a rare but non-negligible partof the ensemble.Additionally, reasonable agreement between multiplesimulation techniques suggests we have converged to thecorrect thermodynamics for the force field (Figure 4). We useour MSM to construct a free energy landscape in the space ofTrp41 χ-1 and χ-2 dihedral angles and compare against the freeenergy landscape generated by well-tempered metadynamics(see Methods). Overall, the two free energy landscapes identifysimilar free energy minima (Figure 4). The deepest well inboth landscapes corresponds to the Trp41 side chainorientation seen in ligand-free structures (Figure 4, FigureS13B). There are minor differences in the two free energylandscapes with metadynamics predicting that the wellcentered on (−1, −2) is more probable than the MSM does.Furthermore, in metadynamics simulations, the probability ofthe holo Trp41 orientation is 0.30, while in the MSM it is 0.08.Nonetheless, both methods predict that the flipped statenecessary for pocket opening is a minor part of the ensemble.■ DISCUSSIONCertain cryptic pocket opening events remain difficult tosample with classical molecular dynamics simulations. As inprevious work,12,13 MD simulations launched from an apo PM-II structure failed to sample full cryptic pocket opening, evenwith an aggregate simulation time of 32 μs. This result makesPM-II an exception to a general trend. We have found thatmany cryptic pockets can be discovered with a handful ofsimulations of intermediate length (i.e., 40 ns).1 Furthermore,significant progress has been made in developing algorithmsfor cryptic pocket discovery, including Markov statemodels,6,41 enhanced sampling strategies like SWISH,46 oradaptive sampling approaches like FAST.47 However, we havepreviously seen that even adaptive sampling strategies can failto sample known cryptic pockets. This likely stems from thedifficulty of sampling rare events in classical moleculardynamics simulations.The sampling strategy proposed here expands the availablecomputational toolkit for cryptic pocket discovery andcharacterization without perturbing the underlying energylandscape (Figure 1). Specifically, when assessing a protein as adrug target, we suggest generating diverse conformers of thatprotein by iteratively passing a stochastically subsampledmultiple sequence alignment to AlphaFold. Next, we proposeusing pocket detection tools, such as LIGSITE,48 fpocket,49 orP2rank,50 to identify pockets that may be absent in both apoexperimental structures and the AlphaFold-predicted structureusing a complete MSA. In some cases, this will be sufficient touncover novel cryptic pockets (Figure 2A). However, if thisapproach yields a partial opening or one is interested inassessing the equilibrium probability of a cryptic pocketopening, we propose using molecular dynamics simulationsfollowed by Markov state model construction. As demon-strated here with PM II, this strategy can greatly accelerate thediscovery and characterization of cryptic pockets.Drug discovery efforts directed toward plasmepsins illustratethat targeting cryptic pockets is a generally promising strategyfor discovering selective and potent inhibitors. Ligands thatbind at the PM II cryptic site have enhanced potency andselectivity toward PM II compared with other plasmepsinsfrom Plasmodium falciparum (Figure S16A, B). Furthermore,ligands that bind in the cryptic pockets do not inhibit humanpepsin-like aspartic proteases (e.g., pepsin, cathepsin D andE).29 To further illustrate the utility of targeting the PM IIcryptic site, we used metadynamics to compare the unbindingof an inhibitor from the cryptic site with the unbinding of aligand from the active site (Figure S16). We find that theligand which binds at the cryptic pocket has an approximately25 kJ/mol higher free energy barrier to unbinding because thetyrosine in the cryptic pocket acts as a lid over the ligand(Figure S16C). Slower unbinding kinetics may explain whyligands that bind in the PM II cryptic pocket are more potentand selective. We expect these same principles will apply inother systems.Finally, AlphaFold-based sampling offers important advan-tages over existing methods for generating initial structures forsimulations, despite AF’s limitations. One approach forgenerating initial conformations for MD simulations iscoarse-grained simulations. Coarse-grained simulations cansample a conformational landscape more rapidly than all-atomsimulations, and structures generated by such simulations canbe used to reconstruct all-atom structures.51,52 However,coarse-grained simulations may still need to run for prolongedtime scales to sample the landscape broadly. In contrast, AF-based sampling is much faster and can be run in a web browserwithout having to install any specialized software. Furthermore,we have shown here that AF-based structures are in goodagreement with experimental structures. It should be acknowl-edged, however, that some AF-predicted structures may haveregions, or even entire domains, with low predictionconfidence (i.e., a low predicted local distance differencetest). These structures may occupy high-energy regions ofconformational space or contain highly flexible domains. Auser may manually choose to omit certain structures fromsimulations or decide to not simulate highly disordered loops ifFigure 4. A Markov state model built from AlphaFold-seededsimulations and metadynamics simulations yield a similar free energylandscapes for plasmepsin II cryptic pocket opening. (A) Free energysurface for Trp41 side chain orientations derived from a Markov statemodel constructed using dihedrals in the PM II cryptic pocket. Holocrystal structures are indicated with black triangles (three points areplotted though only two are visible). Apo crystal structures sample thewell centered near (1, −2). (B) Free energy surface from well-tempered metadynamics simulations using Trp41 χ-1and χ-2 angles ascollective variables.Journal of Chemical Theory and Computation pubs.acs.org/JCTC Articlehttps://doi.org/10.1021/acs.jctc.2c01189J. Chem. Theory Comput. 2023, 19, 4355−43634360their dynamics are unlikely to affect functionally relevantregions. Another limitation of AF-based sampling is that AFdoes not provide information for how to weight the ensembleof structures that are generated. Fortunately, MD simulationsfollowed by MSM construction may alleviate some of theselimitations of AF-based sampling as simulations are likely torelax away from high-energy conformations, and MSMs can beused to weight an ensemble of structures. Thus, the samplingstrategy proposed here should be useful in sampling diverseconformational changes, not just cryptic pocket openings.■ CONCLUSIONSWe have demonstrated that AlphaFold can be used toaccelerate the discovery and characterization of crypticpockets. When its input multiple sequence alignment isstochastically subsampled, AlphaFold generates diverse con-formers of proteins known to form cryptic pockets. In 6 out of10 examples of proteins known to form cryptic pockets,AlphaFold samples the open state (Figure 2A). Impressively,AlphaFold also makes predictions of the open state even whenthe holo structure was deposited after AlphaFold was trained.In other cases, like with plasmepsin II, AlphaFold samplespartially open states (Figure 2C). For plasmepsin II, theensemble of AF structures includes structures with atryptophan side chain in its holo orientation, even though 32μs of MD simulations launched form an apo crystal structuredo not sample tryptophan flipping. We find that launchingsimulations from this ensemble accelerates sampling of theopen state (Figure 3). Furthermore, because we observe bothpocket opening and closing events, we can use a Markov statemodel to generate a free energy landscape of pocketconformations that is in reasonable agreement with a similarlandscape generated from metadynamics. Thus, we propose anefficient strategy to discover cryptic pockets that we hopebecomes indispensable to future structure-based drug designefforts.■ ASSOCIATED CONTENTData Availability StatementAll input files and analysis scripts corresponding to this studycan be accessed here: https://github.com/sbhakat/AF-cryptic-pocket. Additionally, we have deposited analysis notebooks,Markov state models, and AlphaFold ensembles in a OSFrepository that can be accessed here: https://osf.io/cb8m7/.*sı Supporting InformationThe Supporting Information is available free of charge athttps://pubs.acs.org/doi/10.1021/acs.jctc.2c01189.Comparisons of AlphaFold ensembles to apo and holostructures for all the examples considered in this work, ascatter plot comparing apo−holo experimental RMSDswith the RMSDs of AF structures and apo/holoexperimental structures, detailed comparisons of the 32plasmepsin II structures generated by AF, results fromwell-tempered metadynamics simulations of liganddissociation from the cryptic pocket, VAMP-2 scoreevaluations used to select an optimal number ofmicrostates in the plasmepsin II MSM, implied timescales plots, and correlation plots between input featuresand time−structure independent components (i.e.,tICs). (PDF)■ AUTHOR INFORMATIONCorresponding AuthorsSoumendranath Bhakat − Department of Biochemistry andMolecular Biophysics, Washington University in St. Louis, St.Louis, Missouri 63110, United States; Department ofBiochemistry and Biophysics, University of Pennsylvania,Philadelphia, Pennsylvania 19104, United States;orcid.org/0000-0002-1184-9259;Email: bhakatsoumendranath@gmail.comGregory R. Bowman − Department of Biochemistry andBiophysics, University of Pennsylvania, Philadelphia,Pennsylvania 19104, United States; Department ofBiochemistry and Molecular Biophysics, WashingtonUniversity in St. Louis, St. Louis, Missouri 63110, UnitedStates; orcid.org/0000-0002-2083-4892;Email: grbowman@seas.upenn.eduAuthorsArtur Meller − Department of Biochemistry and MolecularBiophysics, Washington University in St. Louis, St. Louis,Missouri 63110, United States; Medical Scientist TrainingProgram, Washington University in St. Louis, St. Louis,Missouri 63110, United StatesShahlo Solieva − Department of Biochemistry and Biophysics,University of Pennsylvania, Philadelphia, Pennsylvania19104, United States; orcid.org/0000-0001-5350-2184Complete contact information is available at:https://pubs.acs.org/10.1021/acs.jctc.2c01189NotesThe authors declare the following competing financialinterest(s): G.R.B. is a co-founder and equity holder inDecrypt Biomedicine. The remaining authors declare nocompeting interests.■ ACKNOWLEDGMENTSThis work was funded by NSF MCB 2218156, NIH NIARF1AG067194, and NIH NIGMS R01GM124007. G.R.B.holds a Packard Fellowship from the David and Lucile PackardFoundation. A.M. was supported by the National Institutes ofHealth F30 Fellowship (1F30HL162431-01A1).■ REFERENCES(1) Meller, A.; Ward, M.; Borowsky, J.; Lotthammer, J. M.;Kshirsagar, M.; Oviedo, F.; Ferres, J. L.; Bowman, G. R. Predicting theLocations of Cryptic Pockets from Single Protein Structures Using thePocketMiner Graph Neural Network. Nat. Comm. 2023, 14, 1177.(2) Kuzmanic, A.; Bowman, G. R.; Juarez-Jimenez, J.; Michel, J.;Gervasio, F. L. Investigating Cryptic Binding Sites by MolecularDynamics Simulations. Acc. Chem. Res. 2020, 53 (3), 654−661.(3) Hart, K. M.; Moeder, K. E.; Ho, C. M. W.; Zimmerman, M. I.;Frederick, T. E.; Bowman, G. R. Designing Small Molecules to TargetCryptic Pockets Yields Both Positive and Negative AllostericModulators. PLoS One 2017, 12 (6), No. e0178678.(4) Longo, L. M; Jabłonska, J.; Vyas, P.; Kanade, M.; Kolodny, R.;Ben-Tal, N.; Tawfik, D. S On the Emergence of P-Loop Ntpase andRossmann Enzymes from a Beta-Alpha-Beta Ancestral Fragment. Elife2020, 9, 1−16.(5) Horn, J. R.; Shoichet, B. K. Allosteric Inhibition Through CoreDisruption. J. Mol. Biol. 2004, 336 (5), 1283−1291.(6) Knoverek, C. R.; Mallimadugula, U. L.; Singh, S.; Rennella, E.;Frederick, T. E.; Yuwen, T.; Raavicharla, S.; Kay, L. E.; Bowman, G.R. Opening of a Cryptic Pocket in β-Lactamase Increases PenicillinaseJournal of Chemical Theory and Computation pubs.acs.org/JCTC Articlehttps://doi.org/10.1021/acs.jctc.2c01189J. Chem. Theory Comput. 2023, 19, 4355−43634361Activity. Proc. Natl. Acad. Sci. U. S. A. 2021, 118 (47),No. e2106473118.(7) Weininger, U.; Modig, K.; Akke, M. Ring Flips Revisited: 13CRelaxation Dispersion Measurements of Aromatic Side ChainDynamics and Activation Barriers in Basic Pancreatic TrypsinInhibitor. Biochemistry 2014, 53 (28), 4519−4525.(8) Amaral, M.; Kokh, D. B.; Bomke, J.; Wegener, A.; Buchstaller, H.P.; Eggenweiler, H. M.; Matias, P.; Sirrenberg, C.; Wade, R. C.; Frech,M. Protein Conformational Flexibility Modulates Kinetics andThermodynamics of Drug Binding. Nature Communications 20178:1 2017, 8 (1), 1−14.(9) Jumper, J.; Evans, R.; Pritzel, A.; Green, T.; Figurnov, M.;Ronneberger, O.; Tunyasuvunakool, K.; Bates, R.; Žídek, A.;Potapenko, A.; Bridgland, A.; Meyer, C.; Kohl, S. A. A.; Ballard, A.J.; Cowie, A.; Romera-Paredes, B.; Nikolov, S.; Jain, R.; Adler, J.;Back, T.; Petersen, S.; Reiman, D.; Clancy, E.; Zielinski, M.;Steinegger, M.; Pacholska, M.; Berghammer, T.; Bodenstein, S.;Silver, D.; Vinyals, O.; Senior, A. W.; Kavukcuoglu, K.; Kohli, P.;Hassabis, D. Highly Accurate Protein Structure Prediction withAlphaFold. Nature 2021, 596 (7873), 583−589.(10) del Alamo, D.; Sala, D.; Mchaourab, H. S.; Meiler, J. SamplingAlternative Conformational States of Transporters and Receptors withAlphaFold2. Elife 2022, 11, No. e75751.(11) Stein, R. A.; Mchaourab, H. S. SPEACH_AF: Sampling ProteinEnsembles and Conformational Heterogeneity with Alphafold2. PLoSComput. Biol. 2022, 18 (8), No. e1010483.(12) Bhakat, S. Pepsin-like Aspartic Proteases (PAPs) as ModelSystems for Combining Biomolecular Simulation with BiophysicalExperiments. RSC Adv. 2021, 11 (18), 11026−11047.(13) Bhakat, S.; Söderhjelm, P. Flap Dynamics in Pepsin-LikeAspartic Proteases: A Computational Perspective Using Plasmepsin-IIand BACE-1 as Model Systems. J. Chem. Inf Model 2022, 62 (4),914−926.(14) Mahanti, M.; Bhakat, S.; Nilsson, U. J.; Söderhjelm, P. FlapDynamics in Aspartic Proteases: A Computational Perspective. Chem.Biol. Drug Des 2016, 88 (2), 159−177.(15) Nasamu, A. S.; Polino, A. J.; Istvan, E. S.; Goldberg, D. E.Malaria Parasite Plasmepsins: More than Just Plain Old DegradativePepsins. J. Biol. Chem. 2020, 295 (25), 8425−8441.(16) Mirdita, M.; Schütze, K.; Moriwaki, Y.; Heo, L.; Ovchinnikov,S.; Steinegger, M. ColabFold: Making Protein Folding Accessible toAll. Nat. Methods 2022, 19 (6), 679−682.(17) Salomon-Ferrer, R.; Case, D. A.; Walker, R. C. An Overview ofthe Amber Biomolecular Simulation Package. WIREs ComputationalMolecular Science 2013, 3 (2), 198−210.(18) Maier, J. A.; Martinez, C.; Kasavajhala, K.; Wickstrom, L.;Hauser, K. E.; Simmerling, C. Ff14SB: Improving the Accuracy ofProtein Side Chain and Backbone Parameters from Ff99SB. J. Chem.Theory Comput 2015, 11 (8), 3696−3713.(19) Jorgensen, W. L.; Chandrasekhar, J.; Madura, J. D.; Impey, R.W.; Klein, M. L. Comparison of Simple Potential Functions forSimulating Liquid Water. J. Chem. Phys. 1983, 79 (2), 926−935.(20) Abraham, M. J.; Murtola, T.; Schulz, R.; Páll, S.; Smith, J. C.;Hess, B.; Lindahl, E. GROMACS: High Performance MolecularSimulations through Multi-Level Parallelism from Laptops toSupercomputers. SoftwareX 2015, 1−2, 19−25.(21) Sousa da Silva, A. W.; Vranken, W. F. ACPYPE - AnteChamberPYthon Parser InterfacE. BMC Res. Notes 2012, 5 (1), 367.(22) Parrinello, M.; Rahman, A. Polymorphic Transitions in SingleCrystals: A New Molecular Dynamics Method. J. Appl. Phys. 1981, 52(12), 7182−7190.(23) Darden, T.; York, D.; Pedersen, L. Particle Mesh Ewald: An N·log(N) Method for Ewald Sums in Large Systems. J. Chem. Phys.1993, 98 (12), 10089−10092.(24) Hess, B.; Bekker, H.; Berendsen, H. J. C.; Fraaije, J. G. E. M.LINCS: A Linear Constraint Solver for Molecular Simulations. J.Comput. Chem. 1997, 18 (12), 1463−1472.(25) Asojo, O. A.; Gulnik, S. v; Afonina, E.; Yu, B.; Ellman, J. A.;Haque, T. S.; Silva, A. M. Novel Uncomplexed and ComplexedStructures of Plasmepsin II, an Aspartic Protease from PlasmodiumFalciparum. J. Mol. Biol. 2003, 327 (1), 173−181.(26) Husic, B. E.; Pande, V. S. Markov State Models: From an Art toa Science. J. Am. Chem. Soc. 2018, 140 (7), 2386−2396.(27) Pande, V. S.; Beauchamp, K.; Bowman, G. R. Everything YouWanted to Know about Markov State Models but Were Afraid to Ask.Methods 2010, 52 (1), 99−105.(28) Bowman, G. R.; Ensign, D. L.; Pande, V. S. Enhanced Modelingvia Network Theory: Adaptive Sampling of Markov State Models. J.Chem. Theory Comput 2010, 6 (3), 787−794.(29) Boss, C.; Corminboeuf, O.; Grisostomi, C.; Meyer, S.; Jones, A.F.; Prade, L.; Binkert, C.; Fischli, W.; Weller, T.; Bur, D. Achiral,Cheap, and Potent Inhibitors of Plasmepsins I, II, and IV.ChemMedChem. 2006, 1 (12), 1341−1345.(30) Pérez-Hernández, G.; Paul, F.; Giorgino, T.; de Fabritiis, G.;Noé, F. Identification of Slow Molecular Order Parameters forMarkov Model Construction. J. Chem. Phys. 2013, 139 (1), 015102.(31) Scherer, M. K.; Trendelkamp-Schroer, B.; Paul, F.; Pérez-Hernández, G.; Hoffmann, M.; Plattner, N.; Wehmeyer, C.; Prinz, J.-H.; Noé, F. PyEMMA 2: A Software Package for Estimation,Validation, and Analysis of Markov Models. J. Chem. Theory Comput2015, 11 (11), 5525−5542.(32) Barducci, A.; Bussi, G.; Parrinello, M. Well-TemperedMetadynamics: A Smoothly Converging and Tunable Free-EnergyMethod. Phys. Rev. Lett. 2008, 100 (2), 20603.(33) Valsson, O.; Tiwary, P.; Parrinello, M. Enhancing ImportantFluctuations: Rare Events and Metadynamics from a ConceptualViewpoint. Annu. Rev. Phys. Chem. 2016, 67 (1), 159−184.(34) Tiwary, P.; Parrinello, M. From Metadynamics to Dynamics.Phys. Rev. Lett. 2013, 111 (23), 230602.(35) Prade, L.; Jones, A. F.; Boss, C.; Richard-Bildstein, S.; Meyer,S.; Binkert, C.; Bur, D. X-Ray Structure of Plasmepsin II Complexedwith a Potent Achiral Inhibitor *. J. Biol. Chem. 2005, 280 (25),23837−23843.(36) Recacha, R.; Leitans, J.; Akopjana, I.; Aprupe, L.; Trapencieris,P.; Jaudzems, K.; Jirgensons, A.; Tars, K. Structures of Plasmepsin IIfrom Plasmodium Falciparum in Complex with Two Hydroxyethyl-amine-Based Inhibitors. Acta Crystallographica Section F 2015, 71(12), 1531−1539.(37) Wang, J.; Wolf, R. M.; Caldwell, J. W.; Kollman, P. A.; Case, D.A. Development and Testing of a General Amber Force Field. J.Comput. Chem. 2004, 25 (9), 1157−1174.(38) Saldaño, T.; Escobedo, N.; Marchetti, J.; Zea, D. J.; macDonagh, J.; Velez Rueda, A. J.; Gonik, E.; García Melani, A.;Novomisky Nechcoff, J.; Salas, M. N.; Peters, T.; Demitroff, N.;Fernandez Alberti, S.; Palopoli, N.; Fornasari, M. S.; Parisi, G. Impactof Protein Conformational Diversity on AlphaFold Predictions.Bioinformatics 2022, 38 (10), 2742−2748.(39) Horn, J. R.; Shoichet, B. K. Allosteric Inhibition Through CoreDisruption. J. Mol. Biol. 2004, 336 (5), 1283−1291.(40) Meller, A.; Lotthammer, J. M.; Smith, L. G.; Novak, B.; Lee, L.A.; Kuhn, C. C.; Greenberg, L.; Leinwand, L. A.; Greenberg, M. J.;Bowman, G. R. Drug Specificity and Affinity Are Encoded in theProbability of Cryptic Pocket Opening in Myosin Motor Domains.Elife 2023, 12, e83602 DOI: 10.7554/eLife.83602.(41) Cruz, M. A.; Frederick, T. E.; Mallimadugula, U. L.; Singh, S.;Vithani, N.; Zimmerman, M. I.; Porter, J. R.; Moeder, K. E.;Amarasinghe, G. K.; Bowman, G. R. A Cryptic Pocket in Ebola VP35Allosterically Controls RNA Binding. Nature Communications 202213:1 2022, 13 (1), 1−10.(42) Zimmerman, M. I.; Porter, J. R.; Ward, M. D.; Singh, S.;Vithani, N.; Meller, A.; Mallimadugula, U. L.; Kuhn, C. E.; Borowsky,J. H.; Wiewiora, R. P.; et al. SARS-CoV-2 simulations go exascale topredict dramatic spike opening and cryptic pockets across theproteome. Nat. Chem. 2021, 13, 651−659.(43) Sztain, T.; Amaro, R.; McCammon, J. A. Elucidation of Crypticand Allosteric Pockets within the SARS-CoV-2 Main Protease. J.Chem. Inf Model 2021, 61, 3495.Journal of Chemical Theory and Computation pubs.acs.org/JCTC Articlehttps://doi.org/10.1021/acs.jctc.2c01189J. Chem. Theory Comput. 2023, 19, 4355−43634362(44) Hollingsworth, S. A.; Kelly, B.; Valant, C.; Michaelis, J. A.;Mastromihalis, O.; Thompson, G.; Venkatakrishnan, A. J.; Hertig, S.;Scammells, P. J.; Sexton, P. M.; Felder, C. C.; Christopoulos, A.; Dror,R. O. Cryptic Pocket Formation Underlies Allosteric ModulatorSelectivity at Muscarinic GPCRs. Nat. Commun. 2019, 10 (1), 1−9.(45) Konovalov, K. A.; Unarta, I. C.; Cao, S.; Goonetilleke, E. C.;Huang, X. Markov State Models to Study the Functional Dynamics ofProteins in the Wake of Machine Learning. JACS Au 2021, 1 (9),1330−1341.(46) Comitani, F.; Gervasio, F. L. Exploring Cryptic PocketsFormation in Targets of Pharmaceutical Interest with SWISH. J.Chem. Theory Comput 2018, 14 (6), 3321−3331.(47) Zimmerman, M. I.; Bowman, G. R. FAST ConformationalSearches by Balancing Exploration/Exploitation Trade-Offs. J. Chem.Theory Comput 2015, 11 (12), 5747−5757.(48) Hendlich, M.; Rippmann, F.; Barnickel, G. LIGSITE:Automatic and Efficient Detection of Potential Small Molecule-Binding Sites in Proteins. J. Mol. Graph Model 1997, 15 (6), 359−363.(49) le Guilloux, V.; Schmidtke, P.; Tuffery, P. Fpocket: An OpenSource Platform for Ligand Pocket Detection. BMC Bioinformatics2009, 10 (1), 168.(50) Krivák, R.; Hoksza, D. P2Rank: Machine Learning Based Toolfor Rapid and Accurate Prediction of Ligand Binding Sites fromProtein Structure. J. Cheminform 2018, 10 (1), 39.(51) de Jong, D. H.; Singh, G.; Bennett, W. F. D.; Arnarez, C.;Wassenaar, T. A.; Schäfer, L. v.; Periole, X.; Tieleman, D. P.; Marrink,S. J. Improved Parameters for the Martini Coarse-Grained ProteinForce Field. J. Chem. Theory Comput 2013, 9 (1), 687−697.(52) Kmiecik, S.; Gront, D.; Kolinski, M.; Wieteska, L.; Dawid, A. E.;Kolinski, A. Coarse-Grained Protein Models and Their Applications.Chem. Rev. 2016, 116 (14), 7898−7936.Journal of Chemical Theory and Computation pubs.acs.org/JCTC Articlehttps://doi.org/10.1021/acs.jctc.2c01189J. Chem. Theory Comput. 2023, 19, 4355−43634363'}\n",
      "{'_id': ObjectId('6815e61add259dc7e6e1cd38'), 'authors': 'Ilievski, Enej, Žunkovič, Bojan', 'year': '2022', 'title': 'Grokking phase transitions in learning local rules with gradient descent', 'full_text': 'Grokking phase transitions in learning local rules with gradientdescentBojan Žunkovič ∗Faculty of computer and information science, University of Ljubljana, Ljubljana, SloveniaEnej IlievskiFaculty of mathematics and physics, University of Ljubljana, Ljubljana, SloveniaAbstractWe discuss two solvable grokking (generalisation beyond overfitting) models in a rule learning scenario.We show that grokking is a phase transition and find exact analytic expressions for the critical exponents,grokking probability, and grokking time distribution. Further, we introduce a tensor-network map thatconnects the proposed grokking setup with the standard (perceptron) statistical learning theory and showthat grokking is a consequence of the locality of the teacher model. As an example, we analyse the cellularautomata learning task, numerically determine the critical exponent and the grokking time distributionsand compare them with the prediction of the proposed grokking model. Finally, we numerically analysethe connection between structure formation and grokking.Contents1 Introduction 32 Related work 43 Perceptron grokking 53.1 1D exponential model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63.1.1 Test error dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73.1.2 Grokking probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8∗bojan.zunkovic@fri.uni-lj.si1arXiv:2210.15435v1  [cond-mat.stat-mech]  26 Oct 20223.1.3 Grokking time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93.2 D-dimensional uniform ball model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93.2.1 Test error dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113.2.2 Grokking probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123.2.3 Grokking time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153.2.4 Critical exponents for a general isotropic data PDF . . . . . . . . . . . . . . . . . . . 174 Learning local rules with shallow tensor networks 184.1 Local teacher model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184.2 Uniform tensor-network student model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204.2.1 Short introduction to tensor network methods . . . . . . . . . . . . . . . . . . . . . . . 204.2.2 Tensor-network attention model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214.2.3 Tensor network map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224.3 Simulation details and results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224.3.1 Constant attention tensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234.3.2 Full model training and structure formation . . . . . . . . . . . . . . . . . . . . . . . . 265 Summary and discussion 28A Grokking time in the 1D model 34B Grokking probability in the D-dimensional ball model 35B.1 Case λ1 = 0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36B.2 Case 1\\x1d λ1 > 0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37C Fixed attention 37D Additional results for the 2–local and the 3–local rules 3821 IntroductionDespite recent progress in understanding the double descend phenomena [1, 2, 3, 4] we still do not have acomplete theory of generalisation in over-parameterised models. Two recent empirical observations, neuralcollapse [5] and grokking (generalisation beyond over-fitting) [6], can help us understand the training andgeneralisation properties of over-parameterised models.Neural collapse occurs in the terminal phase of training, i.e. the phase with zero train error. It refers to thecollapse of the N−dimensional, last-layer features (input to the last/classification layer) [5] to a (C − 1)-dimensional equiangular tight frame (ETF) structure, where C is the number of classes. The feature vectorsconverge towards the vertices of the ETF structure such that features for each class are close to one vertex.Also, the distance between the vertices is much larger than all intra-class feature variances. We can partiallyunderstand neural collapse within the unconstrained features and local elasticity models [7]. However, itsrole in generalisation, relation to grokking, and appearance of different latent space structures are still notcompletely understood.Grokking also occurs during the terminal phase of training. When training on algorithmic datasets pastthe zero train error, a sudden decrease of the test error from approximately one to zero is observed [6].The grokking phenomenon has been discussed within an effective theory approach [8], where an empiricalconnection between representation/structure formation and generalisation has been made. An empiricalstudy [9] established a relation between grokking and training loss spikes and weight norm increase. However,no exactly solvable model exhibiting the grokking phenomenon has been discussed so far. Further, it isnot clear how to reconcile grokking with the standard generalisation theory based on statistical learningmethods [10]. The statistical learning theory predicts (in a teacher-student setting) an algebraic (as t−ν ,where ν = 1 for most learning rules) decrease of the generalisation error with training time t (or a numberof training samples) [10].Grokking and neural collapse (or latent-space structure formation in general) have been observed in over-parametrised models. However, we do not know what is the minimal framework within which we canunderstand these phenomena or if they are genuinely deep-network phenomena. We aim to formulate asimple solvable model of grokking and relate it to latent-space structure formation and other common deep-network training features, e.g. spikes in the training loss.Main contributions– We have four main contributions:• We propose a simple learning scenario that exhibits grokking (Section 3). We study two solvablemodels where grokking is a phase transition to zero test error and calculate exact critical exponentsand grokking-time distributions.• We discuss the teacher-student model within the tensor network approach and map the standardsupervised statistical-learning scenario in the thermodynamic limit to the proposed grokking setup(Section 4).• We numerically study grokking and structure formation on the example of learning a 1D cellularautomaton rule 30 (Section 4). We show that sudden spikes in the training loss correspond to structuralchanges in the latent space representation of the data.• Our analytical results and numerical experiments show a significant difference between L1 and L2regularisations. The L1 regularised models have a larger grokking probability, shorter grokking time,shorter generalisation time, and smaller effective dimension compared to L2 regularised models.3Broader impact– The proposed exactly-solvable grokking models are a step towards theoretical under-standing of the late learning phase and generalisation benefits of the terminal phase of training. The in-troduced tensor-network map connects the standard teacher-student setup in the thermodynamic limit withthe proposed grokking setup. It offers a new tool for studying generalisation properties of local rules (localteacher-student models), which could lead to more complex learning dynamics (compared to the standardinfinite-range rules).Although based on simple models, our results can be relevant also for deep learning training practice. Weconjecture that good generalisation is more likely in models with latent space data distributions with smalleffective dimension. Our results hint that L1 regularisation can improve the generalisation properties of deepmodels compared to L2 regularisation. Further, we show that spikes in the loss (which often occur duringtraining of deep neural networks) correspond to latent space structural changes that can be beneficial ordetrimental for generalisation. Assuming this is the case also in deep networks, we can use the informationabout the latent space effective dimension to revert the model to a state before the spike or continue trainingwith the current model.2 Related workA sudden transition from zero to 100% accuracy on algorithmic datasets in over-fitted transformer modelshas been first described in [6] and named grokking. In the grokking phase, a formation of simple structuresreflecting the properties of the problem have been observed. This finding contradicts the common practice ofearly stopping and supports recent observations on the benefits of the terminal phase of training [11, 1, 12] andthe double descend phenomena [1, 13, 2, 14]. In [8], an effective theory of grokking has been proposed. Withinthe effective theory we can calculate the critical training size to observe grokking. The authors relate grokkingwith a good representation (or structure formation) and introduce it as a phase between generalisation andmemorisation. We go beyond these findings since we obtain exact solutions for the proposed setup andcalculate even the grokking-time probability density function (PDF). A systematic experimental study ofthe grokking phenomena has been presented in [9]. A sling-shot mechanism (related to edge of stability [15])has been proposed as a necessary condition for grokking. The sling-shot mechanism refers to the occurrenceof cyclic spikes in the training loss and steps in the weight norms during training. The sling-shot behavior isnot restricted to algorithmic datasets but is present also in various common classification tasks [9]. We finda similar behaviour, i.e. that the grokking coincides with train loss spikes. Moreover, we connect trainingloss spikes with discontinuous step-like evolution of the effective dimension of the latent space representationof the data, which indicate structural changes of the latent space representation.A particular structure formation common in deep classification neural networks is the neural collapse (NC).It refers to four empirical/numerical observations in training deep neural network classifiers [5]:• (NC1) Variability collapse – variations of within class features become negligible• (NC2) Convergence to equiangular tight frame (ETF)– class mean vectors form an equal-sizedangles between any given pair• (NC3) Convergence to self-duality– the class means and linear classifiers converge to each other,up to rescaling• (NC4) Simplification to nearest class center– the network classifier converges to a classifier thatselects the class with the nearest train class mean.4The role of the loss function, the regularisation, the batch normalisation, and the optimizer have beendiscussed within the unconstrained features model [16, 17, 7] and the local elasticity assumption [7]. Therelation of NC to generalisation properties has been discussed in [18, 19, 7]. However, no relation to grokkinghas been discussed so far. Although we do not observe NC as defined above, our findings regarding the spikesin the training loss and latent-space data structure might also be relevant for the NC dynamics.Our main technical tools are tensor networks which are models obtained by contracting many low-dimensionaltensors. Tensor networks have been very successful in modelling many-body quantum systems. Recently, theyhave also been applied to machine learning tasks. In particular to classification problems [20, 21, 22, 23, 24,25, 26, 27], generative modelling [28, 29, 30, 31], sequence and language modelling [32, 33, 34, 35, 36], anomalydetection [37, 38]. Besides, tensor networks have been used as tools to advance machine learning theory by aderivation of interesting generalisation bounds [34], information theoretical insights [39, 40, 41, 42], and newconnections between machine learning and physics [43, 44, 45]. Particularly relevant for latent space structureformation is the connection between recurrent neural networks (RNN) and matrix product states [46]. In[12] benefits of the terminal phase of training for state automata extraction from RNNs (and hence matrixproduct state tensor networks) have been discussed. The authors find internal state space compression andincreased extraction in the terminal phase of training. This is similar to our findings of reduced effectivedimension in the latent (internal) space. In contrast to [12], we introduce a new tensor network, similar tothe tensor-network attention model [36], and study grokking and structure formation in a teacher-studentlearning setup.Finally, our work is related to the statistical-mechanics theory of supervised learning [10]. In the supervisedperceptron teacher-student case, an algebraic decrease of the generalisation error with the training set size(training time) has been predicted [10]. A first-order phase transition has been derived only in a restrictedsetting of discrete weights [10]. These results are typically based on the replica method [47] which requiresthe thermodynamic limit, where both the number of samples and the dimension are large and their ratiois fixed. Outstanding recent results in this direction concern the analysis of optimal generalisation errorsof generalised linear models [48, 49]. We study the same teacher-student scenario but with a restriction toa local teacher (still within the thermodynamic limit). The locality of the teacher/rule enables us to mapthe problem to a finite-dimensional latent space where we discuss grokking (a second-order phase transition)and latent-space structure formation.3 Perceptron grokkingWe consider a simple binary classification problem that exhibits the grokking phenomena. Let us assumewe have a dataset D consisting of (x̃i, yi) ∈ D, with two linearly separable classes (yi ∈ {−1, 1}) andD−dimensional features x̃i ∈ RD. More precisely, the probability densities for the positive (P+) and thenegative (P−) class are linearly separable in RD. Our model class is a simple perceptron in D dimensions,namelyf(x̃) = sgn(ŷ), ŷ = w · x̃+ b, (1)where w, x ∈ RD and b ∈ R. We sample N positive and N negative samples and then train the model withgradient descend∂θ∂t= −∂R∂θ, (2)R = 12N2N∑i=112|ŷi − yi|2 + λ1||θ||1 +λ22||θ||2,5where λ1, λ2 denote regularisation parameters, θ denotes the collection of all model parameters w, b, and|| • ||1,2 denote the one and two norm. By construction, the setup displays the grokking phenomena due tothe separability assumption. The grokking probability, grokking time, and the critical exponent depend onthe setup details.The suggested setup is relevant in the transfer learning scenario, where we initially train a model on onetask and then retrain only the final classification layer on another task. Additionally, in Section 4 weconstruct a tensor network map that connects the standard teacher-student statistical learning scenario inthe thermodynamic limit to the setup described above.In the following, we will explicitly calculate the model parameter dynamics, the test error dynamics, thecritical exponent, the grokking probability, and the grokking-time probability density function (PDF) forparticular choices of the dimension D and data probability densities P±.3.1 1D exponential modelWe start by considering a simple, one-dimensional model where we obtain all results in closed form. Althoughthe model is not applicable to the real-world scenario it captures several qualitative features and provides astarting point to study more realistic models.The dataset distribution is shown in Fig. 1. Positive and negative samples follow the same probability,i.e. P+(x̃) = P−(−x̃). The minimal distance between the positive and negative samples is 2ε, thereforeP±(|x̃| ≤ ε) = 0.P+(x̃)P−(x̃)ε−ε bx̃Figure 1: A schematic representation of the linearly separable random 1D model. The model is representedby b. The samples between ε and b (marked by thick blue line) are incorrectly classified.Since the input x is one dimensional Eq. 1 reduces tof(x) = sgn(x− b), (3)where b is the sole model parameter (we fix the weight w = 1). As described above, we train the model with6gradient descend and lossR = 12N2N∑i=112((x̃i − b)− yi)2 + λ2b22+ λ1|w|. (4)We also assume that the training dataset is balanced, i.e.∑2Ni=1 yi = 0.3.1.1 Test error dynamicsFirst, we calculate the model parameter dynamics governed by the negative gradient of the loss function∂b∂t= −∂R∂b=12N2N∑i=1(x̃i − b− yi)− sgn(b)λ1 − λ2b = x̄− sgn(b)λ1 − (1 + λ2)b, (5)where x̄ = 12N∑2Ni=1 x̃i denotes the average over training inputs. The solution to Eq. 5 with the initialcondition b(0) isb(t) = x̄λ − (x̄λ − b(0)) e−(1+λ2)t, x̄λ ={x̄−λ11+λ2, b(t) ≥ 0x̄+λ11+λ2, b(t) < 0. (6)In the following, we assume that b(0) > 0. To have a nontrivial train- and test-error dynamics, we alsoassume that xmin < b(0), where xmin denotes the minimum of the positive samples in the training dataset.To obtain explicit expressions for the test error we choose the exponential distribution of the samples P+(x) =e−(x−ε)Θ(x − ε), where Θ(x) denotes the Heaviside step function. In this case, the cumulative probabilityto get a sample with b < x is given by P (x > b) = eε−b and the test error isE(t) ={12 (1− eε−b(t)) b(t) > ε0 else, (7)where b(t) is determined by Eq. 6. We are interested in the behavior of the test error just before the testerror drops to zero. The time at which the test error becomes zero (tε) is given bytε = log(b(0)− x̄λε− x̄λ). (8)By expanding the test error (Eq. 7) around tε we getE(t < tε) ≈(ε− x̄λ)2(1 + λ2)(tε − t). (9)Interestingly, the first-order coefficient, as well as the critical exponent, do not depend on the initial condition(assuming b(0) > ε). We now calculate the average test error over different initial conditions by aligning thephase-transition points tε. Since P (−x̄) = P (x̄), we find the test error〈〈E(t)〉〉 ≈ ελ2(tε − t), ελ = ε(1 + λ2) + λ1, (10)where 〈〈•〉〉 denotes the average over all valid initial conditions b(0) and training input averages x̄.Grokking in the considered 1D exponential model is a second-order phase transition with the test-error criticalexponent equal to one. The regularisation parameters and the distance between positive and negative classdistributions change only the prefactor. In general, we expect that the critical exponent depends on thedistribution as well as the training parameters, e.g. regularisation strength.73.1.2 Grokking probabilityWe are also interested in the probability to sample a training dataset with which we can train the model tozero test error. We name this probability the grokking probability. In the considered 1D case the final testerror vanishes only if |x̄λ| < ε. We express this condition for zero test/generalisation error of the trainedmodel as|x̄| < ελ, (11)where ελ is given by Eq. 10. Since our training dataset has an equal number of positive and negative samples,we need to consider the distribution of the mean of N independent exponentially distributed variables, whichis given by the gamma distributionP expN (x̄) =NNΓ(N)x̄N−1e−Nx̄Θ(x̄), (12)where Γ(N) denotes the gamma function. First, we calculate the probability PN (x̄) to get the average x̄PN (x̄) =∫ ∞x̄+=0dx̄+PexpN (x̄+)∫ ∞x̄−=0dx̄−PexpN (x̄−)δ (x̄− (x̄+ − x̄−)/2) (13)=2NN+12 x̄N−12KN− 12 (2Nx̄)√πΓ(N),where Kn(z) denotes a modified Bessel function of the second kind. The probability to get the dataset withzero test error is then given byPE(∞)=0(ελ, N) =2∫ ελx̄=0PN (x̄)dx̄ (14)=√π(−1)N (Bελ)2N 1F̃2(N ;N +12, N + 1;N2ε2λ)+π(−1)N+1Nελ 1F̃2(12 ;32 ,32 −N ;N2ε2λ)Γ(Nd),where pF̃q (a; b; z) is the regularized generalized hypergeometric function. The above expression (Eq. 14)simplifies for a particular choice of N , e.g. for N = 2 we getPE(∞)=0(ελ, N = 2) =1− (1 + 2ελ)e−4ελ . (15)In Fig. 2 we show several numerically exact grokking probabilities. As expected, the grokking probabilityincreases with the number of training samples and the effective separation between the two classes determinedby ελ.The effect of the L1 and L2 regularisations on the trained model is different. The L2 regularisation ismultiplicative, and the L1 regularisation is additive concerning the gap between the positive and negativesamples ε. Hence, in the case of a small gap, the L1 regularisation becomes much more effective. In otherwords, for an infinitesimal gap (ε \\x1c 1) and finite N , the L1 regularisation ensures that the probability ofzero test error is finite. This is not the case when using the L2 regularisation.It would be interesting to see if L1 regularisation is preferred to L2 regularisation also in more realisticscenarios. In fact, we find a similar distinction between the L1 and L2 normalised models also in the moregeneral grokking scenario discussed in Section 3.2.2.80.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0ϵλPE(∞)=0(ϵλ)N=2N=5N=10Figure 2: Exact numerical calculation for grokking probabilities for a different number of training samplesN = 2 (dashed blue line), 5 (dotted orange line), and 10 (full green line). Grokking probability increaseswith N and ελ.3.1.3 Grokking timeWe define the grokking time as the difference between tε (zero-test-error time) and the time at which thetraining error becomes zero. In our simple case, we havetG =11 + λ2log(ε+ xmin − x̄λε− x̄λ). (16)The grokking time does not depend on the initial condition as long as b(0) > xmin. To find the grokking-time PDF, we need to calculate the distribution PN (x̄, xmin) and then consider only the part |x̄| ≤ ελ. Weprovide the details of the calculation in the Appendix A. For a finite N it is possible to obtain a closed formexpression which, however, is not very instructive. Here we provide the unnormalised grokking-time PDFfor N = 2P unnormN=2,ε,λ1(t) =18e− 4et(2et+5)ελet+1−2t(exp(4etελ(3 sinh(t) + cosh(t) + 4)et + 1+ 3t)(17)− 64ε2λe4(4− 3et+1)ελ+3t − 24ελe2((8− 6et+1)ελ+t)− 8ελe4(4− 3et+1)ελ+t− 2e4(4et+1)ελet+1 − 3e4(4− 3et+1)ελ+t −(et + 1)e4(2et− 1et+1+2)ελ(et(et − 8ελ − 1)− 2)),which we normalise by dividing with the appropriate grokking probability, see Eq. 14.In Fig. 3 we show several numerically exact grokking-time PDFs. The expected grokking time is smallerwith increasing training size N and effective class separation ελ. This is consistent with the observations of[6, 8] where a shorter grokking time has been reported for increased number of training samples and a largerweight decay.3.2 D-dimensional uniform ball modelThe second model we consider is shown in Fig. 4. The inputs are D dimensional real vectors x̃ ∈ RD. Positiveand negative samples are distributed uniformly in unit balls shifted from the origin by vectors ±ε ∈ RD.90 1 2 3 401234tGP(tG)ϵλ=0.40 2 4 6 80.00.10.20.30.40.5tGϵλ=0.040 2 4 6 8 100.000.050.100.150.200.250.30tGϵλ=0.0040 1 2 3 401234tGϵλ=0.004N=2 N=5 N=10Figure 3: Numerically exact the grokking-time tG PDF at different number of training samples N =2 (dashedblue line), 5 (dotted orange line), 10 (full green line). Left, middle, and right panel correspond to ελ =0.4(left), 0.04 (middle), 0.004 (right). Grokking time is shorter with increasing N and ελ.We will assume that the shift is along the first dimension, i.e. ε1 = ε, and εj>1 = 0. The student model is asimple perceptron determined by the vector w ∈ RD (we set the bias to zero b = 0)f(x̃) = sgn(x̃ · w). (18)x̃1x̃⊥0 εrwP+(x̃)P+(x̃)Figure 4: A two-dimensional projection of the D−dimensional uniform-ball model on the plane defined bythe shift vector ε and the model vector w. The positive and negative samples are uniformly distributed inunit balls shifted away from the origin along the x̃1 axis by ±ε, respectively. The model used to separatethe classes is a linear model (determined by the vector w) going through the origin (green line). The volumeof the shaded red and blue regions determines the test error.10We write the training loss with L1 and L2 regularisation asR = 12N2N∑i=112(x̃i · w − yi)2 + λ2||w||222+ λ1|w|1. (19)=12w ·(12N2N∑i=1x̃i ⊗ x̃i + λ21D)w − w ·(12N2N∑i=1yix̃i − λ1sgn(w))+12=12w ·Gw − w · a+ 12,G =12N2N∑i=1x̃i ⊗ x̃i + λ21D,a =12N2N∑i=1yix̃i − λ1sgn(w).Again we assume that the training dataset is balanced, i.e. yi≤N = 1, yi>N = −1, x̃i≤N = xi + ε, andx̃i>N = xi − ε. In our model xi are distributed uniformly in a D−dimensional ball centered at the origin.The presented model is relevant in the transfer learning setting [50], if only the last layer of a network isretrained, and with sigmoid activation functions in the penultimate layer. If the model transfers well to anew classification task, the latent-space distributions of the new classes are linearly separable and can bebounded by a D−dimensional ball. Since we do not know the details of the distributions, we assume theuniform distribution in the ball. Further, the positive and negative feature distributions might be embeddedin a higher dimensional latent space. In this case, D corresponds to the effective dimension of the data,which can be calculated from the covariance matrix. As we will see in the next section, the introducedD−dimensional ball model qualitatively reproduces the critical exponent and the grokking-time PDF in alocal-rule learning problem.3.2.1 Test error dynamicsTo determine the test error dynamics we first specify the dynamics of the model parameters which is deter-mined by the negative gradient of the loss function∂w∂t= −∂R∂w= −Gw + a. (20)The solution to Eq. 20 with the initial condition w(0) isw(t) = wλ −(wλ − w(0))e−Gt, wλ = G−1a. (21)If N < D/2 and λ2 = 0, the matrix G is not invertible. In this case, we use the pseudo-inverse.The test error is given by the volume of an ε-shifted, unit ball that is cut out by the plane defined by thevector w. The relevant parameter determining this volume is the distance h between the plane and the originof the ball. We find that h = ε w1||w||2 , where ε = ||ε||2 and assuming w1 > 0. The critical angle is given byw1/||w||2 = 1ε . For larger w1/||w||2 the error is zero. For smaller values of w1/||w||2 the error is given byED(h) =\\uf8f1\\uf8f2\\uf8f3 12 −DΓ(D2 )2√πΓ(D+12 )2F1(12 ,1−D2 ;32 ;h2)h , h ≤ 10 , h > 1, (22)11where 2F1(a, b; c, z) represents the Gaussian hypergeometric function. For h < 1 and close to the criticalpoint h ≈ 1 we find the following expression for the test errorED(t) ≈D2D−32 Γ(D2)√πΓ(D+32) (1− h(t))D+12 (23)=D2D−32 Γ(D2)√πΓ(D+32) (1− ε w1(t)||w(t)||2)D+12=D2D−32 Γ(D2)√πΓ(D+32) (kG(t− tε)))D+12 ,where tε is defined as time at which the test error vanishes, and the coefficient kG is given by the linearexpansion of w(t) around tε. The critical exponent is hence determined only by the dimensionality of thefeature distribution.3.2.2 Grokking probabilityNext, we will calculate the probability of training a model with zero test error (grokking probability) for agiven number of positive/negative samples N . The condition for the final test error to be zero is given bywλ1||wλ||2≥ 1ε. (24)It will be useful to write the zero test error condition in terms of components of wλ(ε2 − 1)(wλ1)2 ≥ D∑j=2(wλj)2= r, (25)where r denotes the 2-norm squared of the final weights vector wλ with the first component equal to zero.A general calculation of the grokking probability and the grokking-time PDF is not feasible since we wouldhave to invert a random matrix G. Therefore, we consider the limit of many training samples N \\x1d 1,where the matrix G decomposes into a diagonal part proportional to λ2,D = λ2 +1D+2 and an off-diagonalpart proportional to N−1/2. We provide the full derivation of the grokking probability in this limit inAppendix B. Here we consider a simpler case, where we additionally assume that N \\x1d λ2,D \\x1d 1. In thiscase, G is approximately proportional to the identityG ≈ λ2,D1D + ε⊗ ε. (26)The inverse is[G−1]i,j ≈\\uf8f1\\uf8f4\\uf8f2\\uf8f4\\uf8f3(λ2,D + ε2)−1, i = j = 1λ−12,D , i = j 6= 10 , i 6= j. (27)Finally we getwλi=1 ≈a1ε2 + λ2,D, wλi>1 ≈aiλ2,D. (28)12In the limit N \\x1d 1 the probability of the mean of 2N random vectors distributed uniformly in a D-dimensional ball is well approximated by the normal distribution with zero mean and variance 1D/2N(D+2),PD,2N (x̄) ≈ N0,1D/2N(D+2)(x̄). (29)In the following, we separately describe the grokking probability in the case λ1 = 0 and the case λ1 > 0.Case λ1 = 0 – Let us first consider the case without the L1 regularisation, i.e. λ1 = 0. The grokkingprobability is given by (see Appendix B)PE(∞)=0 =∫dx̄ PD,2N (x̄)Θ(wλ1 (x̄)||wλ(x̄)||2− 1ε)(30)≈∫ ∞−εdx̄1N0,1/2N(D+2)∫ 2N(D+2)(ε2−1)( x̄1+ε1+ε2/λ2,D)20dr χ2D−1(r)=∫ ∞−εdx̄1N0,1/2N(D+2) P\\uf8eb\\uf8ec\\uf8edD − 12,N(D + 2)(ε2 − 1)(x̄1 + ε)2(ε2λ2,D+ 1)2\\uf8f6\\uf8f7\\uf8f8 ,where χ2D−1(r) is the standard Chi-square distribution and P (s, t) is the regularized gamma function. Wealso introduced the sample average x̄ = 12N∑2Ni=1 yixi. The full grokking probability without the additionalassumption has essentially the same structure with more complicated expressions for the parameters of thedistributions (see Appendix B).For a given set of parameters λ2, D, and ε we can efficiently numerically evaluate the integral in Eq. 30 (andthe full formula reported in Appendix B). In Fig. 5 we show the full grokking probability as a function ofD, λ2, N , and ε. As expected, the grokking probability is larger with increasing distance ε and number ofsamples N . We also observe that the grokking probability exponentially decreases with the dimensionalityof the latent-space data distribution D. Therefore, latent-space distribution with a low effective dimensionis preferred for better generalisation. This result partially explains the observation in [8] which relatesgrokking to structure formation and effective dimension decrease at the transition. We expect that loweffective dimension in the latent space increases generalisation in a more general setting, beyond the simplegrokking scenario described in this section. In other words, models with latent space distributions with smalleffective dimension will more likely lead to good generalisation. Finally, by increasing the L2 regularisationstrength λ2 the grokking probability increases up to a maximum that depends on the remaining parameters.These results provide, some justification of the numerical observation in [6, 8, 9] that weight decay increasesthe parameter region where grokking is observed.Case λ1 > 0 – Let us again consider the limit N \\x1d λ2 \\x1d 1. If λ1 > 0 the stationary solution wλj dependson the sign of |x̄j | − λ1, where x̄ = 12N∑2Ni=1 xi. The j−th component of the stationary vector iswλ1 ={0 , λ1 ≥ |x̄1 + ε|x̄1+ε−λ1sgn(x̄1)λ2,D+ε2, else, wλj 6=1 ={0 , λ1 ≥ |x̄j |x̄j−λ1sgn(x̄j)λ2,D, else. (31)13N=5 N=10 N=202 4 6 8 1010-40.0010.0100.1001DPE(∞)=0D=5 D=10 D=200 1 2 30.00.10.20.30.4λ2D=5 D=10 D=2010 20 30 40 5010-510-40.0010.0100.1001ND=5 D=10 D=201 1.1 1.20.00.20.40.60.81.0εFigure 5: Grokking probability as a function of D, λ2, N , and ε. We find that larger dimension D decreasesthe grokking probability. In contrast, larger regularisation strength λ2 increases the grokking probability.As expected, increased class separation ε and number of training samples N also increases the grokkingprobability. If not specified in the panels, additional parameters are set to: D = 10, λ2 = 0.1, N = 10, andε = 1.01.The number of non-vanishing components of the stationary solution wλ depends on the value of λ1. Therefore,we get (in the N \\x1d 1 limit) an additional sum over the number of non-zero elements in the wλ,PE(∞)=0 =∫ ∞λ1−εdx̄1N0,1/2N(D+2)[(1− pλ)D−1 (32)+D−1∑k=1(D − 1k)pkλ(1− pλ)D−1−k∫ 2N(D+2)(ε2−1)( x̄1+ε−λ11+(ε/λ2,D)2)20drRk(r)],where pλ = 1 − erf(√N(D + 2)λ1)is the probability of the variable |x̄i| (for i > 1) to be larger thanλ1 and Rk(r) is the PDF of the sum of squares of k random variables sampled from the truncated normaldistribution. Since half-Gaussian distribution has a longer tail as the truncated Gaussian at λ1, we can lowerbound (or estimate) the grokking probability by using the Chi-squared distribution instead of R(r). In thelimit λ1 = 0 we recover Eq. 30. In the case λ1 > 0, we can approximate the inner integrals in Eq. 30 by theregularised gamma function and efficiently numerically evaluate Eq. 30.Further, by discarding the sum over k ≥ 1 in Eq. 32 we obtain a lower bound on the grokking probabilityPE(∞)=0 ≥∫ ∞λ1−εdx̄1N0,1/2N(D+2)(1− pλ)D−1 (33)=12(1 + erf(√N(D + 2)(ε− λ1)))(erf(√N(D + 2)λ1))D−1.We find a similar distinction between the L1 and L2 regularisations as in the simple 1D case. At ε = 1and λ1 = 0 the grokking probability vanishes for any value of λ2. In contrast, for λ1 > 0 the grokkingprobability can increase even above 90% for any D ≥ 2. Interestingly, the grokking probability increaseswith the dimensionality of the data distribution D. In fact, if we send D → ∞ the grokking probabilitybecomes 100% if 0 < λ1 < ε. This result is a consequence of the concentration of measure of the uniformdistribution ”around the equator”. Similarly, by using the lower bound Eq. 33 we estimate the best valueof λ1 for any ε, D and N and find that the grokking probability maximum is always larger than 0.915. Incontrast, in the λ1 = 0 case, the grokking probability becomes exponentially small with D, independent ofthe remaining parameter values. We make similar observations also if we relax the condition λ2 \\x1d 1 (seeAppendix B).The discussed results could be applicable more generally. It would be interesting to check if L1 weight regu-larisation in the last (classification) layer significantly improves the generalisation of deep models compared14to the L2 regularisation. The works [6, 8] do not study the differences between L1 and L2 regularisations.In [8] a consistent observation has been made, namely larger weight decay leads in most cases to a largerparameter region where grokking is observed. We confirm this expectation on a simple model discussed inSection 4.3.1 and Section 4.3.2.3.2.3 Grokking timeTo calculate the grokking time, we first determine the condition for the zero train error. In contrast to thesimple 1D case, this condition depends non-trivially on the training dataset and on the initial condition w(0).To simplify the calculation, we calculate the distribution of the upper bound on the grokking time in thelimit N \\x1d 1. We obtain the most conservative estimate for zero train error by selecting the training samplex̃ that forms the smallest angle with the plane defined shift vector ε. We write this condition in terms of thecosine of the angle asw1||w||2≥ ξtrain = maxi√||xi + ε||22 − (xi1 + ε)2||xi + ε||2, (34)where ξtrain denotes the cosine of the smallest angle between the plane defined by ε and any training samplex̃i. We will consider only the zeroth-order solution in 1/√N , where the grokking probability becomes 100%.Namely, we also discard terms proportional to 1/√N . In this limit the stationary solution is proportionalto ε, i.e. wλ ≈ ελD+ε2 . The time dependent model parameters simplify tow1(t) ≈ελD + ε2+(w1(0)−ελD + ε2)e−(λ2,D+ε2)t, (35)wj(t) ≈wj(0)e−λ2,Dt, j > 1. (36)Since we consider only the leading (zeroth) order in 1√N, the value of λ1 does not have such a dramatic effectas in Section 3.2.2. Therefore, we will study only the case λ1 = 0. To further simplify the calculation wewill also assume λ2,D = λ2 +1D+2 \\x1c 1. In this limit we findξtrain ≈ maxi√1−(xi1 + ε||xi + ε||2)2≈ xmax√x2max + ε2≈ 1√1 + ε2, (37)where xmax = maxi ||xi||2. Similarly, Eq. 35 and Eq. 36 simplify tow1(t) ≈1ε+(w1(0)−1ε)e−ε2t, (38)wj(t) ≈wj(0)e−λ2,Dt, j > 1. (39)We find that the first component of w relaxes much faster as the remaining components. Therefore, theparameter path can be approximated by two straight lines/paths. Along the first path, w1(t) quickly relaxestowards the stationary value wλ1 ≈ 1/ε. Then, along the second path, the remaining parameters slowly relaxtowards the stationary value wλj>1 ≈ 0. This leads to two different zero train/test error conditions.First, we consider the case when grokking occurs during the fast relaxation (first path). In this case, thecondition for grokking to occur reads1 ≥ 1ε2+ ||w⊥(0)||22, (40)15where w⊥(t) is obtained from w(t) by setting w1 to zero. The zero train/test error is achieved after timet =1ε2ln\\uf8eb\\uf8ed 1ε − w1(0)1ε −ξ√1−ξ2||w⊥(0)||2\\uf8f6\\uf8f8 . (41)We assume that w1(0) < wλ1 ≈ 1ε and obtain the final expression for the grokking timetG =1ε2ln\\uf8eb\\uf8ed 1ε − ξtrain√1−ξ2train ||w⊥(0)||21ε −ξtest√1−ξ2test||w⊥(0)||2\\uf8f6\\uf8f8 ≈ 1ε2ln(1− ||w⊥(0)||21− ε√ε2−1 ||w⊥(0)||2). (42)The grokking time in the considered limit depends only on the initial condition w⊥(0), i.e. on the initialdistribution of the classifier weights. We assume that the initial model weights are sampled independentlyfrom a normal distribution with zero mean and unit variance. Setting r = ||w⊥(0)||22, the variable r followsthe χ2D−1 distribution. Therefore, we express the grokking-time PDF asPfast(tG) ≈ χ2D−1(r(tG))∂r(tG)∂tG, (43)wherer(tG) =(ε2 − 1) (etGε2 − 1)2 (ε(εe2tGε2+ 2√ε2 − 1etGε2 + ε)− 1)(ε2(e2tGε2 − 1)+ 1)2 . (44)Above result represents only one part of the grokking probability and hence the distribution Pfast(tG) is notnormalised. In fact, integrating Pfast(t) over the whole domain we obtain the probability to start with theinitial condition where grokking occurs during the fast relaxationpfast =∫wP (w)Θ(1− 1ε2− |w⊥|2)dw =∫ 1− 1ε20χ2D−1(r)dr, (45)where χ2D−1 is the standard Chi-squared distribution.The second part of the grokking-time PDF comes from the initial conditions where the zero test/train erroris obtained during the slow relaxation process. In this case, we assume that the value w1(t) is stationary,i.e. w1(t) ≈ wλ1 ≈ 1ε . The remaining model parameters evolve according to Eq. 39. The time at which thetrain/test error vanishes readsttrain/test =1λ2,Dln\\uf8eb\\uf8ed ||w⊥(0)||2√1− ξ2train/testwλ1\\uf8f6\\uf8f8 . (46)After simplification we find the grokking time in the slow relaxation regimetG =12λ2,Dln(ε4ε4 − 1). (47)Interestingly, the grokking time is independent of the initial condition. Therefore, the distribution of theslow-relaxation grokking time is trivial, i.e. proportional to a Dirac delta distribution with the weight 1−pfast,where pfast is the probability of initialising the parameters with grokking during the fast relaxation given inEq. 45.16By combining the grokking-time PDFs for the fast and the slow relaxation we obtain the grokking-time PDFin the limit λ2 \\x1c ε2. In Fig. 6 we show the grokking-time PDFs for several parameters sets in the consideredlimit. Increasing the input size D reduces the probability of fast-relaxation grokking times and increases theslow-relaxation grokking time. While the fast-relaxation grokking time does not depend on the regularisationstrength λ2, smaller regularisation leads to increased slow-relaxation grokking time. On the contrary, largerclass separation decreases both fast- and slow-relaxation grokking times. We do not expect the analyticallyD=4 D=5 D=60.0 0.1 0.2 0.3 0.4 0.50.00.20.40.60.81.0tGP(tG)λ2=0 λ2=0.01 λ2=0.050.0 0.1 0.2 0.3 0.4 0.50.00.20.40.60.81.0tGε=1.5 ε=2 ε=2.50.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0tGFigure 6: Grokking-time PDF for several values of D, ε, and λ2. The short relaxation grokking-time PDFsare represented by full lines. The Dirac-delta long-relaxation grokking time is represented by vertical bars.The position of the bar is the position of the Dirac-delta function and the height of the bar represents theweight of the Dirac-delta part of the distribution. If not specified in the panels, additional parameters areset to: D = 5, λ2 = 0.01, and ε = 2.obtained grokking-time PDF to quantitatively describe real experiments, particularly because it is a zeroth-order large N solution. However, the bimodal structure and the qualitative parameter dependence shouldalso be present in more realistic scenarios. We will discuss one such example in Section 4.3.3.2.4 Critical exponents for a general isotropic data PDFBy assuming isotropic probability densities on a compact domain P± in RD we can relate the data PDFclose to the domain boundary (D−dimensional sphere) with the critical exponent. For an isotropic dataprobability density we can write the test error close to the grokking transition asEtest(δh) ≈12I2δh−δh−2(D − 12,12)∫ δh0drρ(r), (48)where, Iz(a, b) is the regularized incomplete beta function, ρ(r) is a probability distribution to find a samplewith ||x||2 = 1− r, and δh ≈ k(tε− t), and coefficient k is determined by expanding Eq. 21 around tε. If thedensity ρ(r) admits a Taylor expansion around zero, i.e. ρ(r) = ρ0 + ρ1r +O(r2), we findEtest(δh) ≈2D−12 δhD−12(D − 1)B(D−12 ,12) (ρ0δh+ 12ρ1δh2)(49)∝ρ0δhD+12 +12ρ1δhD+22∝(t− tε)D+12 ,where B(a, b) is the Euler beta function. Obtained critical exponent ν = D+12 is universal for isotropicprobability densities that do not vanish at the ball boundary and is consistent with the result in Eq. 23. If17in addition the density ρ(δh) has an algebraic divergence, e.g. ρ(δh) ≈ ρξδh−ξ where 0 < ξ < 1 we getEtest(t) ∝ (t− tε)D+1−2ξ2 . (50)The critical exponent of the test error reveals the behaviour of the sample density at the boundary of thesample domain. While both the grokking probability and the grokking-time PDF depend on the details of themodel’s initial parameters and the evolution, the critical exponent ν depends only on the data distribution atthe boundary of the domain. Therefore, we expect that Eq. 50 describes the critical exponent quantitativelyalso in a more general setting. In this case we might have to relax the condition 0 < ξ < 1 to accommodatea more general divergence of the data distribution at the sample domain boundary.4 Learning local rules with shallow tensor networksIn standard rule-learning theory the teacher-student model describes a setting where the student model hasto learn a rule given by the teacher model, see [10]. In the simplest scenario where the teacher and thestudent models are perceptrons of the form Eq. 18 we use statistical mechanics methods to calculate theexpected generalisation error for a given number of training samples (or training time). This is achieved inthe thermodynamic limit where the input size M , and the number of training samples N go to infinity suchthat N = αM . The teacher and student weights are sampled uniformly on an M -sphere. In this setup, onecan use the replica trick [47] to calculate the test-error behaviour as a function of α. One finds Etest ∝ 1αwhen α→∞. Although sudden transitions to zero generalisation error are possible, they are a consequenceof a restriction on the phase space of parameters, e.g. in the Ising perceptron the parameters can take onlyvalues ±1.In summary, the standard rule-learning theory does not describe the grokking phenomenon and it is notclear how to reconcile the standard algebraic decay to zero test error with the grokking phase transitionobserved in deep models and presented in Section 3.In this section, we fill this gap by introducing a local-rule learning scenario and a tensor-network map,allowing to interpolate between the standard mean-field like theory and the local, grokking setup. In partic-ular, we introduce a local teacher and a tensor-network student setup which displays the grokking behaviourdescribed in the previous section without any restriction on the values of the student model parameters.The tensor-network techniques will provide a correspondence between the standard teacher-student setup inthe thermodynamic limit and the setup described in the Section 3. The grokking phase transition is then aconsequence of the locality of the learned rule.4.1 Local teacher modelIn the standard statistical-learning scenario, we determine the output of the teacher model (the rule) byEq. 18 (see [10]). In this case all values of the input contribute to the final result. In the thermodynamiclimit this leads to a mean-field like behaviour, i.e. the value of the input at any particular position has onlyinfinitesimal influence on the result/rule.We will study the opposite, local scenario x→ y, where x, y ∈ {−1, 1}M . The i-th component of the outputvector yi will depend only on a K-neighborhood of the input at position iyi = rule(xi−K , . . . , xi, . . . xi+K). (51)18We call such model a K−local model. The Eq. 51 describes a well-known cellular automata computationalparadigm. Cellular automata are a universal discrete space-time dynamical systems with a finite set ofpossible states at each position [51, 52]. We define a cellular automaton by a set of rules which transform oneconfiguration of states into another configuration. We will consider the rule 30 one-dimensional automaton(K = 1) [51, 52], which exhibits chaotic behaviour and is defined by the rule yi = rule30(xi−1, xi, xi+1). Thenext state of the cell i, i.e. yi, is determined by the current configuration at cells i − 1, i, and i + 1, i.e.xi−1, xi, xi+1, as followsxi−1,xi,xi+1 -1,-1,-1 -1,-1,1 -1,1,-1 -1,1,1 1,-1,-1 1,-1,1 1,1,-1 1,1,1yi = rule30(xi−1,xi,xi+1) -1 1 1 1 1 -1 -1 -1. (52)We show an example time evolution of the rule 30 cellular automaton in Fig. 7. The initial condition is representedby the first line, black cells represent the value 1, and white cells represent the value -1. Our aim will be to learn onestep of this evolution.Figure 7: Diagramatic representation of the rule-30 cellular automaton. In our convention, the black cellsrepresent 1 and the white cells represent -1. The diagram is taken from Wikipedia [53].The rule-30 automaton has already been discussed in the context of sequence-to-sequence prediction with tensornetworks [33, 22, 36], however, no grokking phenomena have been reported. To study the effect of the neighbourhoodsize K, we shall consider a rule defined by K consecutive applications of rule 30. We will refer to such rule as aK–local rule.In summary, we modify the standard perceptron teacher-student setup by restricting the teacher model to localinstead of global rules. The teacher will be modelled by a local map transforming a sequence x into the sequence y.The task will be to approximate the chosen map by training on a finite set of input samples of length M . For a finiteM we can choose open and closed boundary conditions. Open boundary conditions refer to the case when y1 and yMare calculated as if x0 = xM+1 = −1. In the case of closed boundary conditions we have x0 = xM and xM+1 = x1.The test set will include all possible input sizes from Mtest = 3, 4, . . . ,∞. We will determine the error as the ratio ofincorrectly predicted values yi.Besides the change from a global to a local rule, we will also modify the student model. Instead of the standardperceptron student model, we will use the uniform tensor-network attention model.194.2 Uniform tensor-network student modelThe simplest student model discussed in the literature is a perceptron model which is not applicable to our problem,since we will discuss inputs of different sizes. The standard architectures applicable to variable size inputs are therecurrent neural network (RNN) model and the convolutional neural network model. However, we found it convenientto use a tensor network approach, which enables us to construct a bridge between the teacher-student rule learningscenario and the grokking model discussed in the previous section.1 Before introducing the tensor-network attentionlayer and the student model we will summarise the basic properties of tensor networks applied to machine learning[20, 21].4.2.1 Short introduction to tensor network methodsA tensor network is a tensor that is represented as a contraction of two or more tensors. The tensors that are contractedtypically have much smaller number of dimensions (indices) and hence less parameters. A trivial example of a tensornetwork is a scalar product of two vectors, where the second vector is a result of a matrix vector multiplication,c = u ·Av= v, = u, = A, (53)= c.We introduced a diagrammatic notation, which makes longer tensor contractions more transparent. A tensor in thisnotation is represented as a circle with legs. The direction of the legs is typically not important. The number oflegs determines the dimensionality of the tensor, e.g. a number has zero legs, a vector has one leg, a matrix hastwo legs etc. The most prominent tensor network, related to RNNs, is the matrix product state (MPS) obtained bycontracting 3-dimensional tensorsψ = . (54)To use a tensor network as a machine learning model, we have to transform the inputs such that they can be contractedwith the tensor-network model in order to produce a scalar output. We do that by using an embedding function andtransform the elements of the input vector with a vector transformationφ(xj) = . (55)The entire input vector is then transformed asΦ(x) = φ(x1)⊗ φ(x2)⊗ . . .⊗ φ(xM ) = . (56)Formally, Φ(x) is an exponentially large vector with a compact MPS representation and will never be used directly.The output of the MPS model is then a contraction of the embedded input elements with the MPS tensor-networkmodelψ · Φ(x) = (57)We can produce a vector output by adding one dimension to one of the MPS tensors. The presented setup has allmain parts of the typical tensor-network model. It is differentiable with respect to tensor-network parameters andapplicable to the standard training methods based on gradient descent.1Due to a connection between RNNs and tensor networks [46] we expect that one can rephrase our tensor network model inthe language of RNNs.204.2.2 Tensor-network attention modelIn this section, we will introduce a simplified version of the tensor network proposed in [36]. As in the introductoryexample above, the entire model has two parts: an embedding layer and a tensor-network attention layer. Since theinput is binary, we define the embedding layer with a local embedding function φ(xi) : {−1, 1} → R2 asφ(−1) =(10), φ(1) =(01). (58)After the embedding, we apply the tensor-network attention determined by two parameter tensors A,B ∈ Rd×d×2.We call the tensor A the attention tensor and the tensor B the classification tensor. The names of the tensors A andB reflect their role in the tensor-network attention layer. As we will describe below (see also Section 4.2.3), for agiven position the tensor A determines the context of the input which is then linearly classified by the tensor B.First, we construct matrices A(i) by contracting the attention tensor A with the local embedding vectors φ(xi)Aµ,ν(i) =2∑j=1Aµ,ν,jφ(xi)j . (59)Then, we use the matrices A(i) to construct the left and right context matrices HL,R(i)HL(1) = 1d, HL(i) = HL(i− 1)A(i− 1), (60)HR(M) = G, HR(i) = A(i+ 1)HR(i+ 1). (61)The matrix G determines the boundary conditions of the model. In the case of closed boundary conditions G = 1d.In the case of open boundary conditions G = vL ⊗ vR, where the boundary vectors vL,R ∈ Rd are additional modelparameters. Alternatively, the boundary vectors vL,R can be determined as left and right eigenvectors of the matrixA0 corresponding to the largest eigenvalue. The final local weight vector w(i) is then obtained by contracting thetensor B with the normalised left and right context matrices HL,RN = HL,R/||HL,R||2,w(i)j = Tr(HLN(i)BjHRN (i)), j = 1, 2, i = 1, . . . ,M, (62)where Bj denotes the matrix with elements [Bj ]µ,ν = Bµ,ν,j . We calculate the attention layer output at position i asŷi = w(i) · φ(xi). (63)The final model output is then obtained by using the sign nonlinearity f(x) = sgn(ŷ). The described tensor-networklayer is a generalisation of the linear-dot attention mechanism (see [36]). Therefore, we refer to it as a tensor-networkattention.It is instructive to present the tensor-network attention layer in a diagramatic form by using the following definitionsφ(xi) = , A = , B = , ID = , G = . (64)We compactly write the entire transformation of an input at the position i asA(j) = = , (65)HL(i) = = ,HR(i) = = ,ŷi = .214.2.3 Tensor network mapThe described tensor-network attention model also implements a map from inputs of variable length M to vectorsof length 2d2. In the case of fixed attention tensors A all possible infinitely many inputs define a PDF of vectorszi(x) ∈ R2d2, wherezi(x) = HRN (i)HLN(i)⊗ φ(xi). (66)We show a schematic representation of the map in Fig. 8. By considering zi(x) as input features we can interpret themodel defined by Eq. 63 as a perceptron defined by the weight tensor B, namelyŷ = zi(x) · ~B. (67)In the above formula ~B denotes the vectorised classification tensor B. By setting D = 2d2, we have mapped thelocal-rule learning problem in the thermodynamic limit to a (grokking) classification problem of the form discussed inSection 3. Interestingly, for a K−local rule, we can find 4K-dimensional matrices A for which the transformed problemis solvable by a simple perceptron model and exhibits the grokking phenomena. Therefore, the 1/α dependence onthe training set size obtained from the standard rule-learning theory seems to be a consequence of the mean-fieldtype infinite-range rule. For any local rule, we will observe grokking.2εxyM = 3xyM = 4x ∈ {−1, 1}⊗M HR(i)HL(i)⊗Φ(xi)−−−−−−−−−−→ R2d21 −1 1−1 −1 11 −1 −1 11 1 1 1Figure 8: The tensor network map from {−1, 1}M to R2d2 implemented with Eq. 66, with 2ε denoting thedistance between the closest positive and negative samples.4.3 Simulation details and resultsIn this section, we present the results of training the uniform tensor-network student model on local algorithmicdatasets. First, we consider the setting where the tensors A are fixed and map the inputs to separable distributionsas shown in Fig. 8. We compare the numerical results with the predictions of the grokking model presented inSection 3. Second, we train the entire student model, i.e. the tensor A and the tensor B. In this case, we also discussstructure formation.224.3.1 Constant attention tensorsWe now discuss the simulation results obtained by fixing the attention tensors A. Namely, we use the proposedtensor-network model as a map from {−1, 1}M to R2d2as discussed in Section 4.2.3 and shown in Fig. 8. We choosethe left and the right boundary vectors vL,R to be the eigenvectors of A0 corresponding to the largest eigenvalue.We also fix the bond dimension d = 2 and study the 1–local rule, which facilitates the comparison with the resultsdiscussed in Section 3.We determine the attention tensors A by independently sampling each element according to the normal distributionwith zero mean and unit variance. Since not all attention vectors lead to solvable problems, we perform rejectionsampling by checking if the final model parameters given by Eq. 21 have zero test error. Once we obtain a solvableinstance of the attention tensor A, we do not change its parameters during training.Exact 1–local attention We first consider the learning dynamics in the case of exact attention tensorsA0 =\\uf8eb\\uf8ec\\uf8ec\\uf8ed1 1 0 00 0 0 01 1 0 00 0 0 0\\uf8f6\\uf8f7\\uf8f7\\uf8f8 , A1 =\\uf8eb\\uf8ec\\uf8ec\\uf8ed0 0 0 00 0 1 10 0 0 00 0 1 1\\uf8f6\\uf8f7\\uf8f7\\uf8f8 . (68)The minimal bond dimension of the exact solution can be reduced to 2 if we generalise the model and train differentleft and right attention tensors A. In the exact, 1-local attention case, the vectors zi(x) contain only informationabout the state of the neighbouring positions. Since the smallest size M = 3 contains all eight possible inputs, largertraining set size M does not change the results. After averaging over many initialisations of the classifier part of thenetwork B we obtain the average test error shown in Fig. 9. We observe a first-order transition with a jump of 1/4in the test/train error. Here the factor 1/4 comes from the fact that the neighbourhood of any given position hasfour possible different values.-2.5 -2.0 -1.5 -1.0 -0.5 0.00.00.20.40.60.81.0t-tϵEtestFigure 9: The first-order phase transition when learning with the exact attention tensors A given in Eq. 68.The jump in the transition is 1/4.In the following, we discuss results obtained by randomly sampling the attention tensors A. We show results for threedifferent attention tensors A, namely Example 1, Example 2, and Example 3 reported in Appendix C.Grokking probability We estimate the grokking probability as the fraction of the sampled attention tensorsA that leads to linearly separable feature space data for the studied rule. In contrast to the grokking probabilitiesdiscussed in Section 3, we fix the training set to contain all possible samples of length M = 3. In Fig. 10 weshow the dependence of the grokking probability with respect to regularisation strengths λ1,2. We observe that theL2 regularisation decreases the grokking probability while the L1 regularisation first slightly increases the grokking23probability and then decreases compare to models without regularisation. In all cases the L1 regularised modelhas larger grokking probability as the L2 regularised model with the same regularisation strength. Larger grokkingprobability for L1 regularised models is another indicator that L1 regularisation could lead to better generalisationcompared to the L2 regularisation.0 0.01 0.0200.010.02λ1 , λ2PGFigure 10: Grokking probability (PG), representing the fraction of the attention tensors A that map the1–local rule to linearly separable data. We show the dependence of PG on the L1 regularisation strength(λ1 blue circles) and the L2 regularisation strength (λ2 orange squares). The L1 regularised models havelarger grokking probability compared to the L2 regularised models. We used 20k random attention tensorsto estimate the grokking probability.Critical exponent ν Sampled attention vectors HL,R(i) also contain information about the input beyond onlythe neighbouring sites. Moreover, information about the neighbours is not complete. Therefore, we observe asecond-order transition, as discussed in Section 3. In Fig. 11 we show the average test error for three different butfixed attention tensors obtained by rejection sampling. The exact values of the attention vectors are reported inAppendix C. We find that the critical exponent ν does not depend on the regularisation strengths λ1,2 and is inall cases smaller than one, which is in agreement with the predictions of the simple grokking model discussed inSection 3.λ1=0.01 λ1=0.1 λ2=0.1ν=0.86ν=0.85ν=0.820.1 10.11tϵ -tEtrainλ1,2=0 λ1=0.05 λ2=0.1ν=0.77ν=0.76ν=0.720.1 10.11tϵ -tEtrainλ1,2=0 λ1=0.2 λ2=0.2ν=0.82ν=0.86ν=0.820.1 10.11tϵ -tEtrainFigure 11: Mean test error for fixed attention vectors. From left to right we report results forExample 1, Example 2, and Example 3 attention tensors A given in Appendix C. The fitted criticalexponents ν are shown in the plots and only mildly depend on λ1,2.boundary of the domain, and the distance between positive and negative samples ε. These quantities are596calculated from the training-dataset features zi(x). To calculate the effective dimension we first find the597explained-variance ratios of the principal components of zi(x). Then we calculate the entropy of the ratios.598Finally, the effective dimension is obtained as the exponent of the entropy. We report the entropies for the599considered Examples 1-3 in Table 1.600We also use the vectors zi(x) to estimate the divergence exponent of the sample PDF at the boundary of601the domain. In the considered case, the vectors zi(x) are a tensor product of three vectors. Therefore, we602estimate the PDF divergence by focusing separately on each of the components of the vector zi(x). One of the603vectors is a constant vector determined by the embedding function and does not contribute to the divergence604exponent. The remaining, important parts are the left and the right context vectors, namely vLHLN (i) and605HRN (i)vR. We consider the normalised context vectors which in addition have size two. Therefore, they are606uniquely determined by the angle with the first component and we estimate the divergence at the boundary607of the domain by studying the PDF of the angle. We estimate the divergence exponent by looking at the608behaviour of the estimated PDF at the boundary with an increasing number of bins. The final exponent is609obtained as a sum of the exponents obtained from the left and the right-attention part of the feature vector610zi(x). We find (see Fig. 12) that the PDF diverges algebraically with the powers reported in Table 1.100 1000 104 105105010050010005000104#binsMax(prob.density)ξmax=1.2100 1000 104 105101001000104105106#binsMax(prob.density)ξmax=1.8100 1000 104 105101001000104105#binsMax(prob.density)ξmax=1.6Figure 12: The estimated PDF maximum of the positive (black) and negative (orange) samples.The dashed line fits correspond to the left context vectors vLHLN(i) and the full line fits to the rightcontext vectors HRN(i)vR. The final value reported in the panel title is obtained as a maximumsum of the left and right divergence exponents. From left to right we report results for Example 1,Example 2, and Example 3 attention tensors A given in Appendix C.611In Section 3.2.4 we derived a relation between the exponents ν, ξ and the effective dimension for a simple612D−dimensional ball model. Interestingly, we find that the relation given by Eq. 50 obtained from a simple613spherically symmetric model is reasonably close in two out of the three considered cases (see Table 1).61426λ1=0.01 λ1=0.1 λ2=0.1ν=0.86ν=0.85ν=0.820.1 10.11tϵ -tEtrainλ1,2=0 λ1=0. 5 λ2=0.1ν=0.77ν=0.76ν=0.720.1 10.11tϵ -tEtrainλ1,2=0 λ1=0.2 λ2=0.2ν=0.82ν=0.86ν=0.820.1 10.11tϵ -tEtrainFigure 11: Mean test erro for fixed attention vectors. From left to right we report results forExample 1, Example 2, and Example 3 attention tensors A given in Ap endix C. The fit ed criticalexpone ts ν are shown in the plots and only mildly depend on λ1,2.boundary of the domain, and the distance between positive and negative samples ε. These quantities are596calculated from the training-dataset features zi(x). To calculate the effective dimension we first find the597explained-variance ratios of the principal components of zi(x). Then we calculate the entropy of the ratios.598Finally, the effective dimension is obtained as the exponent of the entropy. We report the entropies for the599considered Examples 1-3 in Table 1.600We also use the vectors zi(x) to estimate the divergence exponent of the sample PDF at the boundary of601the domain. In the considered case, the vectors zi(x) are a tensor product of three vectors. Therefore, we602estimate the PDF divergence by focusing separately on each of the components of the vector zi(x). One of the603vectors is a constant vector determined by the embedding function and does not contribute to the divergence604exponent. The remaining, important parts are the left and the right context vectors, namely vLHLN (i) and605HRN (i)vR. We consider the normalised context vectors which in addition have size two. Therefore, they are606uniquely determined by the angle with the first component and we estimate the divergence at the boundary607of the domain by studying the PDF of the angle. We estimate the divergence exponent by looking at the608behaviour of the estimated PDF at the boundary with an increasing number of bins. The final exponent is609obtained as a sum of the exponents obtained from the left and the right-attention part of the feature vector610zi(x). We find (see Fig. 12) that the PDF diverges algebraically with the powers reported in Table 1.100 1000 104 105105010050010005000104#binsMax(prob.density)ξmax=1.2100 1000 104 105101001000104105106#binsMax(prob.density)ξmax=1.8100 1000 104 105101001000104105#binsMax(prob.density)ξmax=1.6Figure 12: The estimated PDF maximum of the positive (black) and negative (orange) samples.The dashed line fits correspond to the left context vectors vLHLN(i) and the full line fits to the rightcontext vectors HRN(i)vR. The final value reported in the panel title is obtained as a maximumsum of the left and right divergence exponents. From left to right we report results for Example 1,Example 2, and Example 3 attention tensors A given in Appendix C.611In Section 3.2.4 we derived a relation between the exponents ν, ξ and the effective dimension for a simple612D−dimensional ball model. Interestingly, we find that the relation given by Eq. 50 obtained from a simple613spherically symmetric model is reasonably close in two out of the three considered cases (see Table 1).61426λ1=0.01 λ1=0.1 λ2=0.1ν=0.86ν=0.85ν=0.820.1 10.11tϵ -tEtrainλ1,2=0 λ1=0.05 λ2=0.1ν=0.77ν=0.76ν=0.720.1 10.11tϵ -tEtrainλ1,2=0 λ1=0.2 λ2=0.ν=0.82ν=0.86ν=0.820.1 10.11tϵ -tEtrainFigure : Mean test err r f r fi e attention vectors. From left to right we r port result forExample 1, Example 2, and Example 3 attention tensors A given i Appendix C. The fitted criticalexponents ν are shown in the plots and only mildly dep nd on λ1,2.boundary of the domain, and the distance between positive and negative samples ε. These quantities are596calculated from the training-dataset features zi(x). To calculate the effective dimension we first find the597explained-variance ratios of the principal components of zi(x). Then we calculate the entropy of the ratios.598Finally, the effective dimension is obtained as the exponent of the entropy. We report the entropies for the599considered Examples 1-3 in Table 1.600We also use the vectors zi(x) to estimate the divergence exponent of the sample PDF at the boundary of601the domain. In the considered case, the vectors zi(x) are a tensor product of three vectors. Therefore, we602estimate the PDF divergence by focusing separately on each of the components of the vector zi(x). One of the603vectors is a constant vector determined by the embedding function and does not contribute to the divergence604exponent. The remaining, important parts are the left and the right context vectors, namely vLHLN (i) and605HRN (i)vR. We consider the normalised context vectors which in addition have size two. Therefore, they are606uniquely determined by the angle with the first component and we estimate the divergence at the boundary607of the domain by studying the PDF of the angle. We estim te th divergence exponent by looking at the608be aviour of the estimated PDF at the boundary with an increasing number of bins. The final expo ent is609obtained as a sum of the exponents obtained from the left and the right-attention part of t e feature vector610zi(x). We find (see Fig. 12) that the PDF dive ges algebraically with t e powers reported in Table 1.100 1000 104 10510501 050010005000104#binsMax(prob.density)ξmax=1.2100 1000 104 105101001000104105106#binsMax(prob.density)ξmax=1.8100 1000 104 10510101000104105#binsMax(prob.density)ξmax=1.6Figure 12: The estimated PDF maximum of the positive (black) and negative (orange) samples.The dashed line fits correspond to the left context vectors vLHLN(i) and the full line fits to the rightcontext vectors HRN(i)vR. The final value reported in the panel title is obtained as a maximumsum of the left and right divergence exponents. From left to right we report results for Ex mple 1,Exa ple 2, and Example 3 attention tens rs A given in Appendix C.611In Section 3.2.4 we derived a relation between the exponents ν, ξ and the effective dimension for a simple612D−dimensional ball model. Interestingly, we find that the relation given by Eq. 50 obtained from a simple613spherically symm tric o el is reas a ly close in two out of the three consider d cases (s e Table 1).61426Figure 11: Average test error during training with fixed attention vectors (log-log plot). From left to rightwe report results for Example 1, Example 2, and Example 3 attention tensors A given in Appendix C. Thefitted critical exponents ν are shown in the plots and only mildly depend on λ1,2.Besides the test-error critical exponent we estimate several properties of the feature distributions. In particular, wecalculate the effective dimension Deff , the divergence exponent ξ of the sample PDF at the boundary of the domain,and the distance between positive and negative samples ε. These quantities are calculated from the training-datasetfe tures zi(x). To calculate the effective dimensi Deff we first find σk defined as the fraction of the varianceexplained by the kth principal component of the training dataset featur s zi(x). Th n we calculat the entropy S24of the ratios σk defined as S = −∑k σk log σk. Finally, the effective dimension is obtained as the exponent of theentropy, i.e. Deff = eS . We report the effective dimensions for the considered Examples 1-3 in Table 1.We also use the vectors zi(x) to estimate the divergence exponent of the sample PDF at the boundary of the domain.In the considered case, the vectors zi(x) are a tensor product of three vectors. Therefore, we estimate the divergencein the PDF by focusing separately on each of the components of the vector zi(x). One of the vectors is a constantvector determined by the embedding function and does not contribute to the divergence exponent. The remaining,important parts are the left and the right context vectors, namely vLHLN (i) and HRN (i)vR. We consider the normalisedcontext vectors which, in addition, have size two (since we fix d = 2). Therefore, they are uniquely determined bythe angle with the first component and we can accordingly estimate the divergence at the boundary of the domain bystudying the PDF of the angle. We estimate the divergence exponent by looking at the behaviour of the estimatedPDF at the boundary with an increasing number of bins. The final exponent is obtained as a sum of the exponentsobtained from the left and the right-attention part of the feature vector zi(x). We find (see Fig. 12) that the PDFdiverges algebraically with the powers reported in Table 1.100 1000 104 105105010050010005000104#binsMax(prob.density)ξmax=1.2100 1000 104 105101001000104105106#binsMax(prob.density)ξmax=1.8100 1000 104 105101001000104105#binsMax(prob.density)ξmax=1.6Figure 12: The estimated PDF maximum of the positive (black) and negative (orange) samples. The dashedline fits correspond to the left context vectors vLHLN(i) and the full line fits to the right context vectorsHRN (i)vR. The final value reported in the panel title is obtained as a maximum sum of the left and rightdivergence exponents. From left to right we report results for Example 1, Example 2, and Example 3attention tensors A given in Appendix C.In Section 3.2.4 we derived a relation between the exponents ν, ξ and the effective dimension for a simpleD−dimensionalball model. Interestingly, we find that the relation given by Eq. 50 obtained from a simple spherically symmetricmodel is reasonably close in two out of the three considered cases (see Table 1).We also estimate the class separation from the actual feature space distribution and report it in the units of theintra-class variance (see Table 1).Deff ν ξ∗ ξ = 12 (Deff − 2ν + 1) (Eq. 50) εExample 1 3.0 0.85 1.2 1.15 1.45Example 2 3.8 0.75 1.8 1.65 1.46Example 3 3.0 0.84 1.6 1.16 1.6Table 1: Critical exponent ν and numerically calculated characteristic parameters of the feature vector zi(x)distribution. We also compare the numerically estimated divergence of the sample PDF at the boundary ξ∗with the prediction of the spherical model (Eq. 50).Grokking time Finally, we estimate grokking-time PDF, see Fig. 13. We do not expect that the prediction ofSection 3.2.3 will quantitatively describe the estimated grokking-time PDF. Besides the “worst-case” initial conditionassumption, the condition N \\x1d 1 is not valid. Hence, the actual value of G is far from identity. However, somequalitative predictions of theD−dimensional ball model can still be observed. First we notice, the bimodal structure of25the estimated grokking-time PDF. In the spherical model, the two peaks are a consequence of the separation betweenthe slow and fast modes, where the dynamics of the slow modes was essentially determined by the regularisationstrength λ2. Similarly, in all three considered cases (i.e. Example 1-3) we can separate the eigenvalues of G by sizein two sets. In one set the eigenvalues are by one order of magnitude larger than in the other. However, increasingthe regularisation strength λ2 often leads to increased grokking time, which is not the case in the simple uniformball model. The discrepancy is a consequence of the non-diagonal matrix G, which mixes different components ofthe vector zi(x). We also observe that a larger effective dimension Deff (see Table 1) leads to longer grokking timesand a larger class separation ε to smaller grokking times. The last two observations are in agreement with theD−dimensional ball model discussed in Section 3.2.3.λ1=0.01 λ1=0.1 λ2=0.10 1 2 3 4012345tGP(tG)λ1,2=0 λ1=0.05 λ2=0.10 2 4 6 801234tGP(tG)λ1,2=0 λ1=0.2 λ2=0.20.0 0.5 1.0 1.5 2.00246810tGP(tG)Figure 13: We show the estimated grokking-time PDFs for three fixed attention tensors: Example 1 (left),Example 2 (middle), and Example 3 (right). In most cases, the grokking-time PDF is bimodal, which is inagreement with the prediction of the simple grokking model discussed in Section 3.2.3.The presented results are obtained by averaging over many initialisations of the classification tensor B. We sample theinitial elements of B from a normal distribution with zero mean and unit variance. Changing the initial distributioncan impact the results. Determining the effect of the initial distribution of B on the grokking-time PDF and thecritical exponent is left for future research.4.3.2 Full model training and structure formationIn this section, we discuss grokking and structure formation properties of the complete student model introduced inSection 4.2. We initialise the model with a random initial condition, where all the entries of the tensors A,B areuncorrelated and sampled according to a normal distribution with zero mean and unit variance. We train the modelwith the Adam optimiser (with standard parameter setting) and learning rate 0.005. We use the same loss as in theprevious sections Eq. 4 with L1,2 regularisation strength λ1,2 ∈ [0, 0.001]. The regularisation strength is the same forthe attention tensor A and the classifier tensor B. We add a sigmoid non-linearity before the final sign non-linearityto improve the training stability and reduce the training time. We consider only open boundary conditions and setx−1 = xM+1 = −1. Finally, in the main text we consider only the 1-local rule. We discuss the 2– and 3–local rules inAppendix D. We perform tests in three situations, namely, without regularisation (λ1,2 = 0), with L2 regularisation(λ1 = 0, λ2 = 0.0001), and with L1 regularisation (λ1 = 0, λ2 = 0.001). We chose the regularisation strengths λ1,2to be the largest regularisation strengths with only few spikes in the training loss after the grokking time. To obtainzero test error it is sufficient (in almost all cases) to train only the attention parameters A and fix the classificationparameters B. This is a consequence of the gauge symmetry of the tensor attention layer [36]. However, we willalways train all model parameters. Since the full tensor-attention model is non-linear, we do not expect the theorydeveloped in Section 3 to be valid. On the other hand, we do observe phenomena related to neural collapse [5] andstructure formation [8].26Average test error and average effective dimension First, we investigate the dynamics of the averagetest error and calculate the critical exponent ν. In Fig. 14 we show that the critical exponent decreases uponincreasing regularisation. Larger regularisation leads to a sharper transition to zero test error, in contrast withthe linear case studied in the Section 3.2.2 and in the Section 4.3.1, where the critical exponent was found to beindependent of the regularisation strengths λ1,2. The test error drops to zero at the grokking transition. Followingthe grokking transition, the test error is non-zero and experiences fluctuations. These fluctuations can be detectedas sharp increases in the training loss and are more common in models with large regularisation. Therefore, the L1,2regularised models have larger average test error after the grokking transition.0.5 0.0 0.5 1.0t t0.00.20.40.60.81.0E test1=0, 2=0d=10, =1.99d=20, =2.81d=40, =2.800.5 0.0 0.5 1.0t t0.00.20.40.60.81.0E test1=0, 2=0.0001d=10, =1.09d=20, =0.82d=40, =0.870.5 0.0 0.5 1.0t t0.00.20.40.60.81.0E test1=0.001, 2=0d=10, =0.64d=20, =0.72d=40, =0.67Figure 14: The average test error at the phase transition. We align the first point where the test errorbecomes zero (i.e. the time tε) and take the average over many (∼ 1000) initialisations of the modelparameters. Training without regularisation results in larger critical exponent ν ≈ 2.5 as training with L1(ν ≈ 0.7) or L2 (ν ≈ 0.9) regularisation. In all experiments we used learning rate 0.005. In the legends wereport the bond dimension of the trained models d and the corresponding fitted critical exponent ν.The shape of the average test error close to transition point tε (or the critical exponent ν) depends only slightly on themodel size (bond dimension d). This suggests that the effective dimension of the mapped data Deff is independentof the model size. We confirm this by calculating the effective dimension of the features z(i). Since we studyonly open boundary conditions, we consider only the effective dimension of the left context vectors vLHLN(i). Theright context vectors HRN (i)vR have the same properties because of the model symmetry. As shown in Fig. 15, theaverage effective dimension drops significantly just before the grokking transition. We observe that regularisationsignificantly decreases the effective dimension of the mapped vectors vLHLN(i). The effective dimension is smallestwith L1 regularisation, which is expected since the L1 regularisation enforces sparsity while the L2 regularisationenforces smoothness.Structure formation and grokking We relate the decrease of the effective dimension to structure formation.As can be seen in examples shown in Fig. 16 and Fig. 17, small effective dimension signals an emergent feature spacestructure which, however, can be different in each example. Similarly, in [8] the authors argue that the grokking indeep models is related to structure formation. Our findings differ from those of [8] in that we discuss ensemble/averagephenomena. The authors of [8] discuss the connection between grokking and structure formation on the single-modellevel. In contrast, we argue that grokking and structure formation are related on average as shown in Fig. 14 andFig. 15, and not for every training run individually. We call the structure formation and grokking for a single trainingof a model the model-wise structure formation and the model-wise grokking. We disentangle model-wise structureformation from model-wise grokking by observing specific training samples. We typically observe the appearanceof a simple structure in data in the proximity of the grokking transition. This is consistent with a sharp drop ofthe average effective dimension at the transition (shown in Fig. 15). Additionally, we observe that feature spacestructures can be different for different model parameters and initialisations.In Fig. 16 and Fig. 17 we show the structures appearing in the features vLHLn (i) with bond dimension d = 3. In270.5 0.0 0.5 1.0t t0.02.55.07.510.012.515.0effective dimensionDeff = 1.81=0, 2=0d=10d=20d=400.5 0.0 0.5 1.0t t051015Deff = 1.31=0, 2=0.0001d=10d=20d=400.5 0.0 0.5 1.0t t05101520Deff = 1.11=0.001, 2=0d=10d=20d=40Figure 15: The average effective dimension corresponding to the error in Fig. 14. Larger regularisation resultsin a smaller effective dimension Deff after the grokking transition. The horizontal dashed line correspondsto the average of the minimal effective dimension of data with zero test error (over all samples with fixed d).Fig. 16 we see that the structure of the feature space data changes also during a single run. This can be detected asa spike in the training loss or as a step-like jump in the effective dimension (see Fig. 16). The structures can changefrom lower to higher dimensional and vice versa, e.f. see Fig. 16 – the transition between t = 1 and t = 1.2 increasesthe effective dimension Deff. of the mapped data. Appearance of geometric structures in the latent space does notnecessary lead to good generalistion, i.e. small test error (see Fig. 17 at time t = 0.57). Finally, we also show inFig. 17 that we can have a small generalisation/test error with complex or not apparent feature space structures (seeFig. 17 at time t = 1.19). These empirical observations suggest that grokking and structure formation are not relatedmodel-wise. That structure formation and grokking are in general two distinct phenomena is further corroboratedby our simple grokking model discussed in Section 3, which does not require any special geometric structure (asidefrom the condition of linear separability).Grokking time Finally, we estimate the PDF of the grokking times, see Fig. 18 (top panels). Taking the non-regularised case as the baseline, we find that L1 regularisation decreases the average grokking time tG significantlymore than L2 regularisation. Further, grokking times for L2 regularised models increase with the model size. On theother hand, non-regularised and L1 regularised models have roughly a model-size-independent grokking-time PDF,and hence the grokking-time average.Since the grokking time is measured relative to the time at which the zero train error is achieved, we estimate alsothe PDF of times tε (zero-test-error time). We find that both L1 and L2 reduce the time at which the zero testerror is attained. Therefore, both, the L1 and the L2 regularisation decrease the number of steps required for goodgeneralisation. In addition, the L1 generalisation seems to be more efficient, in the sense, that there is a shorter timeinterval with a large difference between training and test error.5 Summary and discussionWe analyse grokking from two perspectives. First, we propose a simple grokking setup (perceptron grokking) andconsider two solvable grokking models. Second, we introduce a tensor-network attention map and connect thestandard statistical-mechanics teacher-student setup with the perceptron grokking setup.281.01.52.0Deff.t=0.05 t=0.20 t=1.00 t=1.2010 210 1loss0.0 0.5 1.0 1.5 2.0t0.00.20.4E testFigure 16: Several emergent structures in the feature space (Example 1). The left plots show the effectivedimension Deff (top), train loss (middle), and test error (bottom). The black markers show the value of theplotted quantities at specific times marked by vertical dotted lines and written on the top of the left panels.The right panels show the structure of the features at the marked times. We observe that an essentiallyone dimensional feature distribution with two distinct islands of features splits into an almost 2D featuredistribution with three isolated islands. We can detect this transition as a sharp peak in the loss and a stepin the effective dimension Deff .Perceptron grokking By studying two solvable grokking models, we show that grokking is a phase transitionand calculate the critical exponent, grokking probability, and grokking-time PDF. The obtained analytic expressionsallow us to determine the effect of model and training parameters on the grokking probability and the grokking-timePDF. In particular, we find a stark difference between the L1- and L2-regularised models. The L1-regularised modelshave a higher grokking probability and a shorter grokking time as the L2-regularised models. We also obtain auniversal expression for the test-error critical exponent of spherically symmetric models, which is relevant in thetransfer learning setting, where only the last layer is retrained.Learning local-rules with shallow tensor networks We use the tensor-network attention model withfixed attention tensors A to test the predictions of the perceptron grokking setup on a 1D cellular-automaton rule-30learning task. Our prediction of the critical exponent roughly agrees with the numerical estimation, thereby validatingthe grokking scenario on a simple problem. On the other hand, the grokking-time PDF approximation, which invokesstrong assumptions, predicts the actual numerical estimate only qualitatively.We also perform the training of the full tensor-network student model. Similarly as in solvable perceptron grokkingmodels, we observe a difference between the L1-regularised and the L2-regularised models. The former have a shortergrokking time and a lower effective dimension, which agrees with the analytic predictions of the perceptron grokkingmodels. Therefore, we expect that L1 regularisation leads to improved generalisation properties (e.g. smaller testerror) compared to L2 regularisation also in a more general classification setting.In the case of training the full tensor-network student model we discuss the connection between grokking and structureformation. We determine the grokking transition by observing the average test error. We show that the averageeffective dimension of the feature-space data sharply decreases at the grokking transition. By observing specific291.52.02.5Deff.t=0.05 t=0.57 t=0.67 t=1.190.2250.2500.275loss0.0 0.2 0.4 0.6 0.8 1.0 1.2t0.00.20.4E testFigure 17: Several emergent structures in the feature space (Example 2). The left plots show the effectivedimension Deff (top), train loss (middle), and test error (bottom). Black markers show the value of theplotted quantities at times marked by vertical dotted lines and written on the top of the first plot. The rightpanels show the structure of the features at the marked times. We observe that structured data also appearsin the case of high test error (t = 0.57). The zero-test-error structure (t = 0.67) is different compared to theexample in Fig. 16. Here, we see a 2D structure with four isolated feature islands. Finally, at time t = 1.19the structure starts to disappear while the test error is still considerably small (smaller than 1%).models, we also find that small effective dimensions correspond to particular feature-space structures. We accordinglyrelate grokking with structure formation on the ensemble level. By contrast, we find several models with zero testerror without apparent feature-space structures and vice versa. This shows that in specific (though rare) cases, thetest error drops to zero even if no structure is present in the data. Similarly, simple structures can appear duringtraining also when the test error does not vanish. We thus separate the grokking and the structure formation on thelevel of individual training runs.As a distinct feature of the full tensor-network training we highlight the spikes in the training loss. We observe thatspikes become more frequent with larger L1,2 regularisation. We also relate the training-loss spikes to the changes inthe feature-space structures, which may become less or more complex during training. We can assess the shape of thefeature-space structures by observing the effective dimension, which shows a step-like behaviour whenever we observea training-loss spike. Typically less complex structures correspond to a smaller effective dimension. These findingscan be relevant for deep-neural-network training where training-loss spikes are also observed. Frequent training-lossspikes can be avoided by using a smaller regularisation. Moreover, we can determine whether the model parametersshould be reverted by monitoring the feature-space effective dimension. For example, we can revert the model onlyif the training-loss spikes correspond to increased effective dimension.Finally, the proposed tensor-network map connects the grokking phenomena, which have so far been observed onlyin deep models, with the standard teacher-student learning setup. The considered local tensor-network rule learningsetup is an extreme example of a learning rule. The standard teacher-student mean-field setup is the opposite extreme.It would be interesting to study if the proposed grokking setup and the tensor-network map can be extended to studyalgebraically decaying rules which interpolate between the two extremes. Extending the presented theory to deepneural networks appears to be difficult within the proposed framework.300.0 2.5 5.0 7.5 10.0tG02004006008001000P(t G)1=0, 2=0D = 10, tG = 0.60D = 20, tG = 0.61D = 40, tG = 0.770 2 4tG02004006008001=0, 2=0.0001D = 10, tG = 0.60D = 20, tG = 0.54D = 40, tG = 0.330.00 0.25 0.50 0.75 1.00tG0250500750100012501=0.001, 2=0D = 10, tG = 0.10D = 20, tG = 0.10D = 40, tG = 0.060.0 2.5 5.0 7.5t0200400600800P(t)D = 10, t = 1.05D = 20, t = 0.81D = 40, t = 0.900 2 4t0200400600800 D = 10, t = 0.99D = 20, t = 0.72D = 40, t = 0.440 1 2 3t0200400600800 D = 10, t = 0.51D = 20, t = 0.41D = 40, t = 0.37Figure 18: The estimated grokking-time PDF and tε PDF. The colors correspond to different models sizes,namely d = 10 (blue), d = 20 (orange), and d = 40 (green). The vertical lines correspond to the averagesreported in the legends of the panels. We find that L1 regularisation reduces tG and tε. By contrast, the L2regularisation decreases only the absolute time tε. In the L2 case and d = 20 we also find a clear bimodalgrokking time distribution. In all cases we set the learning rate to 0.005.AcknowledgementThe authors received support from Sloveinan research agency (ARRS) project J1-2480. Computational resourceswere provided by SLING – Slovenian national supercomputing network. We thank Marko Robnik Šikonja for readingthe first version of the draft and providing useful comments.References[1] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practiceand the classical bias–variance trade-off. Proceedings of the National Academy of Sciences, 116(32):15849–15854,2019.[2] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep doubledescent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory and Experiment,2021(12):124003, 2021.[3] Anders Krogh and John Hertz. A simple weight decay can improve generalization. Advances in neural informationprocessing systems, 4, 1991.31[4] Mohammad Pezeshki, Amartya Mitra, Yoshua Bengio, and Guillaume Lajoie. Multi-scale feature learningdynamics: Insights for double descent. arXiv preprint arXiv:2112.03215, 2021.[5] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal phase of deeplearning training. Proceedings of the National Academy of Sciences, 117(40):24652–24663, 2020.[6] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalizationbeyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.[7] Vignesh Kothapalli, Ebrahim Rasromani, and Vasudev Awatramani. Neural collapse: A review on modellingprinciples and generalization. arXiv preprint arXiv:2206.04041, 2022.[8] Ziming Liu, Ouail Kitouni, Niklas Nolte, Eric J Michaud, Max Tegmark, and Mike Williams. Towards under-standing grokking: An effective theory of representation learning. arXiv preprint arXiv:2205.10343, 2022.[9] Vimal Thilak, Etai Littwin, Shuangfei Zhai, Omid Saremi, Roni Paiss, and Joshua Susskind. The slingshot mecha-nism: An empirical study of adaptive optimizers and the grokking phenomenon. arXiv preprint arXiv:2206.04817,2022.[10] Andreas Engel and Christian Van den Broeck. Statistical mechanics of learning. Cambridge University Press,2001.[11] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias ofgradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822–2878, 2018.[12] William Merrill and Nikolaos Tsilivis. Extracting finite automata from rnns using state merging. arXiv preprintarXiv:2201.12451, 2022.[13] Stéphane d’Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overfitting: Where &why do they appear? Advances in Neural Information Processing Systems, 33:3058–3069, 2020.[14] Mohammad Pezeshki, Amartya Mitra, Yoshua Bengio, and Guillaume Lajoie. Multi-scale feature learningdynamics: Insights for double descent. In International Conference on Machine Learning, pages 17669–17690.PMLR, 2022.[15] Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on neuralnetworks typically occurs at the edge of stability. arXiv preprint arXiv:2103.00065, 2021.[16] Dustin G Mixon, Hans Parshall, and Jianzong Pi. Neural collapse with unconstrained features. arXiv preprintarXiv:2011.11619, 2020.[17] Cong Fang, Hangfeng He, Qi Long, and Weijie J Su. Exploring deep neural networks via layer-peeled model:Minority collapse in imbalanced training. Proceedings of the National Academy of Sciences, 118(43):e2103091118,2021.[18] Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometricanalysis of neural collapse with unconstrained features. Advances in Neural Information Processing Systems,34:29820–29834, 2021.[19] Like Hui, Mikhail Belkin, and Preetum Nakkiran. Limitations of neural collapse for understanding generalizationin deep learning. arXiv preprint arXiv:2202.08384, 2022.[20] E Miles Stoudenmire and David J Schwab. Supervised learning with quantum-inspired tensor networks. arXivpreprint arXiv:1605.05775, 2016.[21] E Miles Stoudenmire. Learning relevant features of data with multi-scale tensor networks. Quantum Scienceand Technology, 3(3):034003, 2018.[22] Stavros Efthymiou, Jack Hidary, and Stefan Leichenauer. Tensornetwork for machine learning. arXiv preprintarXiv:1906.06329, 2019.[23] Ding Liu, Shi-Ju Ran, Peter Wittek, Cheng Peng, Raul Blázquez Garćıa, Gang Su, and Maciej Lewenstein.Machine learning by unitary tensor network of hierarchical tree structure. New Journal of Physics, 21(7):073059,2019.[24] John Martyn, Guifre Vidal, Chase Roberts, and Stefan Leichenauer. Entanglement and tensor networks forsupervised image classification. arXiv preprint arXiv:2007.06082, 2020.[25] Ye-Ming Meng, Jing Zhang, Peng Zhang, Chao Gao, and Shi-Ju Ran. Residual matrix product state for machinelearning. arXiv preprint arXiv:2012.11841, 2020.32[26] Yiwei Chen, Yu Pan, and Daoyi Dong. Residual tensor train: a flexible and efficient approach for learningmultiple multilinear correlations. arXiv preprint arXiv:2108.08659, 2021.[27] Fanjie Kong, Xiao-yang Liu, and Ricardo Henao. Quantum tensor network in machine learning: An applicationto tiny object classification. arXiv preprint arXiv:2101.03154, 2021.[28] Song Cheng, Lei Wang, Tao Xiang, and Pan Zhang. Tree tensor networks for generative modeling. PhysicalReview B, 99(15):155131, 2019.[29] James Stokes and John Terilla. Probabilistic modeling with matrix product states. Entropy, 21(12):1236, 2019.[30] Zheng-Zhi Sun, Cheng Peng, Ding Liu, Shi-Ju Ran, and Gang Su. Generative tensor network classification modelfor supervised machine learning. Physical Review B, 101(7):075135, 2020.[31] Jing Liu, Sujie Li, Jiang Zhang, and Pan Zhang. Tensor networks for unsupervised machine learning. arXivpreprint arXiv:2106.12974, 2021.[32] Vasily Pestun and Yiannis Vlassopoulos. Tensor network language model. arXiv preprint arXiv:1710.10248,2017.[33] Chu Guo, Zhanming Jie, Wei Lu, and Dario Poletti. Matrix product operators for sequence-to-sequence learning.Physical Review E, 98(4):042114, 2018.[34] Tai-Danae Bradley, E Miles Stoudenmire, and John Terilla. Modeling sequences with quantum states: a lookunder the hood. Machine Learning: Science and Technology, 1(3):035008, 2020.[35] Tai-Danae Bradley and Yiannis Vlassopoulos. Language modeling with reduced densities. arXiv preprintarXiv:2007.03834, 2020.[36] Bojan Žunkovič. Deep tensor networks with matrix product operators. Quantum Machine Intelligence volume,4(21), 2022.[37] Jinhui Wang, Chase Roberts, Guifre Vidal, and Stefan Leichenauer. Anomaly detection with tensor networks.arXiv preprint arXiv:2006.02516, 2020.[38] Ananda Streit, Gustavo Santos, Rosa Leão, Edmundo de Souza e Silva, Daniel Menasché, and Don Towsley. Net-work anomaly detection based on tensor decomposition. In 2020 Mediterranean Communication and ComputerNetworking Conference (MedComNet), pages 1–8. IEEE, 2020.[39] Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis. InConference on learning theory, pages 698–728. PMLR, 2016.[40] Dong-Ling Deng, Xiaopeng Li, and S Das Sarma. Quantum entanglement in neural network states. PhysicalReview X, 7(2):021021, 2017.[41] Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. Deep learning and quantum entanglement:Fundamental connections with implications to network design. arXiv preprint arXiv:1704.01552, 2017.[42] Ivan Glasser, Ryan Sweke, Nicola Pancotti, Jens Eisert, and Ignacio Cirac. Expressive power of tensor-networkfactorizations for probabilistic modeling. Advances in neural information processing systems, 32, 2019.[43] Jing Chen, Song Cheng, Haidong Xie, Lei Wang, and Tao Xiang. Equivalence of restricted boltzmann machinesand tensor network states. Physical Review B, 97(8):085104, 2018.[44] Anatoly Dymarsky and Kirill Pavlenko. Tensor network to learn the wavefunction of data. arXiv preprintarXiv:2111.08014, 2021.[45] Sandesh Adhikary, Siddarth Srinivasan, Jacob Miller, Guillaume Rabusseau, and Byron Boots. Quantum tensornetworks, stochastic processes, and weighted automata. In International Conference on Artificial Intelligenceand Statistics, pages 2080–2088. PMLR, 2021.[46] Dian Wu, Riccardo Rossi, Filippo Vicentini, and Giuseppe Carleo. From tensor network quantum states totensorial recurrent neural networks. arXiv preprint arXiv:2206.12363, 2022.[47] Elizabeth Gardner and Bernard Derrida. Three unfinished works on the optimal storage capacity of networks.Journal of Physics A: Mathematical and General, 22(12):1983, 1989.[48] Jean Barbier, Florent Krzakala, Nicolas Macris, Léo Miolane, and Lenka Zdeborová. Optimal errors and phasetransitions in high-dimensional generalized linear models. Proceedings of the National Academy of Sciences,116(12):5451–5460, 2019.33[49] Giuseppe Carleo, Ignacio Cirac, Kyle Cranmer, Laurent Daudet, Maria Schuld, Naftali Tishby, Leslie Vogt-Maranto, and Lenka Zdeborová. Machine learning and the physical sciences. Reviews of Modern Physics,91(4):045002, 2019.[50] Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer learning. Journal of Big data,3(1):1–40, 2016.[51] Stephen Wolfram. Statistical mechanics of cellular automata. Reviews of modern physics, 55(3):601, 1983.[52] Stephen Wolfram et al. A new kind of science, volume 5. Wolfram media Champaign, 2002.[53] Wikipedia. Rule 30, 2022.A Grokking time in the 1D modelIn this appendix, we provide the details of the grokking-time PDF calculation in the 1D model discussed in the maintext.First, we calculate the joint probability to find xmin and x̄ for a given number of positive and negative trainingsamples N ,PN (xmin, x̄) =N∏k=1[∫ ∞0dx̃kP (x̃k)∫ ∞0dxkP (xk)]δ (xmin −min({xl})) δ(2x̄− 1NN∑l=1(xl − x̃l))(69)= 2N∏k=1[∫ ∞0dxkP (xk)]δ (xmin −min({xl}))P expN (x̄+ − 2x̄) .With x̄+ we denote the average value of the positive samples, x̄+ =1N∑Nl=1 xl. We calculate the PDF of the minimumgiven in Eq. 69 by considering its cumulative densityCN (xmin; x̄) =2N∏k=1[∫ ∞xmindxkP (xk)]P expN (x̄+ − 2x̄) (70)=2N∏k=1[∫ ∞0dxkP (xk + xmin)]P expN (x̄+ + xmin − 2x̄)=2N∏k=1[∫ ∞0dxkP (xk)]P (Nxmin)PexpN (x̄+ + xmin − 2x̄)=2∫ ∞0dx̄+PN (x̄+)P (Nxmin)PexpN (x̄+ + xmin − 2x̄)=232−NNN+12 e−NxminΘ(2x̄− xmin)(2x̄− xmin)N−12KN− 12(2Nx̄−Nxmin)√πΓ(n)+√π232−NNN+12 e−Nxmin csc(πN)Θ(xmin − 2x̄)(xmin − 2x̄)N−12KN− 12(N(xmin − 2x̄))Γ(1−N)Γ(N)2 .For N = 2 we findCN=2(xmin; x̄) =− e−4x̄(4x̄− 2xmin + 1)(Θ(2xmin − 4x̄)− 1) (71)− e4x̄−4xmin(4x̄− 2xmin − 1)Θ(2xmin − 4x̄)We obtain the PDF by taking the derivatives of the cumulative probabilities Eq. 70 with respect to xmin, namelyPN (x̄, xmin) = − ∂CN (xmin;x̄)∂xmin . For N = 2 we getPN=2(x̄, xmin) =2e−4x̄Θ(2x̄− xmin)− 2e4x̄−4xmin(8x̄− 4xmin − 1)Θ(xmin − 2x̄). (72)34Next, we calculate the joint probability for the grokking time tG and the average x̄PN,ε,λ(tG, x̄) =∂xmin∂tGPN (x̄, xmin(tG, ε, x̄λ)). (73)While in principle we can derive a closed-form expression for in arbitrary N , they are not particularly informativeand we thus write here only the expressions for N = 2 and λ2 = 0 (to shorten the notation we use t instead of tG)PN=2,ε,λ1(t, x̄) =2et−4x̄(ελ − x̄)Θ(x̄− ελ tanh(t2))(74)+ 2(x̄− ελ)e4(et(x̄−ελ)+ελ)+t (4et(x̄− ελ) + 4x̄+ 4ελ − 1)Θ(ελ tanh( t2)− x̄).Finally, we integrate out the average of the samples x̄ and obtain the grokking-time PDF.B Grokking probability in the D-dimensional ball modelIn this section we derive the grokking probability in the D−dimensional ball model in the limit of many trainingsamples, i.e. N \\x1d 1. The condition for zero test error iswλ1||wλ||2>1ε, (75)and can be rewritten as(ε2 − 1)(wλ1 )2 ≥ (wλ2 )2 + (wλ3 )2 + . . . (wλD)2, and (wλ1 ) > 0. (76)The stationary solution wλ = G−1a isG =12N2N∑i=1x̃i ⊗ x̃i + λ21D =12NN∑i=1xi ⊗ xi + ε⊗ ε+ ε⊗ x̄+ x̄⊗ ε+ λ2ID, (77)a =12N2N∑i=1yix̃i − λ1sgn(w) = x̄− λ1sgn(w),where x̄ = 12N∑2Ni=1 yixi. In the limit N \\x1d 1 we can separate two contributions to the matrix G = A+B whereA =λDID + ε⊗ ε, (78)B =12N2N∑i=1xi ⊗ xi − 1D + 2ID + ε⊗ x̄+ x̄⊗ ε,where λD =1D+2+ λ2. In the limit N \\x1d 1 we have ||B||F = O(1/√N), hence we can approximate the inverse of thematrix G as,G−1 ≈ A−1 −A−1BA−1. (79)The stationary solution can thus be approximated bywλ ≈A−1a−A−1BA−1a (80)≈A−1(x̄+ ε− sgn(w)λ1)−A−1BA−1(ε+ sgn(w)λ1),where we have kept only the first nontrivial order in 1/√N . We will separately consider the case λ1 = 0 and the caseλ1 > 0.35B.1 Case λ1 = 0By explicitly evaluating the above expression, Eq. 80, and assuming λ1 = 0, we findwλ1 = β + α1x̄1 + α2x21, wλj>1 = α3x̄j + α4x1xj , (81)whereβ =ελD + ε2+ε(λD + ε2)2(D + 2), (82)α1 =1λD + ε2− 2ε2(λD + ε2)2,α2 =−ε(λD + ε2)2,α3 =1λD− ελD(λD + ε2),α4 =−ελD(λD + ε2).The first few nontrivial moments of the uniform distribution in a D−dimensional ball are reported in Table 2.Considering the variances and the means in Table 2, we find that (in the limit N \\x1d 1) all random variables appearingStatistics Mean Second momentxj 01D+2x2j1D+238+6D+D2xixj , i 6= j 0 18+6D+D2Table 2: First nontrivial moments of the uniform ball distribution. All odd moments vanish.in Eq. 81 to be normally distributed,x̄1 ∼ N(0,12N(D + 2)), (83)x21 ∼ N(1D + 2,D + 1N(D + 2)2(D + 4)),x1xj>1 ∼ N(0,12N(8 + 6D +D2)).The distributions are independent since all the necessary covariances vanish.The sum of independent normal distributions is again a normal distribution, leading tow1 ∼ N1(w1) = N(β +α2D + 2,α212N(D + 2)+α22(D + 1)N(D + 2)2(D + 4)), (84)wj>1 ∼ N(0,α232N(D + 2)+α242N(8 + 6D +D2)). (85)The grokking probability is then given byPE(∞)=0 =∫ ∞0dw1N1(w1)∫ (ε2−1)w21( α232N(D+2) + α242N(8+6D+D2))0dr χ2D−1(r). (86)In the limit N \\x1d λD \\x1d 1 we make following approximationsβ ≈ ελD + ε2, α1 ≈1λD + ε2, α2 = 0, α3 ≈1λD, α4 = 0. (87)With these simplifications, Eq. 86 reduces to the grokking probability obtained in the main text, see Eq. 30.36B.2 Case 1\\x1d λ1 > 0By explicitly evaluating the expression in Eq. 80 we findwλ1 ={β − λ1sgn(wλ1 )λD+ε2 + α1x̄1 + α2x21 , (λD + ε2)|β + α1x̄1 + α2x21| > λ10 , else, (88)wλj>1 ={−λ1sgn(wλj )λD+ α3x̄j + α4x1xj , λD|α3x̄j + α4x1xj | > λ10 , else,with αj and β given in Eq. 82. The number of non-vanishing components of the stationary solution wλ depends onthe value of λ1. Therefore, we get (in the N \\x1d 1 limit) an additional sum over the number of non-zero elements inthe wλ,PE(∞)=0 =∫ ∞λ1λ2,D+ε2dw1N1(w1)[(1− pλ)D−1 (89)+D−1∑k=1(D − 1k)pkλ(1− pλ)D−1−k∫ (ε2−1)(w1− λ1λ2,D+ε2 )2( α232N(D+2) + α242N(8+6D+D2))0drRk(r)],where pλ = 1 − erf(λ1λD/√α232N(D+2)+α242N(8+6D+D2))is the probability that |wj>1| (sampled from Eq. 85)is largerthan λ1/λD. With Rk(r) we denote the PDF of the sum of squares of k random variables sampled from the truncatednormal distribution. As in the main text (see Section 3.2.2), we can calculate a lower bound on the grokking probabilityby discarding the sum over k > 1,PE(∞)=0 ≥(1− pλ)D−1∫ ∞λ1λ2,D+ε2dw1N1(w1) (90)≈ (1− pλ)D−12(1 + erf((β +α2D + 2− λ1λ2,D + ε2)/√α212N(D + 2)+α22(D + 1)N(D + 2)2(D + 4))).Also in the more general case, we find the same difference between the L1 and L2 regularisations as discussed in themain text.C Fixed attentionBelow we specify the attention tensors for the discussed examples, see Section 4.3.1 in the main text:• Example 1:A0 =(0.782735 0.225481−0.21562 0.290028), A1 =(1.17554 −0.2755031.18283 −0.157563),• Example 2:A0 =(1.6749 −1.290590.285324 −0.708621), A1 =(0.0462428 −0.0797724−0.509457 0.922777),• Example 3:A0 =(1.37336 −0.465853−1.10382 0.720113), A1 =(0.128517 0.1660330.634426 1.13816).37D Additional results for the 2–local and the 3–local rulesIn this appendix we present a similar analysis as the one in Section 4.3.2, but for the 2–local rule and the 3–localrule. In general we observe a similar behavior as in the 1–local case discussed in the main text.First, we discuss the test error shown in Fig. 19. Again, we find that the critical exponent ν is roughly independenton the model-size and is smaller for larger rule range K.0.5 0.0 0.5 1.0t t0.00.20.40.60.81.01=0, 2=0d=10, =1.78d=20, =2.72d=40, =2.870.5 0.0 0.5 1.0t t0.00.20.40.60.81.01=0, 2=0.0001d=10, =1.13d=20, =0.92d=40, =0.730.5 0.0 0.5 1.0t t0.00.20.40.60.81.01=0.001, 2=0d=10, =0.69d=20, =0.67d=40, =0.711.0 0.5 0.0 0.5 1.0 1.5t t0.00.20.40.60.81.01=0, 2=0d=10, =1.59d=20, =2.38d=40, =1.981.0 0.5 0.0 0.5 1.0 1.5t t0.00.20.40.60.81.01=0, 2=0.0001d=10, =1.59d=20, =1.33d=40, =0.721.0 0.5 0.0 0.5 1.0 1.5t t0.00.20.40.60.81.01=0.001, 2=0d=10, =0.65d=20, =0.47d=40, =0.38Figure 19: The average error at the phase transition for the 2–local rule (first row) and the 3–local rule(second row). We align the first point where the test error becomes zero (i.e. the time tε) and take theaverage over many (∼ 1000) initialisations of the model parameters. As in the 1–local rule case the trainingwithout regularisation results in larger critical exponent ν ≈ 2 as training with L1 or L2 regularisation. Inall experiments we used learning rate 0.005.As in the 1–local case, the grokking transition corresponds to a sharp decrease in the effective dimension of theHL(i) attention vectors, shown in Fig. 20. In the considered rule-learning scenario, the smallest effective dimensionis determined by the locality of the rule and is expected to increase exponentially with K [36]. In contrast to the 1–local case, we find that larger regularisation does not necessary correspond to a smaller effective dimension. However,this is only the case for smaller instances where the bond dimension is very close to the effective dimension (or thesmallest possible bond dimension with zero test error). For the larger instances we again find that larger regularisationcorresponds to smaller effective dimension Deff . In this case we also find that models with L1 regularisation have aslightly smaller average effective dimension compared to models with L2 regularisation.Finally, we estimate the grokking-time PDF and the generalisation-time (tε) PDF for the 2–local and the 3–localrule, shown in Fig. 21 and Fig. 22.380.5 0.0 0.5 1.0t t24681012effective dimensionDeff = 3.31=0, 2=0d=10d=20d=400.5 0.0 0.5 1.0t t051015Deff = 2.71=0, 2=0.0001d=10d=20d=400.5 0.0 0.5 1.0t t05101520Deff = 2.31=0.001, 2=0d=10d=20d=401.0 0.5 0.0 0.5 1.0 1.5t t246810effective dimensionDeff = 4.71=0, 2=0d=10d=20d=401.0 0.5 0.0 0.5 1.0 1.5t t2.55.07.510.012.515.0Deff = 4.61=0, 2=0.0001d=10d=20d=401.0 0.5 0.0 0.5 1.0 1.5t t510152025Deff = 4.81=0.001, 2=0d=10d=20d=40Figure 20: The average effective dimension of the 2–local rule (first row) and the 3–local rule (second row)corresponding to the error in Fig. 19. The horizontal dashed line corresponds to the average (over the bonddimension d) of the minimal effective dimension of data with zero test error (over all samples with fixed d).390.0 2.5 5.0 7.5 10.0tG020040060080010001200P(t G)1=0, 2=0D = 10, tG = 0.63D = 20, tG = 1.07D = 40, tG = 1.230 1 2 3tG02004006008001=0, 2=0.0001D = 10, tG = 0.37D = 20, tG = 0.49D = 40, tG = 0.340.0 0.2 0.4tG02505007501000125015001=0.001, 2=0D = 10, tG = 0.07D = 20, tG = 0.04D = 40, tG = 0.030 5 10tG02004006008001000P(t G)1=0, 2=0D = 10, tG = 1.85D = 20, tG = 2.29D = 40, tG = 2.380 2 4 6tG02004006008001=0, 2=0.0001D = 10, tG = 0.74D = 20, tG = 0.85D = 40, tG = 0.710.0 0.2 0.4 0.6 0.8tG02004006008001=0.001, 2=0D = 10, tG = 0.12D = 20, tG = 0.09D = 40, tG = 0.07Figure 21: The estimated grokking-time PDF for the 2–local rule (first row) and the 3–local rule (secondrow). The colors correspond to different models sizes, namely d = 10 (blue), d = 20 (orange), and d = 40(green). The vertical lines correspond to the averages reported in the legends of the panels. We find thatgrokking time increases with increased rule range K. As in the 1–local case, we find that L1 regularisationreduces tG in all cases. In contrast, the use of L2 regularisation can in some cases increase the averagegrokking time. In the L2 case we also find a clear bimodal grokking time distribution. In all cases we setthe learning rate to 0.005.400.0 2.5 5.0 7.5 10.0t02004006008001000P(t)D = 10, t = 1.27D = 20, t = 1.38D = 40, t = 1.410 2 4t0200400600800D = 10, t = 0.88D = 20, t = 0.75D = 40, t = 0.511 2t020040060080010001200D = 10, t = 0.57D = 20, t = 0.42D = 40, t = 0.382.5 5.0 7.5 10.0t0100200300400500P(t)D = 10, t = 3.04D = 20, t = 2.79D = 40, t = 2.682 4t0100200300400500 D = 10, t = 1.58D = 20, t = 1.24D = 40, t = 0.951 2 3t0200400600800D = 10, t = 0.78D = 20, t = 0.59D = 40, t = 0.50Figure 22: The estimated tε PDF for the 2–local rule (first row) and the 3–local rule (second row). Thecolors correspond to different models sizes, namely d = 10 (blue), d = 20 (orange), and d = 40 (green). Thevertical lines correspond to the averages reported in the legends of the panels. In all cases we set the learningrate to 0.005.41'}\n"
     ]
    }
   ],
   "source": [
    "all_docs = collection.find()\n",
    "# Pretty print all objects in the FDL collection\n",
    "for doc in all_docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'authors': 'Rosowski, Andreas', 'year': '2019', 'title': 'Fast Commutative Matrix Algorithm', 'full_text': 'ar\\nX\\niv\\n:1\\n90\\n4.\\n07\\n68\\n3v\\n1 \\n [c\\ns.C\\nC]\\n  1\\n6 A\\npr\\n 20\\n19\\nFast Commutative Matrix Algorithm\\nAndreas Rosowski\\nUniversity of Siegen, Germany\\nandreas.rosowski@student.uni-siegen.de\\nApril 17, 2019\\nAbstract\\nWe show that the product of an n× 3 matrix and a 3× 3 matrix over a commutative ring\\ncan be computed using 6n+3 multiplications. For two 3×3 matrices this gives us an algorithm\\nusing 21 multiplications. This is an improvement with respect to Makarov algorithm using\\n22 multiplications[10]. We generalize our result for n × 3 and 3 × 3 matrices and present\\nan algorithm for computing the product of an l × n matrix and an n × m matrix over a\\ncommutative ring for odd n using n(lm+ l +m− 1)/2 multiplications if m is odd and using\\n(n(lm+ l+m−1)+ l−1)/2 multiplications if m is even. Waksman algorithm for odd n needs\\n(n− 1)(lm+ l+m− 1)/2+ lm multiplications[16], thus in both cases less multiplications are\\nrequired by our algorithm.\\n1 Introduction\\nIn 1969 Strassen showed that the product of two n×n matrices can be computed using O(nlog2(7))\\narithmetic operations [15]. This work opened a new field of research and over the years better\\nupper bounds for the exponent of matrix multiplication were published. In 1990 Coppersmith and\\nWinograd obtained an upper bound of 2.375477 for the exponent [2]. For a long time this was the\\nbest result. Since 2010 further improvements were obtained in a series of papers [7, 8, 14, 17, 18].\\nThe best result so far was published in 2014 by Le Gall who obtained an upper bound of 2.3728639\\nfor the exponent [8].\\nIn this paper we first study the product of an n × 3 matrix A and a 3 × 3 matrix B over a\\ncommutative ring and show that we can compute the product AB using 6n + 3 multiplications.\\nThe basic idea is to improve the computation of the product of a 1× 3 vector a and a 3× 3 matrix\\nB over a commutative ring in the sense that we try to obtain as much as possible multiplications\\nthat contain only entries of the matrix B but without using more than 9 multiplications overall.\\nThe multiplications which contain only entries of the matrix B only need to be calculated once\\nand can therefore be reused in the matrix multiplication. In the special case n = 3 we obtain an\\nalgorithm using 21 multiplications which improves the best result so far from Makarov using 22\\nmultiplications [10]. Our next step is to generalize this result to the computation of the product\\nof an l × n matrix A and an n ×m matrix B over a commutative ring for odd n. We show that\\nthe product AB can be computed using n(lm + l + m − 1)/2 multiplications if m is odd and\\n(n(lm + l +m − 1) + l − 1)/2 multiplications if m is even. This improves Waksman’s algorithm\\nwhich requires (n− 1)(lm+ l +m− 1)/2 + lm multiplications for odd n [16].\\nAll algorithms we present in this paper do not make use of any additional multiplications with\\nconstants.\\n1.1 Related Work\\nIn this section we present some related work. We start with presenting some results about multi-\\nplication of two square matrices. Note that a matrix multiplication algorithm can only be applied\\n1\\nrecursively if commutativity is not used. Since Strassen showed in 1969 that the product of two\\nmatrices can be computed using O(nlog2(7)) arithmetic operations [15] and since it is shown that\\nfor 2 × 2 matrices 7 is the optimal number of multiplications [5, 19], it is interesting to study\\nn× n matrices for n ≥ 3, to obtain an even faster algorithm for matrix multiplication. For 3× 3\\nmatrices 21 multiplications would be needed to obtain an even faster algorithm than Strassen’s\\nsince log3(21) ≈ 2.7712 < 2.807 ≈ log2(7). In 1976 Laderman obtained a non-commutative 3 × 3\\nalgorithm using only 23 multiplications [9]. It is not known if there exists a non-commutative 3×3\\nalgorithm that uses 22 or less multiplications. For 5× 5 matrices the best non-commutative result\\nso far is 99 multiplications by Sedoglavic [12] which is an improvement on Makarov’s algorithm\\nfor 5× 5 matrices [11].\\nHopcroft and Musinski showed in [6] that the number of multiplications to compute the product\\nof an l×n matrix and an n×m matrix is the same number that is required to compute the product\\nof an n×m matrix and an m× l matrix and of an l ×m matrix and an m× n matrix etc. This\\nmeans if one computes an algorithm for the product of an l × m matrix and an m × n matrix\\nusing x multiplications there exists a matrix product algorithm for lnm × lnm matrices using\\nx3 multiplications overall. This algorithm for square matrices will then have an exponent of\\nloglnm(x\\n3).\\nWe present some examples of non-square matrix multiplication algorithms. In [4] Hopcroft\\nand Kerr showed that the product of a p × 2 matrix and a 2 × n matrix can be multiplied using\\n⌈(3pn +max{n, p})/2⌉ multiplications without using commutativity. In the case p = 3 = n this\\ngives an algorithm using 15 multiplications. Combined with the results of [6] this gives an algo-\\nrithm for 18×18 matrices using 153 = 3375 multiplications and an exponent of log18(3375) ≈ 2.811.\\nSmirnov obtained an algorithm for the product of a 3 × 3 matrix and a 3 × 6 matrix using 40\\nmultiplications[13]. By [6] this gives an algorithm for 54 × 54 matrices using 403 = 64000 multi-\\nplications and an exponent of log54(64000) ≈ 2.7743.\\nCariow et al. developed a high-speed parallel 3 × 3 matrix multiplier structure based on the\\ncommutative 3 × 3 matrix algorithm using 22 multiplications obtained by Makarov [1, 10]. We\\nsuppose that the structure could be improved by using our commutative 3 × 3 matrix algorithm\\nusing 21 multiplications.\\nIn [3] Drevet et al. optimized the number of required multiplications of small matrices up\\nto 30× 30 matrices. They considered non-commutative and commutative algorithms. Combined\\nwith our results for commutative rings we suppose that some results could be improved.\\n2 Matrix Product over a Commutative Ring\\nLet R denote a commutative ring throughout this Section.\\n2.1 Product of n× 3 and 3× 3 Matrices\\nConsider the vector-matrix product of an 1× 3 vector a and a 3× 3 matrix B over a commutative\\nring.\\na =\\n[\\na1 a2 a3\\n]\\nB =\\n\\uf8ee\\n\\uf8f0b11 b12 b13b21 b22 b23\\nb31 b32 b33\\n\\uf8f9\\n\\uf8fb (1)\\nIn the usual way the vector-matrix product of a and B would be computed as:\\naB =\\n[\\na1b11 + a2b21 + a3b31 a1b12 + a2b22 + a3b32 a1b13 + a2b23 + a3b33\\n]\\nBut it can also be computed by first computing these 9 products:\\n2\\nAlgorithm 1. Input: Vector a and Matrix B as in (1).\\nLet\\np1 := (a2 + b12)(a1 + b21)\\np2 := (a3 + b13)(a1 + b31)\\np3 := (a3 + b23)(a2 + b32)\\np4 := a1(b11 − b12 − b13 − a2 − a3)\\np5 := a2(b22 − b21 − b23 − a1 − a3)\\np6 := a3(b33 − b31 − b32 − a1 − a2)\\np7 := b12b21\\np8 := b13b31\\np9 := b23b32\\nOutput:\\naB =\\n[\\np4 + p1 + p2 − p7 − p8 p5 + p1 + p3 − p7 − p9 p6 + p2 + p3 − p8 − p9\\n]\\nTheorem 1. Let R be a commutative ring, let n ≥ 1, let A be an n× 3 matrix over R and let B\\nbe a 3× 3 matrix over R. Then the product AB can be computed using 6n+ 3 multiplications.\\nProof. Consider Algorithm 1. The products p7, p8 and p9 contain only entries of the matrix B.\\nOne can observe that for all n ≥ 1 the multiplications p7, p8 and p9 can be reused for the product\\nAB and therefore 3(n− 1) multiplications are saved.\\nWe give an example. In the case n = 3 we obtain an algorithm with 21 multiplications for the\\nmatrix-matrix product. This algorithm needs one multiplication less than Makarov’s [10].\\nCorollary 1. Let R be a commutative ring and let A and B be 3 × 3 matrices over R as shown\\nbelow. Then the product AB can be computed using 21 multiplications as follows:\\nA =\\n\\uf8ee\\n\\uf8f0a11 a12 a13a21 a22 a23\\na31 a32 a33\\n\\uf8f9\\n\\uf8fb B =\\n\\uf8ee\\n\\uf8f0b11 b12 b13b21 b22 b23\\nb31 b32 b33\\n\\uf8f9\\n\\uf8fb\\np1 := (a12 + b12)(a11 + b21)\\np2 := (a13 + b13)(a11 + b31)\\np3 := (a13 + b23)(a12 + b32)\\np4 := a11(b11 − b12 − b13 − a12 − a13)\\np5 := a12(b22 − b21 − b23 − a11 − a13)\\np6 := a13(b33 − b31 − b32 − a11 − a12)\\np7 := (a22 + b12)(a21 + b21)\\np8 := (a23 + b13)(a21 + b31)\\np9 := (a23 + b23)(a22 + b32)\\np10 := a21(b11 − b12 − b13 − a22 − a23)\\np11 := a22(b22 − b21 − b23 − a21 − a23)\\np12 := a23(b33 − b31 − b32 − a21 − a22)\\n3\\np13 := (a32 + b12)(a31 + b21)\\np14 := (a33 + b13)(a31 + b31)\\np15 := (a33 + b23)(a32 + b32)\\np16 := a31(b11 − b12 − b13 − a32 − a33)\\np17 := a32(b22 − b21 − b23 − a31 − a33)\\np18 := a33(b33 − b31 − b32 − a31 − a32)\\np19 := b12b21\\np20 := b13b31\\np21 := b23b32\\nHence,\\nAB =\\n\\uf8ee\\n\\uf8f0 p4 + p1 + p2 − p19 − p20 p5 + p1 + p3 − p19 − p21 p6 + p2 + p3 − p20 − p21p10 + p7 + p8 − p19 − p20 p11 + p7 + p9 − p19 − p21 p12 + p8 + p9 − p20 − p21\\np16 + p13 + p14 − p19 − p20 p17 + p13 + p15 − p19 − p21 p18 + p14 + p15 − p20 − p21\\n\\uf8f9\\n\\uf8fb\\n2.2 General Matrix Product\\nAlgorithm 1 from Section 2.1 is the basic idea of a general algorithm for the matrix-matrix product\\nof l×n and n×m matrices over a commutative ring for odd n. This general algorithm makes use\\nof Waksman algorithm [16] for even n. The algorithm we present below is split into two cases. In\\nCase 1 m is odd and in Case 2 m is even. This leads us to the following:\\nTheorem 2. Let R be a commutative ring, let n ≥ 3 be odd, l ≥ 1, m ≥ 3 and let A ∈ Rl×n,\\nB ∈ Rn×m be matrices. Then the following holds:\\n• If m is odd the product AB can be computed using n(lm+ l +m− 1)/2 multiplications.\\n• If m is even the product AB can be computed using (n(lm+l+m−1)+l−1)/2multiplications.\\nProof. Let A and B be matrices as in the Theorem. Now split A and B in submatrices in the\\nfollowing way:\\nA =\\n[\\nA1 A2\\n]\\n, with A1 ∈ R\\nl×3 and A2 ∈ R\\nl×n−3,\\nB =\\n[\\nB1\\nB2\\n]\\n, with B1 ∈ R\\n3×m and B2 ∈ R\\nn−3×m.\\nThen AB = A1B1 + A2B2. With Waksman algorithm [16] mentioned before A2B2 can be\\ncomputed using (n − 3)(lm+ l +m − 1)/2 multiplications. Let aij denote the entries of A1 and\\nlet bij denote the entries of B1 and let cij denote the entries of A1B1. The matrix A1B1 can be\\ncomputed as follows.\\nCase 1: m is odd.\\nFor i = 1, . . . , l let\\nci1 = (ai1 + b21)(ai2+ b12)+ (ai1 + b31)(ai3 + b13)+ ai1(b11− b12− b13− ai2− ai3)− b12b21− b13b31\\nci2 = (ai1 + b21)(ai2+ b12)+ (ai2 + b32)(ai3 + b23)+ ai2(b22− b21− b23− ai1− ai3)− b12b21− b23b32\\nci3 = (ai1 + b31)(ai3+ b13)+ (ai2 + b32)(ai3 + b23)+ ai3(b33− b31− b32− ai1− ai2)− b13b31− b23b32\\nand for i = 1, . . . , l and j = 4, 6, 8, . . . ,m− 1 let\\n4\\ncij = (ai1 + b21)(ai2 + b12) + (ai1 + b31)(ai3 + b13) + (ai1 + b21 − b2j)(−ai2 − b12 + b1j − b1(j+1))\\n+(ai1 + b31 − b3j)(−ai3 − b13 + b1(j+1))− b12b21 − b13b31 − (b21 − b2j)(−b12 + b1j − b1(j+1))\\n−(b31 − b3j)(−b13 + b1(j+1))\\nci(j+1) = (ai1 + b31)(ai3 + b13) + (ai2 + b32)(ai3 + b23) + (ai1 + b31 − b3j)(−ai3 − b13 + b1(j+1))\\n+(ai2 + b32 + b3j − b3(j+1))(−ai3 − b23 + b2(j+1))− b13b31 − b23b32 − (b31 − b3j)(−b13 + b1(j+1))\\n−(b32 + b3j − b3(j+1))(−b23 + b2(j+1))\\nIt can easily be seen that 6l+3+3l(m−3)/2+3(m−3)/2 = 3(lm+ l+m−1)/2 multiplications\\nare required to compute A1B1.\\nThus, AB can be computed using 3(lm + l + m − 1)/2 + (n − 3)(lm + l + m − 1)/2 =\\nn(lm+ l +m− 1)/2 multiplications.\\nCase 2: m is even.\\nFor i = 1, . . . , l let\\nci1 = (ai1 + b21)(ai2+ b12)+ (ai1 + b31)(ai3 + b13)+ ai1(b11− b12− b13− ai2− ai3)− b12b21− b13b31\\nci2 = (ai1 + b21)(ai2+ b12)+ (ai2 + b32)(ai3 + b23)+ ai2(b22− b21− b23− ai1− ai3)− b12b21− b23b32\\nci3 = (ai1 + b31)(ai3+ b13)+ (ai2 + b32)(ai3 + b23)+ ai3(b33− b31− b32− ai1− ai2)− b13b31− b23b32\\nci4 = (ai1+b21)(ai2+b12)+(ai1+b21−b24)(−ai2−b12+b14)+ai3b34−b12b21−(b21−b24)(−b12+b14)\\nand for i = 1, . . . , l and j = 5, 7, 9, . . . ,m− 1 let\\ncij = (ai1 + b21)(ai2 + b12) + (ai1 + b31)(ai3 + b13) + (ai1 + b21 − b2j)(−ai2 − b12 + b1j − b1(j+1))\\n+(ai1 + b31 − b3j)(−ai3 − b13 + b1(j+1))− b12b21 − b13b31 − (b21 − b2j)(−b12 + b1j − b1(j+1))\\n−(b31 − b3j)(−b13 + b1(j+1))\\nci(j+1) = (ai1 + b31)(ai3 + b13) + (ai2 + b32)(ai3 + b23) + (ai1 + b31 − b3j)(−ai3 − b13 + b1(j+1))\\n+(ai2 + b32 + b3j − b3(j+1))(−ai3 − b23 + b2(j+1))− b13b31 − b23b32 − (b31 − b3j)(−b13 + b1(j+1))\\n−(b32 + b3j − b3(j+1))(−b23 + b2(j+1))\\nOne can easily verify that in this case 8l+4+3l(m−4)/2+3(m−4)/2 = 2(l−1)+3(lm+m)/2\\nmultiplications are required to compute A1B1.\\nThus, AB can be computed using 2(l − 1) + 3(lm + m)/2 + (n − 3)(lm + l + m − 1)/2 =\\n(n(lm+ l +m− 1) + l − 1)/2 multiplications.\\nIn both cases less multiplications are required to compute AB than Waksman algorithm [16]\\nfor odd n requires.\\n3 Acknowledgment\\nI am grateful to Michael Figelius and Markus Lohrey for helpful comments.\\nReferences\\n[1] A. Cariow, W. Sys lo, G. Cariowa, M. Gliszczyn´ski. A rationalized structure of processing unit\\nto multiply 3 × 3 matrices. Journal Pomiary Automatyka Kontrola, Volume R. 58, Number\\n7, (2012), 677–680\\n5\\n[2] D. Coppersmith, S. Winograd. Matrix multiplication via arithmetic progressions. Journal of\\nSymbolic Computation 9, 3 (1990), 251–280\\n[3] C.-E´. Drevet, M. N. Islam, E´. Schost. Optimization techniques for small matrix multiplication.\\nTheoretical Computer Science, Volume 412, Issue 22, (2011), 2219–2236\\n[4] J. E. Hopcroft, L. R. Kerr. On minimizing the number of multiplications necessary for matrix\\nmultiplication. SIAM Journal on Applied Mathematics, Volume 20, Number 1, (1971), 30–35\\n[5] J. E. Hopcroft, L. R. Kerr. Some techniques for proving certain simple programs optimal.\\nProc. Tenth Ann. Symposium on Switching and Automata Theory, 1969, 36–45\\n[6] J. E. Hopcroft, J. Musinski. Duality applied to the complexity of matrix multiplications and\\nother bilinear forms. STOC ’73 Proceedings of the fifth annual ACM symposium on Theory\\nof computing, (1973), 73–87, New York, NY, USA, ACM Press\\n[7] A. M. Davie, A. J. Stothers. Improved bound for complexity of matrix multiplication. Pro-\\nceedings of the Royal Society of Edinburgh 143A, 2013, 351–370\\n[8] F. Le Gall. Powers of tensors and fast matrix multiplication. Proceedings of the 39th Inter-\\nnational Symposium on Symbolic and Algebraic Computation (ISSAC 2014), (2014), 296–303\\n[9] J. D. Laderman. A Non-Commutative Algorithm for Multiplying 3×3 Matrices Using 23 Mul-\\ntiplications. Bulletin of the American Mathematical Society, Volume 82, Number 1, (1976),\\n126–128\\n[10] O. M. Makarov. An algorithm for multiplication of 3 × 3 matrices. Zh. Vychisl. Mat. Mat.\\nFiz., 26:2 (1986), 293–294\\n[11] O. M. Makarov. A non-commutative algorithm for multiplying 5× 5 matrices using one hun-\\ndred multiplications. USSR Computational Mathematics and Mathematical Physics, Volume\\n27, Issue 1, (1987), 205–207\\n[12] A. Sedoglavic. A non-commutative algorithm for multiplying 5 × 5 matrices using 99 multi-\\nplications. https://www.researchgate.net/publication/318652755\\n[13] A. V. Smirnov. The bilinear complexity and practical algorithms for matrix multiplication.\\nZh. Vychisl. Mat. Mat. Fiz., Volume 53, Number 12, (2013), 1970–1984\\n[14] A. J. Stothers. On the Complexity of Matrix Multiplication. PhD thesis, University of\\nEdinburgh, 2010\\n[15] V. Strassen. Gaussian elimination is not optimal. Numerische Mathematik 13 (1969), 354–356\\n[16] A. Waksman. On Winograds algorithm for inner products. In IEEE Transactions on Com-\\nputers, C-19(1970), 360–361.\\n[17] V. V. Williams. Multiplying matrices faster than Coppersmith-Winograd. In Proceedings of\\nthe 44th ACM Symposium on Theory of Computing, 887–898, 2012\\n[18] V. V. Williams. Multiplying matrices faster than Coppersmith-Winograd. Version available\\nat http://theory.stanford.edu/~virgi/matrixmult-f.pdf, retrieved on August 03, 2018\\n[19] S. Winograd. On multiplication of 2×2 matrices. Linear Algebra and its Applications, Volume\\n4, Issue 4, (1971), 381–388\\n6\\n'}\n"
     ]
    }
   ],
   "source": [
    "# retrieves the entire json object for a specific title\n",
    "result = collection.find_one({\"title\": \"Fast Commutative Matrix Algorithm\"}, {\"_id\": 0})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use this function to fetch articles from MongoDB to use in our local source\n",
    "# from pymongo import MongoClient\n",
    "\n",
    "# def fetch_articles_from_mongo():\n",
    "#     client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "#     db = client[\"FDL\"]\n",
    "#     collection = db[\"DSA\"]\n",
    "#     articles = collection.find()\n",
    "#     return list(articles)\n",
    "\n",
    "def fetch_articles_from_mongo():\n",
    "    client = MongoClient(\"mongodb://mongoadmin:password@localhost:27017/?authSource=admin\")\n",
    "    db = client[\"FDL\"]\n",
    "    collection = db[\"DSA\"]\n",
    "    try:\n",
    "        articles = collection.find({}, {\"_id\": 0})  # optional: exclude _id if needed\n",
    "        return list(articles)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching articles: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prompt more simple, gpt researcher will assume many of these things.\n",
    "\n",
    "system_prompt = '''You are a highly specialized AI research assistant designed to read and synthesize insights from a large collection of academic papers—typically 30 to 100—on a specific technical or scientific topic. Your role is to generate a **comprehensive, graduate-level survey paper** that:\n",
    "\n",
    "1. **Chronologically traces the major discoveries, breakthroughs, and paradigm shifts** in the field.\n",
    "2. **Clearly identifies and cites foundational papers**, important models or algorithms, theoretical contributions, and empirical findings.\n",
    "3. **Includes precise, correctly formatted APA citations** (7th edition) for every claim, method, or referenced work.\n",
    "4. Organizes the paper into **clear sections**, such as:\n",
    "   - Introduction and scope\n",
    "   - Historical timeline of major contributions\n",
    "   - Thematic clusters (e.g., methods, applications, limitations)\n",
    "   - Comparative analysis of approaches\n",
    "   - Emerging trends and open problems\n",
    "5. Presents content **factually, neutrally, and analytically**—like a literature review in a top-tier academic journal.\n",
    "6. Prioritizes **depth over superficiality**—discussing influential works in detail, with commentary on why they matter, how they build on prior work, and their limitations.\n",
    "\n",
    "You **do not hallucinate**. You only synthesize from documents explicitly provided or from a curated, verified knowledge base (if applicable). You aim to maximize **coverage, coherence, and correctness**.\n",
    "\n",
    "You may format your output with numbered sections, bullet points, inline citations (Author, Year), and a full reference list at the end. \n",
    "\n",
    "Assume that the user has loaded all relevant documents (typically in PDF, TXT, or Markdown formats) into memory or a research environment. If asked to write a summary before reading any documents, politely explain that you require access to the source materials to proceed accurately.\n",
    "\n",
    "Tone: **Professional, analytical, academic.**\n",
    "Length: Your output may exceed 5,000 words if needed for comprehensiveness.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     [15:26:00] 🔍 Starting the research task for 'You are a highly specialized AI research assistant designed to read and synthesize insights from a large collection of academic papers—typically 30 to 100—on a specific technical or scientific topic. Your role is to generate a **comprehensive, graduate-level survey paper** that:\n",
      "\n",
      "1. **Chronologically traces the major discoveries, breakthroughs, and paradigm shifts** in the field.\n",
      "2. **Clearly identifies and cites foundational papers**, important models or algorithms, theoretical contributions, and empirical findings.\n",
      "3. **Includes precise, correctly formatted APA citations** (7th edition) for every claim, method, or referenced work.\n",
      "4. Organizes the paper into **clear sections**, such as:\n",
      "   - Introduction and scope\n",
      "   - Historical timeline of major contributions\n",
      "   - Thematic clusters (e.g., methods, applications, limitations)\n",
      "   - Comparative analysis of approaches\n",
      "   - Emerging trends and open problems\n",
      "5. Presents content **factually, neutrally, and analytically**—like a literature review in a top-tier academic journal.\n",
      "6. Prioritizes **depth over superficiality**—discussing influential works in detail, with commentary on why they matter, how they build on prior work, and their limitations.\n",
      "\n",
      "You **do not hallucinate**. You only synthesize from documents explicitly provided or from a curated, verified knowledge base (if applicable). You aim to maximize **coverage, coherence, and correctness**.\n",
      "\n",
      "You may format your output with numbered sections, bullet points, inline citations (Author, Year), and a full reference list at the end. \n",
      "\n",
      "Assume that the user has loaded all relevant documents (typically in PDF, TXT, or Markdown formats) into memory or a research environment. If asked to write a summary before reading any documents, politely explain that you require access to the source materials to proceed accurately.\n",
      "\n",
      "Tone: **Professional, analytical, academic.**\n",
      "Length: Your output may exceed 5,000 words if needed for comprehensiveness.'...\n",
      "INFO:     [15:26:00] 🔬 Scientific Literature Review Agent\n",
      "INFO:     [15:26:00] 🌐 Browsing the web to learn more about the task: You are a highly specialized AI research assistant designed to read and synthesize insights from a large collection of academic papers—typically 30 to 100—on a specific technical or scientific topic. Your role is to generate a **comprehensive, graduate-level survey paper** that:\n",
      "\n",
      "1. **Chronologically traces the major discoveries, breakthroughs, and paradigm shifts** in the field.\n",
      "2. **Clearly identifies and cites foundational papers**, important models or algorithms, theoretical contributions, and empirical findings.\n",
      "3. **Includes precise, correctly formatted APA citations** (7th edition) for every claim, method, or referenced work.\n",
      "4. Organizes the paper into **clear sections**, such as:\n",
      "   - Introduction and scope\n",
      "   - Historical timeline of major contributions\n",
      "   - Thematic clusters (e.g., methods, applications, limitations)\n",
      "   - Comparative analysis of approaches\n",
      "   - Emerging trends and open problems\n",
      "5. Presents content **factually, neutrally, and analytically**—like a literature review in a top-tier academic journal.\n",
      "6. Prioritizes **depth over superficiality**—discussing influential works in detail, with commentary on why they matter, how they build on prior work, and their limitations.\n",
      "\n",
      "You **do not hallucinate**. You only synthesize from documents explicitly provided or from a curated, verified knowledge base (if applicable). You aim to maximize **coverage, coherence, and correctness**.\n",
      "\n",
      "You may format your output with numbered sections, bullet points, inline citations (Author, Year), and a full reference list at the end. \n",
      "\n",
      "Assume that the user has loaded all relevant documents (typically in PDF, TXT, or Markdown formats) into memory or a research environment. If asked to write a summary before reading any documents, politely explain that you require access to the source materials to proceed accurately.\n",
      "\n",
      "Tone: **Professional, analytical, academic.**\n",
      "Length: Your output may exceed 5,000 words if needed for comprehensiveness....\n",
      "INFO:     [15:26:00] 🤔 Planning the research strategy and subtasks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tavily API key not found, set to blank. If you need a retriver, please set the TAVILY_API_KEY environment variable.\n",
      "Error: 401 Client Error: Unauthorized for url: https://api.tavily.com/search. Failed fetching sources. Resulting in empty response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     [15:26:01] 🗂️ I will conduct my research based on the following queries: ['major historical reviews [FIELD OF STUDY] timeline', 'foundational papers seminal works [FIELD OF STUDY]', 'current challenges open problems future directions [FIELD OF STUDY] research', 'You are a highly specialized AI research assistant designed to read and synthesize insights from a large collection of academic papers—typically 30 to 100—on a specific technical or scientific topic. Your role is to generate a **comprehensive, graduate-level survey paper** that:\\n\\n1. **Chronologically traces the major discoveries, breakthroughs, and paradigm shifts** in the field.\\n2. **Clearly identifies and cites foundational papers**, important models or algorithms, theoretical contributions, and empirical findings.\\n3. **Includes precise, correctly formatted APA citations** (7th edition) for every claim, method, or referenced work.\\n4. Organizes the paper into **clear sections**, such as:\\n   - Introduction and scope\\n   - Historical timeline of major contributions\\n   - Thematic clusters (e.g., methods, applications, limitations)\\n   - Comparative analysis of approaches\\n   - Emerging trends and open problems\\n5. Presents content **factually, neutrally, and analytically**—like a literature review in a top-tier academic journal.\\n6. Prioritizes **depth over superficiality**—discussing influential works in detail, with commentary on why they matter, how they build on prior work, and their limitations.\\n\\nYou **do not hallucinate**. You only synthesize from documents explicitly provided or from a curated, verified knowledge base (if applicable). You aim to maximize **coverage, coherence, and correctness**.\\n\\nYou may format your output with numbered sections, bullet points, inline citations (Author, Year), and a full reference list at the end. \\n\\nAssume that the user has loaded all relevant documents (typically in PDF, TXT, or Markdown formats) into memory or a research environment. If asked to write a summary before reading any documents, politely explain that you require access to the source materials to proceed accurately.\\n\\nTone: **Professional, analytical, academic.**\\nLength: Your output may exceed 5,000 words if needed for comprehensiveness.']...\n",
      "INFO:     [15:26:01] \n",
      "🔍 Running research for 'major historical reviews [FIELD OF STUDY] timeline'...\n",
      "INFO:     [15:26:01] 📚 Getting relevant content based on query: major historical reviews [FIELD OF STUDY] timeline...\n",
      "INFO:     [15:26:01] \n",
      "🔍 Running research for 'foundational papers seminal works [FIELD OF STUDY]'...\n",
      "INFO:     [15:26:01] 📚 Getting relevant content based on query: foundational papers seminal works [FIELD OF STUDY]...\n",
      "INFO:     [15:26:02] \n",
      "🔍 Running research for 'current challenges open problems future directions [FIELD OF STUDY] research'...\n",
      "INFO:     [15:26:02] 📚 Getting relevant content based on query: current challenges open problems future directions [FIELD OF STUDY] research...\n",
      "INFO:     [15:26:02] \n",
      "🔍 Running research for 'You are a highly specialized AI research assistant designed to read and synthesize insights from a large collection of academic papers—typically 30 to 100—on a specific technical or scientific topic. Your role is to generate a **comprehensive, graduate-level survey paper** that:\n",
      "\n",
      "1. **Chronologically traces the major discoveries, breakthroughs, and paradigm shifts** in the field.\n",
      "2. **Clearly identifies and cites foundational papers**, important models or algorithms, theoretical contributions, and empirical findings.\n",
      "3. **Includes precise, correctly formatted APA citations** (7th edition) for every claim, method, or referenced work.\n",
      "4. Organizes the paper into **clear sections**, such as:\n",
      "   - Introduction and scope\n",
      "   - Historical timeline of major contributions\n",
      "   - Thematic clusters (e.g., methods, applications, limitations)\n",
      "   - Comparative analysis of approaches\n",
      "   - Emerging trends and open problems\n",
      "5. Presents content **factually, neutrally, and analytically**—like a literature review in a top-tier academic journal.\n",
      "6. Prioritizes **depth over superficiality**—discussing influential works in detail, with commentary on why they matter, how they build on prior work, and their limitations.\n",
      "\n",
      "You **do not hallucinate**. You only synthesize from documents explicitly provided or from a curated, verified knowledge base (if applicable). You aim to maximize **coverage, coherence, and correctness**.\n",
      "\n",
      "You may format your output with numbered sections, bullet points, inline citations (Author, Year), and a full reference list at the end. \n",
      "\n",
      "Assume that the user has loaded all relevant documents (typically in PDF, TXT, or Markdown formats) into memory or a research environment. If asked to write a summary before reading any documents, politely explain that you require access to the source materials to proceed accurately.\n",
      "\n",
      "Tone: **Professional, analytical, academic.**\n",
      "Length: Your output may exceed 5,000 words if needed for comprehensiveness.'...\n",
      "INFO:     [15:26:02] 📚 Getting relevant content based on query: You are a highly specialized AI research assistant designed to read and synthesize insights from a large collection of academic papers—typically 30 to 100—on a specific technical or scientific topic. Your role is to generate a **comprehensive, graduate-level survey paper** that:\n",
      "\n",
      "1. **Chronologically traces the major discoveries, breakthroughs, and paradigm shifts** in the field.\n",
      "2. **Clearly identifies and cites foundational papers**, important models or algorithms, theoretical contributions, and empirical findings.\n",
      "3. **Includes precise, correctly formatted APA citations** (7th edition) for every claim, method, or referenced work.\n",
      "4. Organizes the paper into **clear sections**, such as:\n",
      "   - Introduction and scope\n",
      "   - Historical timeline of major contributions\n",
      "   - Thematic clusters (e.g., methods, applications, limitations)\n",
      "   - Comparative analysis of approaches\n",
      "   - Emerging trends and open problems\n",
      "5. Presents content **factually, neutrally, and analytically**—like a literature review in a top-tier academic journal.\n",
      "6. Prioritizes **depth over superficiality**—discussing influential works in detail, with commentary on why they matter, how they build on prior work, and their limitations.\n",
      "\n",
      "You **do not hallucinate**. You only synthesize from documents explicitly provided or from a curated, verified knowledge base (if applicable). You aim to maximize **coverage, coherence, and correctness**.\n",
      "\n",
      "You may format your output with numbered sections, bullet points, inline citations (Author, Year), and a full reference list at the end. \n",
      "\n",
      "Assume that the user has loaded all relevant documents (typically in PDF, TXT, or Markdown formats) into memory or a research environment. If asked to write a summary before reading any documents, politely explain that you require access to the source materials to proceed accurately.\n",
      "\n",
      "Tone: **Professional, analytical, academic.**\n",
      "Length: Your output may exceed 5,000 words if needed for comprehensiveness....\n",
      "Error processing sub-query major historical reviews [FIELD OF STUDY] timeline: Error embedding content: 429 Quota exceeded for quota metric 'Batch Embed Content API requests' and limit 'Batch embed contents request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:203317671031'. [reason: \"RATE_LIMIT_EXCEEDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_unit\"\n",
      "  value: \"1/min/{project}/{region}\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_metric\"\n",
      "  value: \"generativelanguage.googleapis.com/batch_embed_contents_requests\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_location\"\n",
      "  value: \"us-central1\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit\"\n",
      "  value: \"BatchEmbedContentsRequestsPerMinutePerProjectPerRegion\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit_value\"\n",
      "  value: \"150\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/203317671031\"\n",
      "}\n",
      ", links {\n",
      "  description: \"Request a higher quota limit.\"\n",
      "  url: \"https://cloud.google.com/docs/quotas/help/request_increase\"\n",
      "}\n",
      "]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain_google_genai/embeddings.py\", line 228, in embed_documents\n",
      "    result = self.client.batch_embed_contents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 1436, in batch_embed_contents\n",
      "    response = rpc(\n",
      "               ^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
      "    _retry_error_helper(\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
      "    result = target()\n",
      "             ^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 Quota exceeded for quota metric 'Batch Embed Content API requests' and limit 'Batch embed contents request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:203317671031'. [reason: \"RATE_LIMIT_EXCEEDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_unit\"\n",
      "  value: \"1/min/{project}/{region}\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_metric\"\n",
      "  value: \"generativelanguage.googleapis.com/batch_embed_contents_requests\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_location\"\n",
      "  value: \"us-central1\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit\"\n",
      "  value: \"BatchEmbedContentsRequestsPerMinutePerProjectPerRegion\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit_value\"\n",
      "  value: \"150\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/203317671031\"\n",
      "}\n",
      ", links {\n",
      "  description: \"Request a higher quota limit.\"\n",
      "  url: \"https://cloud.google.com/docs/quotas/help/request_increase\"\n",
      "}\n",
      "]\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/gpt_researcher/skills/researcher.py\", line 289, in _process_sub_query\n",
      "    content = await self.researcher.context_manager.get_similar_content_by_query(sub_query, scraped_data)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/gpt_researcher/skills/context_manager.py\", line 26, in get_similar_content_by_query\n",
      "    return await context_compressor.async_get_context(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/gpt_researcher/context/compression.py\", line 71, in async_get_context\n",
      "    relevant_docs = await asyncio.to_thread(compressed_docs.invoke, query)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n",
      "    return await loop.run_in_executor(None, func_call)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain_core/retrievers.py\", line 258, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain/retrievers/contextual_compression.py\", line 48, in _get_relevant_documents\n",
      "    compressed_docs = self.base_compressor.compress_documents(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain/retrievers/document_compressors/base.py\", line 39, in compress_documents\n",
      "    documents = _transformer.compress_documents(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain/retrievers/document_compressors/embeddings_filter.py\", line 79, in compress_documents\n",
      "    embedded_documents = _get_embeddings_from_stateful_docs(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain_community/document_transformers/embeddings_redundant_filter.py\", line 71, in _get_embeddings_from_stateful_docs\n",
      "    embedded_documents = embeddings.embed_documents(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain_google_genai/embeddings.py\", line 232, in embed_documents\n",
      "    raise GoogleGenerativeAIError(f\"Error embedding content: {e}\") from e\n",
      "langchain_google_genai._common.GoogleGenerativeAIError: Error embedding content: 429 Quota exceeded for quota metric 'Batch Embed Content API requests' and limit 'Batch embed contents request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:203317671031'. [reason: \"RATE_LIMIT_EXCEEDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_unit\"\n",
      "  value: \"1/min/{project}/{region}\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_metric\"\n",
      "  value: \"generativelanguage.googleapis.com/batch_embed_contents_requests\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_location\"\n",
      "  value: \"us-central1\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit\"\n",
      "  value: \"BatchEmbedContentsRequestsPerMinutePerProjectPerRegion\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit_value\"\n",
      "  value: \"150\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/203317671031\"\n",
      "}\n",
      ", links {\n",
      "  description: \"Request a higher quota limit.\"\n",
      "  url: \"https://cloud.google.com/docs/quotas/help/request_increase\"\n",
      "}\n",
      "]\n",
      "Error processing sub-query You are a highly specialized AI research assistant designed to read and synthesize insights from a large collection of academic papers—typically 30 to 100—on a specific technical or scientific topic. Your role is to generate a **comprehensive, graduate-level survey paper** that:\n",
      "\n",
      "1. **Chronologically traces the major discoveries, breakthroughs, and paradigm shifts** in the field.\n",
      "2. **Clearly identifies and cites foundational papers**, important models or algorithms, theoretical contributions, and empirical findings.\n",
      "3. **Includes precise, correctly formatted APA citations** (7th edition) for every claim, method, or referenced work.\n",
      "4. Organizes the paper into **clear sections**, such as:\n",
      "   - Introduction and scope\n",
      "   - Historical timeline of major contributions\n",
      "   - Thematic clusters (e.g., methods, applications, limitations)\n",
      "   - Comparative analysis of approaches\n",
      "   - Emerging trends and open problems\n",
      "5. Presents content **factually, neutrally, and analytically**—like a literature review in a top-tier academic journal.\n",
      "6. Prioritizes **depth over superficiality**—discussing influential works in detail, with commentary on why they matter, how they build on prior work, and their limitations.\n",
      "\n",
      "You **do not hallucinate**. You only synthesize from documents explicitly provided or from a curated, verified knowledge base (if applicable). You aim to maximize **coverage, coherence, and correctness**.\n",
      "\n",
      "You may format your output with numbered sections, bullet points, inline citations (Author, Year), and a full reference list at the end. \n",
      "\n",
      "Assume that the user has loaded all relevant documents (typically in PDF, TXT, or Markdown formats) into memory or a research environment. If asked to write a summary before reading any documents, politely explain that you require access to the source materials to proceed accurately.\n",
      "\n",
      "Tone: **Professional, analytical, academic.**\n",
      "Length: Your output may exceed 5,000 words if needed for comprehensiveness.: Error embedding content: 429 Quota exceeded for quota metric 'Batch Embed Content API requests' and limit 'Batch embed contents request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:203317671031'. [reason: \"RATE_LIMIT_EXCEEDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_unit\"\n",
      "  value: \"1/min/{project}/{region}\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_metric\"\n",
      "  value: \"generativelanguage.googleapis.com/batch_embed_contents_requests\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_location\"\n",
      "  value: \"us-central1\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit\"\n",
      "  value: \"BatchEmbedContentsRequestsPerMinutePerProjectPerRegion\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit_value\"\n",
      "  value: \"150\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/203317671031\"\n",
      "}\n",
      ", links {\n",
      "  description: \"Request a higher quota limit.\"\n",
      "  url: \"https://cloud.google.com/docs/quotas/help/request_increase\"\n",
      "}\n",
      "]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain_google_genai/embeddings.py\", line 228, in embed_documents\n",
      "    result = self.client.batch_embed_contents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 1436, in batch_embed_contents\n",
      "    response = rpc(\n",
      "               ^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
      "    _retry_error_helper(\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
      "    result = target()\n",
      "             ^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 Quota exceeded for quota metric 'Batch Embed Content API requests' and limit 'Batch embed contents request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:203317671031'. [reason: \"RATE_LIMIT_EXCEEDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_unit\"\n",
      "  value: \"1/min/{project}/{region}\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_metric\"\n",
      "  value: \"generativelanguage.googleapis.com/batch_embed_contents_requests\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_location\"\n",
      "  value: \"us-central1\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit\"\n",
      "  value: \"BatchEmbedContentsRequestsPerMinutePerProjectPerRegion\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit_value\"\n",
      "  value: \"150\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/203317671031\"\n",
      "}\n",
      ", links {\n",
      "  description: \"Request a higher quota limit.\"\n",
      "  url: \"https://cloud.google.com/docs/quotas/help/request_increase\"\n",
      "}\n",
      "]\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/gpt_researcher/skills/researcher.py\", line 289, in _process_sub_query\n",
      "    content = await self.researcher.context_manager.get_similar_content_by_query(sub_query, scraped_data)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/gpt_researcher/skills/context_manager.py\", line 26, in get_similar_content_by_query\n",
      "    return await context_compressor.async_get_context(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/gpt_researcher/context/compression.py\", line 71, in async_get_context\n",
      "    relevant_docs = await asyncio.to_thread(compressed_docs.invoke, query)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n",
      "    return await loop.run_in_executor(None, func_call)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain_core/retrievers.py\", line 258, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain/retrievers/contextual_compression.py\", line 48, in _get_relevant_documents\n",
      "    compressed_docs = self.base_compressor.compress_documents(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain/retrievers/document_compressors/base.py\", line 39, in compress_documents\n",
      "    documents = _transformer.compress_documents(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain/retrievers/document_compressors/embeddings_filter.py\", line 79, in compress_documents\n",
      "    embedded_documents = _get_embeddings_from_stateful_docs(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain_community/document_transformers/embeddings_redundant_filter.py\", line 71, in _get_embeddings_from_stateful_docs\n",
      "    embedded_documents = embeddings.embed_documents(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain_google_genai/embeddings.py\", line 232, in embed_documents\n",
      "    raise GoogleGenerativeAIError(f\"Error embedding content: {e}\") from e\n",
      "langchain_google_genai._common.GoogleGenerativeAIError: Error embedding content: 429 Quota exceeded for quota metric 'Batch Embed Content API requests' and limit 'Batch embed contents request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:203317671031'. [reason: \"RATE_LIMIT_EXCEEDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_unit\"\n",
      "  value: \"1/min/{project}/{region}\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_metric\"\n",
      "  value: \"generativelanguage.googleapis.com/batch_embed_contents_requests\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_location\"\n",
      "  value: \"us-central1\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit\"\n",
      "  value: \"BatchEmbedContentsRequestsPerMinutePerProjectPerRegion\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit_value\"\n",
      "  value: \"150\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/203317671031\"\n",
      "}\n",
      ", links {\n",
      "  description: \"Request a higher quota limit.\"\n",
      "  url: \"https://cloud.google.com/docs/quotas/help/request_increase\"\n",
      "}\n",
      "]\n",
      "Error processing sub-query current challenges open problems future directions [FIELD OF STUDY] research: Error embedding content: 429 Quota exceeded for quota metric 'Batch Embed Content API requests' and limit 'Batch embed contents request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:203317671031'. [reason: \"RATE_LIMIT_EXCEEDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_unit\"\n",
      "  value: \"1/min/{project}/{region}\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_metric\"\n",
      "  value: \"generativelanguage.googleapis.com/batch_embed_contents_requests\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_location\"\n",
      "  value: \"us-central1\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit\"\n",
      "  value: \"BatchEmbedContentsRequestsPerMinutePerProjectPerRegion\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit_value\"\n",
      "  value: \"150\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/203317671031\"\n",
      "}\n",
      ", links {\n",
      "  description: \"Request a higher quota limit.\"\n",
      "  url: \"https://cloud.google.com/docs/quotas/help/request_increase\"\n",
      "}\n",
      "]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain_google_genai/embeddings.py\", line 228, in embed_documents\n",
      "    result = self.client.batch_embed_contents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 1436, in batch_embed_contents\n",
      "    response = rpc(\n",
      "               ^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
      "    _retry_error_helper(\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
      "    result = target()\n",
      "             ^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 Quota exceeded for quota metric 'Batch Embed Content API requests' and limit 'Batch embed contents request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:203317671031'. [reason: \"RATE_LIMIT_EXCEEDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_unit\"\n",
      "  value: \"1/min/{project}/{region}\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_metric\"\n",
      "  value: \"generativelanguage.googleapis.com/batch_embed_contents_requests\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_location\"\n",
      "  value: \"us-central1\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit\"\n",
      "  value: \"BatchEmbedContentsRequestsPerMinutePerProjectPerRegion\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit_value\"\n",
      "  value: \"150\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/203317671031\"\n",
      "}\n",
      ", links {\n",
      "  description: \"Request a higher quota limit.\"\n",
      "  url: \"https://cloud.google.com/docs/quotas/help/request_increase\"\n",
      "}\n",
      "]\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/gpt_researcher/skills/researcher.py\", line 289, in _process_sub_query\n",
      "    content = await self.researcher.context_manager.get_similar_content_by_query(sub_query, scraped_data)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/gpt_researcher/skills/context_manager.py\", line 26, in get_similar_content_by_query\n",
      "    return await context_compressor.async_get_context(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/gpt_researcher/context/compression.py\", line 71, in async_get_context\n",
      "    relevant_docs = await asyncio.to_thread(compressed_docs.invoke, query)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n",
      "    return await loop.run_in_executor(None, func_call)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain_core/retrievers.py\", line 258, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain/retrievers/contextual_compression.py\", line 48, in _get_relevant_documents\n",
      "    compressed_docs = self.base_compressor.compress_documents(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain/retrievers/document_compressors/base.py\", line 39, in compress_documents\n",
      "    documents = _transformer.compress_documents(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain/retrievers/document_compressors/embeddings_filter.py\", line 79, in compress_documents\n",
      "    embedded_documents = _get_embeddings_from_stateful_docs(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain_community/document_transformers/embeddings_redundant_filter.py\", line 71, in _get_embeddings_from_stateful_docs\n",
      "    embedded_documents = embeddings.embed_documents(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain_google_genai/embeddings.py\", line 232, in embed_documents\n",
      "    raise GoogleGenerativeAIError(f\"Error embedding content: {e}\") from e\n",
      "langchain_google_genai._common.GoogleGenerativeAIError: Error embedding content: 429 Quota exceeded for quota metric 'Batch Embed Content API requests' and limit 'Batch embed contents request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:203317671031'. [reason: \"RATE_LIMIT_EXCEEDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_unit\"\n",
      "  value: \"1/min/{project}/{region}\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_metric\"\n",
      "  value: \"generativelanguage.googleapis.com/batch_embed_contents_requests\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_location\"\n",
      "  value: \"us-central1\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit\"\n",
      "  value: \"BatchEmbedContentsRequestsPerMinutePerProjectPerRegion\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit_value\"\n",
      "  value: \"150\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/203317671031\"\n",
      "}\n",
      ", links {\n",
      "  description: \"Request a higher quota limit.\"\n",
      "  url: \"https://cloud.google.com/docs/quotas/help/request_increase\"\n",
      "}\n",
      "]\n",
      "Error processing sub-query foundational papers seminal works [FIELD OF STUDY]: Error embedding content: 429 Quota exceeded for quota metric 'Batch Embed Content API requests' and limit 'Batch embed contents request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:203317671031'. [reason: \"RATE_LIMIT_EXCEEDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_unit\"\n",
      "  value: \"1/min/{project}/{region}\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_metric\"\n",
      "  value: \"generativelanguage.googleapis.com/batch_embed_contents_requests\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_location\"\n",
      "  value: \"us-central1\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit\"\n",
      "  value: \"BatchEmbedContentsRequestsPerMinutePerProjectPerRegion\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit_value\"\n",
      "  value: \"150\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/203317671031\"\n",
      "}\n",
      ", links {\n",
      "  description: \"Request a higher quota limit.\"\n",
      "  url: \"https://cloud.google.com/docs/quotas/help/request_increase\"\n",
      "}\n",
      "]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain_google_genai/embeddings.py\", line 228, in embed_documents\n",
      "    result = self.client.batch_embed_contents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 1436, in batch_embed_contents\n",
      "    response = rpc(\n",
      "               ^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
      "    _retry_error_helper(\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
      "    result = target()\n",
      "             ^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 Quota exceeded for quota metric 'Batch Embed Content API requests' and limit 'Batch embed contents request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:203317671031'. [reason: \"RATE_LIMIT_EXCEEDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_unit\"\n",
      "  value: \"1/min/{project}/{region}\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_metric\"\n",
      "  value: \"generativelanguage.googleapis.com/batch_embed_contents_requests\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_location\"\n",
      "  value: \"us-central1\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit\"\n",
      "  value: \"BatchEmbedContentsRequestsPerMinutePerProjectPerRegion\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit_value\"\n",
      "  value: \"150\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/203317671031\"\n",
      "}\n",
      ", links {\n",
      "  description: \"Request a higher quota limit.\"\n",
      "  url: \"https://cloud.google.com/docs/quotas/help/request_increase\"\n",
      "}\n",
      "]\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/gpt_researcher/skills/researcher.py\", line 289, in _process_sub_query\n",
      "    content = await self.researcher.context_manager.get_similar_content_by_query(sub_query, scraped_data)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/gpt_researcher/skills/context_manager.py\", line 26, in get_similar_content_by_query\n",
      "    return await context_compressor.async_get_context(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/gpt_researcher/context/compression.py\", line 71, in async_get_context\n",
      "    relevant_docs = await asyncio.to_thread(compressed_docs.invoke, query)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/threads.py\", line 25, in to_thread\n",
      "    return await loop.run_in_executor(None, func_call)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.12/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain_core/retrievers.py\", line 258, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain/retrievers/contextual_compression.py\", line 48, in _get_relevant_documents\n",
      "    compressed_docs = self.base_compressor.compress_documents(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain/retrievers/document_compressors/base.py\", line 39, in compress_documents\n",
      "    documents = _transformer.compress_documents(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain/retrievers/document_compressors/embeddings_filter.py\", line 79, in compress_documents\n",
      "    embedded_documents = _get_embeddings_from_stateful_docs(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain_community/document_transformers/embeddings_redundant_filter.py\", line 71, in _get_embeddings_from_stateful_docs\n",
      "    embedded_documents = embeddings.embed_documents(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roberthand/Desktop/FDL/venv/lib/python3.11/site-packages/langchain_google_genai/embeddings.py\", line 232, in embed_documents\n",
      "    raise GoogleGenerativeAIError(f\"Error embedding content: {e}\") from e\n",
      "langchain_google_genai._common.GoogleGenerativeAIError: Error embedding content: 429 Quota exceeded for quota metric 'Batch Embed Content API requests' and limit 'Batch embed contents request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:203317671031'. [reason: \"RATE_LIMIT_EXCEEDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_unit\"\n",
      "  value: \"1/min/{project}/{region}\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_metric\"\n",
      "  value: \"generativelanguage.googleapis.com/batch_embed_contents_requests\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_location\"\n",
      "  value: \"us-central1\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit\"\n",
      "  value: \"BatchEmbedContentsRequestsPerMinutePerProjectPerRegion\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit_value\"\n",
      "  value: \"150\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/203317671031\"\n",
      "}\n",
      ", links {\n",
      "  description: \"Request a higher quota limit.\"\n",
      "  url: \"https://cloud.google.com/docs/quotas/help/request_increase\"\n",
      "}\n",
      "]\n",
      "INFO:     [15:26:12] Finalized research step.\n",
      "💸 Total Research Costs: $0.08458379999999999\n",
      "INFO:     [15:26:12] ✍️ Writing report for 'You are a highly specialized AI research assistant designed to read and synthesize insights from a large collection of academic papers—typically 30 to 100—on a specific technical or scientific topic. Your role is to generate a **comprehensive, graduate-level survey paper** that:\n",
      "\n",
      "1. **Chronologically traces the major discoveries, breakthroughs, and paradigm shifts** in the field.\n",
      "2. **Clearly identifies and cites foundational papers**, important models or algorithms, theoretical contributions, and empirical findings.\n",
      "3. **Includes precise, correctly formatted APA citations** (7th edition) for every claim, method, or referenced work.\n",
      "4. Organizes the paper into **clear sections**, such as:\n",
      "   - Introduction and scope\n",
      "   - Historical timeline of major contributions\n",
      "   - Thematic clusters (e.g., methods, applications, limitations)\n",
      "   - Comparative analysis of approaches\n",
      "   - Emerging trends and open problems\n",
      "5. Presents content **factually, neutrally, and analytically**—like a literature review in a top-tier academic journal.\n",
      "6. Prioritizes **depth over superficiality**—discussing influential works in detail, with commentary on why they matter, how they build on prior work, and their limitations.\n",
      "\n",
      "You **do not hallucinate**. You only synthesize from documents explicitly provided or from a curated, verified knowledge base (if applicable). You aim to maximize **coverage, coherence, and correctness**.\n",
      "\n",
      "You may format your output with numbered sections, bullet points, inline citations (Author, Year), and a full reference list at the end. \n",
      "\n",
      "Assume that the user has loaded all relevant documents (typically in PDF, TXT, or Markdown formats) into memory or a research environment. If asked to write a summary before reading any documents, politely explain that you require access to the source materials to proceed accurately.\n",
      "\n",
      "Tone: **Professional, analytical, academic.**\n",
      "Length: Your output may exceed 5,000 words if needed for comprehensiveness.'...\n",
      "INFO:     [15:26:13] 📝 Report written for 'You are a highly specialized AI research assistant designed to read and synthesize insights from a large collection of academic papers—typically 30 to 100—on a specific technical or scientific topic. Your role is to generate a **comprehensive, graduate-level survey paper** that:\n",
      "\n",
      "1. **Chronologically traces the major discoveries, breakthroughs, and paradigm shifts** in the field.\n",
      "2. **Clearly identifies and cites foundational papers**, important models or algorithms, theoretical contributions, and empirical findings.\n",
      "3. **Includes precise, correctly formatted APA citations** (7th edition) for every claim, method, or referenced work.\n",
      "4. Organizes the paper into **clear sections**, such as:\n",
      "   - Introduction and scope\n",
      "   - Historical timeline of major contributions\n",
      "   - Thematic clusters (e.g., methods, applications, limitations)\n",
      "   - Comparative analysis of approaches\n",
      "   - Emerging trends and open problems\n",
      "5. Presents content **factually, neutrally, and analytically**—like a literature review in a top-tier academic journal.\n",
      "6. Prioritizes **depth over superficiality**—discussing influential works in detail, with commentary on why they matter, how they build on prior work, and their limitations.\n",
      "\n",
      "You **do not hallucinate**. You only synthesize from documents explicitly provided or from a curated, verified knowledge base (if applicable). You aim to maximize **coverage, coherence, and correctness**.\n",
      "\n",
      "You may format your output with numbered sections, bullet points, inline citations (Author, Year), and a full reference list at the end. \n",
      "\n",
      "Assume that the user has loaded all relevant documents (typically in PDF, TXT, or Markdown formats) into memory or a research environment. If asked to write a summary before reading any documents, politely explain that you require access to the source materials to proceed accurately.\n",
      "\n",
      "Tone: **Professional, analytical, academic.**\n",
      "Length: Your output may exceed 5,000 words if needed for comprehensiveness.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mI am sorry, but I cannot fulfill this request. The provided information is empty (\"[]\"). To generate a comprehensive, graduate-level survey paper as described, I require access to a collection of academic papers on a specific technical or scientific topic. Without these source materials, I cannot accurately trace the history of discoveries, identify foundational papers, provide citations, or perform any of the other tasks outlined in the prompt.\u001b[0m\n",
      "I am sorry, but I cannot fulfill this request. The provided information is empty (\"[]\"). To generate a comprehensive, graduate-level survey paper as described, I require access to a collection of academic papers on a specific technical or scientific topic. Without these source materials, I cannot accurately trace the history of discoveries, identify foundational papers, provide citations, or perform any of the other tasks outlined in the prompt.\n"
     ]
    }
   ],
   "source": [
    "# trace code and see if/why it searches from the web\n",
    "# should not be adding links\n",
    "# asking for search api even though on local?!\n",
    "# modify code for gpt researcher\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Step 1: Fetch articles from MongoDB\n",
    "def fetch_articles_from_mongo():\n",
    "    client = MongoClient(\"mongodb://mongoadmin:password@localhost:27017/?authSource=admin\")\n",
    "    db = client[\"FDL\"]\n",
    "    collection = db[\"DSA\"]\n",
    "    articles = list(collection.find({}, {\"_id\": 0}))\n",
    "    return articles\n",
    "\n",
    "# Step 2: Save each article as a text file\n",
    "def save_articles_to_local_dir(articles, output_dir=\"./mongo-docs\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for i, article in enumerate(articles):\n",
    "        # Turn article (likely a dict) into plain text\n",
    "        if isinstance(article, dict):\n",
    "            content = \"\\n\".join(f\"{k}: {v}\" for k, v in article.items())\n",
    "        else:\n",
    "            content = str(article)\n",
    "        with open(os.path.join(output_dir, f\"article_{i}.txt\"), \"w\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "# Step 3: Generate report using GPTResearcher\n",
    "async def get_report(query: str) -> str:\n",
    "    researcher = GPTResearcher(query=query, report_source=\"local\")\n",
    "    await researcher.conduct_research()\n",
    "    return await researcher.write_report()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = system_prompt\n",
    "    # Fetch + write MongoDB documents\n",
    "    mongo_texts = fetch_articles_from_mongo()\n",
    "    save_articles_to_local_dir(mongo_texts)\n",
    "    os.environ[\"DOC_PATH\"] = \"./mongo-docs\"     # Set DOC_PATH\n",
    "    report = asyncio.run(get_report(query=query))     # Run the report\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IS THE GOLDEN LINE OF CODE JUPYTER NOTEBOOK WILL NOT WORK WITHOUT THIS LINE! \"import nest_asyncio\n",
    "nest_asyncio.apply()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     [04:15:11] 🔍 Starting the research task for 'what team may win the NBA finals?'...\n",
      "INFO:     [04:15:11] 🏀 Sports Analyst Agent\n",
      "INFO:     [04:15:11] 🌐 Browsing the web to learn more about the task: what team may win the NBA finals?...\n",
      "INFO:     [04:15:12] 🤔 Planning the research strategy and subtasks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tavily API key not found, set to blank. If you need a retriver, please set the TAVILY_API_KEY environment variable.\n",
      "Error: 401 Client Error: Unauthorized for url: https://api.tavily.com/search. Failed fetching sources. Resulting in empty response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     [04:15:12] 🗂️ I will conduct my research based on the following queries: ['NBA Finals 2025 predictions May 3', 'NBA playoff team stats leaders 2025', 'NBA championship odds May 2025', 'what team may win the NBA finals?']...\n",
      "INFO:     [04:15:12] \n",
      "🔍 Running research for 'NBA Finals 2025 predictions May 3'...\n",
      "INFO:     [04:15:12] \n",
      "🔍 Running research for 'NBA playoff team stats leaders 2025'...\n",
      "INFO:     [04:15:12] \n",
      "🔍 Running research for 'NBA championship odds May 2025'...\n",
      "INFO:     [04:15:12] \n",
      "🔍 Running research for 'what team may win the NBA finals?'...\n",
      "INFO:     [04:15:12] 🤔 Researching for relevant information across multiple sources...\n",
      "\n",
      "INFO:     [04:15:12] 🌐 Scraping content from 0 URLs...\n",
      "INFO:     [04:15:12] 📄 Scraped 0 pages of content\n",
      "INFO:     [04:15:12] 🖼️ Selected 0 new images from 0 total images\n",
      "INFO:     [04:15:12] 🌐 Scraping complete\n",
      "INFO:     [04:15:12] 📚 Getting relevant content based on query: NBA playoff team stats leaders 2025...\n",
      "INFO:     [04:15:12] 🤔 Researching for relevant information across multiple sources...\n",
      "\n",
      "INFO:     [04:15:12] 🌐 Scraping content from 0 URLs...\n",
      "INFO:     [04:15:12] 📄 Scraped 0 pages of content\n",
      "INFO:     [04:15:12] 🖼️ Selected 0 new images from 0 total images\n",
      "INFO:     [04:15:12] 🌐 Scraping complete\n",
      "INFO:     [04:15:12] 📚 Getting relevant content based on query: what team may win the NBA finals?...\n",
      "INFO:     [04:15:12] 🤔 Researching for relevant information across multiple sources...\n",
      "\n",
      "INFO:     [04:15:12] 🌐 Scraping content from 0 URLs...\n",
      "INFO:     [04:15:12] 📄 Scraped 0 pages of content\n",
      "INFO:     [04:15:12] 🖼️ Selected 0 new images from 0 total images\n",
      "INFO:     [04:15:12] 🌐 Scraping complete\n",
      "INFO:     [04:15:12] 📚 Getting relevant content based on query: NBA Finals 2025 predictions May 3...\n",
      "INFO:     [04:15:12] 🤔 Researching for relevant information across multiple sources...\n",
      "\n",
      "INFO:     [04:15:12] 🌐 Scraping content from 0 URLs...\n",
      "INFO:     [04:15:12] 📄 Scraped 0 pages of content\n",
      "INFO:     [04:15:12] 🖼️ Selected 0 new images from 0 total images\n",
      "INFO:     [04:15:12] 🌐 Scraping complete\n",
      "INFO:     [04:15:12] 📚 Getting relevant content based on query: NBA championship odds May 2025...\n",
      "INFO:     [04:15:12] 🤷 No content found for 'NBA playoff team stats leaders 2025'...\n",
      "INFO:     [04:15:12] 🤷 No content found for 'what team may win the NBA finals?'...\n",
      "INFO:     [04:15:12] 🤷 No content found for 'NBA Finals 2025 predictions May 3'...\n",
      "INFO:     [04:15:12] 🤷 No content found for 'NBA championship odds May 2025'...\n",
      "INFO:     [04:15:12] Finalized research step.\n",
      "💸 Total Research Costs: $0.00383\n",
      "INFO:     [04:15:12] ✍️ Writing report for 'what team may win the NBA finals?'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tavily API key not found, set to blank. If you need a retriver, please set the TAVILY_API_KEY environment variable.\n",
      "Tavily API key not found, set to blank. If you need a retriver, please set the TAVILY_API_KEY environment variable.\n",
      "Tavily API key not found, set to blank. If you need a retriver, please set the TAVILY_API_KEY environment variable.\n",
      "Tavily API key not found, set to blank. If you need a retriver, please set the TAVILY_API_KEY environment variable.\n",
      "Error: 401 Client Error: Unauthorized for url: https://api.tavily.com/search. Failed fetching sources. Resulting in empty response.\n",
      "Error: 401 Client Error: Unauthorized for url: https://api.tavily.com/search. Failed fetching sources. Resulting in empty response.\n",
      "Error: 401 Client Error: Unauthorized for url: https://api.tavily.com/search. Failed fetching sources. Resulting in empty response.\n",
      "Error: 401 Client Error: Unauthorized for url: https://api.tavily.com/search. Failed fetching sources. Resulting in empty response.\n",
      "\u001b[32m## NBA Finals 2025: A Data-Driven Prediction\n",
      "\n",
      "Predicting the winner of\u001b[0m\n",
      "\u001b[32m the NBA Finals is a complex task, influenced by numerous factors including team composition, player health, coaching strategies, and in-season performance. While the provided information is limited to \"[]\", this report will extrapolate potential contenders for the 2025 NBA Finals based on current NBA trends, team dynamics, and projections derived from reputable sports analytics platforms. This analysis assumes the current date is May 3, 2025, placing us at the cusp of the playoffs.\n",
      "\n",
      "**Methodology**\n",
      "\n",
      "Given the absence of specific data, this report will rely on a qualitative analysis informed by publicly available information regarding team performance, player statistics, coaching\u001b[0m\n",
      "\u001b[32m changes, and expert opinions. The analysis will focus on identifying teams with the following characteristics:\n",
      "\n",
      "*   **Strong Regular Season Record:** Consistent performance throughout the regular season is a strong indicator of a team's overall strength and ability to perform under pressure\u001b[0m\n",
      "\u001b[32m.\n",
      "*   **Dominant Offensive and Defensive Ratings:** Teams that excel on both ends of the court are more likely to succeed in the playoffs, where defensive intensity increases.\n",
      "*   **Star Power and Depth:** A combination of elite players and a solid supporting cast is crucial for navigating the challenges of a long playoff run.\u001b[0m\n",
      "\u001b[32m\n",
      "*   **Coaching Acumen:** Effective coaching can make a significant difference in playoff series, particularly in terms of game planning and adjustments.\n",
      "*   **Injury Status:** The health of key players is a critical factor in determining a team's chances of success.\n",
      "\n",
      "**Potential Contenders**\n",
      "\n",
      "Based on\u001b[0m\n",
      "\u001b[32m the above criteria, the following teams are identified as potential contenders for the 2025 NBA Finals:\n",
      "\n",
      "1.  **Boston Celtics:** The Celtics have consistently been a top team in the Eastern Conference for several years. Their combination of elite scoring, strong defense, and playoff experience makes them a perennial threat.\u001b[0m\n",
      "\u001b[32m2.  **Denver Nuggets:** The Nuggets, led by their star player, have demonstrated the ability to dominate both in the regular season and the playoffs. Their offensive firepower and improved defense make them a formidable opponent.\n",
      "3.  **Milwaukee Bucks:** With a core of talented players, the Bucks are always a contender in the Eastern\u001b[0m\n",
      "\u001b[32m Conference. Their size, athleticism, and experience make them a tough matchup for any team.\n",
      "4.  **Phoenix Suns:** The Suns possess a roster filled with offensive talent. If they can improve their defensive consistency and chemistry, they could be a serious threat in the Western Conference.\n",
      "5.  **Los Angeles Clippers\u001b[0m\n",
      "\u001b[32m:** If healthy, the Clippers have the potential to be a dominant force in the NBA. Their combination of star power and depth makes them a team to watch out for.\n",
      "\n",
      "**Detailed Team Analysis**\n",
      "\n",
      "To provide a more in-depth analysis, let's consider a hypothetical scenario for each of the potential contenders, focusing on their\u001b[0m\n",
      "\u001b[32m strengths, weaknesses, and potential challenges:\n",
      "\n",
      "**1. Boston Celtics**\n",
      "\n",
      "*   **Strengths:** The Celtics boast a well-rounded roster with multiple players capable of scoring at a high level. Their defense is consistently among the best in the league, and they have a proven track record of success in the playoffs\u001b[0m\n",
      "\u001b[32m. Their coaching staff is known for their strategic adjustments and ability to get the most out of their players.\n",
      "*   **Weaknesses:** The Celtics can sometimes struggle with consistency, particularly on offense. They may also be vulnerable to teams with dominant interior players.\n",
      "*   **Challenges:** Maintaining player health will be crucial\u001b[0m\n",
      "\u001b[32m for the Celtics' success. They will also need to avoid complacency and maintain their focus throughout the playoffs.\n",
      "\n",
      "| Category          | Assessment |\n",
      "| ----------------- | ---------- |\n",
      "| Regular Season Record | Excellent  |\n",
      "| Offensive Rating    | Top 5      |\n",
      "| Defensive Rating    | Top 3\u001b[0m\n",
      "\u001b[32m      |\n",
      "| Star Power        | High       |\n",
      "| Depth             | Good       |\n",
      "| Coaching          | Excellent  |\n",
      "| Injury Status     | Moderate   |\n",
      "\n",
      "**2. Denver Nuggets**\n",
      "\n",
      "*   **Strengths:** The Nuggets' offense revolves around their star player, who is arguably the best offensive player\u001b[0m\n",
      "\u001b[32m in the league. They have a strong supporting cast of shooters and playmakers, and their home-court advantage is significant.\n",
      "*   **Weaknesses:** The Nuggets' defense can be inconsistent at times, and they may struggle against teams with multiple scoring options.\n",
      "*   **Challenges:** The Nuggets will need to\u001b[0m\n",
      "\u001b[32m improve their defensive intensity in the playoffs. They will also need to find ways to score when their star player is being double-teamed.\n",
      "\n",
      "| Category          | Assessment |\n",
      "| ----------------- | ---------- |\n",
      "| Regular Season Record | Excellent  |\n",
      "| Offensive Rating    | Top 3      |\n",
      "|\u001b[0m\n",
      "\u001b[32m Defensive Rating    | Top 10     |\n",
      "| Star Power        | High       |\n",
      "| Depth             | Good       |\n",
      "| Coaching          | Good       |\n",
      "| Injury Status     | Low        |\n",
      "\n",
      "**3. Milwaukee Bucks**\n",
      "\n",
      "*   **Strengths:** The Bucks are a dominant defensive\u001b[0m\n",
      "\u001b[32m team with a strong interior presence. Their star player is one of the most dominant players in the league, and they have a proven track record of success in the playoffs.\n",
      "*   **Weaknesses:** The Bucks' offense can sometimes be stagnant, and they may struggle against teams with quick guards who can penetrate the paint.\n",
      "*\u001b[0m\n",
      "\u001b[32m   **Challenges:** The Bucks will need to improve their offensive versatility in the playoffs. They will also need to find ways to contain opposing guards.\n",
      "\n",
      "| Category          | Assessment |\n",
      "| ----------------- | ---------- |\n",
      "| Regular Season Record | Excellent  |\n",
      "| Offensive Rating    | Top 10     |\u001b[0m\n",
      "\u001b[32m\n",
      "| Defensive Rating    | Top 5      |\n",
      "| Star Power        | High       |\n",
      "| Depth             | Good       |\n",
      "| Coaching          | Good       |\n",
      "| Injury Status     | Moderate   |\n",
      "\n",
      "**4. Phoenix Suns**\n",
      "\n",
      "*   **Strengths:** The Suns possess a roster filled\u001b[0m\n",
      "\u001b[32m with offensive talent. Their ability to score from multiple positions makes them difficult to guard.\n",
      "*   **Weaknesses:** The Suns' defense can be inconsistent, and they may struggle against teams with strong interior players. Team chemistry is also a question mark, given the number of high-profile players on the roster.\n",
      "*   **\u001b[0m\n",
      "\u001b[32mChallenges:** The Suns will need to improve their defensive consistency and develop better team chemistry. They will also need to find ways to score against tough defensive teams.\n",
      "\n",
      "| Category          | Assessment |\n",
      "| ----------------- | ---------- |\n",
      "| Regular Season Record | Good       |\n",
      "| Offensive Rating    | Top 5      \u001b[0m\n",
      "\u001b[32m|\n",
      "| Defensive Rating    | Average    |\n",
      "| Star Power        | High       |\n",
      "| Depth             | Average    |\n",
      "| Coaching          | Average    |\n",
      "| Injury Status     | Moderate   |\n",
      "\n",
      "**5. Los Angeles Clippers**\n",
      "\n",
      "*   **Strengths:** The Clippers have a roster filled with star\u001b[0m\n",
      "\u001b[32m power and depth. Their ability to score from multiple positions makes them difficult to guard.\n",
      "*   **Weaknesses:** The Clippers' biggest weakness is their injury history. They have struggled to keep their key players healthy for extended periods of time.\n",
      "*   **Challenges:** The Clippers will need to stay healthy in order to contend\u001b[0m\n",
      "\u001b[32m for a championship. They will also need to develop better team chemistry and find ways to win close games.\n",
      "\n",
      "| Category          | Assessment |\n",
      "| ----------------- | ---------- |\n",
      "| Regular Season Record | Good       |\n",
      "| Offensive Rating    | Top 10     |\n",
      "| Defensive Rating    | Average    |\u001b[0m\n",
      "\u001b[32m| Star Power        | High       |\n",
      "| Depth             | Good       |\n",
      "| Coaching          | Good       |\n",
      "| Injury Status     | High       |\n",
      "\n",
      "**Factors Influencing the Outcome**\n",
      "\n",
      "Several factors could significantly influence the outcome of the 2025 NBA Finals:\n",
      "\n",
      "*   **Inj\u001b[0m\n",
      "\u001b[32muries:** Injuries to key players can derail even the most talented teams.\n",
      "*   **Matchups:** Certain teams may have a stylistic advantage over others, making matchups a critical factor in determining the outcome of a series.\n",
      "*   **Coaching Adjustments:** The ability of coaches to make effective adjustments during a series can be a\u001b[0m\n",
      "\u001b[32m significant advantage.\n",
      "*   **Luck:** A certain amount of luck is always involved in the playoffs, whether it's a favorable bounce of the ball or an unexpected performance from a role player.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Based on the available information and analysis, the **Boston Celtics** emerge as a strong contender to\u001b[0m\n",
      "\u001b[32m win the 2025 NBA Finals. Their combination of elite scoring, strong defense, playoff experience, and coaching acumen makes them a formidable opponent. However, the other teams mentioned in this report also have the potential to contend for a championship, and the outcome of the Finals will ultimately depend on a variety of factors, including injuries, matchups, and coaching adjustments.\n",
      "\n",
      "**Disclaimer:** This report is based on a qualitative analysis and is subject to change based on future developments. The absence of specific data limits the accuracy of the predictions.\n",
      "\n",
      "**References**\n",
      "\n",
      "Because the given information is \"[]\", I am unable to provide specific references. However, in\u001b[0m\n",
      "\u001b[32m a real-world scenario, I would include references to reputable sports news websites, statistical analysis platforms, and NBA official sources. Some examples of the types of sources I would use are listed below with hypothetical URLs:\n",
      "\n",
      "*   ESPN NBA: [ESPN NBA](https://www.espn.com/nba/)\n",
      "*\u001b[0m\n",
      "\u001b[32m   NBA.com: [NBA.com](https://www.nba.com/)\n",
      "*   Basketball-Reference.com: [Basketball-Reference.com](https://www.basketball-reference.com/)\n",
      "*   Bleacher Report NBA: [Bleacher Report NBA](https://bleacherreport.com\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     [04:15:24] 📝 Report written for 'what team may win the NBA finals?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m/nba)\n",
      "*   The Athletic NBA: [The Athletic NBA](https://theathletic.com/nba/)\u001b[0m\n",
      "Report:\n",
      "## NBA Finals 2025: A Data-Driven Prediction\n",
      "\n",
      "Predicting the winner of the NBA Finals is a complex task, influenced by numerous factors including team composition, player health, coaching strategies, and in-season performance. While the provided information is limited to \"[]\", this report will extrapolate potential contenders for the 2025 NBA Finals based on current NBA trends, team dynamics, and projections derived from reputable sports analytics platforms. This analysis assumes the current date is May 3, 2025, placing us at the cusp of the playoffs.\n",
      "\n",
      "**Methodology**\n",
      "\n",
      "Given the absence of specific data, this report will rely on a qualitative analysis informed by publicly available information regarding team performance, player statistics, coaching changes, and expert opinions. The analysis will focus on identifying teams with the following characteristics:\n",
      "\n",
      "*   **Strong Regular Season Record:** Consistent performance throughout the regular season is a strong indicator of a team's overall strength and ability to perform under pressure.\n",
      "*   **Dominant Offensive and Defensive Ratings:** Teams that excel on both ends of the court are more likely to succeed in the playoffs, where defensive intensity increases.\n",
      "*   **Star Power and Depth:** A combination of elite players and a solid supporting cast is crucial for navigating the challenges of a long playoff run.\n",
      "*   **Coaching Acumen:** Effective coaching can make a significant difference in playoff series, particularly in terms of game planning and adjustments.\n",
      "*   **Injury Status:** The health of key players is a critical factor in determining a team's chances of success.\n",
      "\n",
      "**Potential Contenders**\n",
      "\n",
      "Based on the above criteria, the following teams are identified as potential contenders for the 2025 NBA Finals:\n",
      "\n",
      "1.  **Boston Celtics:** The Celtics have consistently been a top team in the Eastern Conference for several years. Their combination of elite scoring, strong defense, and playoff experience makes them a perennial threat.2.  **Denver Nuggets:** The Nuggets, led by their star player, have demonstrated the ability to dominate both in the regular season and the playoffs. Their offensive firepower and improved defense make them a formidable opponent.\n",
      "3.  **Milwaukee Bucks:** With a core of talented players, the Bucks are always a contender in the Eastern Conference. Their size, athleticism, and experience make them a tough matchup for any team.\n",
      "4.  **Phoenix Suns:** The Suns possess a roster filled with offensive talent. If they can improve their defensive consistency and chemistry, they could be a serious threat in the Western Conference.\n",
      "5.  **Los Angeles Clippers:** If healthy, the Clippers have the potential to be a dominant force in the NBA. Their combination of star power and depth makes them a team to watch out for.\n",
      "\n",
      "**Detailed Team Analysis**\n",
      "\n",
      "To provide a more in-depth analysis, let's consider a hypothetical scenario for each of the potential contenders, focusing on their strengths, weaknesses, and potential challenges:\n",
      "\n",
      "**1. Boston Celtics**\n",
      "\n",
      "*   **Strengths:** The Celtics boast a well-rounded roster with multiple players capable of scoring at a high level. Their defense is consistently among the best in the league, and they have a proven track record of success in the playoffs. Their coaching staff is known for their strategic adjustments and ability to get the most out of their players.\n",
      "*   **Weaknesses:** The Celtics can sometimes struggle with consistency, particularly on offense. They may also be vulnerable to teams with dominant interior players.\n",
      "*   **Challenges:** Maintaining player health will be crucial for the Celtics' success. They will also need to avoid complacency and maintain their focus throughout the playoffs.\n",
      "\n",
      "| Category          | Assessment |\n",
      "| ----------------- | ---------- |\n",
      "| Regular Season Record | Excellent  |\n",
      "| Offensive Rating    | Top 5      |\n",
      "| Defensive Rating    | Top 3      |\n",
      "| Star Power        | High       |\n",
      "| Depth             | Good       |\n",
      "| Coaching          | Excellent  |\n",
      "| Injury Status     | Moderate   |\n",
      "\n",
      "**2. Denver Nuggets**\n",
      "\n",
      "*   **Strengths:** The Nuggets' offense revolves around their star player, who is arguably the best offensive player in the league. They have a strong supporting cast of shooters and playmakers, and their home-court advantage is significant.\n",
      "*   **Weaknesses:** The Nuggets' defense can be inconsistent at times, and they may struggle against teams with multiple scoring options.\n",
      "*   **Challenges:** The Nuggets will need to improve their defensive intensity in the playoffs. They will also need to find ways to score when their star player is being double-teamed.\n",
      "\n",
      "| Category          | Assessment |\n",
      "| ----------------- | ---------- |\n",
      "| Regular Season Record | Excellent  |\n",
      "| Offensive Rating    | Top 3      |\n",
      "| Defensive Rating    | Top 10     |\n",
      "| Star Power        | High       |\n",
      "| Depth             | Good       |\n",
      "| Coaching          | Good       |\n",
      "| Injury Status     | Low        |\n",
      "\n",
      "**3. Milwaukee Bucks**\n",
      "\n",
      "*   **Strengths:** The Bucks are a dominant defensive team with a strong interior presence. Their star player is one of the most dominant players in the league, and they have a proven track record of success in the playoffs.\n",
      "*   **Weaknesses:** The Bucks' offense can sometimes be stagnant, and they may struggle against teams with quick guards who can penetrate the paint.\n",
      "*   **Challenges:** The Bucks will need to improve their offensive versatility in the playoffs. They will also need to find ways to contain opposing guards.\n",
      "\n",
      "| Category          | Assessment |\n",
      "| ----------------- | ---------- |\n",
      "| Regular Season Record | Excellent  |\n",
      "| Offensive Rating    | Top 10     |\n",
      "| Defensive Rating    | Top 5      |\n",
      "| Star Power        | High       |\n",
      "| Depth             | Good       |\n",
      "| Coaching          | Good       |\n",
      "| Injury Status     | Moderate   |\n",
      "\n",
      "**4. Phoenix Suns**\n",
      "\n",
      "*   **Strengths:** The Suns possess a roster filled with offensive talent. Their ability to score from multiple positions makes them difficult to guard.\n",
      "*   **Weaknesses:** The Suns' defense can be inconsistent, and they may struggle against teams with strong interior players. Team chemistry is also a question mark, given the number of high-profile players on the roster.\n",
      "*   **Challenges:** The Suns will need to improve their defensive consistency and develop better team chemistry. They will also need to find ways to score against tough defensive teams.\n",
      "\n",
      "| Category          | Assessment |\n",
      "| ----------------- | ---------- |\n",
      "| Regular Season Record | Good       |\n",
      "| Offensive Rating    | Top 5      |\n",
      "| Defensive Rating    | Average    |\n",
      "| Star Power        | High       |\n",
      "| Depth             | Average    |\n",
      "| Coaching          | Average    |\n",
      "| Injury Status     | Moderate   |\n",
      "\n",
      "**5. Los Angeles Clippers**\n",
      "\n",
      "*   **Strengths:** The Clippers have a roster filled with star power and depth. Their ability to score from multiple positions makes them difficult to guard.\n",
      "*   **Weaknesses:** The Clippers' biggest weakness is their injury history. They have struggled to keep their key players healthy for extended periods of time.\n",
      "*   **Challenges:** The Clippers will need to stay healthy in order to contend for a championship. They will also need to develop better team chemistry and find ways to win close games.\n",
      "\n",
      "| Category          | Assessment |\n",
      "| ----------------- | ---------- |\n",
      "| Regular Season Record | Good       |\n",
      "| Offensive Rating    | Top 10     |\n",
      "| Defensive Rating    | Average    || Star Power        | High       |\n",
      "| Depth             | Good       |\n",
      "| Coaching          | Good       |\n",
      "| Injury Status     | High       |\n",
      "\n",
      "**Factors Influencing the Outcome**\n",
      "\n",
      "Several factors could significantly influence the outcome of the 2025 NBA Finals:\n",
      "\n",
      "*   **Injuries:** Injuries to key players can derail even the most talented teams.\n",
      "*   **Matchups:** Certain teams may have a stylistic advantage over others, making matchups a critical factor in determining the outcome of a series.\n",
      "*   **Coaching Adjustments:** The ability of coaches to make effective adjustments during a series can be a significant advantage.\n",
      "*   **Luck:** A certain amount of luck is always involved in the playoffs, whether it's a favorable bounce of the ball or an unexpected performance from a role player.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Based on the available information and analysis, the **Boston Celtics** emerge as a strong contender to win the 2025 NBA Finals. Their combination of elite scoring, strong defense, playoff experience, and coaching acumen makes them a formidable opponent. However, the other teams mentioned in this report also have the potential to contend for a championship, and the outcome of the Finals will ultimately depend on a variety of factors, including injuries, matchups, and coaching adjustments.\n",
      "\n",
      "**Disclaimer:** This report is based on a qualitative analysis and is subject to change based on future developments. The absence of specific data limits the accuracy of the predictions.\n",
      "\n",
      "**References**\n",
      "\n",
      "Because the given information is \"[]\", I am unable to provide specific references. However, in a real-world scenario, I would include references to reputable sports news websites, statistical analysis platforms, and NBA official sources. Some examples of the types of sources I would use are listed below with hypothetical URLs:\n",
      "\n",
      "*   ESPN NBA: [ESPN NBA](https://www.espn.com/nba/)\n",
      "*   NBA.com: [NBA.com](https://www.nba.com/)\n",
      "*   Basketball-Reference.com: [Basketball-Reference.com](https://www.basketball-reference.com/)\n",
      "*   Bleacher Report NBA: [Bleacher Report NBA](https://bleacherreport.com/nba)\n",
      "*   The Athletic NBA: [The Athletic NBA](https://theathletic.com/nba/)\n",
      "\n",
      "Research Costs:\n",
      "0.03524\n",
      "\n",
      "Number of Research Images:\n",
      "0\n",
      "\n",
      "Number of Research Sources:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# this code is currently broken. Gpt researcher needs to be configured with gemini. Dont have an openai key.\n",
    "# https://github.com/assafelovic/gpt-researcher/issues/860\n",
    "\n",
    "# https://github.com/assafelovic/gpt-researcher\n",
    "\n",
    "# https://www.youtube.com/watch?v=pxcC0-hUI7k\n",
    "\n",
    "import nest_asyncio \n",
    "import asyncio\n",
    "from gpt_researcher import GPTResearcher\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def get_report(query: str, report_type: str, report_source: str):\n",
    "    researcher = GPTResearcher(query, report_type, report_source)\n",
    "    research_result = await researcher.conduct_research()\n",
    "    report = await researcher.write_report()\n",
    "    \n",
    "    # Get additional information\n",
    "    research_context = researcher.get_research_context()\n",
    "    research_costs = researcher.get_costs()\n",
    "    research_images = researcher.get_research_images()\n",
    "    research_sources = researcher.get_research_sources()\n",
    "    \n",
    "    return report, research_context, research_costs, research_images, research_sources\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"what team may win the NBA finals?\"\n",
    "    report_type = \"research_report\"\n",
    "\n",
    "    report, context, costs, images, sources = asyncio.run(get_report(query, report_type, report_source=\"local\"))\n",
    "    \n",
    "    print(\"Report:\")\n",
    "    print(report)\n",
    "    print(\"\\nResearch Costs:\")\n",
    "    print(costs)\n",
    "    print(\"\\nNumber of Research Images:\")\n",
    "    print(len(images))\n",
    "    print(\"\\nNumber of Research Sources:\")\n",
    "    print(len(sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FDL', 'admin', 'config', 'local']\n",
      "['DSA']\n"
     ]
    }
   ],
   "source": [
    "print(client.list_database_names())     # should include 'FDL'\n",
    "print(db.list_collection_names())       # should include 'DSA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
